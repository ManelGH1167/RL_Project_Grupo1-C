{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah\n",
    "* Alumno 2: Cuesta Cifuentes, Jair\n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2. Preparar Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "El entorno de Colab está preinstalado con una serie de librerías por defecto. Para trabajar en base a las especificaciones del ejercicio se necesitan intalar unas librerías que bajen de versión las existentes en Colab. Entre ellas tensorflow. El problema de realizar esta acción es que para que todas las versiones sean consideradas por el entorno hay que reiniciar la sesión, sino se mantienen dependencias y los import no funcionan. <br>\n",
    "Es decir tras los \"pip install\" hay que hacer un **\"Runtime > Restart runtime\"** o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\".<br>\n",
    "En este punto se ha de tener presente que se ha reiniciado y **se han perdido las variables** que se hayan establecido, por ese motivo repetiremos el código para identificar si estamos en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "Ignorar los errores que puedan aparecer, son incompatibilidades con librerías avanzadas que no utilizamos ni necesitamos para nuestro código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB:  \n",
    "# =========================\n",
    "#  Entorno Colab nativo con todo lo compatible.\n",
    "#  Sólo recordar que se debe REINICIAR EL RUNTIME (al acabar)\n",
    "# =========================  \n",
    "  print(\" [INFO] - Instalando paquetes adicionales...\")  \n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Ignorar los errores que aparecen:\")\n",
    "  print(\"   Son incompatibilidades que aparecen con librería avanzadas\")\n",
    "  print(\"   que no necesitamos ni vamos a utilizar\")  \n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12.1 --quiet\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"INSTALACIÓN COMPLETADA\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\")\n",
    "  print(\"1. Ve a Runtime > Restart runtime\")\n",
    "  print(\"2. Después ejecuta las importaciones\")\n",
    "  print(\"=\"*60)  \n",
    "  INSTALL_LOCAL = False\n",
    "  IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB_ENV:\n",
    "# =========================\n",
    "#  Colab con env --> \n",
    "#    no funciona muy bien pues aunque se cree el entorno, Colab sigue\n",
    "#    utilizando el suyo con sus librería y se necesita usar %%writefile\n",
    "# =========================      \n",
    "  # 1. Instalar virtualenv\n",
    "  !pip install virtualenv --quiet\n",
    "\n",
    "  # 3. Crear el entorno virtual llamado \"miar_rl\"\n",
    "  !virtualenv miar_rl\n",
    "\n",
    "  # 4. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
    "  !./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
    "  !./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
    "  !./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
    "  !./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
    "  !./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
    "\n",
    "  # 5. Librerías adicionales\n",
    "  !./miar_rl/bin/pip install Pillow\n",
    "  !./miar_rl/bin/pip install matplotlib\n",
    "  !./miar_rl/bin/pip install tqdm\n",
    "  INSTALL_LOCAL = False\n",
    "    \n",
    "if INSTALL_LOCAL:    \n",
    "# =========================\n",
    "#  Librería para trabajar en local, si NO se cargaron las \n",
    "#    librerías desde fichero requirements\n",
    "# =========================        \n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm\n",
    "  %pip install imageio==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.3. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE:**<br>\n",
    "Recordar que antes de seguir (si hemos decidido el entorno de Colab nativo - IN_COLAB=True -) \n",
    "* Hay que hacer un <font color='red'>\"Runtime > Restart runtime\"</font> o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_1\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.4. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "3lN7KLe05NSa",
    "outputId": "47c41c84-3bfd-425f-9b8d-6d6aa525afdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.viminfo', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_dqn (1).ipynb', '08MIAR_dqn (2).ipynb', '08MIAR_dqn (3).ipynb', '08MIAR_dqn (4).ipynb', '08MIAR_dqn (5).ipynb', '08MIAR_dqn.ipynb', '08miar_dqn.py', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoint', 'checkpoints', 'Configuración local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'diagnosticos', 'Documents', 'Downloads', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'fffff.py', 'ffmpeg', 'ffmpeg-2025-06-17-git-ee1f79b0fa-essentials_build.7z', 'ffmpeg-2025-06-17-git-ee1f79b0fa-full_build.7z', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'lista.txt', 'menory.txt', 'Menú Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'models', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'Plantillas', 'ppppp.ipynb', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_práctico_Grupo1_C (1).ipynb', 'RL_Proyecto_práctico_Grupo1_C - copia.ipynb', 'RL_Proyecto_práctico_Grupo1_C.ipynb', 'RL_Proyecto_práctico_Grupo1_C.py', 'RL_Proyecto_práctico_Grupo1_C_v0.ipynb', 'RL_Proyecto_práctico_Grupo1_C_v2.ipynb', 'RL_Proyecto_práctico_Grupo1_C_v4.ipynb', 'RL_Proyecto_práctico_Grupo1_C_VERSION_COLAB_V3.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'start.bat', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled221.ipynb', 'Untitled3.ipynb', 'v3.ipynb', 'Videos', '_RL_Proyecto_práctico_Grupo1_C_v2.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Cambiar al directorio en Google Drive que deseas usar\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\" [INFO] - Estamos ejecutando en Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Montar Google Drive en el punto de montaje\n",
    "    print(\" [INFO] - Colab: montando Google drive en: \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Crear drive_root si no existe\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\n [INFO] - Colab: Asegurando que \", drive_root, \" existe.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Cambiar al directorio\n",
    "    print(\"\\n [INFO] - Colab: Cambiamos el directorio a: \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verificar que estamos en el directorio de trabajo correcto\n",
    "%pwd\n",
    "print(\" [INFO] - Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihTI9TOD43ML"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIAR9zQv43MO"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas\n",
    "\n",
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K4o4-N4T43MO"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "import json\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "if IN_COLAB:  \n",
    "  from tensorflow.keras.optimizers.legacy import Adam\n",
    "else:\n",
    "  from tensorflow.keras.optimizers import Adam\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "puzS2kTzc1Fd"
   },
   "outputs": [],
   "source": [
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "except ImportError:\n",
    "    print(\" [WARNING] - gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8iRCStcc90p",
    "outputId": "96045a07-febd-4dce-e5c8-bbec04475b33"
   },
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (x cores)\n",
    "def optimizar_tensorflow():\n",
    "    \"\"\"Configura TensorFlow para rendimiento óptimo en CPU/GPU\"\"\"\n",
    "    # Limpiar sesión previa\n",
    "    gc.collect()\n",
    "\n",
    "    # Optimización de GPU si está disponible\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\" [INFO] - GPU optimizada para crecimiento adaptativo de memoria\")\n",
    "        except Exception as e:\n",
    "            print(f\" [INFO] - Error al configurar GPU: {e}\")\n",
    "\n",
    "    # Optimización de CPU\n",
    "    num_cpu_cores = os.cpu_count() or 8  # Fallback a 8 si no se puede detectar\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_cpu_cores // 2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(max(2, num_cpu_cores // 4))\n",
    "\n",
    "    # Modo eager solo si es necesario\n",
    "    # Para entrenamiento, es mejor desactivarlo por rendimiento\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "\n",
    "    print(f\" [INFO] - TensorFlow optimizado para {num_cpu_cores} cores CPU\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faNbnMuOdNDP"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7WFE0sqPdLsy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Crear el entorno\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJEMPLO TONTO DE VIDEO... PERO SIRVE PARA LUEGO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturando frames...\n",
      " [INFO] - Episodio terminado en el paso 303\n",
      " [INFO] - Guardando 304 frames en video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Video guardado: checkpoints/videos\\spaceinvaders_game.mp4\n",
      "✓ Tamaño del archivo: 69568 bytes\n",
      "\n",
      " [INFO] - Recompensa total: 55.0\n"
     ]
    }
   ],
   "source": [
    "# SOLUCIÓN MÁS SIMPLE Y CONFIABLE\n",
    "import gym\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# Crear directorio\n",
    "video_dir = 'checkpoints/videos'\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Crear entorno\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "# Lista para almacenar frames\n",
    "frames = []\n",
    "\n",
    "# Ejecutar episodio y capturar frames\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Capturando frames...\")\n",
    "for step in range(100000):\n",
    "    # Renderizar y capturar frame\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Acción aleatoria\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Ejecutar paso\n",
    "    step_result = env.step(action)\n",
    "    \n",
    "    if len(step_result) == 5:\n",
    "        observation, reward, done, truncated, info = step_result\n",
    "        done = done or truncated\n",
    "    else:\n",
    "        observation, reward, done, info = step_result\n",
    "    \n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        print(f\" [INFO] - Episodio terminado en el paso {step}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Guardar video\n",
    "if frames:\n",
    "    video_path = os.path.join(video_dir, 'spaceinvaders_game.mp4')\n",
    "    print(f\" [INFO] - Guardando {len(frames)} frames en video...\")\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    print(f\"✓ Video guardado: {video_path}\")\n",
    "    print(f\"✓ Tamaño del archivo: {os.path.getsize(video_path)} bytes\")\n",
    "else:\n",
    "    print(\"❌ [ERROR] - No se capturaron frames\")\n",
    "\n",
    "print(f\"\\n [INFO] - Recompensa total: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcjoyH3idqh4",
    "outputId": "85af5e47-6b37-472b-ddbe-ce71bea7eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgw7iHVRduGa"
   },
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TshMrqTjdxja"
   },
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "# Hiperparámetros optimizados\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "CHANNELS = 1                    # Canal para grises\n",
    "USE_FRAMESTACK = True           # Cambiar a True si quieres detección de movimiento\n",
    "WINDOW_LENGTH = 4 if USE_FRAMESTACK else 1   # Número de fotogramas apilados          # La mayoría de implementaciones usan 4 frames\n",
    "batch_size = 32                 # Tamaño de batch óptimo\n",
    "gamma = 0.99                    # Factor de descuento (mejor que 0.95 para recompensas a largo plazo)\n",
    "learning_rate = 0.00025         # Tasa de aprendizaje estándar para DQN\n",
    "memory_size = 1000000           # Buffer de memoria grande para mejor estabilidad\n",
    "TARGET_UPDATE_INTERVAL = 10000  # Actualización de red objetivo cada 10,000 pasos\n",
    "WARMUP_STEPS = 50000            # Pasos iniciales para llenar la memoria (experiencia aleatoria)\n",
    "NUM_TRAINING_STEPS = 2000000    # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
    "EPSILON_STEPS = 500000          # Total de pasos de evaluación del modelo\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH)   # Dimensiones de cada frame\n",
    "\n",
    "# Single frame shape (height, width, channels)\n",
    "FRAME_SHAPE = (HEIGHT, WIDTH, CHANNELS)  # (84, 84, 1)\n",
    "MODEL_INPUT_SHAPE = (HEIGHT, WIDTH, WINDOW_LENGTH)  # Forma para el modelo (channels_last)\n",
    "SEQ_INPUT_SHAPE = (WINDOW_LENGTH,HEIGHT, WIDTH)  # Forma para el modelo (channels_last)\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "# Definir shape consistente\n",
    "if USE_FRAMESTACK:\n",
    "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
    "else:\n",
    "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
    "\n",
    "state_size = (*INPUT_SHAPE, WINDOW_LENGTH)   # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "input_shape = (*INPUT_SHAPE, WINDOW_LENGTH)  # Para la API de keras-rl\n",
    "action_size = env.action_space.n       # 6 acciones posibles\n",
    "learning_rate =  0.00025               # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST      # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST      # Máximo de pasos posibles por episodio\n",
    "NUM_TRAINING_STEPS = 10000    # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
    "total_episodios = 5                # Episodios totales para el entrenamiento\n",
    "max_steps       = 3000               # Máximo de pasos posibles por episodio\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.1             # Probabilidad mínima de exploración\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "TARGET_REWARD = 20.0\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjFRyr4Ld1Hp"
   },
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "06wZVH5c43MP"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(INPUT_SHAPE)):\n",
    "        self.input_shape = input_shape\n",
    "        # Precargar una imagen negra para inicialización\n",
    "        self.black_frame = np.zeros(input_shape, dtype=np.uint8)\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        # Si la observación es None, devolver un marco negro\n",
    "        if observation is None:\n",
    "            return self.black_frame\n",
    "\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        # Crop the screen (remove the part below the player)\n",
    "        # [Up: Down, Left: right]\n",
    "        cropped_img = observation[18:-12, 4:-12]\n",
    "        # Optimización: usar cv2 para redimensionar y convertir a escala de grises (más rápido que PIL)\n",
    "        resized = cv2.resize(cropped_img, self.input_shape)\n",
    "        processed_observation = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY) if len(resized.shape) == 3 else resized\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype(np.uint8)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Normaliza la recompensa al rango [-1, 1].\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            reward (float): Recompensa original del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada al rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        \"\"\"\n",
    "        Procesa un paso completo del entorno.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation: Observación del entorno.\n",
    "            reward: Recompensa obtenida.\n",
    "            done: Indicador de fin de episodio.\n",
    "            info: Información adicional del entorno.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "            tuple: (observación procesada, recompensa procesada, done, info)\n",
    "        \"\"\"\n",
    "        processed_observation = self.process_observation(observation)\n",
    "        processed_reward = self.process_reward(reward)\n",
    "        return processed_observation, processed_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptwluQRXedZP"
   },
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VwBXOBk43MP",
    "outputId": "88dbba58-92a4-4d75-cf4c-799a04c36cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Numero de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "print(\" [INFO] - Numero de acciones disponibles: \" + str(nb_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT6osc3H43MP",
    "outputId": "cbe27690-c40d-49b1-d420-80b7d045ce26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Formato de las observaciones:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" [INFO] - Formato de las observaciones:\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "7-BoOu_eeiAE",
    "outputId": "5b52561e-70d0-47c1-f55e-c882dda20b66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for i in range(22):\n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "vHJaGcAVelRY",
    "outputId": "f2686c2b-a159-4c1f-e857-984352520683"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-yCJoGjf2Fg"
   },
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ewKKozUaf-mG"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-asignar arrays con el tamaño exacto\n",
    "        # Usar uint8 para estados (más eficiente que float32)\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        print(f\" [INFO] - ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
    "        memory_size = (\n",
    "            self.states.nbytes + self.next_states.nbytes +\n",
    "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "        ) / (1024 * 1024)\n",
    "        print(f\" [INFO] - Memoria asignada: {memory_size:.2f} MB\")\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Añade una experiencia al buffer de forma eficiente\"\"\"\n",
    "        # Convertir a uint8 para ahorrar memoria (estados son imágenes 0-255)\n",
    "        if state.dtype != np.uint8:\n",
    "            state = (state * 255).astype(np.uint8)\n",
    "        if next_state.dtype != np.uint8:\n",
    "            next_state = (next_state * 255).astype(np.uint8)\n",
    "\n",
    "        # Almacenar directamente en el array pre-asignado\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        # Actualizar posición circular\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
    "\n",
    "        # Generar índices aleatorios\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "\n",
    "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
    "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
    "        batch_actions = self.actions[indices]\n",
    "        batch_rewards = self.rewards[indices]\n",
    "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
    "        batch_dones = self.dones[indices]\n",
    "\n",
    "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        # No es necesario limpiar los arrays, se sobrescriben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-SaTPRkQoK"
   },
   "source": [
    "#### Clase PerformanceMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente no se usa, si se necesitase mayor detalle de la evolución de los entrenamientos se podría incluir en el Callback antes del entrenamiento. De momento no se usa!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Wf2A5kDokNdS"
   },
   "outputs": [],
   "source": [
    "# Clase para monitoreo de memoria y rendimiento\n",
    "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path='diagnosticos'):\n",
    "        self.save_path = save_path\n",
    "        self.episode_times = []\n",
    "        self.memory_usage = []\n",
    "        self.current_episode = 0\n",
    "        self.episode_start_time = None\n",
    "        self.episode_start_memory = None\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        self.episode_start_time = time.time()\n",
    "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        self.current_episode = episode\n",
    "        print(f\" [INFO] - Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        end_time = time.time()\n",
    "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        episode_duration = end_time - self.episode_start_time\n",
    "\n",
    "        self.episode_times.append(episode_duration)\n",
    "        self.memory_usage.append(final_memory)\n",
    "\n",
    "        print(f\" [INFO] - Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
    "        print(f\" [INFO] - Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
    "\n",
    "        # Guardar diagnóstico cada 5 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            self.save_diagnostics(episode)\n",
    "\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()\n",
    "\n",
    "    def save_diagnostics(self, episode):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.episode_times)\n",
    "        plt.title('Tiempo por episodio')\n",
    "        plt.ylabel('Segundos')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.memory_usage)\n",
    "        plt.title('Uso de memoria')\n",
    "        plt.ylabel('MB')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTgDOJoCgISN"
   },
   "source": [
    "### 1. Implementación de la red neuronal\n",
    "\n",
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFAzP0UigPVg"
   },
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ0dGSAUgP0a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kP2vNpKnzkTl"
   },
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo DQN usando SOLO Keras estándar. Base común para redes DQN y DDQN\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "    Construye un modelo que acepta channels_first y convierte internamente\n",
    "\n",
    "    Esta función implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación RELU.\n",
    "    Esto evita completamente los problemas de grafos múltiples\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DQN estándar: input_shape={input_shape}, actions={nb_actions}\")\n",
    "       \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DQN_Model')\n",
    "    memory = None\n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model, memory, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_models(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea modelos para Double DQN (principal y objetivo)\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelos DDQN: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Modelo principal\n",
    "    main_model, memory, _ = create_dqn_model(input_shape, nb_actions, memory_size)\n",
    "    main_model._name = 'DDQN_Main_Model'    \n",
    "    \n",
    "    # Modelo objetivo (copia exacta)\n",
    "    target_model = tf.keras.models.clone_model(main_model)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    target_model._name = 'DDQN_Target_Model'\n",
    "    \n",
    "    print(\"✅ Modelos DDQN creados exitosamente\")    \n",
    "    return main_model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_replay_model(input_shape, nb_actions, memory_size):\n",
    "    print(f\"🏗️ Creando modelos DDQN_replay: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DDQN_replay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "    target_model = clone_model(model)  # Create target model for DDQN\n",
    "    target_model.set_weights(model.get_weights())  # Initialize with same weights\n",
    "    target_model._name = 'DDQN_replay_Target_Model'    \n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dueling_dqn_replay_model(input_shape, action_size, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo Dueling DQN con replay.\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape={input_shape}, actions={action_size}\")\n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)    \n",
    "    x = Permute((2, 3, 1))(inputs)\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    # Value stream\n",
    "    value = Dense(512, activation='relu')(x)\n",
    "    value = Dense(1, activation='linear')(value)\n",
    "    # Advantage stream\n",
    "    advantage = Dense(512, activation='relu')(x)\n",
    "    advantage = Dense(action_size, activation='linear')(advantage)\n",
    "    # Combine streams\n",
    "    outputs = Add()([value, Lambda(lambda a: a - K.mean(a, axis=1, keepdims=True))(advantage)])\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='DuelingDQNReplay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)   \n",
    "    target_model = clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())    \n",
    "    target_model._name = 'DuelingDQNReplay_Target_Model'     \n",
    "   \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0DHvKNshvQo"
   },
   "source": [
    "### 2. Implementación de la solución DQN\n",
    "\n",
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para monitorear el progreso del entrenamiento de un agente DQN.\n",
    "\n",
    "    Registra el avance en términos de pasos completados, porcentaje, velocidad de entrenamiento\n",
    "    (pasos por segundo) y tiempo estimado de finalización (ETA).\n",
    "\n",
    "    Atributos:\n",
    "        total_steps (int): Número total de pasos de entrenamiento.\n",
    "        print_interval (int): Intervalo de pasos para imprimir el progreso (por defecto: 10,000).\n",
    "        start_time (float): Tiempo de inicio del entrenamiento (en segundos).\n",
    "        last_step (int): Último paso registrado (inicializado en 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, total_steps, print_interval=100):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "\n",
    "        Args:\n",
    "            total_steps (int): Número total de pasos de entrenamiento.\n",
    "            print_interval (int): Intervalo de pasos para imprimir el progreso.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.print_interval = print_interval\n",
    "        self.step_counter = 0\n",
    "        self.start_time = time.time()\n",
    "        self.episode_rewards = []  # Store clipped episode rewards\n",
    "        self.episode_steps = []\n",
    "        self.current_episode_reward = 0.0  # Track clipped reward for current episode\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al inicio del entrenamiento.\n",
    "\n",
    "        Inicializa el tiempo de inicio y muestra un mensaje de comienzo.\n",
    "\n",
    "        Args:\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"🚀 Entrenamiento iniciado: {self.total_steps:,} pasos\")\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al final de cada paso de entrenamiento.\n",
    "\n",
    "        Calcula y muestra el progreso, incluyendo porcentaje completado, velocidad\n",
    "        (pasos por segundo) y tiempo estimado de finalización (ETA) en horas.\n",
    "\n",
    "        Args:\n",
    "            step (int): Número del paso actual.\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.step_counter += 1        \n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        clipped_reward = np.clip(raw_reward, -1.0, 1.0)  # Match AtariProcessor clipping\n",
    "        self.current_episode_reward += clipped_reward\n",
    "        if self.step_counter % self.print_interval == 0:\n",
    "            progress = (self.step_counter / self.total_steps) * 100\n",
    "            elapsed_time = (time.time() - self.start_time)\n",
    "            steps_per_sec = self.step_counter / elapsed_time\n",
    "            eta_hours = (self.total_steps - self.step_counter) / steps_per_sec / 3600\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "            print(f\"📊 Paso {self.step_counter:,}/{self.total_steps:,} ({progress:.1f}%) - \"\n",
    "                  f\"{steps_per_sec:.1f} pasos/seg - ETA: {eta_hours:.1f}h - Memoria: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        nb_steps = logs.get('nb_episode_steps', 1)\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.episode_steps.append(nb_steps)\n",
    "        mean_reward = self.current_episode_reward / nb_steps if nb_steps > 0 else 0\n",
    "        print(f\"📈 Episodio {episode+1}: Recompensa total (clipped): {self.current_episode_reward:.3f}, \"\n",
    "              f\"Pasos: {nb_steps}, Mean Reward Calculado: {mean_reward:.6f} (Recompensa/Pasos)\")\n",
    "        # Reset for next episode\n",
    "        self.current_episode_reward = 0.0            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback que monitorea el progreso hacia una media de episode_reward objetivo\n",
    "    e integra con nuestro sistema de checkpoints.\n",
    "    \"\"\"      \n",
    "    def __init__(self, dqn, target_avg_reward=20.0, name_model=None, window_size=100, save_best=True, checkpoint_dir=checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.target_avg_reward = target_avg_reward\n",
    "        self.window_size = window_size\n",
    "        self.save_best = save_best\n",
    "        # Obtener el nombre del modelo sin ruta\n",
    "        self.model = dqn\n",
    "        self.model_name = name_model\n",
    "        self.episode_count = 0\n",
    "        self.episode_rewards = []\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.episodes_at_target = 0\n",
    "        self.consecutive_target_episodes = 0\n",
    "        self.checkpoint_dir =  f\"{checkpoint_path}/{self.model_name}\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)       \n",
    "        \n",
    "        print(f\"🎯 OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
    "        print(f\"📊 Ventana de evaluación: {window_size} episodios\")        \n",
    "\n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        self.episode_count += 1\n",
    "        episode_reward = logs.get('episode_reward', 0)\n",
    "        # Convert NumPy types to Python types\n",
    "        if isinstance(episode_reward, np.floating):\n",
    "            episode_reward = episode_reward.item()\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular media móvil\n",
    "        if len(self.episode_rewards) >= self.window_size or self.episode_count % 10 == 0:\n",
    "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
    "            current_avg = np.mean(recent_rewards)\n",
    "            \n",
    "            # Verificar si alcanzamos el objetivo\n",
    "            target_reached = current_avg >= self.target_avg_reward\n",
    "            \n",
    "            if target_reached:\n",
    "                self.episodes_at_target += 1\n",
    "                self.consecutive_target_episodes += 1\n",
    "            else:\n",
    "                self.consecutive_target_episodes = 0\n",
    "            \n",
    "            # Guardar si es el mejor promedio\n",
    "            if current_avg > self.best_avg_reward or self.episode_count % 10 == 0:\n",
    "                self.best_avg_reward = current_avg\n",
    "                if self.save_best and hasattr(self, 'model'):\n",
    "                    # Formato del sufijo para el mejor modelo con su promedio\n",
    "                    best_suffix = f\"best_avg{current_avg:.1f}\"                               \n",
    "                    try:\n",
    "                        epsilon = self.model.policy.eps if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'eps') else 0.1\n",
    "                       \n",
    "                        # Usar nuestro sistema de checkpoint para guardar el mejor modelo\n",
    "                        if hasattr(self.model, 'model'): # Por si el agente es un DQNAgent con un .model\n",
    "                            save_model_checkpoint(   self.model,   self.model_name,\n",
    "                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "                                checkpoint_dir=self.checkpoint_dir,   suffix=best_suffix, \n",
    "                                epsilon=epsilon\n",
    "                            )                                             \n",
    "                            # También actualizar el checkpoint \"best\" general\n",
    "                            save_model_checkpoint(   self.model,    self.model_name,\n",
    "                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "                                checkpoint_dir=self.checkpoint_dir,   suffix=\"best\", \n",
    "                                epsilon=epsilon\n",
    "                            )\n",
    "                        else:\n",
    "                            # Si no es un agente completo, guardar solo los pesos\n",
    "                            best_filename = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_model.h5\"\n",
    "                            self.model.save_weights(best_filename, overwrite=True)                        \n",
    "                            \n",
    "                        # Guardar métricas en JSON\n",
    "                        metrics = {\n",
    "                            \"episode\": int(self.episode_count),\n",
    "                            \"avg_reward\": float(current_avg),\n",
    "                            \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                            \"timestamp\": int(time.time()),\n",
    "                            \"consecutive_target_episodes\": int(self.consecutive_target_episodes),\n",
    "                            \"episodes_at_target\": int(self.episodes_at_target)\n",
    "                        }\n",
    "                        metrics_file = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_metrics.json\"\n",
    "                        with open(metrics_file, 'w') as f:\n",
    "                            json.dump(metrics, f, indent=2, default=str)\n",
    "                        \n",
    "                        print(f\"💾 NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado en {self.checkpoint_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ [ERROR] - Error guardando mejor modelo: {e}\")\n",
    "            \n",
    "            # Mostrar progreso cada 50 episodios\n",
    "            if self.episode_count % 50 == 0:\n",
    "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
    "                target_status = \"🎯 OBJETIVO ALCANZADO!\" if target_reached else f\"📈 {progress_pct:.1f}% del objetivo\"\n",
    "                \n",
    "                print(f\"\\n📊 EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
    "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
    "                print(f\"   Media últimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
    "                print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "                print(f\"   Estado: {target_status}\")\n",
    "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
    "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
    "                \n",
    "                if self.consecutive_target_episodes >= 50:\n",
    "                    print(f\"🏆 ¡MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
    "        logs = logs or {}\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
    "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
    "            \n",
    "            print(f\"\\n🏁 RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "            print(f\"   Total episodios: {self.episode_count}\")\n",
    "            print(f\"   Media final últimos {self.window_size}: {final_avg:.2f}\")\n",
    "            print(f\"   Objetivo ({self.target_avg_reward}): {'✅ ALCANZADO' if objetivo_alcanzado else '❌ NO ALCANZADO'}\")\n",
    "            print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")\n",
    "            \n",
    "            # Save final metrics to JSON\n",
    "            final_metrics = {\n",
    "                \"total_episodes\": int(self.episode_count),\n",
    "                \"final_avg_reward\": float(final_avg),\n",
    "                \"target_reached\": bool(objetivo_alcanzado),\n",
    "                \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                \"episodes_at_target\": int(self.episodes_at_target),\n",
    "                \"consecutive_target_episodes\": int(self.consecutive_target_episodes)  \n",
    "            }\n",
    "            final_log_path = f\"{self.checkpoint_dir}/final_metrics.json\"\n",
    "            try:\n",
    "                with open(final_log_path, 'w') as f:\n",
    "                    json.dump(final_metrics, f, indent=2, default=str)                                    \n",
    "                print(f\"💾 Métricas finales guardadas en: {final_log_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error al guardar métricas finales: {e}\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFileLogger(FileLogger):\n",
    "    def __init__(self, filepath, interval=100):\n",
    "        super().__init__(filepath, interval)\n",
    "        self.step = 0  \n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.logs = {}        \n",
    "    \n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.step += 1  \n",
    "        if self.step % self.interval == 0:\n",
    "            episode_logs = {}\n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        self.current_episode_reward += np.clip(raw_reward, -1.0, 1.0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        metrics = logs.copy()\n",
    "        metrics['episode_reward'] = self.current_episode_reward\n",
    "        metrics['mean_reward_step'] = self.current_episode_reward / metrics.get('nb_episode_steps', 1)\n",
    "        metrics = {k: float(v) if isinstance(v, np.floating) else v for k, v in metrics.items()}\n",
    "        self.metrics[int(episode)] = metrics\n",
    "        if self.step % self.interval == 0:\n",
    "            with open(self.filepath, 'w') as f:\n",
    "                json.dump(self.metrics, f, indent=2, default=str)                \n",
    "        self.current_episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback para guardar checkpoints episódicamente usando nuestras funciones personalizadas.\n",
    "    \"\"\"    \n",
    "    def __init__(self, dqnet, checkpoint_path, save_freq=100, model_name='DQN'):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "        \n",
    "        Parametros:\n",
    "            dqnet: El agente DQN a guardar\n",
    "            checkpoint_path: Ruta donde guardar los checkpoints\n",
    "            save_freq: Frecuencia de episodios para guardar\n",
    "            model_name: Nombre base para los archivos de checkpoint\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_path = f\"{checkpoint_path}/{self.model_name}\"\n",
    "        self.episode_counter = 0\n",
    "        self.dqnet = dqnet\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        \"\"\"\n",
    "        Guarda checkpoint al final de ciertos episodios.\n",
    "        \"\"\"        \n",
    "        self.episode_counter += 1\n",
    "        if self.episode_counter % self.save_freq == 0:\n",
    "            try:                                \n",
    "                \n",
    "                # Guardar con nombre específico del episodio\n",
    "                epsilon = self.dqnet.policy.eps if hasattr(dqn, 'policy') and hasattr(self.dqnet.policy, 'eps') else 0.1\n",
    "                save_model_checkpoint(\n",
    "                    self.dqnet, \n",
    "                    self.model_name,\n",
    "                    episode=self.dqnet.episode,\n",
    "                    steps=self.dqnet.step, \n",
    "                    checkpoint_dir=self.checkpoint_path, \n",
    "                    suffix=f\"ep{self.episode_counter}\",\n",
    "                    epsilon=epsilon\n",
    "                )        \n",
    "                print(f\"DEBUG ---------------{self.dqnet.episode},{self.dqnet.step},{episode}\")\n",
    "                # También actualizar el checkpoint \"lastest\"\n",
    "                save_model_checkpoint(\n",
    "                    self.dqnet, \n",
    "                    self.model_name,\n",
    "                    episode=self.dqnet.episode,\n",
    "                    steps=self.dqnet.step, \n",
    "                    checkpoint_dir=self.checkpoint_path, \n",
    "                    suffix=\"lastest\",\n",
    "                    epsilon=epsilon\n",
    "                )                \n",
    "                print(f\"✅ Checkpoint guardado para episodio {self.episode_counter}\")        \n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ [ERROR] - Error al guardar checkpoint para episodio {self.episode_counter}: {str(e)}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model, memory, target_model, model_name, checkpoint_dir=\"checkpoints\", suffix=\"lastest\"):\n",
    "    \"\"\"\n",
    "    Carga pesos y estado para componentes separados (modelo, memoria, target_model).\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Tipo del modelo ('DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.)\n",
    "        checkpoint_dir: Directorio donde buscar los checkpoints\n",
    "        suffix: Tipo de checkpoint a cargar (\"lastest\", \"best\", o \"epXXX\")\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (episode, steps, epsilon) - El episodio, pasos y epsilon desde donde continuar\n",
    "               Si no se encuentra el checkpoint, devuelve (0, 0, 0.1)\n",
    "    \"\"\"  \n",
    "    # Valores predeterminados\n",
    "    episode = 0\n",
    "    steps = 0\n",
    "    epsilon = 0.1  # Valor por defecto\n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "      \n",
    "    try:    \n",
    "        # Cargar estado del entrenamiento\n",
    "        if not os.path.exists(state_path):\n",
    "            print(f\"⚠️ No se encontró el checkpoint {suffix} para {model_name}\")\n",
    "            return episode, steps, epsilon \n",
    "        else:\n",
    "            print(f\"📂 Se cargó: {state_path}\")        \n",
    "\n",
    "        with open(state_path, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            \n",
    "        # Extraer información básica\n",
    "        episode = state.get('episode', 0)\n",
    "        steps = state.get('global_steps', 0)\n",
    "        epsilon = state.get('epsilon', 0.1)\n",
    "        \n",
    "        print(f\"📂 Cargando checkpoint {suffix} (episodio: {episode}, pasos: {steps}, epsilon: {epsilon})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error al cargar el archivo de estado: {e}\")\n",
    "        \n",
    "    # Cargar modelo principal\n",
    "    if os.path.exists(main_model_path):\n",
    "        try:\n",
    "            model.load_weights(main_model_path)\n",
    "            print(f\"📂 Modelo principal cargado: {main_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ [ERROR] - Error al cargar el modelo principal: {e}\")\n",
    "            return 0, 0, 0.1  # Si falla la carga del modelo principal, mejor empezar desde cero\n",
    "    else:\n",
    "        print(f\"❌ [ERROR] - No se encontró el archivo del modelo principal: {main_model_path}\")\n",
    "        return 0, 0, 0.1       \n",
    "\n",
    "    # Cargar modelo target si existe y se proporcionó\n",
    "    if target_model is not None:\n",
    "        if os.path.exists(target_model_path):\n",
    "            try:\n",
    "                target_model.load_weights(target_model_path)\n",
    "                print(f\"📂 Modelo target cargado: {target_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ [WARNING] - Error al cargar el modelo target: {e}\")\n",
    "                # Si hay error, sincronizar con el principal\n",
    "                print(\"🔄 Sincronizando red target desde la principal...\")\n",
    "                target_model.set_weights(model.get_weights())\n",
    "        else:\n",
    "            # Si no existe el archivo, sincronizar con el principal\n",
    "            print(\"🔄 No se encontró archivo target, sincronizando desde la principal...\")\n",
    "            target_model.set_weights(model.get_weights())\n",
    "            \n",
    "    # Cargar memoria si corresponde al tipo de modelo\n",
    "    if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and memory is not None and os.path.exists(memory_path):\n",
    "        try:\n",
    "            with open(memory_path, 'rb') as f:\n",
    "                loaded_memory = pickle.load(f)\n",
    "\n",
    "            # Transfer content, not replace object\n",
    "            memory_loaded = False\n",
    "\n",
    "            # Try SequentialMemory structure (Keras-RL)\n",
    "            if hasattr(loaded_memory, 'observations') and hasattr(memory, 'observations'):\n",
    "                memory.observations = loaded_memory.observations\n",
    "                memory.actions = loaded_memory.actions\n",
    "                memory.rewards = loaded_memory.rewards\n",
    "                memory.terminals = loaded_memory.terminals\n",
    "                if hasattr(loaded_memory, 'observations_'):\n",
    "                    memory.observations_ = loaded_memory.observations_\n",
    "                memory_loaded = True\n",
    "\n",
    "            # Try ReplayBuffer structure\n",
    "            elif hasattr(loaded_memory, 'buffer') and hasattr(memory, 'buffer'):\n",
    "                memory.buffer = loaded_memory.buffer\n",
    "                memory.position = loaded_memory.position\n",
    "                if hasattr(loaded_memory, 'size'):\n",
    "                    memory.size = loaded_memory.size\n",
    "                memory_loaded = True\n",
    "\n",
    "            if memory_loaded:\n",
    "                print(f\"📂 Memoria cargada correctamente: {memory_path}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Estructura de memoria desconocida, no se pudo cargar\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al cargar la memoria: {e}\")\n",
    "            \n",
    "    \n",
    "    return episode, steps, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para convertir objetos no serializables a JSON\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convierte tipos numpy a tipos Python nativos para serialización JSON\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "        return obj.isoformat()\n",
    "    else:\n",
    "        # Para cualquier otro tipo no reconocido\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(agent, model_name, episode=0, steps=0, \n",
    "                          checkpoint_dir=\"checkpoints\", suffix=\"lastest\", epsilon=0.1):\n",
    "    \"\"\"           \n",
    "    Guarda el modelo, la memoria de repetición y el estado del entrenamiento.\n",
    "    \n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.    \n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Nombre base del modelo (ej: 'DQN', 'DDQN_REPLAY')\n",
    "        episode: Número del episodio actual\n",
    "        steps: Pasos globales acumulados\n",
    "        checkpoint_dir: Directorio donde guardar los checkpoints\n",
    "        suffix: Tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\")\n",
    "        \n",
    "        model : tensorflow.keras.Model-  El modelo principal de red neuronal que se desea guardar.\n",
    "        memory : objeto de memoria-      La memoria de repetición utilizada para almacenar experiencias de entrenamiento.\n",
    "                                         Puede ser SequentialMemory, ReplayBuffer u otra implementación compatible.\n",
    "        target_model : tensorflow.keras.Model-         El modelo target utilizado en algoritmos como DDQN. Es una copia del modelo principal\n",
    "            que se actualiza periódicamente durante el entrenamiento.\n",
    "        model_name : str-                Nombre identificativo del modelo ('DQN', 'DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.).\n",
    "                                         Se utiliza para nombrar los archivos de checkpoint.\n",
    "        episode : int, opcional-         Número del episodio actual de entrenamiento. Por defecto es 0.\n",
    "        steps : int, opcional-           Número total de pasos (interacciones con el entorno) realizados. Por defecto es 0.\n",
    "\n",
    "        checkpoint_dir : str, opcional-  Directorio donde se guardarán los archivos de checkpoint. Por defecto es \"checkpoints\".\n",
    "\n",
    "        suffix : str, opcional-          Sufijo para identificar el tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\"). Por defecto es \"lastest\".\n",
    "\n",
    "        epsilon : float, opcional-       Valor actual de epsilon para la política epsilon-greedy. Por defecto es 0.1.\n",
    "        force_override: Si es True, sobrescribe incluso los checkpoints 'best'    \n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "    \"\"\"   \n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "    \n",
    "    print(f\"DEBUG ------------{episode}\")\n",
    "    # PROTECCIÓN DE CHECKPOINTS EXISTENTES\n",
    "    # Si es un checkpoint \"best\" y estamos intentando guardarlo con episodio 0\n",
    "    if (\"best\" in suffix or \"lastest\" in suffix) and episode == 0:\n",
    "        # Verificar si ya existe un mejor checkpoint con episodio > 0\n",
    "        state_path = f\"{checkpoint_dir}/{model_name}_best_state.json\"\n",
    "        if os.path.exists(state_path):\n",
    "            try:\n",
    "                with open(state_path, 'r') as f:\n",
    "                    existing_state = json.load(f)\n",
    "                existing_episode = existing_state.get('episode', 0)\n",
    "                \n",
    "                if existing_episode > 0:\n",
    "                    print(f\"🛡️ Protegiendo checkpoint '{suffix}' existente (ep: {existing_episode})\")\n",
    "                    print(f\"❌ NO se guardará un nuevo checkpoint con episodio 0\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error verificando checkpoint existente: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo principal\n",
    "        if hasattr(agent, 'model'):\n",
    "            agent.model.save_weights(main_model_path)\n",
    "            print(f\"💾 Guardado modelo principal {suffix}: {main_model_path}\")               \n",
    "        else:\n",
    "            print(f\"⚠️ El agente no tiene el atributo 'model'\")          \n",
    "        \n",
    "        # Guardar pesos del modelo target si existe\n",
    "        if hasattr(agent, 'target_model') and agent.target_model is not None:\n",
    "            agent.target_model.save_weights(target_model_path)      \n",
    "            print(f\"💾 Guardado modelo target {suffix}: {target_model_path}\")         \n",
    "        else:\n",
    "            print(f\"⚠️ El agente no tiene el atributo 'target_model' o es None\")            \n",
    "\n",
    "        # Preparar el estado del entrenamiento - Con conversión a tipos Python nativos\n",
    "        state = {\n",
    "            'episode': int(episode),  \n",
    "            'global_steps': int(steps),  \n",
    "            'epsilon': float(epsilon),        \n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }              \n",
    "        \n",
    "        # Si hay una política específica en el agente\n",
    "        if hasattr(agent, 'policy'):\n",
    "            policy_state = {}\n",
    "            \n",
    "            # Intentar guardar el estado de la política\n",
    "            if hasattr(agent.policy, 'eps'):\n",
    "                policy_state['eps'] = float(agent.policy.eps)  # Convertir a float Python\n",
    "        \n",
    "            # Si la política tiene más atributos relevantes\n",
    "            for attr in ['value_max', 'value_min', 'value_test', 'nb_steps']:\n",
    "                if hasattr(agent.policy, attr):\n",
    "                    value = getattr(agent.policy, attr)\n",
    "                    if isinstance(value, (np.integer, np.floating, np.bool_)):\n",
    "                        value = value.item()  # Convierte cualquier tipo numpy a su equivalente Python\n",
    "                    policy_state[attr] = value\n",
    "            \n",
    "            if policy_state:\n",
    "                state['policy'] = policy_state                  \n",
    "               \n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and hasattr(agent, 'memory') and agent.memory is not None:\n",
    "            try:\n",
    "                with open(memory_path, 'wb') as f:\n",
    "                    pickle.dump(agent.memory, f)\n",
    "                print(f\"💾 Memoria {suffix} guardada: {memory_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ No se pudo guardar la memoria: {e}\")                    \n",
    "                \n",
    "        # Guardar el estado\n",
    "        with open(state_path, 'w') as f:\n",
    "            json.dump(state, f, indent=2, default=convert_to_json_serializable)\n",
    "        \n",
    "        print(f\"💾 Checkpoint {suffix} guardado (ep: {episode}, pasos: {steps})\")  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint {suffix}: {e}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgbzJyUjmTzs"
   },
   "source": [
    "#### **ENTRENAMIENTO** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(env,  model_name,  model, memory, target_model, model_instance=False, \n",
    "        start_episode=0, start_steps=0,\n",
    "        batch_size=batch_size, learning_rate=learning_rate, checkpoint_path='checkpoints',\n",
    "        input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size, warmup_steps=WARMUP_STEPS,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau, epsilon_start=epsilon_start,\n",
    "        epsilon_min=0.1, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS, target_reward=TARGET_REWARD,\n",
    "        enable_double_dqn = False):     \n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN con el entorno proporcionado\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        env: El entorno de Gym\n",
    "        model_name: Nombre identificador del modelo ('DQN', 'DDQN', etc.)\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None para DQN)        \n",
    "        model_instance: Flag que determina si existe modelo cargado (si False, se crea uno nuevo)\n",
    "        checkpoint_path: Ruta donde guardar checkpoints\n",
    "        start_episode: Episodio desde donde continuar el entrenamiento\n",
    "        start_steps: Pasos desde donde continuar el entrenamiento\n",
    "        batch_size: Tamaño del lote para el entrenamiento\n",
    "        learning_rate: Tasa de aprendizaje del optimizador\n",
    "        input_shape: Forma de la entrada para el modelo\n",
    "        memoria_tamano: Tamaño de la memoria de repetición\n",
    "        warmup_steps: Pasos de calentamiento antes del entrenamiento\n",
    "        target_update_interval: Intervalo para actualizar la red objetivo\n",
    "        target_update_tau: Factor de actualización suave para la red objetivo\n",
    "        epsilon_start: Valor inicial de epsilon para exploración\n",
    "        epsilon_min: Valor mínimo de epsilon para exploración\n",
    "        epsilon_steps: Número de pasos para decrementar epsilon\n",
    "        num_steps: Número total de pasos de entrenamiento\n",
    "        target_reward: Recompensa objetivo para considerar resuelto el entorno\n",
    "        enable_double_dqn: Si es True, usa DDQN; si no, usa DQN estándar\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (agente_entrenado, éxito) - Modelo entrenado y booleano indicando éxito    \n",
    "    \"\"\"\n",
    "    # Nombre del modelo para logs y checkpoints\n",
    "    name_model = model_name.upper()    \n",
    "    print(f\"🤖 {'Continuando' if model_instance else 'Creando'} entrenamiento para {name_model}...\")\n",
    "        \n",
    "    # Inicializar dqn desde el principio para evitar referencia antes de asignación\n",
    "    dqn = None      \n",
    "    save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"\n",
    "    # Inicializar callbacks al principio para asegurarnos de que siempre esté definido\n",
    "    callbacks = []\n",
    "    # Verificar target_model solo si estamos usando DDQN\n",
    "    if enable_double_dqn and target_model is None:\n",
    "        raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "            \n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()   \n",
    "    try:  \n",
    "        # Verificar que tenemos un modelo válido\n",
    "        if model is None:\n",
    "            raise ValueError(\"El modelo principal no puede ser None.\")      \n",
    "        \n",
    "        # Verificar target_model solo si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is None:\n",
    "            raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "\n",
    "        # Verificar si la memoria ya se creó o necesitamos crearla\n",
    "        if memory is None:\n",
    "            print(\"Creando nueva memoria de experiencia...\")\n",
    "            memory = SequentialMemory(limit=memoria_tamano, window_length=WINDOW_LENGTH)\n",
    "        \n",
    "        # Política de exploración\n",
    "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                     attr='eps',\n",
    "                                     value_max=epsilon_start, \n",
    "                                     value_min=epsilon_min, \n",
    "                                     value_test=.05,\n",
    "                                     nb_steps=epsilon_steps)  \n",
    "        # Crear agente DQN\n",
    "        dqn = DQNAgent(\n",
    "            model=model,\n",
    "            nb_actions=env.action_space.n,\n",
    "            memory=memory,\n",
    "            processor=processor,\n",
    "            nb_steps_warmup=warmup_steps,\n",
    "            target_model_update=target_update_interval if enable_double_dqn else 10000,\n",
    "            enable_double_dqn=enable_double_dqn,\n",
    "            policy=policy,\n",
    "            gamma=0.99,\n",
    "            train_interval=4,\n",
    "            delta_clip=1.0,\n",
    "            batch_size=batch_size\n",
    "        )        \n",
    "        \n",
    "        # Después de crear el agente, reemplazar el target_model si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is not None:\n",
    "            dqn.target_model = target_model            \n",
    "\n",
    "        # Compilar el agente\n",
    "        optimizer = Adam(learning_rate=learning_rate)  \n",
    "        dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al crear o compilar el agente: {str(e)}\")\n",
    "        return None, False\n",
    "        \n",
    "    # Verificar que tenemos un agente válido antes de continuar\n",
    "    if dqn is None:\n",
    "        print(f\"❌ Error: No se pudo inicializar el agente DQN\")\n",
    "        return None, False        \n",
    "    try:\n",
    "        # DEBUG --------------------\n",
    "        # callbacks = [\n",
    "        #     log_filename = f'{checkpoint_path}/{name_model}_log.json'\n",
    "        #     progress_callback,\n",
    "        #     ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
    "        #     CustomFileLogger(log_filename, interval=1000),\n",
    "        #     PerformanceMonitor(save_path='diagnosticos')\n",
    "        # ]\n",
    "        # Callbacks optimizados para el objetivo\n",
    "        callbacks = [\n",
    "            SimpleProgressCallback(num_steps, print_interval=20000),         \n",
    "            TargetRewardTracker(dqn,target_avg_reward=target_reward, name_model=name_model, window_size=100, \n",
    "                                save_best=True, checkpoint_dir=checkpoint_path),\n",
    "            EpisodeCheckpointCallback(dqn, checkpoint_path=checkpoint_path, save_freq=100, model_name=name_model)    \n",
    "        ]         \n",
    "          \n",
    "         # Ajustar el número de pasos restantes si estamos continuando el entrenamiento\n",
    "        adjusted_steps = max(0, num_steps - start_steps)\n",
    "        if start_steps > 0:\n",
    "            print(f\"Continuando desde el paso {start_steps} (quedan {adjusted_steps} pasos)\")\n",
    "\n",
    "        print(f\"Iniciando entrenamiento de {name_model} por {adjusted_steps} pasos...\")\n",
    "        start_time = time.time()       \n",
    "        # Fit del agente al entorno\n",
    "        history = dqn.fit(env, nb_steps=adjusted_steps, callbacks=callbacks, verbose=2)\n",
    "        \n",
    "        training_time = (time.time() - start_time) / 60\n",
    "        print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        # Verificar correctamente las claves disponibles y usar la adecuada\n",
    "        if hasattr(history, 'history'):\n",
    "            if 'episode' in history.history and history.history['episode']:\n",
    "                episode = start_episode + history.history['episode'][-1]\n",
    "            elif 'nb_episode' in history.history and history.history['nb_episode']:\n",
    "                episode = start_episode + history.history['nb_episode'][-1]\n",
    "            else:\n",
    "                # Intentar obtener el episodio del agente directamente\n",
    "                episode = getattr(dqn, 'episode', start_episode)\n",
    "        else:\n",
    "            episode = start_episode        \n",
    "        steps = start_steps + adjusted_steps\n",
    "        epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1          \n",
    "        try:\n",
    "            print(f\"DEBUG --------sss------------{episode}\")\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"lastest\", \n",
    "                                epsilon=epsilon)\n",
    "                \n",
    "            # También guardar como \"best\" si no hay un checkpoint \"best\" previo\n",
    "            best_path = f\"{checkpoint_path}/{name_model}_best_model.h5\"\n",
    "            if not os.path.exists(best_path):\n",
    "                save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                    checkpoint_dir=save_checkpoint_path, suffix=\"best\", \n",
    "                                    epsilon=epsilon)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al guardar el modelo: {str(e)}\")\n",
    "                                                        \n",
    "        return dqn, True    \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido por el usuario\") \n",
    "        # Guardar pesos de emergencia\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            episode = start_episode  # No podemos saber el episodio exacto después de la interrupción\n",
    "            steps = start_steps + dqn.step if hasattr(dqn, 'step') else start_steps\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"emergency\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"✅ Modelo guardado en estado de emergencia\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al guardar el modelo de emergencia: {str(e)}\")\n",
    "        return dqn, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ [ERROR] - Error durante el entrenamiento: {str(e)}\")\n",
    "        # Intentar guardar en estado de error\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            save_model_checkpoint(dqn, name_model, episode=start_episode, \n",
    "                                steps=start_steps + (dqn.step if hasattr(dqn, 'step') else 0),\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"error_recovery\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"✅ Modelo guardado en estado de recuperación de error\")\n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ Error al guardar modelo de recuperación: {str(e2)}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVALUACION** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6GZzkrK72yj1"
   },
   "outputs": [],
   "source": [
    "# Función para evaluar el modelo\n",
    "def evaluar_modelo(agent, env, num_episodes=200, render=True, record_video=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo DQN o DDQN y si el modelo alcanza el objetivo de media\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        agent: Agente DQN entrenado\n",
    "        env: Entorno de gym\n",
    "        num_episodes: Número de episodios para evaluar\n",
    "        render: Si se debe mostrar la visualización\n",
    "        record_video: Si se debe grabar video\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        Lista de recompensas por episodio\n",
    "    \"\"\"\n",
    "    print(f\"🎯 EVALUANDO MODELO DUELING DQN\")\n",
    "    print(f\"📊 Evaluando por {num_episodes} episodios...\")\n",
    "    rewards = []\n",
    "    \n",
    "    # Setup for frame stacking\n",
    "    window_length = 4  # As specified in your model input shape\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the frame buffer with the initial observation\n",
    "            if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                processed_obs = agent.processor.process_observation(observation)\n",
    "            else:\n",
    "                processed_obs = observation\n",
    "                \n",
    "            # Create a frame stack buffer of the right shape\n",
    "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
    "            \n",
    "            # Fill buffer with the first observation\n",
    "            for i in range(window_length):\n",
    "                frame_buffer[i] = processed_obs\n",
    "            \n",
    "            while not done and step < 2000:\n",
    "                try:\n",
    "                    # Prepare input in the format expected by the model: (batch, channels=window_length, h, w)\n",
    "                    # Format directly to channels_first, correcting for the specific shape expected\n",
    "                    input_data = np.expand_dims(frame_buffer, axis=0)  # Add batch dimension\n",
    "                    \n",
    "                    # Get Q-values directly from the model\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        q_values = agent.model.predict(input_data)\n",
    "                        \n",
    "                        # Handle the output format for Dueling DQN\n",
    "                        if isinstance(q_values, list):\n",
    "                            q_values = q_values[0]  # Take first output for value function\n",
    "                      \n",
    "                        action = np.argmax(q_values)\n",
    "                    else:\n",
    "                        # Fallback to random action\n",
    "                        print(\"⚠️ No se encuentra ningún modelo, utilizando una acción aleatoria\")\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    # Execute action\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    # Render if enabled\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    \n",
    "                    # Process new observation\n",
    "                    if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                        processed_obs = agent.processor.process_observation(observation)\n",
    "                    else:\n",
    "                        processed_obs = observation\n",
    "                    \n",
    "                    # Update frame buffer - shift frames\n",
    "                    for i in range(window_length-1):\n",
    "                        frame_buffer[i] = frame_buffer[i+1]\n",
    "                    frame_buffer[window_length-1] = processed_obs\n",
    "                    \n",
    "                    # Update counters\n",
    "                    total_reward += reward\n",
    "                    step += 1\n",
    "                    \n",
    "                except Exception as step_error:\n",
    "                    print(f\"⚠️ Error en el paso {step}: {step_error}\")\n",
    "                    break\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "            print(f\"   Episodio {episode + 1}/{num_episodes}: reward = {total_reward:.1f}\")\n",
    "        \n",
    "        # Análisis final\n",
    "        if len(rewards) > 0:\n",
    "            avg_reward = np.mean(rewards)\n",
    "            std_reward = np.std(rewards)\n",
    "            max_reward = np.max(rewards)\n",
    "            min_reward = np.min(rewards)\n",
    "            \n",
    "            objetivo_alcanzado = avg_reward >= TARGET_REWARD\n",
    "            \n",
    "            print(f\"\\n📊 RESULTADOS DE EVALUACIÓN:\")\n",
    "            print(f\"   Media: {avg_reward:.2f} {'✅' if objetivo_alcanzado else '❌'}\")\n",
    "            print(f\"   Desviación: ±{std_reward:.2f}\")\n",
    "            print(f\"   Máximo: {max_reward:.2f}\")\n",
    "            print(f\"   Mínimo: {min_reward:.2f}\")\n",
    "            print(f\"   Episodios sobre {TARGET_REWARD}: {sum(1 for r in rewards if r >= TARGET_REWARD)} / {len(rewards)}\")\n",
    "            \n",
    "            if objetivo_alcanzado:\n",
    "                print(f\"🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de {avg_reward:.2f}\")\n",
    "            else:\n",
    "                print(f\"📈 Progreso: {(avg_reward/TARGET_REWARD)*100:.1f}% del objetivo\")\n",
    "        else:\n",
    "            print(\"❌ No se completaron episodios correctamente\")\n",
    "            avg_reward = float('nan')\n",
    "            objetivo_alcanzado = False\n",
    "        \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()            \n",
    "        return rewards, objetivo_alcanzado        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ [ERROR] - Error durante evaluación: {e}\")    \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()           \n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabar_video_del_modelo(model, model_name, env=None, video_dir=\"checkpoints/videos\", fps=30, max_steps=10000):\n",
    "    \"\"\"\n",
    "    Graba un video del comportamiento del modelo en el entorno.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model: El agente entrenado (DQN, DDQN, etc.)\n",
    "    model_name: Nombre del modelo para identificar el archivo de video\n",
    "    video_dir: Directorio donde se guardará el video\n",
    "    fps: Frames por segundo para el video\n",
    "    max_steps: Número máximo de pasos a ejecutar\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    str: Ruta al archivo de video grabado, o None si hubo un error\n",
    "    \"\"\"\n",
    "    # Crear directorio para videos\n",
    "    video_dir = os.path.join(\"checkpoints\", \"videos\")\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Nombre del archivo\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    video_path = os.path.join(video_dir, f\"{model_name}_{timestamp}.mp4\")\n",
    "    \n",
    "    # Crear un nuevo entorno\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    frames = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"📹 Grabando video del modelo {model_name}...\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Inicializar episodio\n",
    "            observation = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < max_steps:\n",
    "                # Capturar frame para el video\n",
    "                frame = env.render(mode='rgb_array')\n",
    "                frames.append(frame)\n",
    "                \n",
    "                # Obtener acción\n",
    "                if hasattr(model, 'forward'):\n",
    "                    # Para agentes keras-rl\n",
    "                    action = model.forward(observation)\n",
    "                elif hasattr(model, 'predict'):\n",
    "                    # Para otros tipos de agentes\n",
    "                    action = model.predict(observation)[0]\n",
    "                else:\n",
    "                    # Fallback\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                # Ejecutar acción\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "            \n",
    "            print(f\"  Episodio {episode+1}: Recompensa = {total_reward}\")\n",
    "        \n",
    "        # Guardar video\n",
    "        if frames:\n",
    "            print(f\"💾 Guardando video con {len(frames)} frames...\")\n",
    "            import imageio\n",
    "            imageio.mimsave(video_path, frames, fps=30)\n",
    "            print(f\"✅ Video guardado en: {video_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Cerrar entorno\n",
    "        env.close()\n",
    "    \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡¡¡¡¡¡¡ **EJECUCION - MAIN** !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True\n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "    episode_render = False\n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    # Optimizar configuración de TensorFlow\n",
    "    optimizar_tensorflow()\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    gc.collect()    \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    # Ejecutar prueba\n",
    "    print(\"🚀 EJECUTANDO SOLUCIÓN...\")\n",
    "    print(f\"🎯 OBJETIVO: Conseguir media de episode_reward = {TARGET_REWARD} (con clipping)\")    \n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', create_dqn_model, False),\n",
    "        ('DDQN', create_ddqn_models, False),\n",
    "        ('DDQN_REPLAY', create_ddqn_replay_model, False),\n",
    "        ('DUELING_DQN_REPLAY', create_dueling_dqn_replay_model, True)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for model_name, model_process, training_specific_flag in modelos_a_procesar:\n",
    "            # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "            # Verificar el tipo de modelo y establecer enable_double_dqn correctamente\n",
    "            enable_double_dqn = model_name in ['DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']            \n",
    "            entrenarSN = training_global and training_specific_flag\n",
    "            model_instance = False    \n",
    "            agent = None\n",
    "            save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"            \n",
    "            if entrenarSN:\n",
    "                # Crear una nueva sesión para cada modelo\n",
    "                tf.keras.backend.clear_session()            \n",
    "                # Intentar cargar un modelo previamente guardado (independientemente de si entrenaremos o no)\n",
    "                try:        \n",
    "                    # Crear una instancia del modelo - crear primero la arquitectura antes de poder cargar los pesos en ella\n",
    "                    if enable_double_dqn:\n",
    "                        model, memory, target_model = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                    else:\n",
    "                        model, memory, _ = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                        target_model = None    \n",
    "\n",
    "                    # Intentar cargar el mejor checkpoint (o lastest si best no existe)\n",
    "                    start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                 checkpoint_dir=save_checkpoint_path, suffix=\"best\")\n",
    "                    if start_episode == 0:\n",
    "                        # Si no encontró el mejor, intentar con el último guardado\n",
    "                        start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                     checkpoint_dir=save_checkpoint_path, suffix=\"lastest\")\n",
    "                    if start_episode > 0:\n",
    "                        model_instance = True\n",
    "                        print(f\"✅ Modelo {model_name} cargado exitosamente desde el episodio {start_episode}\")\n",
    "                        # Si se debe entrenar, continuar desde donde quedó\n",
    "                        if entrenarSN:\n",
    "                            print(f\"⏩ Continuando entrenamiento de {model_name} desde episodio {start_episode+1}\")\n",
    "                            # Establecer parámetros para continuar el entrenamiento\n",
    "                            epsilon_actual = epsilon\n",
    "\n",
    "                            # Entrenar el modelo desde donde quedó\n",
    "                            agent, success = entrenar_modelo(\n",
    "                                env=env,\n",
    "                                model_name=model_name,\n",
    "                                model=model, \n",
    "                                memory=memory, \n",
    "                                target_model=target_model,\n",
    "                                model_instance=model_instance,\n",
    "                                checkpoint_path=checkpoint_path,\n",
    "                                start_episode=start_episode+1,\n",
    "                                start_steps=global_steps,\n",
    "                                epsilon_start=epsilon_actual,  # Usar el epsilon guardado\n",
    "                                epsilon_min=epsilon_stop,\n",
    "                                epsilon_steps=EPSILON_STEPS,\n",
    "                                num_steps=NUM_TRAINING_STEPS,\n",
    "                                warmup_steps=WARMUP_STEPS,\n",
    "                                target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                                target_update_tau=tau,\n",
    "                                enable_double_dqn = enable_double_dqn\n",
    "                            )                                                           \n",
    "                    else:\n",
    "                        print(f\"🆕 Creando y entrenando un nuevo modelo {model_name}\")\n",
    "                        model_instance = False   \n",
    "                        # Entrenar el modelo desde cero\n",
    "                        agent, success = entrenar_modelo(\n",
    "                            env=env,\n",
    "                            model_name=model_name,\n",
    "                            model=model, \n",
    "                            memory=memory, \n",
    "                            target_model=target_model,                        \n",
    "                            model_instance=model_instance,\n",
    "                            checkpoint_path=checkpoint_path,\n",
    "                            start_episode=0,  # Añadido: Especificar episodio inicial\n",
    "                            start_steps=0,    # Añadido: Especificar pasos iniciales\n",
    "                            epsilon_start=epsilon_start,\n",
    "                            epsilon_min=epsilon_stop,\n",
    "                            epsilon_steps=EPSILON_STEPS,\n",
    "                            num_steps=NUM_TRAINING_STEPS,\n",
    "                            warmup_steps=WARMUP_STEPS,\n",
    "                            target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                            target_update_tau=tau,\n",
    "                            enable_double_dqn = enable_double_dqn\n",
    "                        )                \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ [ERROR] - Error al cargar o entrenar modelo {model_name}: {e}\")\n",
    "\n",
    "            # Si tenemos un modelo válido (cargado o entrenado), evaluarlo y guardarlo en el diccionario\n",
    "            # Evaluación rápida con 10 episodios para:\n",
    "            # - Comprobar el rendimiento básico del modelo\n",
    "            # - Decidir si vale la pena guardarlo como \"best\"\n",
    "            # - Mostrar un feedback rápido sobre su desempeño\n",
    "            if agent:\n",
    "                rewards, _ = evaluar_modelo(agent, env, num_episodes=10, render=False, record_video=False)\n",
    "                avg_reward = np.mean(rewards)\n",
    "                trained_models[model_name] = agent                       \n",
    "                print(f\"📊 Recompensa promedio para {model_name}: {avg_reward:.2f}\")\n",
    "                # Guardar el modelo como \"best\" si supera umbral de evaluación\n",
    "                if avg_reward >= TARGET_REWARD * 0.8:  # 80% del objetivo como umbral mínimo\n",
    "                    save_model_checkpoint(agent, model_name, episode=start_episode, steps=global_steps,  # Corregido: Usar el episodio y pasos actuales\n",
    "                                         checkpoint_dir=save_checkpoint_path, suffix=\"best_reward\", \n",
    "                                         epsilon=epsilon_start)\n",
    "                    print(f\"🏆 Modelo {model_name} guardado como 'best_reward' con recompensa {avg_reward:.2f}\")        \n",
    "\n",
    "        # ------------------------------------------------------    \n",
    "        # Búsqueda del mejor modelo entre todos los entrenados/cargados: comparar su rendimiento\n",
    "        # con el mismo número de episodios (10) para mantener una comparación justa\n",
    "        # Ayuda a determinar cuál es el mejor modelo para la evaluación final\n",
    "        best_model_name = None\n",
    "        best_reward = -float('inf')    \n",
    "        for name, model in trained_models.items():\n",
    "            rewards, objetivo_conseguido = evaluar_modelo(model, env, num_episodes=10, render=False, record_video=False)\n",
    "            avg_reward = np.mean(rewards)\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                best_model_name = name        \n",
    "\n",
    "        if best_model_name:\n",
    "            print(\"\\n✅ SOLUCIÓN EXITOSA - Entrenamiento completado\")        \n",
    "            print(f\"\\n🥇 El mejor modelo es {best_model_name} con recompensa promedio {best_reward:.2f}\")\n",
    "            best_model = trained_models[best_model_name]\n",
    "\n",
    "            # Evaluación final y más exhaustiva del mejor modelo: Se hace con muchos más episodios (200) \n",
    "            #   para obtener resultados estadísticamente más significativos. Es la evaluación definitiva \n",
    "            #   para determinar si se cumple el objetivo --> conclusión final del proceso        \n",
    "            #   print(f\"\\n🎯 EVALUACIÓN FINAL DEL OBJETIVO\")\n",
    "            #   rewards_eval, objetivo_conseguido = evaluar_modelo(best_model, env, num_episodes=200)                \n",
    "\n",
    "            # Grabar video del mejor modelo\n",
    "            video_path = grabar_video_del_modelo(best_model, best_model_name, env)\n",
    "            if video_path:\n",
    "                print(f\"🎬 Se ha grabado una demostración del modelo en: {video_path}\")        \n",
    "\n",
    "            if objetivo_conseguido:\n",
    "                print(f\"🏆 ¡FELICIDADES EQUIPO! El modelo alcanzó el objetivo de media {TARGET_REWARD}\")\n",
    "            else:\n",
    "                print(f\"📈 El modelo necesita más entrenamiento para alcanzar media {TARGET_REWARD}\")\n",
    "        else:\n",
    "            print(\"❌ [ERROR] - No se pudo entrenar ningún modelo correctamente\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️ Entrenamiento interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n❌ Error inesperado: {e}\")\n",
    "    finally:\n",
    "        # Asegurar que siempre se cierra el entorno\n",
    "        env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - TensorFlow optimizado para 20 cores CPU\n",
      "🚀 EJECUTANDO SOLUCIÓN...\n",
      "🎯 OBJETIVO: Conseguir media de episode_reward = 20.0 (con clipping)\n",
      "🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape=(84, 84, 4), actions=6\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "✅ Modelo creado exitosamente\n",
      "📊 Resumen del modelo:\n",
      "Model: \"DuelingDQNReplay_Main_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_channels_first (InputLay  [(None, 4, 84, 84)]  0          []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 84, 84, 4)    0           ['input_channels_first[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 20, 20, 32)   8224        ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 9, 9, 64)     32832       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 7, 64)     36928       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          1606144     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          1606144     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            3078        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 6)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6)            0           ['dense_1[0][0]',                \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,293,863\n",
      "Trainable params: 3,293,863\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "📂 Se cargó: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_state.json\n",
      "📂 Cargando checkpoint best (episodio: 10, pasos: 5355, epsilon: 0.1)\n",
      "📂 Modelo principal cargado: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "📂 Modelo target cargado: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "📂 Memoria cargada correctamente: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "✅ Modelo DUELING_DQN_REPLAY cargado exitosamente desde el episodio 10\n",
      "⏩ Continuando entrenamiento de DUELING_DQN_REPLAY desde episodio 11\n",
      "🤖 Continuando entrenamiento para DUELING_DQN_REPLAY...\n",
      "🎯 OBJETIVO: Media de episode_reward = 20.0\n",
      "📊 Ventana de evaluación: 100 episodios\n",
      "Continuando desde el paso 5355 (quedan 4645 pasos)\n",
      "Iniciando entrenamiento de DUELING_DQN_REPLAY por 4645 pasos...\n",
      "🚀 Entrenamiento iniciado: 10,000 pasos\n",
      "Training for 4645 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1: Recompensa total (clipped): 16.000, Pasos: 511, Mean Reward Calculado: 0.031311 (Recompensa/Pasos)\n",
      "  511/4645: episode: 1, duration: 3.982s, episode steps: 511, steps per second: 128, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 2: Recompensa total (clipped): 13.000, Pasos: 412, Mean Reward Calculado: 0.031553 (Recompensa/Pasos)\n",
      "  923/4645: episode: 2, duration: 2.951s, episode steps: 412, steps per second: 140, episode reward: 13.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 3: Recompensa total (clipped): 17.000, Pasos: 747, Mean Reward Calculado: 0.022758 (Recompensa/Pasos)\n",
      " 1670/4645: episode: 3, duration: 5.880s, episode steps: 747, steps per second: 127, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 4: Recompensa total (clipped): 17.000, Pasos: 515, Mean Reward Calculado: 0.033010 (Recompensa/Pasos)\n",
      " 2185/4645: episode: 4, duration: 4.094s, episode steps: 515, steps per second: 126, episode reward: 17.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 5: Recompensa total (clipped): 7.000, Pasos: 440, Mean Reward Calculado: 0.015909 (Recompensa/Pasos)\n",
      " 2625/4645: episode: 5, duration: 3.434s, episode steps: 440, steps per second: 128, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 6: Recompensa total (clipped): 14.000, Pasos: 480, Mean Reward Calculado: 0.029167 (Recompensa/Pasos)\n",
      " 3105/4645: episode: 6, duration: 3.512s, episode steps: 480, steps per second: 137, episode reward: 14.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 7: Recompensa total (clipped): 22.000, Pasos: 722, Mean Reward Calculado: 0.030471 (Recompensa/Pasos)\n",
      " 3827/4645: episode: 7, duration: 5.270s, episode steps: 722, steps per second: 137, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 8: Recompensa total (clipped): 5.000, Pasos: 291, Mean Reward Calculado: 0.017182 (Recompensa/Pasos)\n",
      " 4118/4645: episode: 8, duration: 2.235s, episode steps: 291, steps per second: 130, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "📈 Episodio 9: Recompensa total (clipped): 13.000, Pasos: 475, Mean Reward Calculado: 0.027368 (Recompensa/Pasos)\n",
      " 4593/4645: episode: 9, duration: 3.617s, episode steps: 475, steps per second: 131, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "done, took 35.317 seconds\n",
      "Entrenamiento completado en 0.59 minutos\n",
      "DEBUG --------sss------------11\n",
      "DEBUG ------------11\n",
      "💾 Guardado modelo principal lastest: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_model.h5\n",
      "💾 Guardado modelo target lastest: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_target.h5\n",
      "💾 Memoria lastest guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_memory.pkl\n",
      "💾 Checkpoint lastest guardado (ep: 11, pasos: 10000)\n",
      "DEBUG ------------11\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 11, pasos: 10000)\n",
      "🎯 EVALUANDO MODELO DUELING DQN\n",
      "📊 Evaluando por 10 episodios...\n",
      "   Episodio 1/10: reward = 390.0\n",
      "   Episodio 2/10: reward = 180.0\n",
      "   Episodio 3/10: reward = 175.0\n",
      "   Episodio 4/10: reward = 75.0\n",
      "   Episodio 5/10: reward = 170.0\n",
      "   Episodio 6/10: reward = 225.0\n",
      "   Episodio 7/10: reward = 210.0\n",
      "   Episodio 8/10: reward = 225.0\n",
      "   Episodio 9/10: reward = 75.0\n",
      "   Episodio 10/10: reward = 30.0\n",
      "\n",
      "📊 RESULTADOS DE EVALUACIÓN:\n",
      "   Media: 175.50 ✅\n",
      "   Desviación: ±96.71\n",
      "   Máximo: 390.00\n",
      "   Mínimo: 30.00\n",
      "   Episodios sobre 20.0: 10 / 10\n",
      "🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de 175.50\n",
      "📊 Recompensa promedio para DUELING_DQN_REPLAY: 175.50\n",
      "DEBUG ------------10\n",
      "💾 Guardado modelo principal best_reward: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_reward_model.h5\n",
      "💾 Guardado modelo target best_reward: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_reward_target.h5\n",
      "💾 Memoria best_reward guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_reward_memory.pkl\n",
      "💾 Checkpoint best_reward guardado (ep: 10, pasos: 5355)\n",
      "🏆 Modelo DUELING_DQN_REPLAY guardado como 'best_reward' con recompensa 175.50\n",
      "🎯 EVALUANDO MODELO DUELING DQN\n",
      "📊 Evaluando por 10 episodios...\n",
      "   Episodio 1/10: reward = 200.0\n",
      "   Episodio 2/10: reward = 205.0\n",
      "   Episodio 3/10: reward = 175.0\n",
      "   Episodio 4/10: reward = 205.0\n",
      "   Episodio 5/10: reward = 360.0\n",
      "   Episodio 6/10: reward = 35.0\n",
      "   Episodio 7/10: reward = 210.0\n",
      "   Episodio 8/10: reward = 420.0\n",
      "   Episodio 9/10: reward = 145.0\n",
      "   Episodio 10/10: reward = 25.0\n",
      "\n",
      "📊 RESULTADOS DE EVALUACIÓN:\n",
      "   Media: 198.00 ✅\n",
      "   Desviación: ±116.45\n",
      "   Máximo: 420.00\n",
      "   Mínimo: 25.00\n",
      "   Episodios sobre 20.0: 10 / 10\n",
      "🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de 198.00\n",
      "\n",
      "✅ SOLUCIÓN EXITOSA - Entrenamiento completado\n",
      "\n",
      "🥇 El mejor modelo es DUELING_DQN_REPLAY con recompensa promedio 198.00\n",
      "📹 Grabando video del modelo DUELING_DQN_REPLAY...\n",
      "❌ Error: name 'num_episodes' is not defined\n",
      "🎬 Se ha grabado una demostración del modelo en: checkpoints\\videos\\DUELING_DQN_REPLAY_20250624_045933.mp4\n",
      "\n",
      "\n",
      "❌ Error inesperado: name 'objetivo_conseguido' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68dzFiUS2U6L"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
