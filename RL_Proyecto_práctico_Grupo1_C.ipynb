{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah\n",
    "* Alumno 2: Cuesta Cifuentes, Jair\n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2. Preparar Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "El entorno de Colab está preinstalado con una serie de librerías por defecto. Para trabajar en base a las especificaciones del ejercicio se necesitan intalar unas librerías que bajen de versión las existentes en Colab. Entre ellas tensorflow. El problema de realizar esta acción es que para que todas las versiones sean consideradas por el entorno hay que reiniciar la sesión, sino se mantienen dependencias y los import no funcionan. <br>\n",
    "Es decir tras los \"pip install\" hay que hacer un **\"Runtime > Restart runtime\"** o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\".<br>\n",
    "En este punto se ha de tener presente que se ha reiniciado y **se han perdido las variables** que se hayan establecido, por ese motivo repetiremos el código para identificar si estamos en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "Ignorar los errores que puedan aparecer, son incompatibilidades con librerías avanzadas que no utilizamos ni necesitamos para nuestro código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB:  \n",
    "# =========================\n",
    "#  Entorno Colab nativo con todo lo compatible.\n",
    "#  Sólo recordar que se debe REINICIAR EL RUNTIME (al acabar)\n",
    "# =========================  \n",
    "  print(\"Instalando paquetes adicionales...\")  \n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Ignorar los errores que aparecen:\")\n",
    "  print(\"   Son incompatibilidades que aparecen con librería avanzadas\")\n",
    "  print(\"   que no necesitamos ni vamos a utilizar\")  \n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12.1 --quiet\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"INSTALACIÓN COMPLETADA\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\")\n",
    "  print(\"1. Ve a Runtime > Restart runtime\")\n",
    "  print(\"2. Después ejecuta las importaciones\")\n",
    "  print(\"=\"*60)  \n",
    "  INSTALL_LOCAL = False\n",
    "  IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB_ENV:\n",
    "# =========================\n",
    "#  Colab con env --> \n",
    "#    no funciona muy bien pues aunque se cree el entorno, Colab sigue\n",
    "#    utilizando el suyo con sus librería y se necesita usar %%writefile\n",
    "# =========================      \n",
    "  # 1. Instalar virtualenv\n",
    "  !pip install virtualenv --quiet\n",
    "\n",
    "  # 3. Crear el entorno virtual llamado \"miar_rl\"\n",
    "  !virtualenv miar_rl\n",
    "\n",
    "  # 4. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
    "  !./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
    "  !./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
    "  !./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
    "  !./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
    "  !./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
    "\n",
    "  # 5. Librerías adicionales\n",
    "  !./miar_rl/bin/pip install Pillow\n",
    "  !./miar_rl/bin/pip install matplotlib\n",
    "  !./miar_rl/bin/pip install tqdm\n",
    "  INSTALL_LOCAL = False\n",
    "    \n",
    "if INSTALL_LOCAL:    \n",
    "# =========================\n",
    "#  Librería para trabajar en local, si NO se cargaron las \n",
    "#    librerías desde fichero requirements\n",
    "# =========================        \n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm\n",
    "  %pip install imageio==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.3. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE:**<br>\n",
    "Recordar que antes de seguir (si hemos decidido el entorno de Colab nativo - IN_COLAB=True -) \n",
    "* Hay que hacer un <font color='red'>\"Runtime > Restart runtime\"</font> o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_1\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.4. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "3lN7KLe05NSa",
    "outputId": "47c41c84-3bfd-425f-9b8d-6d6aa525afdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.viminfo', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_dqn (1).ipynb', '08MIAR_dqn (2).ipynb', '08MIAR_dqn (3).ipynb', '08MIAR_dqn (4).ipynb', '08MIAR_dqn (5).ipynb', '08MIAR_dqn.ipynb', '08miar_dqn.py', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoint', 'checkpoints', 'Configuración local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'diagnosticos', 'Documents', 'Downloads', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'fffff.py', 'ffmpeg', 'ffmpeg-2025-06-17-git-ee1f79b0fa-essentials_build.7z', 'ffmpeg-2025-06-17-git-ee1f79b0fa-full_build.7z', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'lista.txt', 'menory.txt', 'Menú Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'models', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'Plantillas', 'ppppp.ipynb', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_práctico_Grupo1_C (1).ipynb', 'RL_Proyecto_práctico_Grupo1_C - copia.ipynb', 'RL_Proyecto_práctico_Grupo1_C.ipynb', 'RL_Proyecto_práctico_Grupo1_C.py', 'RL_Proyecto_práctico_Grupo1_C_v0.ipynb', 'RL_Proyecto_práctico_Grupo1_C_v2.ipynb', 'RL_Proyecto_práctico_Grupo1_C_v4.ipynb', 'RL_Proyecto_práctico_Grupo1_C_VERSION_COLAB_V3.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'start.bat', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled221.ipynb', 'Untitled3.ipynb', 'v3.ipynb', 'Videos', '_RL_Proyecto_práctico_Grupo1_C_v2.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Cambiar al directorio en Google Drive que deseas usar\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"Estamos ejecutando en Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Montar Google Drive en el punto de montaje\n",
    "    print(\"Colab: montando Google drive en: \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Crear drive_root si no existe\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: Asegurando que \", drive_root, \" existe.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Cambiar al directorio\n",
    "    print(\"\\nColab: Cambiamos el directorio a: \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verificar que estamos en el directorio de trabajo correcto\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihTI9TOD43ML"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIAR9zQv43MO"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas\n",
    "\n",
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K4o4-N4T43MO"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "import json\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "puzS2kTzc1Fd"
   },
   "outputs": [],
   "source": [
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "except ImportError:\n",
    "    print(\"WARNING: gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8iRCStcc90p",
    "outputId": "96045a07-febd-4dce-e5c8-bbec04475b33"
   },
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (x cores)\n",
    "def optimizar_tensorflow():\n",
    "    \"\"\"Configura TensorFlow para rendimiento óptimo en CPU/GPU\"\"\"\n",
    "    # Limpiar sesión previa\n",
    "    gc.collect()\n",
    "\n",
    "    # Optimización de GPU si está disponible\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"GPU optimizada para crecimiento adaptativo de memoria\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al configurar GPU: {e}\")\n",
    "\n",
    "    # Optimización de CPU\n",
    "    num_cpu_cores = os.cpu_count() or 8  # Fallback a 8 si no se puede detectar\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_cpu_cores // 2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(max(2, num_cpu_cores // 4))\n",
    "\n",
    "    # Modo eager solo si es necesario\n",
    "    # Para entrenamiento, es mejor desactivarlo por rendimiento\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "\n",
    "    print(f\"TensorFlow optimizado para {num_cpu_cores} cores CPU\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faNbnMuOdNDP"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7WFE0sqPdLsy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Crear el entorno\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJEMPLO TONTO DE VIDEO... PERO SIRVE PARA LUEGO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturando frames...\n",
      "Episodio terminado en el paso 694\n",
      "Guardando 695 frames en video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Video guardado: checkpoints/videos\\spaceinvaders_game.mp4\n",
      "✓ Tamaño del archivo: 181311 bytes\n",
      "\n",
      "Recompensa total: 230.0\n"
     ]
    }
   ],
   "source": [
    "# SOLUCIÓN MÁS SIMPLE Y CONFIABLE\n",
    "import gym\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# Crear directorio\n",
    "video_dir = 'checkpoints/videos'\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Crear entorno\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "# Lista para almacenar frames\n",
    "frames = []\n",
    "\n",
    "# Ejecutar episodio y capturar frames\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Capturando frames...\")\n",
    "for step in range(100000):\n",
    "    # Renderizar y capturar frame\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Acción aleatoria\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Ejecutar paso\n",
    "    step_result = env.step(action)\n",
    "    \n",
    "    if len(step_result) == 5:\n",
    "        observation, reward, done, truncated, info = step_result\n",
    "        done = done or truncated\n",
    "    else:\n",
    "        observation, reward, done, info = step_result\n",
    "    \n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Episodio terminado en el paso {step}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Guardar video\n",
    "if frames:\n",
    "    video_path = os.path.join(video_dir, 'spaceinvaders_game.mp4')\n",
    "    print(f\"Guardando {len(frames)} frames en video...\")\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    print(f\"✓ Video guardado: {video_path}\")\n",
    "    print(f\"✓ Tamaño del archivo: {os.path.getsize(video_path)} bytes\")\n",
    "else:\n",
    "    print(\"❌ No se capturaron frames\")\n",
    "\n",
    "print(f\"\\nRecompensa total: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcjoyH3idqh4",
    "outputId": "85af5e47-6b37-472b-ddbe-ce71bea7eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgw7iHVRduGa"
   },
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TshMrqTjdxja"
   },
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "# Hiperparámetros optimizados\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "CHANNELS = 1                 # Canal para grises\n",
    "USE_FRAMESTACK = True        # Cambiar a True si quieres detección de movimiento\n",
    "WINDOW_LENGTH = 4 if USE_FRAMESTACK else 1   # Número de fotogramas apilados          # La mayoría de implementaciones usan 4 frames\n",
    "batch_size = 32              # Tamaño de batch óptimo\n",
    "gamma = 0.99                 # Factor de descuento (mejor que 0.95 para recompensas a largo plazo)\n",
    "learning_rate = 0.00025      # Tasa de aprendizaje estándar para DQN\n",
    "memory_size = 1000000        # Buffer de memoria grande para mejor estabilidad\n",
    "TARGET_UPDATE_INTERVAL = 10000  # Actualización de red objetivo cada 10,000 pasos\n",
    "WARMUP_STEPS = 50000         # Pasos iniciales para llenar la memoria (experiencia aleatoria)\n",
    "NUM_TRAINING_STEPS = 5000000 # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
    "EPSILON_STEPS = 500000       # Total de pasos de evaluación del modelo\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH)                 # Dimensiones de cada frame\n",
    "\n",
    "# Single frame shape (height, width, channels)\n",
    "FRAME_SHAPE = (HEIGHT, WIDTH, CHANNELS)  # (84, 84, 1)\n",
    "MODEL_INPUT_SHAPE = (HEIGHT, WIDTH, WINDOW_LENGTH)  # Forma para el modelo (channels_last)\n",
    "SEQ_INPUT_SHAPE = (WINDOW_LENGTH,HEIGHT, WIDTH)  # Forma para el modelo (channels_last)\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "# Definir shape consistente\n",
    "if USE_FRAMESTACK:\n",
    "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
    "else:\n",
    "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
    "\n",
    "state_size = (*INPUT_SHAPE, WINDOW_LENGTH)   # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "input_shape = (*INPUT_SHAPE, WINDOW_LENGTH)  # Para la API de keras-rl\n",
    "action_size = env.action_space.n       # 6 acciones posibles\n",
    "learning_rate =  0.00025               # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST      # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST      # Máximo de pasos posibles por episodio\n",
    "total_episodios = 100                # Episodios totales para el entrenamiento\n",
    "max_steps       = 3000               # Máximo de pasos posibles por episodio\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.1             # Probabilidad mínima de exploración\n",
    "decay_rate = 0.00001           # Tasa de decaimiento exponencial para la probabilidad de exploración\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "TARGET_REWARD = 20.0\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "\n",
    "### CAMBIA ESTO A FALSE SI SOLO QUIERES VER AL AGENTE ENTRENADO\n",
    "training = False\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjFRyr4Ld1Hp"
   },
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "06wZVH5c43MP"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(INPUT_SHAPE)):\n",
    "        self.input_shape = input_shape\n",
    "        # Precargar una imagen negra para inicialización\n",
    "        self.black_frame = np.zeros(input_shape, dtype=np.uint8)\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        # Si la observación es None, devolver un marco negro\n",
    "        if observation is None:\n",
    "            return self.black_frame\n",
    "\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        # Crop the screen (remove the part below the player)\n",
    "        # [Up: Down, Left: right]\n",
    "        cropped_img = observation[18:-12, 4:-12]\n",
    "        # Optimización: usar cv2 para redimensionar y convertir a escala de grises (más rápido que PIL)\n",
    "        resized = cv2.resize(cropped_img, self.input_shape)\n",
    "        processed_observation = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY) if len(resized.shape) == 3 else resized\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype(np.uint8)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        \"\"\"\n",
    "        Procesa un paso completo del entorno.\n",
    "        \"\"\"\n",
    "        processed_observation = self.process_observation(observation)\n",
    "        processed_reward = self.process_reward(reward)\n",
    "        return processed_observation, processed_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptwluQRXedZP"
   },
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VwBXOBk43MP",
    "outputId": "88dbba58-92a4-4d75-cf4c-799a04c36cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de acciones disponibles: \" + str(nb_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT6osc3H43MP",
    "outputId": "cbe27690-c40d-49b1-d420-80b7d045ce26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de las observaciones:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Formato de las observaciones:\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "7-BoOu_eeiAE",
    "outputId": "5b52561e-70d0-47c1-f55e-c882dda20b66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for i in range(22):\n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "vHJaGcAVelRY",
    "outputId": "f2686c2b-a159-4c1f-e857-984352520683"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtnUlEQVR4nO3dfXRU1b3/8U+AZIhCJhDJhEgCKUWiIiogEPCuXiGClKUokVu78IqitUJAHu4FjRTUZSFcudcHLA9WLeBSRLEFfCosjRaXEp5CUVEbQLkSC0nUa2YQIdDk/P5oOz/OOQNhJjPZM/H9Wmuv1b1nz5nv7C/m2zPnKcmyLEsAALSwNqYDAAD8MFGAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARsSsAC1ZskQ9evRQ+/btNWjQIG3fvj1WHwUASEBJsbgX3IsvvqhbbrlFy5cv16BBg/TYY49p7dq1qqysVGZm5hnf29jYqEOHDqljx45KSkqKdmgAgBizLEtHjhxRdna22rQ5w36OFQMDBw60iouLg/2GhgYrOzvbKi0tbfK9VVVVliQajUajJXirqqo649/7doqyEydOqKKiQiUlJcGxNm3aqLCwUOXl5a759fX1qq+vD/atf+yQjRgxQsnJydEOr0mvv/56k3PGjBlj6ze1Vxct69evd4199dVXtn6XLl1cc66//voYRWRXW1vrGtuwYcMZ3zN69OhYhdOks8l1vHPme+zYsS3yuaFyvW7duibf9/DDD8cinCbNnj27yTlTpkxxjeXm5sYiHJfFixe7xr788ktb31SuJXe+zybXktSxY8czvh71AvT111+roaFBPp/PNu7z+fSXv/zFNb+0tFQPPvigazw5OdlIATobzrhSUlJa5HPPuCt7hjktFV8k+YrXHCcKZ77jOdeSlJqaGuVIosfj8bjGWireSP7bbqlcS5Hnu6nDKFE/BnTo0CGdf/752rJliwoKCoLjs2fP1ubNm7Vt2zbbfOceUCAQUE5OTjRDAgAY4Pf7lZaWdtrXo74HdN5556lt27aqqamxjdfU1CgrK8s13+PxhPx/HgCA1i3qp2GnpKSof//+KisrC441NjaqrKzMtkcEAPhhi/oekCTNnDlTEyZM0IABAzRw4EA99thjOnr0qG677bZYfBwAIAHFpAD97Gc/01dffaV58+apurpal112mTZu3Og6MQEA8MMVkwtRmyMQCMjr9ZoOAwDQTE2dhMC94AAARlCAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARlCAAABGUIAAAEZQgAAARsTkiag/dNddd51r7JVXXmnyfdOmTbP1H3/88ajFdKqrrrrK1n/nnXeafE+o73T06FFbv6ysrHmB/YPH47H1Qz3Q6quvvrL1u3TpYutPnTrV9Z558+ZFITo359rEc66lyPIdz7mW3PmOVa4RXewBAQCMoAABAIygAAEAjKAAAQCMSLIsyzIdxKkCgYC8Xq/pMMIya9YsW3/lypWuOc6DpN98841rTqwORDvdeOONtv7555/vmpORkWHrP/HEE645zoPD0VJZWWnrO9dXkgYPHmzrb9261dY/mxMBIhEqFme+Q50A4cy3qVxL7nw7cy258x3PuZZil280j9/vD3liyT+xBwQAMIICBAAwggIEADCCY0Ax8MYbb7jGli9fbutffvnlrjljx4619S+99NLoBvYPxcXFtn5hYaFrzmWXXWbrHz582DXnkUcesfVffvnl5gcn6dxzz7X1P/zwQ9ecTZs22foffPCBrR/qO40bNy4K0bk58+3MteTOt6lcS+61ceZacuc7nnMtub9TrHKN8HAMCAAQlyhAAAAjKEAAACMoQAAAIzgJIQZqa2sjep/zwO7kyZOjEY7L0qVLbf1QFyuejczMzGiE4+LM/759+8LeRq9evVxjfr8/4pjOJJJ8m8q1FFm+4znXkjvfsco1wsNJCACAuEQBAgAYEXYBevfdd3XttdcqOztbSUlJWr9+ve11y7I0b948de3aVampqSosLIx4txoA0HqF/UTUo0eP6tJLL9XEiRNdF9NJ0sMPP6zFixdr1apVysvL09y5czVy5Eh98sknat++fVSCjjehbqDo1Lt3b1t//vz5rjnO3+ZjdVzA+TmhLiqcM2eOre+8aaTk/t6LFi2KQnRndxzAeUzCeRwm1DaicRwjklxL7nybyrXkzrcz11LTNwmNp1yH2k6sjlkhusIuQKNGjdKoUaNCvmZZlh577DH96le/0pgxYyRJzz77rHw+n9avX6+bbrqpedECAFqNqB4DOnDggKqrq223xfB6vRo0aJDKy8tDvqe+vl6BQMDWAACtX1QLUHV1tSTJ5/PZxn0+X/A1p9LSUnm93mDLycmJZkgAgDjVrOuAkpKStG7dOl1//fWSpC1btmjo0KE6dOiQunbtGpz3b//2b0pKStKLL77o2kZ9fb3q6+uD/UAgQBECgFagRa8DysrKkiTV1NTYxmtqaoKvOXk8HqWlpdkaAKD1i2oBysvLU1ZWlsrKyoJjgUBA27ZtU0FBQTQ/CgCQ4MI+C+67777T/v37g/0DBw5o9+7d6ty5s3JzczV9+nT9+te/Vq9evYKnYWdnZwd/pgMAQJJkhemdd96xJLnahAkTLMuyrMbGRmvu3LmWz+ezPB6PNXz4cKuysvKst+/3+0Nun0aj0WiJ1fx+/xn/3nMzUgBATHAzUgBAXKIAAQCMoAABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMCPtWPGhat27dXGNn88TL++67LxbhNGnBggVNzlm6dKlr7Msvv4xFOC6TJk1yjTnvmF5VVWXrL1u2LKYxncqZ73jOtRRZvuM511LL5hvRwx4QAMAIChAAwAgKEADACI4BxcBLL73kGjv1CbGStHLlStcc5+/uZ3MsIRLOz3E+QFCSbr31Vlv/X//1X11zhgwZEs2wgpw3o21sbHTNueOOO2z9uXPnnnEb0t9vjBgLznw7cy25820q15I7385cS+58x3OuQ20nVrlGdLEHBAAwggIEADCCAgQAMIJjQFEwa9YsWz/UdRbO6yj+4z/+wzVny5Yt0Q3sND766CNbP9Tv+zfccIOtH+raJuf3XrRoURSik3bt2mXrf/HFF645AwcOtPV/97vf2fqzZ892vadnz57Njs35nSV3vkNdM+PMt6lcS+58O3MtufMdz7mW3PmORq4Re+wBAQCMoAABAIygAAEAjKAAAQCM4CSEGHjttddcY5WVlbb+nDlzWiocl6+++srW37Bhg2vOiy++aOv37t3bNefCCy+MbmD/8Le//c3WHzZsmGtObW2trZ+ZmWnrO9c7lpz5DvXZpvLtzLXkzrcz15I73/Gca6ll843oYQ8IAGAEBQgAYAQFCABgBMeAYqBfv36uMefFdKWlpa45b7/9dsxiOpNQv7s74w31nVrKxo0bXWPO4wCh5rQU59o4105y59tUriV3vkPFayrf8Z5rRBd7QAAAIyhAAAAjKEAAACMoQAAAI5Isy7JMB3GqQCAQ8mmWAIDE4vf7lZaWdtrX2QMCABhBAQIAGBFWASotLdUVV1yhjh07KjMzU9dff73rHkzHjx9XcXGxMjIy1KFDBxUVFammpiaqQQMAEl9YBWjz5s0qLi7W1q1b9eabb+rkyZMaMWKEjh49GpwzY8YMvfrqq1q7dq02b96sQ4cOaezYsVEPHACQ4KxmqK2ttSRZmzdvtizLsurq6qzk5GRr7dq1wTmffvqpJckqLy8/q236/X5LEo1Go9ESvPn9/jP+vW/WMSC/3y9J6ty5sySpoqJCJ0+eVGFhYXBOfn6+cnNzVV5eHnIb9fX1CgQCtgYAaP0iLkCNjY2aPn26hg4dqj59+kiSqqurlZKSovT0dNtcn8+n6urqkNspLS2V1+sNtpycnEhDAgAkkIgLUHFxsfbs2aM1a9Y0K4CSkhL5/f5gq6qqatb2AACJIaK7YU+ZMkWvvfaa3n33XXXr1i04npWVpRMnTqiurs62F1RTU6OsrKyQ2/J4PPJ4PJGEAQBIYGHtAVmWpSlTpmjdunV6++23lZeXZ3u9f//+Sk5OVllZWXCssrJSBw8eVEFBQXQiBgC0CmHtARUXF2v16tXasGGDOnbsGDyu4/V6lZqaKq/Xq9tvv10zZ85U586dlZaWpqlTp6qgoECDBw+OyRcAACSocE671mlOtVuxYkVwzrFjx6zJkydbnTp1ss455xzrhhtusA4fPnzWn8Fp2DQajdY6WlOnYXMzUgBATHAzUgBAXKIAAQCMoAABAIyI6Dog2M2aNcvWX7RoUUTbufHGG239l19+OeKYWuJzovW9nZx3WO/du3eztxHpdpyc31mK7HubynWknxXPuY7mdtCy2AMCABhBAQIAGEEBAgAYQQECABjBSQgx0K9fP9fYxo0bm3zf5MmTYxFOk2pra5ucc80117RAJKGFWjvnGu/ataulwnFxxhLPuZbiO9/xnmtEF3tAAAAjKEAAACMoQAAAI7gZaQtxXig3Z84c15xYXYzYlFAXK86fP9/Wj7cL+5zHMTIzMw1F4hbqIlhnvk3lWnLn25lrKb7yHc+5xplxM1IAQFyiAAEAjKAAAQCMoAABAIzgQtQY6Nixo2tsy5Yttn5GRkZLhdOkULE44w31nY4cORKzmE7Vv39/11hZWdkZ51RUVMQ0plM518a5dlJ85ztUvM7vFM+5llo234ge9oAAAEZQgAAARlCAAABGcCFqFAwfPtzWv+CCC1xzli1b1uR2FixYYOvfd999zQssip8zadIk19jevXttfedv9ZFyxrd06VLXnC+//NLW79atm60f6maf0VhPZ64ld77jOddn+1nOfMdzriV3vmO1nggPF6ICAOISBQgAYAQFCABgBMeAAAAxwTEgAEBcogABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMCKsALVu2TH379lVaWprS0tJUUFCgP/7xj8HXjx8/ruLiYmVkZKhDhw4qKipSTU1N1IMGACS+sApQt27dtHDhQlVUVGjnzp0aNmyYxowZo48//liSNGPGDL366qtau3atNm/erEOHDmns2LExCRwAkOCsZurUqZP19NNPW3V1dVZycrK1du3a4GuffvqpJckqLy8/6+35/X5LEo1Go9ESvPn9/jP+vY/4GFBDQ4PWrFmjo0ePqqCgQBUVFTp58qQKCwuDc/Lz85Wbm6vy8vLTbqe+vl6BQMDWAACtX9gF6KOPPlKHDh3k8Xh01113ad26dbroootUXV2tlJQUpaen2+b7fD5VV1efdnulpaXyer3BlpOTE/aXAAAknrALUO/evbV7925t27ZNkyZN0oQJE/TJJ59EHEBJSYn8fn+wVVVVRbwtAEDiaBfuG1JSUvTjH/9YktS/f3/t2LFDjz/+uH72s5/pxIkTqqurs+0F1dTUKCsr67Tb83g88ng84Ucex/r16+ca27hxY5Pvy8zMjEU4TaqtrW1yzjXXXOMa27VrVyzCcQm1ds41dsYSKt5YccYSz7mWIst3POdaatl8I3qafR1QY2Oj6uvr1b9/fyUnJ9se1VtZWamDBw+qoKCguR8DAGhlwtoDKikp0ahRo5Sbm6sjR45o9erV+tOf/qRNmzbJ6/Xq9ttv18yZM9W5c2elpaVp6tSpKigo0ODBg2MVPwAgQYVVgGpra3XLLbfo8OHD8nq96tu3rzZt2qSrr75akvToo4+qTZs2KioqUn19vUaOHKmlS5fGJHAAQGLjiahRcMcdd9j6CxYscM0ZMWKErR/qZ8nZs2fb+nl5eVGIzu3AgQO2/nvvveeaM2TIEFv/iy++cM1ZvXq1rf/0009HITpp586dtn5ubq5rzpNPPmnrb9u2zdZ/4IEHXO8ZMGBAs2Nz5lpy59uZa8mdb1O5ltz5duZacuc7nnMtufMdjVyj+XgiKgAgLlGAAABGUIAAAEZwDCgGQl0HVFJSYutfcMEFrjmXXnppzGI6kw8++MA1tnfvXlu/tLTUNcfktSHffvutrd+pUydb3+R1QM5cS+58m8q15M63M9eSO9/xnGuJ64DiFceAAABxiQIEADCCAgQAMIICBAAwIuybkaJpoS4yveWWW2z9a6+91jXnqquusvXfeeed6AZ2ms+ZP3++a86rr75q60+cONE1J1YHpp03pw31PKkHH3zQ1r///vvPuA3p78+eigVnvp25ltz5NpVryZ1vZ64ld77jOdehthOrXCO62AMCABhBAQIAGEEBAgAYwTGgKBg+fLitH+qprseOHbP1X3rpJdecadOm2fqxOi7Qt29fW//xxx9v8j2hvpPze5/6LKjmmDNnjq3/xBNPNPke513XnduQpHnz5jUvMLm/s+ReG2euJXe+TeVaiizf8ZzrUNuJRq4Re+wBAQCMoAABAIygAAEAjKAAAQCM4G7YAICY4G7YAIC4RAECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGMETUWPA4/G4xrp27drk+/73f/83BtE0rUePHk3OOXz4sGusvr4+BtG4ZWdnu8ZSUlJs/RMnTtj6hw4dimlMp3LmO55zLUWW73jOtdSy+Ub0sAcEADCCAgQAMKJZBWjhwoVKSkrS9OnTg2PHjx9XcXGxMjIy1KFDBxUVFammpqa5cQIAWpmIjwHt2LFDTz75pPr27WsbnzFjhl5//XWtXbtWXq9XU6ZM0dixY/X+++83O9h45fxNffv27U2+p6yszDXWvn17W/+GG25oVlyns27dOlt/6NChrjnO+AYOHOiaM3z4cFs/Wsc1rrvuOlv/6aefds1xftbXX39t6y9fvtz1nldeeaXZsYU6fhJJvk3lWnLnO9S/RWe+4znXkjvf0cg1Yi+iPaDvvvtO48eP11NPPaVOnToFx/1+v5555hk98sgjGjZsmPr3768VK1Zoy5Yt2rp1a9SCBgAkvogKUHFxsUaPHq3CwkLbeEVFhU6ePGkbz8/PV25ursrLy0Nuq76+XoFAwNYAAK1f2D/BrVmzRrt27dKOHTtcr1VXVyslJUXp6em2cZ/Pp+rq6pDbKy0t1YMPPhhuGACABBfWHlBVVZWmTZum559/3vUbdqRKSkrk9/uDraqqKirbBQDEt7D2gCoqKlRbW6t+/foFxxoaGvTuu+/qN7/5jTZt2qQTJ06orq7OthdUU1OjrKyskNv0eDwhL9xMJOPGjQv7PaEO/C5YsCAa4TQp1EkHTs74LrjgAtcc5/detGhR8wL7h1AHop0+++wzW7+ysrLJbWRmZjYvMEWWa8m9nvGca8md73jOdajtRCPXiL2wCtDw4cP10Ucf2cZuu+025efn65577lFOTo6Sk5NVVlamoqIiSX//x3Lw4EEVFBREL2oAQMILqwB17NhRffr0sY2de+65ysjICI7ffvvtmjlzpjp37qy0tDRNnTpVBQUFGjx4cPSiBgAkvKjfC+7RRx9VmzZtVFRUpPr6eo0cOVJLly6N9scAABJckmVZlukgThUIBOT1ek2H0SwbN250jT333HO2/i233OKaM2LEiJjFdCrn/yF45JFHXHNGjRpl64e6kHjXrl3RDewfnPnv2bOna86pxyEl6cYbb7T1r7nmmugHdhrOfDtzLbnzbSrXkjvfzlxL7nzHc66lls03zp7f71daWtppX+decAAAIyhAAAAjKEAAACN4IF0LcR4XGDJkiKFI3Pbv3+8ae/bZZ239UL/Nm+S87iPUcQFTQh0Diud8O3MtxVe+4znXaB72gAAARlCAAABGUIAAAEZQgAAARnAhagyEemqm09GjR11j5557rq0fradOOnXp0uWMnytJX3zxha3fvXt31xy/32/rf/vtt1GITkpKSgr7s8/m30ys1jOSfJvKdajPduZacq85uUYkuBAVABCXKEAAACMoQAAAIzgGBACICY4BAQDiEgUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYEQ70wG0Rtddd51rbNGiRbb+nDlzXHO++eYbW/+dd96JbmD/cNVVV9n6GRkZrjnz58+39WfNmuWa88orr0Q3sH/weDy2fqibGX788ce2/sUXX2zrBwIB13vq6+ujEJ2bM9/OXEvufJvKteTOtzPXkjvf8ZxryZ3vWOUa0cUeEADACAoQAMAIChAAwAgKEADACJ6IGgXOA7a33nqra47z4OqWLVtcc5wHjDMzM5sfXAi1tbW2fqgD4EOGDLH1Qx3UX7lypa0f6uB7JJzxbdiwwTVnzJgxZ5zjfF2KznqGOhnDme9QB9Kd+TaVa8mdb2euJXe+4znXoebEaj0RHp6ICgCISxQgAIARYRWgBx54QElJSbaWn58ffP348eMqLi5WRkaGOnTooKKiItXU1EQ9aABA4gvrGNADDzygl19+WW+99VZwrF27djrvvPMkSZMmTdLrr7+ulStXyuv1asqUKWrTpo3ef//9sw4oEY8BOW3cuNE1ds011zT5vqVLl9r6kydPjlpMzf2cSL9TJJz579mzp2vOrl27bP1+/frZ+p999pnrPX6/PwrRuTnXJp5zfbafFcl3ikQ0ci258x2rXCM8TR0DCvtOCO3atVNWVlbID3rmmWe0evVqDRs2TJK0YsUKXXjhhdq6dasGDx4c7kcBAFqxsI8B7du3T9nZ2frRj36k8ePH6+DBg5KkiooKnTx5UoWFhcG5+fn5ys3NVXl5+Wm3V19fr0AgYGsAgNYvrAI0aNAgrVy5Uhs3btSyZct04MAB/cu//IuOHDmi6upqpaSkKD093fYen8+n6urq026ztLRUXq832HJyciL6IgCAxBLWT3CjRo0K/u++fftq0KBB6t69u1566SWlpqZGFEBJSYlmzpwZ7AcCAYoQAPwANOtu2Onp6brgggu0f/9+XX311Tpx4oTq6upse0E1NTUhjxn9k8fjcd0RN9Hccccdtv7EiRObfM/NN9/sGovVgWinUBfBOp16ookkjR49OlbhuPz+979v8rOda/7GG2/Y+rE6CO38XCmyfMdzrqWWy3c0ci1x0kGiatZ1QN99950+++wzde3aVf3791dycrLKysqCr1dWVurgwYMqKChodqAAgNYlrD2g//zP/9S1116r7t2769ChQ7r//vvVtm1b/fznP5fX69Xtt9+umTNnqnPnzkpLS9PUqVNVUFDAGXAAAJewCtCXX36pn//85/rmm2/UpUsXXXnlldq6dau6dOkiSXr00UfVpk0bFRUVqb6+XiNHjgx5HQIAAGEVoDVr1pzx9fbt22vJkiVasmRJs4JKNJ06dbL1Dx065JpTWVlp6/fu3TumMZ3J8ePHbf0bb7zRNefU0+lbmvMklFBPtzybNY8F5+eG+mxnriVz+XbmWnLnm1zDFO4FBwAwggIEADCCAgQAMIIH0gEAYoIH0gEA4hIFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgRNgF6K9//atuvvlmZWRkKDU1VZdccol27twZfN2yLM2bN09du3ZVamqqCgsLtW/fvqgGDQBIfGEVoG+//VZDhw5VcnKy/vjHP+qTTz7R//zP/6hTp07BOQ8//LAWL16s5cuXa9u2bTr33HM1cuRIHT9+POrBAwASmBWGe+65x7ryyitP+3pjY6OVlZVlLVq0KDhWV1dneTwe64UXXjirz/D7/ZYkGo1GoyV48/v9Z/x7H9Ye0CuvvKIBAwZo3LhxyszM1OWXX66nnnoq+PqBAwdUXV2twsLC4JjX69WgQYNUXl4ecpv19fUKBAK2BgBo/cIqQJ9//rmWLVumXr16adOmTZo0aZLuvvturVq1SpJUXV0tSfL5fLb3+Xy+4GtOpaWl8nq9wZaTkxPJ9wAAJJiwClBjY6P69eunBQsW6PLLL9edd96pX/ziF1q+fHnEAZSUlMjv9wdbVVVVxNsCACSOsApQ165dddFFF9nGLrzwQh08eFCSlJWVJUmqqamxzampqQm+5uTxeJSWlmZrAIDWL6wCNHToUFVWVtrG9u7dq+7du0uS8vLylJWVpbKysuDrgUBA27ZtU0FBQRTCBQC0Gmd3/tvfbd++3WrXrp01f/58a9++fdbzzz9vnXPOOdZzzz0XnLNw4UIrPT3d2rBhg/Xhhx9aY8aMsfLy8qxjx45xFhyNRqP9gFpTZ8GFVYAsy7JeffVVq0+fPpbH47Hy8/Ot3/72t7bXGxsbrblz51o+n8/yeDzW8OHDrcrKyrPePgWIRqPRWkdrqgAlWZZlKY4EAgF5vV7TYQAAmsnv95/xuD73ggMAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABjRznQASAz/fOz6qfr06WPr79mzxzVnwoQJMYspkVVUVDQ5x7l2odb3h8j5704K/e/zVP37949VOGgG9oAAAEZQgAAARlCAAABGUIAAAEbwRFSE5MzB22+/HdF2hg0bZuv7/f6IY0pkDz30kK3/05/+NOxtcCD9787mBA6nN954wzU2d+7caISDM+CJqACAuEQBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABjBE1ER0g033BCT7axcuTIq2000PXr0aPY2Qj0J9IfwlNRQ3ztc0Vh/RB97QAAAIyhAAAAjwipAPXr0UFJSkqsVFxdLko4fP67i4mJlZGSoQ4cOKioqUk1NTUwCBwAktrCOAe3YsUMNDQ3B/p49e3T11Vdr3LhxkqQZM2bo9ddf19q1a+X1ejVlyhSNHTtW77//fnSjRsxNnTo1Jtv5oR4Duuiii5q9jSFDhrjGfgjHgEJ973BFY/0RfWEVoC5dutj6CxcuVM+ePfWTn/xEfr9fzzzzjFavXh18CuaKFSt04YUXauvWrRo8eHD0ogYAJLyIjwGdOHFCzz33nCZOnKikpCRVVFTo5MmTKiwsDM7Jz89Xbm6uysvLT7ud+vp6BQIBWwMAtH4RF6D169errq5Ot956qySpurpaKSkpSk9Pt83z+Xyqrq4+7XZKS0vl9XqDLScnJ9KQAAAJJOIC9Mwzz2jUqFHKzs5uVgAlJSXy+/3BVlVV1aztAQASQ0QXon7xxRd666239Ic//CE4lpWVpRMnTqiurs62F1RTU6OsrKzTbsvj8cjj8UQSBgAggUW0B7RixQplZmZq9OjRwbH+/fsrOTlZZWVlwbHKykodPHhQBQUFzY8UANCqhL0H1NjYqBUrVmjChAlq1+7/v93r9er222/XzJkz1blzZ6WlpWnq1KkqKCjgDDgAgEvYBeitt97SwYMHNXHiRNdrjz76qNq0aaOioiLV19dr5MiRWrp0aVQCBQC0LmEXoBEjRsiyrJCvtW/fXkuWLNGSJUuaHRjiy+zZs11j+/bts/V79erlmvPwww/HLKZEdjY3e120aFELRJJ49u/f7xqbNWvWGd+zbt26WIWDZuBecAAAIyhAAAAjKEAAACMoQAAAI5Ks051RYEggEJDX6zUdBgCgmfx+v9LS0k77OntAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAIyhAAAAjKEAAACMoQAAAI9qZDgDR1a1bN9fY008/bevfe++9tv7u3btjGRJiaNCgQbb+1q1bW+Rzd+zY4RobOHBgi3w2Wg/2gAAARlCAAABGUIAAAEYkWZZlmQ7iVIFAQF6v13QYCeuFF15wjXXp0sXW//bbb239cePGxTQmJJ7t27fb+ldccYWtzzEgnA2/36+0tLTTvs4eEADACAoQAMCIsApQQ0OD5s6dq7y8PKWmpqpnz5566KGHdOqveJZlad68eeratatSU1NVWFioffv2RT1wAEBiC+s6oP/6r//SsmXLtGrVKl188cXauXOnbrvtNnm9Xt19992SpIcffliLFy/WqlWrlJeXp7lz52rkyJH65JNP1L59+5h8Cfx/Dz74oGvsN7/5ja2/fPnylgoHMWbqOiAgGsIqQFu2bNGYMWM0evRoSVKPHj30wgsvBA9YWpalxx57TL/61a80ZswYSdKzzz4rn8+n9evX66abbopy+ACARBXWT3BDhgxRWVmZ9u7dK0n64IMP9N5772nUqFGSpAMHDqi6ulqFhYXB93i9Xg0aNEjl5eUht1lfX69AIGBrAIDWL6w9oHvvvVeBQED5+flq27atGhoaNH/+fI0fP16SVF1dLUny+Xy29/l8vuBrTqWlpSF/NgIAtG5h7QG99NJLev7557V69Wrt2rVLq1at0n//939r1apVEQdQUlIiv98fbFVVVRFvCwCQOMLaA5o1a5buvffe4LGcSy65RF988YVKS0s1YcIEZWVlSZJqamrUtWvX4Ptqamp02WWXhdymx+ORx+OJMHx07NjR1neecBBKSUmJrb9r1y7XHOfFqohPzhvJ9urVKyrb/f3vf2/r9+3bNyrbBU4V1h7Q999/rzZt7G9p27atGhsbJUl5eXnKyspSWVlZ8PVAIKBt27apoKAgCuECAFqLsPaArr32Ws2fP1+5ubm6+OKL9ec//1mPPPKIJk6cKElKSkrS9OnT9etf/1q9evUKnoadnZ2t66+/PhbxAwASVFgF6IknntDcuXM1efJk1dbWKjs7W7/85S81b9684JzZs2fr6NGjuvPOO1VXV6crr7xSGzdu5BogAIBNq7kZqfOGm6cba22GDx9u648YMSLsbfz5z392ja1ZsybimNByevfubesvXry4RT73008/dY1Nnz69RT4b8a+hoUGVlZXcjBQAEJ8oQAAAIyhAAAAjKEAAACPi9iSEoUOHql2705+kd+r95qQfxgkHofz4xz+29ffv328oEgCS9Mtf/tLWf/LJJw1FYs6xY8c0Y8YMTkIAAMQnChAAwIiwLkRtCf/8RfBvf/vbGecdP37c1j927FjMYopnR48etfV/qOsAxAvnI2V+iP9N/vPvc1NHeOLuGNCXX36pnJwc02EAAJqpqqpK3bp1O+3rcVeAGhsbdejQIXXs2FFHjhxRTk6OqqqqznggC5EJBAKsbwyxvrHF+sZWc9bXsiwdOXJE2dnZrhtYnyrufoJr06ZNsGImJSVJktLS0vgHFkOsb2yxvrHF+sZWpOt7NrdU4yQEAIARFCAAgBFxXYA8Ho/uv/9+npgaI6xvbLG+scX6xlZLrG/cnYQAAPhhiOs9IABA60UBAgAYQQECABhBAQIAGEEBAgAYEbcFaMmSJerRo4fat2+vQYMGafv27aZDSkilpaW64oor1LFjR2VmZur6669XZWWlbc7x48dVXFysjIwMdejQQUVFRaqpqTEUceJauHChkpKSNH369OAYa9t8f/3rX3XzzTcrIyNDqampuuSSS7Rz587g65Zlad68eeratatSU1NVWFioffv2GYw4cTQ0NGju3LnKy8tTamqqevbsqYceesh2E9GYrq8Vh9asWWOlpKRYv/vd76yPP/7Y+sUvfmGlp6dbNTU1pkNLOCNHjrRWrFhh7dmzx9q9e7f105/+1MrNzbW+++674Jy77rrLysnJscrKyqydO3dagwcPtoYMGWIw6sSzfft2q0ePHlbfvn2tadOmBcdZ2+b5v//7P6t79+7Wrbfeam3bts36/PPPrU2bNln79+8Pzlm4cKHl9Xqt9evXWx988IF13XXXWXl5edaxY8cMRp4Y5s+fb2VkZFivvfaadeDAAWvt2rVWhw4drMcffzw4J5brG5cFaODAgVZxcXGw39DQYGVnZ1ulpaUGo2odamtrLUnW5s2bLcuyrLq6Ois5Odlau3ZtcM6nn35qSbLKy8tNhZlQjhw5YvXq1ct68803rZ/85CfBAsTaNt8999xjXXnllad9vbGx0crKyrIWLVoUHKurq7M8Ho/1wgsvtESICW306NHWxIkTbWNjx461xo8fb1lW7Nc37n6CO3HihCoqKmyP3G7Tpo0KCwtVXl5uMLLWwe/3S5I6d+4sSaqoqNDJkydt652fn6/c3FzW+ywVFxdr9OjRrsfEs7bN98orr2jAgAEaN26cMjMzdfnll+upp54Kvn7gwAFVV1fb1tjr9WrQoEGs8VkYMmSIysrKtHfvXknSBx98oPfee0+jRo2SFPv1jbu7YX/99ddqaGiQz+ezjft8Pv3lL38xFFXr0NjYqOnTp2vo0KHq06ePJKm6ulopKSlKT0+3zfX5fKqurjYQZWJZs2aNdu3apR07drheY22b7/PPP9eyZcs0c+ZM3XfffdqxY4fuvvtupaSkaMKECcF1DPX3gjVu2r333qtAIKD8/Hy1bdtWDQ0Nmj9/vsaPHy9JMV/fuCtAiJ3i4mLt2bNH7733nulQWoWqqipNmzZNb775ptq3b286nFapsbFRAwYM0IIFCyRJl19+ufbs2aPly5drwoQJhqNLfC+99JKef/55rV69WhdffLF2796t6dOnKzs7u0XWN+5+gjvvvPPUtm1b15lCNTU1ysrKMhRV4psyZYpee+01vfPOO7YnFGZlZenEiROqq6uzzWe9m1ZRUaHa2lr169dP7dq1U7t27bR582YtXrxY7dq1k8/nY22bqWvXrrroootsYxdeeKEOHjwoScF15O9FZGbNmqV7771XN910ky655BL9+7//u2bMmKHS0lJJsV/fuCtAKSkp6t+/v8rKyoJjjY2NKisrU0FBgcHIEpNlWZoyZYrWrVunt99+W3l5ebbX+/fvr+TkZNt6V1ZW6uDBg6x3E4YPH66PPvpIu3fvDrYBAwZo/Pjxwf/N2jbP0KFDXZcN7N27V927d5ck5eXlKSsry7bGgUBA27ZtY43Pwvfff+96Ymnbtm3V2NgoqQXWt9mnMcTAmjVrLI/HY61cudL65JNPrDvvvNNKT0+3qqurTYeWcCZNmmR5vV7rT3/6k3X48OFg+/7774Nz7rrrLis3N9d6++23rZ07d1oFBQVWQUGBwagT16lnwVkWa9tc27dvt9q1a2fNnz/f2rdvn/X8889b55xzjvXcc88F5yxcuNBKT0+3NmzYYH344YfWmDFjOA37LE2YMME6//zzg6dh/+EPf7DOO+88a/bs2cE5sVzfuCxAlmVZTzzxhJWbm2ulpKRYAwcOtLZu3Wo6pIQkKWRbsWJFcM6xY8esyZMnW506dbLOOecc64YbbrAOHz5sLugE5ixArG3zvfrqq1afPn0sj8dj5efnW7/97W9trzc2Nlpz5861fD6f5fF4rOHDh1uVlZWGok0sgUDAmjZtmpWbm2u1b9/e+tGPfmTNmTPHqq+vD86J5fryPCAAgBFxdwwIAPDDQAECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABjx/wBxuidLAu/E7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVe0dhbsfvc6"
   },
   "source": [
    "#### Clase FrameStack para apilar frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gSJsJa41fzo7"
   },
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Clase para gestionar una pila de fotogramas consecutivos del entorno, utilizada para capturar\n",
    "    el contexto temporal en juegos de Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Mantiene una cola (deque) de fotogramas preprocesados con un tamaño máximo definido por\n",
    "    max_length, apilándolos para formar un estado con información de movimiento.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "        frames (deque): Cola de fotogramas preprocesados con longitud máxima max_length.\n",
    "        max_length (int): Número máximo de fotogramas a apilar (e.g., WINDOW_LENGTH).\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        append(frame): Añade un nuevo fotograma a la pila, eliminando el más antiguo si es necesario.\n",
    "        get_stacked_state(): Devuelve el estado apilado como un array NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frames=4):\n",
    "        \"\"\"\n",
    "        Inicializa la pila de fotogramas.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            max_length (int): Número máximo de fotogramas a mantener en la pila.\n",
    "        \"\"\"\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque([np.zeros((INPUT_SHAPE), dtype=np.int) for i in range(self.num_frames)], maxlen=self.num_frames)\n",
    "\n",
    "    def reset(self):\n",
    "        self.frames.clear()\n",
    "\n",
    "    def add_frame(self, frame, is_new_episode):\n",
    "        \"\"\"\n",
    "        Añade un fotograma preprocesado a la pila.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            frame (np.ndarray): Fotograma preprocesado (e.g., imagen en escala de grises de 84x84).\n",
    "        \"\"\"\n",
    "        # Si es el primer frame, llenamos el deque\n",
    "        if is_new_episode:\n",
    "            self.frames = deque([np.zeros((INPUT_SHAPE), dtype=np.int) for i in range(self.num_frames)], maxlen=self.num_frames)\n",
    "            for _ in range(self.num_frames):\n",
    "                self.frames.append(frame)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Devuelve el estado apilado como un array NumPy con los fotogramas actuales.\n",
    "\n",
    "        Si la pila no está llena, repite el último fotograma hasta completar max_length.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Array de forma (84, 84, max_length) con los fotogramas apilados.\n",
    "        \"\"\"\n",
    "        # Convertir a array con shape (84, 84, 4)\n",
    "        return np.stack(self.frames, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-yCJoGjf2Fg"
   },
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ewKKozUaf-mG"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-asignar arrays con el tamaño exacto\n",
    "        # Usar uint8 para estados (más eficiente que float32)\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        print(f\"ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
    "        memory_size = (\n",
    "            self.states.nbytes + self.next_states.nbytes +\n",
    "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "        ) / (1024 * 1024)\n",
    "        print(f\"Memoria asignada: {memory_size:.2f} MB\")\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Añade una experiencia al buffer de forma eficiente\"\"\"\n",
    "        # Convertir a uint8 para ahorrar memoria (estados son imágenes 0-255)\n",
    "        if state.dtype != np.uint8:\n",
    "            state = (state * 255).astype(np.uint8)\n",
    "        if next_state.dtype != np.uint8:\n",
    "            next_state = (next_state * 255).astype(np.uint8)\n",
    "\n",
    "        # Almacenar directamente en el array pre-asignado\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        # Actualizar posición circular\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
    "\n",
    "        # Generar índices aleatorios\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "\n",
    "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
    "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
    "        batch_actions = self.actions[indices]\n",
    "        batch_rewards = self.rewards[indices]\n",
    "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
    "        batch_dones = self.dones[indices]\n",
    "\n",
    "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        # No es necesario limpiar los arrays, se sobrescriben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-SaTPRkQoK"
   },
   "source": [
    "#### Clase PerformanceMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Wf2A5kDokNdS"
   },
   "outputs": [],
   "source": [
    "# Clase para monitoreo de memoria y rendimiento\n",
    "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path='diagnosticos'):\n",
    "        self.save_path = save_path\n",
    "        self.episode_times = []\n",
    "        self.memory_usage = []\n",
    "        self.current_episode = 0\n",
    "        self.episode_start_time = None\n",
    "        self.episode_start_memory = None\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        self.episode_start_time = time.time()\n",
    "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        self.current_episode = episode\n",
    "        print(f\"Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        end_time = time.time()\n",
    "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        episode_duration = end_time - self.episode_start_time\n",
    "\n",
    "        self.episode_times.append(episode_duration)\n",
    "        self.memory_usage.append(final_memory)\n",
    "\n",
    "        print(f\"Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
    "        print(f\"Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
    "\n",
    "        # Guardar diagnóstico cada 5 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            self.save_diagnostics(episode)\n",
    "\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()\n",
    "\n",
    "    def save_diagnostics(self, episode):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.episode_times)\n",
    "        plt.title('Tiempo por episodio')\n",
    "        plt.ylabel('Segundos')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.memory_usage)\n",
    "        plt.title('Uso de memoria')\n",
    "        plt.ylabel('MB')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTgDOJoCgISN"
   },
   "source": [
    "### 1. Implementación de la red neuronal\n",
    "\n",
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFAzP0UigPVg"
   },
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ0dGSAUgP0a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kP2vNpKnzkTl"
   },
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo DQN usando SOLO Keras estándar. Base común para redes DQN y DDQN\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "    Construye un modelo que acepta channels_first y convierte internamente\n",
    "\n",
    "    Esta función implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación RELU.\n",
    "    Esto evita completamente los problemas de grafos múltiples\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DQN estándar: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # CRÍTICO: Limpiar completamente antes de crear\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "       \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DQN_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_models(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea modelos para Double DQN (principal y objetivo)\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelos DDQN: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Modelo principal\n",
    "    main_model, memory = create_dqn_model(input_shape, nb_actions, memory_size)\n",
    "    main_model._name = 'DDQN_Main_Model'    \n",
    "    \n",
    "    # Modelo objetivo (copia exacta)\n",
    "    target_model = tf.keras.models.clone_model(main_model)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    target_model._name = 'DDQN_Target_Model'\n",
    "    \n",
    "    print(\"✅ Modelos DDQN creados exitosamente\")    \n",
    "    return main_model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_replay_model(input_shape, action_space, memory_size):\n",
    "    print(f\"🏗️ Creando modelos DDQN_replay: input_shape={input_shape}, actions={nb_actions}\")\n",
    "\n",
    "    # CRÍTICO: Limpiar completamente antes de crear\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DDQN_replay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "    target_model = clone_model(model)  # Create target model for DDQN\n",
    "    target_model.set_weights(model.get_weights())  # Initialize with same weights\n",
    "    target_model._name = 'DDQN_replay_Target_Model'    \n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dueling_dqn_replay_model(state_size, action_size, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo Dueling DQN con replay.\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape={state_size}, actions={action_size}\")\n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)    \n",
    "    x = Permute((2, 3, 1))(inputs)\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    # Value stream\n",
    "    value = Dense(512, activation='relu')(x)\n",
    "    value = Dense(1, activation='linear')(value)\n",
    "    # Advantage stream\n",
    "    advantage = Dense(512, activation='relu')(x)\n",
    "    advantage = Dense(action_size, activation='linear')(advantage)\n",
    "    # Combine streams\n",
    "    outputs = Add()([value, Lambda(lambda a: a - K.mean(a, axis=1, keepdims=True))(advantage)])\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='DuelingDQNReplay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)   \n",
    "    target_model = clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())    \n",
    "    target_model._name = 'DuelingDQNReplay_Target_Model'     \n",
    "   \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0DHvKNshvQo"
   },
   "source": [
    "### 2. Implementación de la solución DQN\n",
    "\n",
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para monitorear el progreso del entrenamiento de un agente DQN.\n",
    "\n",
    "    Registra el avance en términos de pasos completados, porcentaje, velocidad de entrenamiento\n",
    "    (pasos por segundo) y tiempo estimado de finalización (ETA).\n",
    "\n",
    "    Atributos:\n",
    "        total_steps (int): Número total de pasos de entrenamiento.\n",
    "        print_interval (int): Intervalo de pasos para imprimir el progreso (por defecto: 10,000).\n",
    "        start_time (float): Tiempo de inicio del entrenamiento (en segundos).\n",
    "        last_step (int): Último paso registrado (inicializado en 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, total_steps, print_interval=100):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "\n",
    "        Args:\n",
    "            total_steps (int): Número total de pasos de entrenamiento.\n",
    "            print_interval (int): Intervalo de pasos para imprimir el progreso.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.print_interval = print_interval\n",
    "        self.step_counter = 0\n",
    "        self.start_time = time.time()\n",
    "        self.episode_rewards = []  # Store clipped episode rewards\n",
    "        self.episode_steps = []\n",
    "        self.current_episode_reward = 0.0  # Track clipped reward for current episode\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al inicio del entrenamiento.\n",
    "\n",
    "        Inicializa el tiempo de inicio y muestra un mensaje de comienzo.\n",
    "\n",
    "        Args:\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"🚀 Entrenamiento iniciado: {self.total_steps:,} pasos\")\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al final de cada paso de entrenamiento.\n",
    "\n",
    "        Calcula y muestra el progreso, incluyendo porcentaje completado, velocidad\n",
    "        (pasos por segundo) y tiempo estimado de finalización (ETA) en horas.\n",
    "\n",
    "        Args:\n",
    "            step (int): Número del paso actual.\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.step_counter += 1        \n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        clipped_reward = np.clip(raw_reward, -1.0, 1.0)  # Match AtariProcessor clipping\n",
    "        self.current_episode_reward += clipped_reward\n",
    "        if self.step_counter % self.print_interval == 0:\n",
    "            progress = (self.step_counter / self.total_steps) * 100\n",
    "            elapsed_time = (time.time() - self.start_time)\n",
    "            steps_per_sec = self.step_counter / elapsed_time\n",
    "            eta_hours = (self.total_steps - self.step_counter) / steps_per_sec / 3600\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "            print(f\"📊 Paso {self.step_counter:,}/{self.total_steps:,} ({progress:.1f}%) - \"\n",
    "                  f\"{steps_per_sec:.1f} pasos/seg - ETA: {eta_hours:.1f}h - Memoria: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        nb_steps = logs.get('nb_episode_steps', 1)\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.episode_steps.append(nb_steps)\n",
    "        mean_reward = self.current_episode_reward / nb_steps if nb_steps > 0 else 0\n",
    "        print(f\"📈 Episodio {episode+1}: Recompensa total (clipped): {self.current_episode_reward:.3f}, \"\n",
    "              f\"Pasos: {nb_steps}, Mean Reward Calculado: {mean_reward:.6f} (Recompensa/Pasos)\")\n",
    "        # Reset for next episode\n",
    "        self.current_episode_reward = 0.0            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback que monitorea el progreso hacia una media de episode_reward = 20\n",
    "    \"\"\"\n",
    "    def __init__(self, target_avg_reward=TARGET_REWARD, name_model=None, window_size=100, save_best=True):\n",
    "        super().__init__()\n",
    "        self.target_avg_reward = target_avg_reward\n",
    "        self.window_size = window_size\n",
    "        self.save_best = save_best\n",
    "        self.name_model = name_model\n",
    "        \n",
    "        self.episode_count = 0\n",
    "        self.episode_rewards = []\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.episodes_at_target = 0\n",
    "        self.consecutive_target_episodes = 0\n",
    "        \n",
    "        print(f\"🎯 OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
    "        print(f\"📊 Ventana de evaluación: {window_size} episodios\")\n",
    "        self.save_better = f\"{self.name_model}_better-trains_{int(time.time())}\"\n",
    "        os.makedirs(self.save_better, exist_ok=True)       \n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.episode_count += 1\n",
    "        episode_reward = logs.get('episode_reward', 0)\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular media móvil\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
    "            current_avg = np.mean(recent_rewards)\n",
    "            \n",
    "            # Verificar si alcanzamos el objetivo\n",
    "            target_reached = current_avg >= self.target_avg_reward\n",
    "            \n",
    "            if target_reached:\n",
    "                self.episodes_at_target += 1\n",
    "                self.consecutive_target_episodes += 1\n",
    "            else:\n",
    "                self.consecutive_target_episodes = 0\n",
    "            \n",
    "            # Guardar si es el mejor promedio\n",
    "            if current_avg > self.best_avg_reward:\n",
    "                self.best_avg_reward = current_avg\n",
    "                if self.save_best:\n",
    "                    best_filename = f\"{self.save_better}/best_model_avg{current_avg:.1f}_ep{self.episode_count}.h5\"\n",
    "                    self.model.save_weights(best_filename, overwrite=True)\n",
    "                    print(f\"💾 NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado: {best_filename}\")\n",
    "            \n",
    "            # Mostrar progreso cada 50 episodios\n",
    "            if self.episode_count % 50 == 0:\n",
    "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
    "                target_status = \"🎯 OBJETIVO ALCANZADO!\" if target_reached else f\"📈 {progress_pct:.1f}% del objetivo\"\n",
    "                \n",
    "                print(f\"\\n📊 EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
    "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
    "                print(f\"   Media últimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
    "                print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "                print(f\"   Estado: {target_status}\")\n",
    "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
    "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
    "                \n",
    "                if self.consecutive_target_episodes >= 50:\n",
    "                    print(f\"🏆 ¡MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
    "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
    "            \n",
    "            print(f\"\\n🏁 RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "            print(f\"   Total episodios: {self.episode_count}\")\n",
    "            print(f\"   Media final últimos {self.window_size}: {final_avg:.2f}\")\n",
    "            print(f\"   Objetivo ({self.target_avg_reward}): {'✅ ALCANZADO' if objetivo_alcanzado else '❌ NO ALCANZADO'}\")\n",
    "            print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback que monitorea el progreso hacia una media de episode_reward = 20\n",
    "    \"\"\"\n",
    "    def __init__(self, target_avg_reward=20.0, name_model=None, window_size=100, save_best=True):\n",
    "        super().__init__()\n",
    "        self.target_avg_reward = target_avg_reward\n",
    "        self.window_size = window_size\n",
    "        self.save_best = save_best\n",
    "        self.name_model = name_model or \"model\"\n",
    "        self.episode_count = 0\n",
    "        self.episode_rewards = []\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.episodes_at_target = 0\n",
    "        self.consecutive_target_episodes = 0\n",
    "        self.save_better = f\"{self.name_model}_better-trains_{int(time.time())}\"\n",
    "        os.makedirs(self.save_better, exist_ok=True)\n",
    "        \n",
    "        print(f\"🎯 OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
    "        print(f\"📊 Ventana de evaluación: {window_size} episodios\")\n",
    "\n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        self.episode_count += 1\n",
    "        episode_reward = logs.get('episode_reward', 0)\n",
    "        # Convert NumPy types to Python types\n",
    "        if isinstance(episode_reward, np.floating):\n",
    "            episode_reward = episode_reward.item()\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular media móvil\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
    "            current_avg = np.mean(recent_rewards)\n",
    "            \n",
    "            # Verificar si alcanzamos el objetivo\n",
    "            target_reached = current_avg >= self.target_avg_reward\n",
    "            \n",
    "            if target_reached:\n",
    "                self.episodes_at_target += 1\n",
    "                self.consecutive_target_episodes += 1\n",
    "            else:\n",
    "                self.consecutive_target_episodes = 0\n",
    "            \n",
    "            # Guardar si es el mejor promedio\n",
    "            if current_avg > self.best_avg_reward:\n",
    "                self.best_avg_reward = current_avg\n",
    "                if self.save_best:\n",
    "                    best_filename = f\"{self.save_better}/best_model_avg{current_avg:.1f}_ep{self.episode_count}.h5f\"\n",
    "                    try:\n",
    "                        self.model.save_weights(best_filename, overwrite=True)\n",
    "                        # Save metrics to JSON\n",
    "                        metrics = {\n",
    "                            \"episode\": int(self.episode_count),  # Cast to Python int\n",
    "                            \"avg_reward\": float(current_avg),   # Cast to Python float\n",
    "                            \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                            \"timestamp\": int(time.time())\n",
    "                        }\n",
    "                        with open(f\"{best_filename}.json\", 'w') as f:\n",
    "                            json.dump(metrics, f, indent=2, default=str)\n",
    "                        print(f\"💾 NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado: {best_filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Error saving best model: {e}\")\n",
    "            \n",
    "            # Mostrar progreso cada 50 episodios\n",
    "            if self.episode_count % 50 == 0:\n",
    "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
    "                target_status = \"🎯 OBJETIVO ALCANZADO!\" if target_reached else f\"📈 {progress_pct:.1f}% del objetivo\"\n",
    "                \n",
    "                print(f\"\\n📊 EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
    "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
    "                print(f\"   Media últimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
    "                print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "                print(f\"   Estado: {target_status}\")\n",
    "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
    "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
    "                \n",
    "                if self.consecutive_target_episodes >= 50:\n",
    "                    print(f\"🏆 ¡MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        logs = logs or {}    \n",
    "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
    "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
    "            \n",
    "            print(f\"\\n🏁 RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "            print(f\"   Total episodios: {self.episode_count}\")\n",
    "            print(f\"   Media final últimos {self.window_size}: {final_avg:.2f}\")\n",
    "            print(f\"   Objetivo ({self.target_avg_reward}): {'✅ ALCANZADO' if objetivo_alcanzado else '❌ NO ALCANZADO'}\")\n",
    "            print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")\n",
    "            \n",
    "            # Save final metrics to JSON\n",
    "            final_metrics = {\n",
    "                \"total_episodes\": int(self.episode_count),\n",
    "                \"final_avg_reward\": float(final_avg),\n",
    "                \"target_reached\": bool(objetivo_alcanzado),\n",
    "                \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                \"episodes_at_target\": int(self.episodes_at_target)\n",
    "            }\n",
    "            final_log_path = f\"{self.save_better}/final_metrics.json\"\n",
    "            try:\n",
    "                with open(final_log_path, 'w') as f:\n",
    "                    json.dump(final_metrics, f, indent=2, default=str)                                    \n",
    "                print(f\"💾 Métricas finales guardadas en: {final_log_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error saving final metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFileLogger(FileLogger):\n",
    "    def __init__(self, filepath, interval=100):\n",
    "        super().__init__(filepath, interval)\n",
    "        self.step = 0  \n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.logs = {}        \n",
    "    \n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.step += 1  \n",
    "        if self.step % self.interval == 0:\n",
    "            episode_logs = {}\n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        self.current_episode_reward += np.clip(raw_reward, -1.0, 1.0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        metrics = logs.copy()\n",
    "        metrics['episode_reward'] = self.current_episode_reward\n",
    "        metrics['mean_reward_step'] = self.current_episode_reward / metrics.get('nb_episode_steps', 1)\n",
    "        metrics = {k: float(v) if isinstance(v, np.floating) else v for k, v in metrics.items()}\n",
    "        self.metrics[int(episode)] = metrics\n",
    "        if self.step % self.interval == 0:\n",
    "            with open(self.filepath, 'w') as f:\n",
    "                json.dump(self.metrics, f, indent=2, default=str)                \n",
    "        self.current_episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dqnet, checkpoint_path, save_freq=100, model_name='DQN'):\n",
    "        super().__init__()\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.save_freq = save_freq\n",
    "        self.model_name = model_name\n",
    "        self.episode_counter = 0\n",
    "        self.dqnet = dqnet\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.episode_counter += 1\n",
    "        if self.episode_counter % self.save_freq == 0:\n",
    "            weights_filename = f\"{self.checkpoint_path}/{self.model_name}_weights_episode_{self.episode_counter}.h5f\"\n",
    "            try:\n",
    "                self.model.save_weights(weights_filename, overwrite=True)\n",
    "                print(f\"✅ Pesos guardados en {weights_filename} tras el episodio {self.episode_counter}\")\n",
    "                \n",
    "                # Guardar memoria de repetición para modelos con replay                                \n",
    "                if self.model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
    "                    weights_filename = f\"{self.checkpoint_path}/{self.model_name}_memory_episode_{self.episode_counter}.pkl\"\n",
    "                    try:\n",
    "                        with open(self.checkpoint_path, 'wb') as f:\n",
    "                            pickle.dump(self.dqnet.memory, f)\n",
    "                        print(f\"💾 Memoria guardada: {self.checkpoint_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Error guardando memoria: {e}\")                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error al guardar pesos en {weights_filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_memory(dqnet, model_type, suffix=\"final\", checkpoint_path=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Guarda los pesos del modelo y, si aplica, la memoria de repetición en archivos.\n",
    "\n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay. Debe tener un método `save_weights` para guardar pesos y, si usa memoria, un\n",
    "            atributo `memory`.\n",
    "        model_type: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para determinar el prefijo del nombre de archivo y verificar si el modelo soporta memoria de repetición.     \n",
    "        episode: int. Número del episodio actual del entrenamiento. Se usa para nombrar los archivos de checkpoint y\n",
    "            memoria (e.g., `checkpoint_ep10.h5` para el episodio 10).\n",
    "        checkpoint_dir: str, opcional. Directorio donde se guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Se crea el directorio si no existe. Por defecto es \"checkpoints\".\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la escritura de archivos, pero no interrumpe la ejecución.\n",
    "    \"\"\"\n",
    "    # Crear directorio de checkpoints si no existe\n",
    "    # os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    memory_path_save = None\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo\n",
    "        checkpoint_path_save =  f\"{checkpoint_path}/{model_type}_weights_{suffix}.h5\"\n",
    "        dqnet.main_network.save_weights(checkpoint_path_save)\n",
    "        print(f\"💾 Guardado modelo {suffix}: {checkpoint_path_save}\")\n",
    "\n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if model_type in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
    "            memory_path_save = os.path.join(checkpoint_path, f'{model_type}_memory_{suffix}.pkl')\n",
    "            try:\n",
    "                with open(memory_path_save, 'wb') as f:\n",
    "                    pickle.dump(dqnet.memory, f)\n",
    "                print(f\"💾 Memoria {suffix} guardada: {memory_path_save}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error guardando memoria {suffix}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint {suffix}: {e}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgbzJyUjmTzs"
   },
   "source": [
    "#### **ENTRENAMIENTO** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(env, model_type, model_process, batch_size=batch_size, learning_rate=learning_rate, checkpoint_path='checkpoints',\n",
    "        input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size, warmup_steps=WARMUP_STEPS,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau, epsilon_start=epsilon_start,\n",
    "        epsilon_min=0.1, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS, target_reward=TARGET_REWARD):\n",
    "    \"\"\"\n",
    "    Agente simple con conversión automática - NO toca AtariProcessor\n",
    "    \"\"\"\n",
    "    name_model = model_type.upper()    \n",
    "    print(f\"🤖 Creando agente {name_model} con conversión automática...\")\n",
    "    \n",
    "    # Limpiar\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = None\n",
    "\n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()\n",
    "    # Verificar el tipo de modelo y establecer enable_double_dqn correctamente\n",
    "    enable_double_dqn = name_model in ['DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']\n",
    "\n",
    "    print(\"-\" * 60)    \n",
    "    try:\n",
    "        if enable_double_dqn:\n",
    "            model, memory, target_model = model_process(input_shape, env.action_space.n, memoria_tamano)\n",
    "        else:\n",
    "            model, memory = model_process(input_shape, env.action_space.n, memoria_tamano)\n",
    "            target_model = None\n",
    "       \n",
    "        if model is None:\n",
    "            raise ValueError(\"El modelo no se creó correctamente.\")\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al crear el modelo: {str(e)}\")\n",
    "        return None, False\n",
    "    \n",
    "    memory = SequentialMemory(limit=memoria_tamano, window_length=WINDOW_LENGTH)\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                attr='eps',\n",
    "                                value_max=epsilon_start, \n",
    "                                value_min=epsilon_min, \n",
    "                                value_test=.05,\n",
    "                                nb_steps=epsilon_steps)  \n",
    "    # Crear agente\n",
    "    try:\n",
    "        dqn = DQNAgent(\n",
    "            model=model,\n",
    "            nb_actions=env.action_space.n,\n",
    "            memory=memory,\n",
    "            processor=processor,\n",
    "            nb_steps_warmup=warmup_steps,\n",
    "            target_model_update=TARGET_UPDATE_INTERVAL if enable_double_dqn else 10000,  # Hard update every 10,000 steps\n",
    "            enable_double_dqn=enable_double_dqn,\n",
    "            gamma=gamma,\n",
    "            train_interval=4,\n",
    "            delta_clip=1.0,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Compilar el agente\n",
    "        dqn.compile(Adam(learning_rate=learning_rate), metrics=['mae'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al crear o compilar el agente: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "    # Configurar callbacks\n",
    "    tipo_str = name_model\n",
    "    weights_filename = f'{tipo_str}_weights.h5f'\n",
    "    checkpoint_weights_filename = f'{checkpoint_path}/{tipo_str}_weights_{{step}}.h5f'\n",
    "    log_filename = f'{checkpoint_path}/{tipo_str}_log.json'\n",
    "\n",
    "    # DEBUG --------------------\n",
    "    # callbacks = [\n",
    "    #     progress_callback,\n",
    "    #     ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
    "    #     CustomFileLogger(log_filename, interval=1000),\n",
    "    #     PerformanceMonitor(save_path='diagnosticos')\n",
    "    # ]\n",
    "    # Callbacks optimizados para el objetivo\n",
    "    training_dir = f\"{checkpoint_path}/{name_model}\"\n",
    "    callbacks = [\n",
    "            #ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),      \n",
    "            CustomFileLogger(log_filename, interval=1000),\n",
    "            # Progreso cada 20k pasos\n",
    "            SimpleProgressCallback(num_steps, print_interval=20000),         \n",
    "            # Tracker del objetivo principal\n",
    "            TargetRewardTracker( target_avg_reward=target_reward, name_model=training_dir, window_size=100, save_best=True),\n",
    "            EpisodeCheckpointCallback(dqn, checkpoint_path=checkpoint_path, save_freq=100, model_name=tipo_str)    \n",
    "        ]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Iniciando entrenamiento de {tipo_str} por {num_steps} pasos...\")\n",
    "        start_time = time.time()\n",
    "        trained_dqn = dqn.fit(env, nb_steps=num_steps, callbacks=callbacks, verbose=2)\n",
    "        training_time = (time.time() - start_time) / 60\n",
    "        print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n",
    "        \n",
    "        # Guardar pesos finales\n",
    "        save_checkpoint_memory(dqn, name_model)\n",
    "        return trained_dqn, True\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido por el usuario\") \n",
    "        # Guardar pesos de emergencia\n",
    "        save_checkpoint_memory(dqn, name_model, suffix=\"emergency\")        \n",
    "        return trained_dqn, True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante el entrenamiento: {str(e)}\")\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVALUACION** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6GZzkrK72yj1"
   },
   "outputs": [],
   "source": [
    "# Función para evaluar el modelo\n",
    "def evaluar_modelo(dqn, env, num_episodes=200, render=True, record_video=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo DQN o DDQN y si el modelo alcanza el objetivo de media\n",
    "\n",
    "    Args:\n",
    "        dqn: Agente DQN entrenado\n",
    "        env: Entorno de gym\n",
    "        num_episodes: Número de episodios para evaluar\n",
    "        render: Si se debe mostrar la visualización\n",
    "        record_video: Si se debe grabar video\n",
    "\n",
    "    Returns:\n",
    "        Lista de recompensas por episodio\n",
    "    \"\"\"\n",
    "    print(f\"🎯 EVALUANDO OBJETIVO: Media de episode_reward = {TARGET_REWARD}\")\n",
    "    print(f\"📊 Evaluando por {num_episodes} episodios...\")\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < 2000:  # Límite de pasos por episodio\n",
    "                action = dqn.forward(state)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "            if (episode + 1) % 50 == 0:\n",
    "                current_avg = np.mean(rewards)\n",
    "                print(f\"   Episodio {episode + 1}: avg actual = {current_avg:.2f}\")\n",
    "        \n",
    "        # Análisis final\n",
    "        avg_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards)\n",
    "        max_reward = np.max(rewards)\n",
    "        min_reward = np.min(rewards)\n",
    "        \n",
    "        objetivo_alcanzado = avg_reward >= 20.0\n",
    "        \n",
    "        print(f\"\\n📊 RESULTADOS DE EVALUACIÓN:\")\n",
    "        print(f\"   Media: {avg_reward:.2f} {'✅' if objetivo_alcanzado else '❌'}\")\n",
    "        print(f\"   Desviación: ±{std_reward:.2f}\")\n",
    "        print(f\"   Máximo: {max_reward:.2f}\")\n",
    "        print(f\"   Mínimo: {min_reward:.2f}\")\n",
    "        print(f\"   Episodios sobre 20: {sum(1 for r in rewards if r >= 20)} / {num_episodes}\")\n",
    "        \n",
    "        if objetivo_alcanzado:\n",
    "            print(f\"🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de {avg_reward:.2f}\")\n",
    "        else:\n",
    "            print(f\"📈 Progreso: {(avg_reward/20)*100:.1f}% del objetivo\")\n",
    "        \n",
    "        return rewards, objetivo_alcanzado\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error durante evaluación: {e}\")\n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡¡¡¡¡¡¡ **EJECUCION - MAIN** !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow optimizado para 20 cores CPU\n",
      "🚀 EJECUTANDO SOLUCIÓN...\n",
      "🎯 OBJETIVO: Conseguir media de episode_reward = 20.0 (con clipping)\n",
      "Cargando pesos desde checkpoints\\DDQN_weights_episode_200.h5f.index\n",
      "🤖 Creando agente DDQN con conversión automática...\n",
      "------------------------------------------------------------\n",
      "🏗️ Creando modelos DDQN: input_shape=(84, 84, 4), actions=6\n",
      "🏗️ Creando modelo DQN estándar: input_shape=(84, 84, 4), actions=6\n",
      "✅ Modelo creado exitosamente\n",
      "📊 Resumen del modelo:\n",
      "Model: \"DQN_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_channels_first (Input  [(None, 4, 84, 84)]      0         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " convert_to_channels_last (P  (None, 84, 84, 4)        0         \n",
      " ermute)                                                         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 512)               1606144   \n",
      "                                                                 \n",
      " q_values (Dense)            (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "✅ Modelos DDQN creados exitosamente\n",
      "🎯 OBJETIVO: Media de episode_reward = 20.0\n",
      "📊 Ventana de evaluación: 100 episodios\n",
      "Iniciando entrenamiento de DDQN por 5000000 pasos...\n",
      "🚀 Entrenamiento iniciado: 5,000,000 pasos\n",
      "Training for 5000000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1: Recompensa total (clipped): 20.000, Pasos: 649, Mean Reward Calculado: 0.030817 (Recompensa/Pasos)\n",
      "     649/5000000: episode: 1, duration: 4.520s, episode steps: 649, steps per second: 144, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.641 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 2: Recompensa total (clipped): 19.000, Pasos: 659, Mean Reward Calculado: 0.028832 (Recompensa/Pasos)\n",
      "    1308/5000000: episode: 2, duration: 4.560s, episode steps: 659, steps per second: 145, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 3: Recompensa total (clipped): 17.000, Pasos: 679, Mean Reward Calculado: 0.025037 (Recompensa/Pasos)\n",
      "    1987/5000000: episode: 3, duration: 4.520s, episode steps: 679, steps per second: 150, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.672 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 4: Recompensa total (clipped): 13.000, Pasos: 398, Mean Reward Calculado: 0.032663 (Recompensa/Pasos)\n",
      "    2385/5000000: episode: 4, duration: 2.793s, episode steps: 398, steps per second: 143, episode reward: 13.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.314 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 5: Recompensa total (clipped): 12.000, Pasos: 527, Mean Reward Calculado: 0.022770 (Recompensa/Pasos)\n",
      "    2912/5000000: episode: 5, duration: 3.612s, episode steps: 527, steps per second: 146, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 6: Recompensa total (clipped): 11.000, Pasos: 420, Mean Reward Calculado: 0.026190 (Recompensa/Pasos)\n",
      "    3332/5000000: episode: 6, duration: 3.020s, episode steps: 420, steps per second: 139, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.162 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 7: Recompensa total (clipped): 15.000, Pasos: 547, Mean Reward Calculado: 0.027422 (Recompensa/Pasos)\n",
      "    3879/5000000: episode: 7, duration: 3.932s, episode steps: 547, steps per second: 139, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.166 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 8: Recompensa total (clipped): 8.000, Pasos: 461, Mean Reward Calculado: 0.017354 (Recompensa/Pasos)\n",
      "    4340/5000000: episode: 8, duration: 3.246s, episode steps: 461, steps per second: 142, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 9: Recompensa total (clipped): 8.000, Pasos: 477, Mean Reward Calculado: 0.016771 (Recompensa/Pasos)\n",
      "    4817/5000000: episode: 9, duration: 3.210s, episode steps: 477, steps per second: 149, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.945 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 10: Recompensa total (clipped): 11.000, Pasos: 602, Mean Reward Calculado: 0.018272 (Recompensa/Pasos)\n",
      "    5419/5000000: episode: 10, duration: 4.105s, episode steps: 602, steps per second: 147, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 11: Recompensa total (clipped): 10.000, Pasos: 488, Mean Reward Calculado: 0.020492 (Recompensa/Pasos)\n",
      "    5907/5000000: episode: 11, duration: 3.268s, episode steps: 488, steps per second: 149, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.250 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 12: Recompensa total (clipped): 20.000, Pasos: 1017, Mean Reward Calculado: 0.019666 (Recompensa/Pasos)\n",
      "    6924/5000000: episode: 12, duration: 7.232s, episode steps: 1017, steps per second: 141, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 13: Recompensa total (clipped): 15.000, Pasos: 499, Mean Reward Calculado: 0.030060 (Recompensa/Pasos)\n",
      "    7423/5000000: episode: 13, duration: 3.432s, episode steps: 499, steps per second: 145, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 14: Recompensa total (clipped): 28.000, Pasos: 902, Mean Reward Calculado: 0.031042 (Recompensa/Pasos)\n",
      "    8325/5000000: episode: 14, duration: 6.112s, episode steps: 902, steps per second: 148, episode reward: 28.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 15: Recompensa total (clipped): 17.000, Pasos: 636, Mean Reward Calculado: 0.026730 (Recompensa/Pasos)\n",
      "    8961/5000000: episode: 15, duration: 4.335s, episode steps: 636, steps per second: 147, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 16: Recompensa total (clipped): 12.000, Pasos: 605, Mean Reward Calculado: 0.019835 (Recompensa/Pasos)\n",
      "    9566/5000000: episode: 16, duration: 4.083s, episode steps: 605, steps per second: 148, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.673 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 17: Recompensa total (clipped): 24.000, Pasos: 590, Mean Reward Calculado: 0.040678 (Recompensa/Pasos)\n",
      "   10156/5000000: episode: 17, duration: 4.035s, episode steps: 590, steps per second: 146, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.363 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 18: Recompensa total (clipped): 6.000, Pasos: 482, Mean Reward Calculado: 0.012448 (Recompensa/Pasos)\n",
      "   10638/5000000: episode: 18, duration: 3.419s, episode steps: 482, steps per second: 141, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 19: Recompensa total (clipped): 15.000, Pasos: 517, Mean Reward Calculado: 0.029014 (Recompensa/Pasos)\n",
      "   11155/5000000: episode: 19, duration: 3.393s, episode steps: 517, steps per second: 152, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 20: Recompensa total (clipped): 10.000, Pasos: 429, Mean Reward Calculado: 0.023310 (Recompensa/Pasos)\n",
      "   11584/5000000: episode: 20, duration: 2.893s, episode steps: 429, steps per second: 148, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.979 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 21: Recompensa total (clipped): 26.000, Pasos: 1196, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
      "   12780/5000000: episode: 21, duration: 8.192s, episode steps: 1196, steps per second: 146, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.294 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 22: Recompensa total (clipped): 25.000, Pasos: 668, Mean Reward Calculado: 0.037425 (Recompensa/Pasos)\n",
      "   13448/5000000: episode: 22, duration: 4.498s, episode steps: 668, steps per second: 149, episode reward: 25.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.686 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 23: Recompensa total (clipped): 16.000, Pasos: 491, Mean Reward Calculado: 0.032587 (Recompensa/Pasos)\n",
      "   13939/5000000: episode: 23, duration: 3.418s, episode steps: 491, steps per second: 144, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.587 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 24: Recompensa total (clipped): 12.000, Pasos: 442, Mean Reward Calculado: 0.027149 (Recompensa/Pasos)\n",
      "   14381/5000000: episode: 24, duration: 2.690s, episode steps: 442, steps per second: 164, episode reward: 12.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.213 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 25: Recompensa total (clipped): 20.000, Pasos: 746, Mean Reward Calculado: 0.026810 (Recompensa/Pasos)\n",
      "   15127/5000000: episode: 25, duration: 5.055s, episode steps: 746, steps per second: 148, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.257 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 26: Recompensa total (clipped): 14.000, Pasos: 613, Mean Reward Calculado: 0.022838 (Recompensa/Pasos)\n",
      "   15740/5000000: episode: 26, duration: 4.217s, episode steps: 613, steps per second: 145, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.330 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 27: Recompensa total (clipped): 9.000, Pasos: 497, Mean Reward Calculado: 0.018109 (Recompensa/Pasos)\n",
      "   16237/5000000: episode: 27, duration: 3.488s, episode steps: 497, steps per second: 142, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.300 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 28: Recompensa total (clipped): 6.000, Pasos: 370, Mean Reward Calculado: 0.016216 (Recompensa/Pasos)\n",
      "   16607/5000000: episode: 28, duration: 2.700s, episode steps: 370, steps per second: 137, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 29: Recompensa total (clipped): 33.000, Pasos: 1214, Mean Reward Calculado: 0.027183 (Recompensa/Pasos)\n",
      "   17821/5000000: episode: 29, duration: 7.891s, episode steps: 1214, steps per second: 154, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.674 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 30: Recompensa total (clipped): 15.000, Pasos: 560, Mean Reward Calculado: 0.026786 (Recompensa/Pasos)\n",
      "   18381/5000000: episode: 30, duration: 3.885s, episode steps: 560, steps per second: 144, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.795 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 31: Recompensa total (clipped): 11.000, Pasos: 530, Mean Reward Calculado: 0.020755 (Recompensa/Pasos)\n",
      "   18911/5000000: episode: 31, duration: 3.734s, episode steps: 530, steps per second: 142, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.291 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 32: Recompensa total (clipped): 6.000, Pasos: 482, Mean Reward Calculado: 0.012448 (Recompensa/Pasos)\n",
      "   19393/5000000: episode: 32, duration: 3.296s, episode steps: 482, steps per second: 146, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 33: Recompensa total (clipped): 9.000, Pasos: 594, Mean Reward Calculado: 0.015152 (Recompensa/Pasos)\n",
      "   19987/5000000: episode: 33, duration: 3.961s, episode steps: 594, steps per second: 150, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.212 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📊 Paso 20,000/5,000,000 (0.4%) - 146.1 pasos/seg - ETA: 9.5h - Memoria: 593.64 MB\n",
      "📈 Episodio 34: Recompensa total (clipped): 22.000, Pasos: 770, Mean Reward Calculado: 0.028571 (Recompensa/Pasos)\n",
      "   20757/5000000: episode: 34, duration: 5.267s, episode steps: 770, steps per second: 146, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 35: Recompensa total (clipped): 22.000, Pasos: 805, Mean Reward Calculado: 0.027329 (Recompensa/Pasos)\n",
      "   21562/5000000: episode: 35, duration: 5.334s, episode steps: 805, steps per second: 151, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 36: Recompensa total (clipped): 24.000, Pasos: 581, Mean Reward Calculado: 0.041308 (Recompensa/Pasos)\n",
      "   22143/5000000: episode: 36, duration: 4.170s, episode steps: 581, steps per second: 139, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.114 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 37: Recompensa total (clipped): 15.000, Pasos: 589, Mean Reward Calculado: 0.025467 (Recompensa/Pasos)\n",
      "   22732/5000000: episode: 37, duration: 4.339s, episode steps: 589, steps per second: 136, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 38: Recompensa total (clipped): 14.000, Pasos: 608, Mean Reward Calculado: 0.023026 (Recompensa/Pasos)\n",
      "   23340/5000000: episode: 38, duration: 4.265s, episode steps: 608, steps per second: 143, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.388 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 39: Recompensa total (clipped): 7.000, Pasos: 473, Mean Reward Calculado: 0.014799 (Recompensa/Pasos)\n",
      "   23813/5000000: episode: 39, duration: 3.283s, episode steps: 473, steps per second: 144, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 40: Recompensa total (clipped): 16.000, Pasos: 684, Mean Reward Calculado: 0.023392 (Recompensa/Pasos)\n",
      "   24497/5000000: episode: 40, duration: 4.718s, episode steps: 684, steps per second: 145, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 41: Recompensa total (clipped): 15.000, Pasos: 477, Mean Reward Calculado: 0.031447 (Recompensa/Pasos)\n",
      "   24974/5000000: episode: 41, duration: 3.332s, episode steps: 477, steps per second: 143, episode reward: 15.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 42: Recompensa total (clipped): 26.000, Pasos: 744, Mean Reward Calculado: 0.034946 (Recompensa/Pasos)\n",
      "   25718/5000000: episode: 42, duration: 5.201s, episode steps: 744, steps per second: 143, episode reward: 26.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.343 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 43: Recompensa total (clipped): 6.000, Pasos: 383, Mean Reward Calculado: 0.015666 (Recompensa/Pasos)\n",
      "   26101/5000000: episode: 43, duration: 2.836s, episode steps: 383, steps per second: 135, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 44: Recompensa total (clipped): 13.000, Pasos: 496, Mean Reward Calculado: 0.026210 (Recompensa/Pasos)\n",
      "   26597/5000000: episode: 44, duration: 3.510s, episode steps: 496, steps per second: 141, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.365 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 45: Recompensa total (clipped): 7.000, Pasos: 502, Mean Reward Calculado: 0.013944 (Recompensa/Pasos)\n",
      "   27099/5000000: episode: 45, duration: 3.439s, episode steps: 502, steps per second: 146, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.203 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 46: Recompensa total (clipped): 13.000, Pasos: 598, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
      "   27697/5000000: episode: 46, duration: 3.543s, episode steps: 598, steps per second: 169, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.467 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 47: Recompensa total (clipped): 10.000, Pasos: 365, Mean Reward Calculado: 0.027397 (Recompensa/Pasos)\n",
      "   28062/5000000: episode: 47, duration: 2.536s, episode steps: 365, steps per second: 144, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.868 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 48: Recompensa total (clipped): 15.000, Pasos: 694, Mean Reward Calculado: 0.021614 (Recompensa/Pasos)\n",
      "   28756/5000000: episode: 48, duration: 4.930s, episode steps: 694, steps per second: 141, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.229 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 49: Recompensa total (clipped): 18.000, Pasos: 737, Mean Reward Calculado: 0.024423 (Recompensa/Pasos)\n",
      "   29493/5000000: episode: 49, duration: 4.764s, episode steps: 737, steps per second: 155, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.186 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 50: Recompensa total (clipped): 13.000, Pasos: 525, Mean Reward Calculado: 0.024762 (Recompensa/Pasos)\n",
      "   30018/5000000: episode: 50, duration: 3.459s, episode steps: 525, steps per second: 152, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 51: Recompensa total (clipped): 14.000, Pasos: 502, Mean Reward Calculado: 0.027888 (Recompensa/Pasos)\n",
      "   30520/5000000: episode: 51, duration: 3.600s, episode steps: 502, steps per second: 139, episode reward: 14.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.369 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 52: Recompensa total (clipped): 13.000, Pasos: 557, Mean Reward Calculado: 0.023339 (Recompensa/Pasos)\n",
      "   31077/5000000: episode: 52, duration: 3.678s, episode steps: 557, steps per second: 151, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.447 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 53: Recompensa total (clipped): 9.000, Pasos: 511, Mean Reward Calculado: 0.017613 (Recompensa/Pasos)\n",
      "   31588/5000000: episode: 53, duration: 3.621s, episode steps: 511, steps per second: 141, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.096 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 54: Recompensa total (clipped): 12.000, Pasos: 504, Mean Reward Calculado: 0.023810 (Recompensa/Pasos)\n",
      "   32092/5000000: episode: 54, duration: 3.581s, episode steps: 504, steps per second: 141, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 55: Recompensa total (clipped): 12.000, Pasos: 477, Mean Reward Calculado: 0.025157 (Recompensa/Pasos)\n",
      "   32569/5000000: episode: 55, duration: 3.355s, episode steps: 477, steps per second: 142, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.398 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 56: Recompensa total (clipped): 24.000, Pasos: 965, Mean Reward Calculado: 0.024870 (Recompensa/Pasos)\n",
      "   33534/5000000: episode: 56, duration: 6.522s, episode steps: 965, steps per second: 148, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.291 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 57: Recompensa total (clipped): 9.000, Pasos: 419, Mean Reward Calculado: 0.021480 (Recompensa/Pasos)\n",
      "   33953/5000000: episode: 57, duration: 2.961s, episode steps: 419, steps per second: 141, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 58: Recompensa total (clipped): 19.000, Pasos: 673, Mean Reward Calculado: 0.028232 (Recompensa/Pasos)\n",
      "   34626/5000000: episode: 58, duration: 4.332s, episode steps: 673, steps per second: 155, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.270 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 59: Recompensa total (clipped): 16.000, Pasos: 813, Mean Reward Calculado: 0.019680 (Recompensa/Pasos)\n",
      "   35439/5000000: episode: 59, duration: 5.351s, episode steps: 813, steps per second: 152, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 60: Recompensa total (clipped): 11.000, Pasos: 376, Mean Reward Calculado: 0.029255 (Recompensa/Pasos)\n",
      "   35815/5000000: episode: 60, duration: 2.531s, episode steps: 376, steps per second: 149, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.199 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 61: Recompensa total (clipped): 14.000, Pasos: 489, Mean Reward Calculado: 0.028630 (Recompensa/Pasos)\n",
      "   36304/5000000: episode: 61, duration: 3.288s, episode steps: 489, steps per second: 149, episode reward: 14.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.431 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 62: Recompensa total (clipped): 20.000, Pasos: 820, Mean Reward Calculado: 0.024390 (Recompensa/Pasos)\n",
      "   37124/5000000: episode: 62, duration: 5.637s, episode steps: 820, steps per second: 145, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.834 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 63: Recompensa total (clipped): 12.000, Pasos: 541, Mean Reward Calculado: 0.022181 (Recompensa/Pasos)\n",
      "   37665/5000000: episode: 63, duration: 3.683s, episode steps: 541, steps per second: 147, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 64: Recompensa total (clipped): 12.000, Pasos: 519, Mean Reward Calculado: 0.023121 (Recompensa/Pasos)\n",
      "   38184/5000000: episode: 64, duration: 3.473s, episode steps: 519, steps per second: 149, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.247 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 65: Recompensa total (clipped): 12.000, Pasos: 513, Mean Reward Calculado: 0.023392 (Recompensa/Pasos)\n",
      "   38697/5000000: episode: 65, duration: 3.422s, episode steps: 513, steps per second: 150, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 66: Recompensa total (clipped): 7.000, Pasos: 371, Mean Reward Calculado: 0.018868 (Recompensa/Pasos)\n",
      "   39068/5000000: episode: 66, duration: 2.715s, episode steps: 371, steps per second: 137, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📊 Paso 40,000/5,000,000 (0.8%) - 145.7 pasos/seg - ETA: 9.5h - Memoria: 739.71 MB\n",
      "📈 Episodio 67: Recompensa total (clipped): 30.000, Pasos: 1047, Mean Reward Calculado: 0.028653 (Recompensa/Pasos)\n",
      "   40115/5000000: episode: 67, duration: 7.752s, episode steps: 1047, steps per second: 135, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 68: Recompensa total (clipped): 10.000, Pasos: 521, Mean Reward Calculado: 0.019194 (Recompensa/Pasos)\n",
      "   40636/5000000: episode: 68, duration: 3.751s, episode steps: 521, steps per second: 139, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.975 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 69: Recompensa total (clipped): 18.000, Pasos: 490, Mean Reward Calculado: 0.036735 (Recompensa/Pasos)\n",
      "   41126/5000000: episode: 69, duration: 3.536s, episode steps: 490, steps per second: 139, episode reward: 18.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.208 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 70: Recompensa total (clipped): 13.000, Pasos: 604, Mean Reward Calculado: 0.021523 (Recompensa/Pasos)\n",
      "   41730/5000000: episode: 70, duration: 4.100s, episode steps: 604, steps per second: 147, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.123 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 71: Recompensa total (clipped): 15.000, Pasos: 672, Mean Reward Calculado: 0.022321 (Recompensa/Pasos)\n",
      "   42402/5000000: episode: 71, duration: 4.568s, episode steps: 672, steps per second: 147, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.201 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 72: Recompensa total (clipped): 19.000, Pasos: 421, Mean Reward Calculado: 0.045131 (Recompensa/Pasos)\n",
      "   42823/5000000: episode: 72, duration: 2.914s, episode steps: 421, steps per second: 144, episode reward: 19.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 73: Recompensa total (clipped): 16.000, Pasos: 718, Mean Reward Calculado: 0.022284 (Recompensa/Pasos)\n",
      "   43541/5000000: episode: 73, duration: 5.382s, episode steps: 718, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 74: Recompensa total (clipped): 17.000, Pasos: 658, Mean Reward Calculado: 0.025836 (Recompensa/Pasos)\n",
      "   44199/5000000: episode: 74, duration: 4.987s, episode steps: 658, steps per second: 132, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.120 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 75: Recompensa total (clipped): 14.000, Pasos: 542, Mean Reward Calculado: 0.025830 (Recompensa/Pasos)\n",
      "   44741/5000000: episode: 75, duration: 3.807s, episode steps: 542, steps per second: 142, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 4.111 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 76: Recompensa total (clipped): 20.000, Pasos: 685, Mean Reward Calculado: 0.029197 (Recompensa/Pasos)\n",
      "   45426/5000000: episode: 76, duration: 4.780s, episode steps: 685, steps per second: 143, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.358 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 77: Recompensa total (clipped): 12.000, Pasos: 722, Mean Reward Calculado: 0.016620 (Recompensa/Pasos)\n",
      "   46148/5000000: episode: 77, duration: 4.956s, episode steps: 722, steps per second: 146, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 78: Recompensa total (clipped): 17.000, Pasos: 600, Mean Reward Calculado: 0.028333 (Recompensa/Pasos)\n",
      "   46748/5000000: episode: 78, duration: 4.194s, episode steps: 600, steps per second: 143, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 79: Recompensa total (clipped): 13.000, Pasos: 684, Mean Reward Calculado: 0.019006 (Recompensa/Pasos)\n",
      "   47432/5000000: episode: 79, duration: 4.677s, episode steps: 684, steps per second: 146, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 80: Recompensa total (clipped): 20.000, Pasos: 742, Mean Reward Calculado: 0.026954 (Recompensa/Pasos)\n",
      "   48174/5000000: episode: 80, duration: 5.166s, episode steps: 742, steps per second: 144, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.629 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 81: Recompensa total (clipped): 8.000, Pasos: 467, Mean Reward Calculado: 0.017131 (Recompensa/Pasos)\n",
      "   48641/5000000: episode: 81, duration: 3.353s, episode steps: 467, steps per second: 139, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "📈 Episodio 82: Recompensa total (clipped): 12.000, Pasos: 672, Mean Reward Calculado: 0.017857 (Recompensa/Pasos)\n",
      "   49313/5000000: episode: 82, duration: 4.669s, episode steps: 672, steps per second: 144, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 83: Recompensa total (clipped): 16.000, Pasos: 751, Mean Reward Calculado: 0.021305 (Recompensa/Pasos)\n",
      "   50064/5000000: episode: 83, duration: 10.306s, episode steps: 751, steps per second:  73, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.613 [0.000, 5.000],  loss: 0.012944, mae: 0.028648, mean_q: 0.062642\n",
      "📈 Episodio 84: Recompensa total (clipped): 18.000, Pasos: 574, Mean Reward Calculado: 0.031359 (Recompensa/Pasos)\n",
      "   50638/5000000: episode: 84, duration: 35.834s, episode steps: 574, steps per second:  16, episode reward: 18.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.571 [0.000, 5.000],  loss: 0.011139, mae: 0.020085, mean_q: 0.036276\n",
      "📈 Episodio 85: Recompensa total (clipped): 3.000, Pasos: 273, Mean Reward Calculado: 0.010989 (Recompensa/Pasos)\n",
      "   50911/5000000: episode: 85, duration: 16.245s, episode steps: 273, steps per second:  17, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.586 [0.000, 5.000],  loss: 0.010111, mae: 0.024434, mean_q: 0.039662\n",
      "📈 Episodio 86: Recompensa total (clipped): 23.000, Pasos: 666, Mean Reward Calculado: 0.034535 (Recompensa/Pasos)\n",
      "   51577/5000000: episode: 86, duration: 39.517s, episode steps: 666, steps per second:  17, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.901 [0.000, 5.000],  loss: 0.011258, mae: 0.023520, mean_q: 0.039601\n",
      "📈 Episodio 87: Recompensa total (clipped): 22.000, Pasos: 968, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
      "   52545/5000000: episode: 87, duration: 59.890s, episode steps: 968, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.011434, mae: 0.024479, mean_q: 0.038401\n",
      "📈 Episodio 88: Recompensa total (clipped): 13.000, Pasos: 649, Mean Reward Calculado: 0.020031 (Recompensa/Pasos)\n",
      "   53194/5000000: episode: 88, duration: 41.490s, episode steps: 649, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.012827, mae: 0.028863, mean_q: 0.042286\n",
      "📈 Episodio 89: Recompensa total (clipped): 11.000, Pasos: 730, Mean Reward Calculado: 0.015068 (Recompensa/Pasos)\n",
      "   53924/5000000: episode: 89, duration: 44.556s, episode steps: 730, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.512 [0.000, 5.000],  loss: 0.011907, mae: 0.028498, mean_q: 0.042166\n",
      "📈 Episodio 90: Recompensa total (clipped): 18.000, Pasos: 663, Mean Reward Calculado: 0.027149 (Recompensa/Pasos)\n",
      "   54587/5000000: episode: 90, duration: 41.231s, episode steps: 663, steps per second:  16, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.011847, mae: 0.028938, mean_q: 0.043642\n",
      "📈 Episodio 91: Recompensa total (clipped): 2.000, Pasos: 279, Mean Reward Calculado: 0.007168 (Recompensa/Pasos)\n",
      "   54866/5000000: episode: 91, duration: 17.683s, episode steps: 279, steps per second:  16, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.010354, mae: 0.024501, mean_q: 0.035814\n",
      "📈 Episodio 92: Recompensa total (clipped): 18.000, Pasos: 852, Mean Reward Calculado: 0.021127 (Recompensa/Pasos)\n",
      "   55718/5000000: episode: 92, duration: 55.185s, episode steps: 852, steps per second:  15, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.010603, mae: 0.026433, mean_q: 0.039628\n",
      "📈 Episodio 93: Recompensa total (clipped): 11.000, Pasos: 507, Mean Reward Calculado: 0.021696 (Recompensa/Pasos)\n",
      "   56225/5000000: episode: 93, duration: 31.768s, episode steps: 507, steps per second:  16, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.576 [0.000, 5.000],  loss: 0.011277, mae: 0.023408, mean_q: 0.035824\n",
      "📈 Episodio 94: Recompensa total (clipped): 4.000, Pasos: 269, Mean Reward Calculado: 0.014870 (Recompensa/Pasos)\n",
      "   56494/5000000: episode: 94, duration: 16.805s, episode steps: 269, steps per second:  16, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 0.896 [0.000, 5.000],  loss: 0.011147, mae: 0.023210, mean_q: 0.032381\n",
      "📈 Episodio 95: Recompensa total (clipped): 2.000, Pasos: 261, Mean Reward Calculado: 0.007663 (Recompensa/Pasos)\n",
      "   56755/5000000: episode: 95, duration: 16.433s, episode steps: 261, steps per second:  16, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.010962, mae: 0.024732, mean_q: 0.035311\n",
      "📈 Episodio 96: Recompensa total (clipped): 9.000, Pasos: 592, Mean Reward Calculado: 0.015203 (Recompensa/Pasos)\n",
      "   57347/5000000: episode: 96, duration: 35.920s, episode steps: 592, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.011022, mae: 0.026810, mean_q: 0.033995\n",
      "📈 Episodio 97: Recompensa total (clipped): 19.000, Pasos: 798, Mean Reward Calculado: 0.023810 (Recompensa/Pasos)\n",
      "   58145/5000000: episode: 97, duration: 49.873s, episode steps: 798, steps per second:  16, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.010102, mae: 0.022135, mean_q: 0.026083\n",
      "📈 Episodio 98: Recompensa total (clipped): 21.000, Pasos: 793, Mean Reward Calculado: 0.026482 (Recompensa/Pasos)\n",
      "   58938/5000000: episode: 98, duration: 49.151s, episode steps: 793, steps per second:  16, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.010557, mae: 0.023785, mean_q: 0.030895\n",
      "📈 Episodio 99: Recompensa total (clipped): 13.000, Pasos: 502, Mean Reward Calculado: 0.025896 (Recompensa/Pasos)\n",
      "   59440/5000000: episode: 99, duration: 31.671s, episode steps: 502, steps per second:  16, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.008790, mae: 0.022508, mean_q: 0.032255\n",
      "📊 Paso 60,000/5,000,000 (1.2%) - 61.9 pasos/seg - ETA: 22.2h - Memoria: 934.36 MB\n",
      "📈 Episodio 100: Recompensa total (clipped): 15.000, Pasos: 629, Mean Reward Calculado: 0.023847 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.57 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.6_ep100.h5f\n",
      "\n",
      "📊 EPISODIO 100 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 15.00\n",
      "   Media últimos 100: 14.57 / 20.0\n",
      "   Mejor promedio histórico: 14.57\n",
      "   Estado: 📈 72.9% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "✅ Pesos guardados en checkpoints/DDQN_weights_episode_100.h5f tras el episodio 100\n",
      "   60069/5000000: episode: 100, duration: 39.901s, episode steps: 629, steps per second:  16, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.968 [0.000, 5.000],  loss: 0.012800, mae: 0.031416, mean_q: 0.047084\n",
      "📈 Episodio 101: Recompensa total (clipped): 10.000, Pasos: 472, Mean Reward Calculado: 0.021186 (Recompensa/Pasos)\n",
      "   60541/5000000: episode: 101, duration: 30.206s, episode steps: 472, steps per second:  16, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012727, mae: 0.053616, mean_q: 0.072509\n",
      "📈 Episodio 102: Recompensa total (clipped): 20.000, Pasos: 846, Mean Reward Calculado: 0.023641 (Recompensa/Pasos)\n",
      "   61387/5000000: episode: 102, duration: 51.989s, episode steps: 846, steps per second:  16, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.739 [0.000, 5.000],  loss: 0.011185, mae: 0.047671, mean_q: 0.065972\n",
      "📈 Episodio 103: Recompensa total (clipped): 13.000, Pasos: 702, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
      "   62089/5000000: episode: 103, duration: 43.657s, episode steps: 702, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.013283, mae: 0.054783, mean_q: 0.075446\n",
      "📈 Episodio 104: Recompensa total (clipped): 27.000, Pasos: 828, Mean Reward Calculado: 0.032609 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.58 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.6_ep104.h5f\n",
      "   62917/5000000: episode: 104, duration: 51.899s, episode steps: 828, steps per second:  16, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.011357, mae: 0.047362, mean_q: 0.068441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 105: Recompensa total (clipped): 14.000, Pasos: 681, Mean Reward Calculado: 0.020558 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.60 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.6_ep105.h5f\n",
      "   63598/5000000: episode: 105, duration: 43.804s, episode steps: 681, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010241, mae: 0.045374, mean_q: 0.065608\n",
      "📈 Episodio 106: Recompensa total (clipped): 10.000, Pasos: 626, Mean Reward Calculado: 0.015974 (Recompensa/Pasos)\n",
      "   64224/5000000: episode: 106, duration: 39.593s, episode steps: 626, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.011472, mae: 0.050803, mean_q: 0.071816\n",
      "📈 Episodio 107: Recompensa total (clipped): 5.000, Pasos: 311, Mean Reward Calculado: 0.016077 (Recompensa/Pasos)\n",
      "   64535/5000000: episode: 107, duration: 19.852s, episode steps: 311, steps per second:  16, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.011157, mae: 0.047905, mean_q: 0.073549\n",
      "📈 Episodio 108: Recompensa total (clipped): 17.000, Pasos: 616, Mean Reward Calculado: 0.027597 (Recompensa/Pasos)\n",
      "   65151/5000000: episode: 108, duration: 39.301s, episode steps: 616, steps per second:  16, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.009740, mae: 0.048351, mean_q: 0.071210\n",
      "📈 Episodio 109: Recompensa total (clipped): 17.000, Pasos: 661, Mean Reward Calculado: 0.025719 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.67 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.7_ep109.h5f\n",
      "   65812/5000000: episode: 109, duration: 40.830s, episode steps: 661, steps per second:  16, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.010812, mae: 0.048988, mean_q: 0.067325\n",
      "📈 Episodio 110: Recompensa total (clipped): 13.000, Pasos: 496, Mean Reward Calculado: 0.026210 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.69 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.7_ep110.h5f\n",
      "   66308/5000000: episode: 110, duration: 31.529s, episode steps: 496, steps per second:  16, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.131 [0.000, 5.000],  loss: 0.011037, mae: 0.050873, mean_q: 0.071766\n",
      "📈 Episodio 111: Recompensa total (clipped): 9.000, Pasos: 485, Mean Reward Calculado: 0.018557 (Recompensa/Pasos)\n",
      "   66793/5000000: episode: 111, duration: 30.948s, episode steps: 485, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.342 [0.000, 5.000],  loss: 0.011463, mae: 0.052880, mean_q: 0.072433\n",
      "📈 Episodio 112: Recompensa total (clipped): 6.000, Pasos: 461, Mean Reward Calculado: 0.013015 (Recompensa/Pasos)\n",
      "   67254/5000000: episode: 112, duration: 28.637s, episode steps: 461, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.011821, mae: 0.048783, mean_q: 0.071296\n",
      "📈 Episodio 113: Recompensa total (clipped): 11.000, Pasos: 493, Mean Reward Calculado: 0.022312 (Recompensa/Pasos)\n",
      "   67747/5000000: episode: 113, duration: 31.119s, episode steps: 493, steps per second:  16, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.010845, mae: 0.052655, mean_q: 0.074009\n",
      "📈 Episodio 114: Recompensa total (clipped): 19.000, Pasos: 768, Mean Reward Calculado: 0.024740 (Recompensa/Pasos)\n",
      "   68515/5000000: episode: 114, duration: 47.748s, episode steps: 768, steps per second:  16, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011043, mae: 0.052744, mean_q: 0.075566\n",
      "📈 Episodio 115: Recompensa total (clipped): 7.000, Pasos: 480, Mean Reward Calculado: 0.014583 (Recompensa/Pasos)\n",
      "   68995/5000000: episode: 115, duration: 31.688s, episode steps: 480, steps per second:  15, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.011888, mae: 0.051365, mean_q: 0.071716\n",
      "📈 Episodio 116: Recompensa total (clipped): 3.000, Pasos: 295, Mean Reward Calculado: 0.010169 (Recompensa/Pasos)\n",
      "   69290/5000000: episode: 116, duration: 17.603s, episode steps: 295, steps per second:  17, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.011837, mae: 0.052748, mean_q: 0.071977\n",
      "📈 Episodio 117: Recompensa total (clipped): 5.000, Pasos: 394, Mean Reward Calculado: 0.012690 (Recompensa/Pasos)\n",
      "   69684/5000000: episode: 117, duration: 25.657s, episode steps: 394, steps per second:  15, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.010628, mae: 0.048712, mean_q: 0.072077\n",
      "📈 Episodio 118: Recompensa total (clipped): 11.000, Pasos: 375, Mean Reward Calculado: 0.029333 (Recompensa/Pasos)\n",
      "   70059/5000000: episode: 118, duration: 25.353s, episode steps: 375, steps per second:  15, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.829 [0.000, 5.000],  loss: 0.009805, mae: 0.048243, mean_q: 0.069838\n",
      "📈 Episodio 119: Recompensa total (clipped): 15.000, Pasos: 594, Mean Reward Calculado: 0.025253 (Recompensa/Pasos)\n",
      "   70653/5000000: episode: 119, duration: 37.779s, episode steps: 594, steps per second:  16, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.009884, mae: 0.072373, mean_q: 0.103516\n",
      "📈 Episodio 120: Recompensa total (clipped): 24.000, Pasos: 875, Mean Reward Calculado: 0.027429 (Recompensa/Pasos)\n",
      "   71528/5000000: episode: 120, duration: 55.845s, episode steps: 875, steps per second:  16, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.010857, mae: 0.075072, mean_q: 0.109315\n",
      "📈 Episodio 121: Recompensa total (clipped): 12.000, Pasos: 494, Mean Reward Calculado: 0.024291 (Recompensa/Pasos)\n",
      "   72022/5000000: episode: 121, duration: 30.834s, episode steps: 494, steps per second:  16, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.009269, mae: 0.070269, mean_q: 0.099930\n",
      "📈 Episodio 122: Recompensa total (clipped): 8.000, Pasos: 405, Mean Reward Calculado: 0.019753 (Recompensa/Pasos)\n",
      "   72427/5000000: episode: 122, duration: 24.935s, episode steps: 405, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.622 [0.000, 5.000],  loss: 0.010374, mae: 0.073616, mean_q: 0.103833\n",
      "📈 Episodio 123: Recompensa total (clipped): 5.000, Pasos: 305, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
      "   72732/5000000: episode: 123, duration: 19.811s, episode steps: 305, steps per second:  15, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.446 [0.000, 5.000],  loss: 0.010404, mae: 0.077355, mean_q: 0.105908\n",
      "📈 Episodio 124: Recompensa total (clipped): 13.000, Pasos: 614, Mean Reward Calculado: 0.021173 (Recompensa/Pasos)\n",
      "   73346/5000000: episode: 124, duration: 39.219s, episode steps: 614, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.009803, mae: 0.072207, mean_q: 0.103764\n",
      "📈 Episodio 125: Recompensa total (clipped): 9.000, Pasos: 325, Mean Reward Calculado: 0.027692 (Recompensa/Pasos)\n",
      "   73671/5000000: episode: 125, duration: 20.911s, episode steps: 325, steps per second:  16, episode reward:  9.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.008827, mae: 0.067827, mean_q: 0.096326\n",
      "📈 Episodio 126: Recompensa total (clipped): 6.000, Pasos: 429, Mean Reward Calculado: 0.013986 (Recompensa/Pasos)\n",
      "   74100/5000000: episode: 126, duration: 25.931s, episode steps: 429, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.011109, mae: 0.076530, mean_q: 0.106828\n",
      "📈 Episodio 127: Recompensa total (clipped): 5.000, Pasos: 493, Mean Reward Calculado: 0.010142 (Recompensa/Pasos)\n",
      "   74593/5000000: episode: 127, duration: 29.772s, episode steps: 493, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.010712, mae: 0.072405, mean_q: 0.098274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 128: Recompensa total (clipped): 18.000, Pasos: 944, Mean Reward Calculado: 0.019068 (Recompensa/Pasos)\n",
      "   75537/5000000: episode: 128, duration: 58.056s, episode steps: 944, steps per second:  16, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.010848, mae: 0.073765, mean_q: 0.104114\n",
      "📈 Episodio 129: Recompensa total (clipped): 13.000, Pasos: 567, Mean Reward Calculado: 0.022928 (Recompensa/Pasos)\n",
      "   76104/5000000: episode: 129, duration: 34.749s, episode steps: 567, steps per second:  16, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.009801, mae: 0.074698, mean_q: 0.105959\n",
      "📈 Episodio 130: Recompensa total (clipped): 5.000, Pasos: 303, Mean Reward Calculado: 0.016502 (Recompensa/Pasos)\n",
      "   76407/5000000: episode: 130, duration: 18.799s, episode steps: 303, steps per second:  16, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.686 [0.000, 5.000],  loss: 0.010987, mae: 0.075175, mean_q: 0.103486\n",
      "📈 Episodio 131: Recompensa total (clipped): 6.000, Pasos: 394, Mean Reward Calculado: 0.015228 (Recompensa/Pasos)\n",
      "   76801/5000000: episode: 131, duration: 24.887s, episode steps: 394, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.145 [0.000, 5.000],  loss: 0.009920, mae: 0.073473, mean_q: 0.100756\n",
      "📈 Episodio 132: Recompensa total (clipped): 21.000, Pasos: 935, Mean Reward Calculado: 0.022460 (Recompensa/Pasos)\n",
      "   77736/5000000: episode: 132, duration: 60.003s, episode steps: 935, steps per second:  16, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.009152, mae: 0.072504, mean_q: 0.103625\n",
      "📈 Episodio 133: Recompensa total (clipped): 19.000, Pasos: 768, Mean Reward Calculado: 0.024740 (Recompensa/Pasos)\n",
      "   78504/5000000: episode: 133, duration: 51.419s, episode steps: 768, steps per second:  15, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.009651, mae: 0.072069, mean_q: 0.101063\n",
      "📈 Episodio 134: Recompensa total (clipped): 25.000, Pasos: 702, Mean Reward Calculado: 0.035613 (Recompensa/Pasos)\n",
      "   79206/5000000: episode: 134, duration: 45.508s, episode steps: 702, steps per second:  15, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.008583, mae: 0.068623, mean_q: 0.096546\n",
      "📈 Episodio 135: Recompensa total (clipped): 8.000, Pasos: 492, Mean Reward Calculado: 0.016260 (Recompensa/Pasos)\n",
      "   79698/5000000: episode: 135, duration: 31.095s, episode steps: 492, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.055 [0.000, 5.000],  loss: 0.010248, mae: 0.074889, mean_q: 0.103200\n",
      "📈 Episodio 136: Recompensa total (clipped): 3.000, Pasos: 291, Mean Reward Calculado: 0.010309 (Recompensa/Pasos)\n",
      "   79989/5000000: episode: 136, duration: 17.683s, episode steps: 291, steps per second:  16, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.010197, mae: 0.071217, mean_q: 0.099250\n",
      "📊 Paso 80,000/5,000,000 (1.6%) - 35.8 pasos/seg - ETA: 38.1h - Memoria: 1113.55 MB\n",
      "📈 Episodio 137: Recompensa total (clipped): 16.000, Pasos: 684, Mean Reward Calculado: 0.023392 (Recompensa/Pasos)\n",
      "   80673/5000000: episode: 137, duration: 43.300s, episode steps: 684, steps per second:  16, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.010834, mae: 0.113631, mean_q: 0.156832\n",
      "📈 Episodio 138: Recompensa total (clipped): 18.000, Pasos: 769, Mean Reward Calculado: 0.023407 (Recompensa/Pasos)\n",
      "   81442/5000000: episode: 138, duration: 47.026s, episode steps: 769, steps per second:  16, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.011678, mae: 0.113565, mean_q: 0.157619\n",
      "📈 Episodio 139: Recompensa total (clipped): 12.000, Pasos: 602, Mean Reward Calculado: 0.019934 (Recompensa/Pasos)\n",
      "   82044/5000000: episode: 139, duration: 37.895s, episode steps: 602, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.009872, mae: 0.108955, mean_q: 0.147770\n",
      "📈 Episodio 140: Recompensa total (clipped): 14.000, Pasos: 609, Mean Reward Calculado: 0.022989 (Recompensa/Pasos)\n",
      "   82653/5000000: episode: 140, duration: 38.618s, episode steps: 609, steps per second:  16, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.009306, mae: 0.111792, mean_q: 0.151818\n",
      "📈 Episodio 141: Recompensa total (clipped): 12.000, Pasos: 544, Mean Reward Calculado: 0.022059 (Recompensa/Pasos)\n",
      "   83197/5000000: episode: 141, duration: 34.822s, episode steps: 544, steps per second:  16, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.010803, mae: 0.111508, mean_q: 0.148925\n",
      "📈 Episodio 142: Recompensa total (clipped): 15.000, Pasos: 520, Mean Reward Calculado: 0.028846 (Recompensa/Pasos)\n",
      "   83717/5000000: episode: 142, duration: 31.933s, episode steps: 520, steps per second:  16, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.010630, mae: 0.107525, mean_q: 0.141948\n",
      "📈 Episodio 143: Recompensa total (clipped): 15.000, Pasos: 609, Mean Reward Calculado: 0.024631 (Recompensa/Pasos)\n",
      "   84326/5000000: episode: 143, duration: 36.440s, episode steps: 609, steps per second:  17, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.007310, mae: 0.103166, mean_q: 0.138182\n",
      "📈 Episodio 144: Recompensa total (clipped): 19.000, Pasos: 673, Mean Reward Calculado: 0.028232 (Recompensa/Pasos)\n",
      "   84999/5000000: episode: 144, duration: 40.991s, episode steps: 673, steps per second:  16, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011142, mae: 0.115440, mean_q: 0.151482\n",
      "📈 Episodio 145: Recompensa total (clipped): 2.000, Pasos: 255, Mean Reward Calculado: 0.007843 (Recompensa/Pasos)\n",
      "   85254/5000000: episode: 145, duration: 16.467s, episode steps: 255, steps per second:  15, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.227 [0.000, 5.000],  loss: 0.007658, mae: 0.104190, mean_q: 0.139997\n",
      "📈 Episodio 146: Recompensa total (clipped): 15.000, Pasos: 626, Mean Reward Calculado: 0.023962 (Recompensa/Pasos)\n",
      "   85880/5000000: episode: 146, duration: 39.745s, episode steps: 626, steps per second:  16, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.010586, mae: 0.111507, mean_q: 0.150572\n",
      "📈 Episodio 147: Recompensa total (clipped): 12.000, Pasos: 482, Mean Reward Calculado: 0.024896 (Recompensa/Pasos)\n",
      "   86362/5000000: episode: 147, duration: 29.965s, episode steps: 482, steps per second:  16, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.266 [0.000, 5.000],  loss: 0.009901, mae: 0.111227, mean_q: 0.153179\n",
      "📈 Episodio 148: Recompensa total (clipped): 8.000, Pasos: 257, Mean Reward Calculado: 0.031128 (Recompensa/Pasos)\n",
      "   86619/5000000: episode: 148, duration: 16.297s, episode steps: 257, steps per second:  16, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.009408, mae: 0.107725, mean_q: 0.144180\n",
      "📈 Episodio 149: Recompensa total (clipped): 6.000, Pasos: 375, Mean Reward Calculado: 0.016000 (Recompensa/Pasos)\n",
      "   86994/5000000: episode: 149, duration: 23.792s, episode steps: 375, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.008457, mae: 0.106001, mean_q: 0.144199\n",
      "📈 Episodio 150: Recompensa total (clipped): 16.000, Pasos: 888, Mean Reward Calculado: 0.018018 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 150 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 16.00\n",
      "   Media últimos 100: 13.20 / 20.0\n",
      "   Mejor promedio histórico: 14.69\n",
      "   Estado: 📈 66.0% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "   87882/5000000: episode: 150, duration: 57.561s, episode steps: 888, steps per second:  15, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.009054, mae: 0.109401, mean_q: 0.148249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 151: Recompensa total (clipped): 13.000, Pasos: 604, Mean Reward Calculado: 0.021523 (Recompensa/Pasos)\n",
      "   88486/5000000: episode: 151, duration: 35.497s, episode steps: 604, steps per second:  17, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.008617, mae: 0.109476, mean_q: 0.148696\n",
      "📈 Episodio 152: Recompensa total (clipped): 9.000, Pasos: 404, Mean Reward Calculado: 0.022277 (Recompensa/Pasos)\n",
      "   88890/5000000: episode: 152, duration: 23.361s, episode steps: 404, steps per second:  17, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.480 [0.000, 5.000],  loss: 0.008078, mae: 0.103216, mean_q: 0.142767\n",
      "📈 Episodio 153: Recompensa total (clipped): 22.000, Pasos: 628, Mean Reward Calculado: 0.035032 (Recompensa/Pasos)\n",
      "   89518/5000000: episode: 153, duration: 37.685s, episode steps: 628, steps per second:  17, episode reward: 22.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.008725, mae: 0.107934, mean_q: 0.147818\n",
      "📈 Episodio 154: Recompensa total (clipped): 7.000, Pasos: 298, Mean Reward Calculado: 0.023490 (Recompensa/Pasos)\n",
      "   89816/5000000: episode: 154, duration: 17.487s, episode steps: 298, steps per second:  17, episode reward:  7.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.007408, mae: 0.101252, mean_q: 0.136677\n",
      "📈 Episodio 155: Recompensa total (clipped): 9.000, Pasos: 297, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
      "   90113/5000000: episode: 155, duration: 17.549s, episode steps: 297, steps per second:  17, episode reward:  9.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.862 [0.000, 5.000],  loss: 0.008347, mae: 0.108777, mean_q: 0.144854\n",
      "📈 Episodio 156: Recompensa total (clipped): 11.000, Pasos: 758, Mean Reward Calculado: 0.014512 (Recompensa/Pasos)\n",
      "   90871/5000000: episode: 156, duration: 43.840s, episode steps: 758, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.010502, mae: 0.129334, mean_q: 0.174326\n",
      "📈 Episodio 157: Recompensa total (clipped): 6.000, Pasos: 289, Mean Reward Calculado: 0.020761 (Recompensa/Pasos)\n",
      "   91160/5000000: episode: 157, duration: 17.400s, episode steps: 289, steps per second:  17, episode reward:  6.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.010450, mae: 0.129632, mean_q: 0.171506\n",
      "📈 Episodio 158: Recompensa total (clipped): 7.000, Pasos: 500, Mean Reward Calculado: 0.014000 (Recompensa/Pasos)\n",
      "   91660/5000000: episode: 158, duration: 30.708s, episode steps: 500, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.010604, mae: 0.135976, mean_q: 0.179987\n",
      "📈 Episodio 159: Recompensa total (clipped): 12.000, Pasos: 575, Mean Reward Calculado: 0.020870 (Recompensa/Pasos)\n",
      "   92235/5000000: episode: 159, duration: 34.345s, episode steps: 575, steps per second:  17, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.009022, mae: 0.130404, mean_q: 0.176727\n",
      "📈 Episodio 160: Recompensa total (clipped): 6.000, Pasos: 299, Mean Reward Calculado: 0.020067 (Recompensa/Pasos)\n",
      "   92534/5000000: episode: 160, duration: 19.757s, episode steps: 299, steps per second:  15, episode reward:  6.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.007300, mae: 0.130098, mean_q: 0.175140\n",
      "📈 Episodio 161: Recompensa total (clipped): 9.000, Pasos: 399, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
      "   92933/5000000: episode: 161, duration: 26.442s, episode steps: 399, steps per second:  15, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.009852, mae: 0.126695, mean_q: 0.169238\n",
      "📈 Episodio 162: Recompensa total (clipped): 13.000, Pasos: 518, Mean Reward Calculado: 0.025097 (Recompensa/Pasos)\n",
      "   93451/5000000: episode: 162, duration: 31.959s, episode steps: 518, steps per second:  16, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.008213, mae: 0.126181, mean_q: 0.169899\n",
      "📈 Episodio 163: Recompensa total (clipped): 16.000, Pasos: 489, Mean Reward Calculado: 0.032720 (Recompensa/Pasos)\n",
      "   93940/5000000: episode: 163, duration: 30.692s, episode steps: 489, steps per second:  16, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.008944, mae: 0.126344, mean_q: 0.170030\n",
      "📈 Episodio 164: Recompensa total (clipped): 8.000, Pasos: 436, Mean Reward Calculado: 0.018349 (Recompensa/Pasos)\n",
      "   94376/5000000: episode: 164, duration: 28.074s, episode steps: 436, steps per second:  16, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009077, mae: 0.126238, mean_q: 0.170618\n",
      "📈 Episodio 165: Recompensa total (clipped): 18.000, Pasos: 781, Mean Reward Calculado: 0.023047 (Recompensa/Pasos)\n",
      "   95157/5000000: episode: 165, duration: 49.134s, episode steps: 781, steps per second:  16, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.010397, mae: 0.134244, mean_q: 0.178882\n",
      "📈 Episodio 166: Recompensa total (clipped): 6.000, Pasos: 285, Mean Reward Calculado: 0.021053 (Recompensa/Pasos)\n",
      "   95442/5000000: episode: 166, duration: 16.937s, episode steps: 285, steps per second:  17, episode reward:  6.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.008994, mae: 0.132217, mean_q: 0.178644\n",
      "📈 Episodio 167: Recompensa total (clipped): 9.000, Pasos: 520, Mean Reward Calculado: 0.017308 (Recompensa/Pasos)\n",
      "   95962/5000000: episode: 167, duration: 32.416s, episode steps: 520, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.009373, mae: 0.126290, mean_q: 0.167904\n",
      "📈 Episodio 168: Recompensa total (clipped): 16.000, Pasos: 594, Mean Reward Calculado: 0.026936 (Recompensa/Pasos)\n",
      "   96556/5000000: episode: 168, duration: 35.867s, episode steps: 594, steps per second:  17, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.007540, mae: 0.117332, mean_q: 0.156781\n",
      "📈 Episodio 169: Recompensa total (clipped): 21.000, Pasos: 838, Mean Reward Calculado: 0.025060 (Recompensa/Pasos)\n",
      "   97394/5000000: episode: 169, duration: 49.615s, episode steps: 838, steps per second:  17, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.007781, mae: 0.123004, mean_q: 0.162638\n",
      "📈 Episodio 170: Recompensa total (clipped): 4.000, Pasos: 290, Mean Reward Calculado: 0.013793 (Recompensa/Pasos)\n",
      "   97684/5000000: episode: 170, duration: 18.024s, episode steps: 290, steps per second:  16, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.141 [0.000, 5.000],  loss: 0.009065, mae: 0.125405, mean_q: 0.167552\n",
      "📈 Episodio 171: Recompensa total (clipped): 14.000, Pasos: 560, Mean Reward Calculado: 0.025000 (Recompensa/Pasos)\n",
      "   98244/5000000: episode: 171, duration: 34.708s, episode steps: 560, steps per second:  16, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.518 [0.000, 5.000],  loss: 0.005927, mae: 0.116488, mean_q: 0.157211\n",
      "📈 Episodio 172: Recompensa total (clipped): 15.000, Pasos: 1134, Mean Reward Calculado: 0.013228 (Recompensa/Pasos)\n",
      "   99378/5000000: episode: 172, duration: 72.765s, episode steps: 1134, steps per second:  16, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.656 [0.000, 5.000],  loss: 0.007443, mae: 0.123727, mean_q: 0.165095\n",
      "📈 Episodio 173: Recompensa total (clipped): 2.000, Pasos: 401, Mean Reward Calculado: 0.004988 (Recompensa/Pasos)\n",
      "   99779/5000000: episode: 173, duration: 25.575s, episode steps: 401, steps per second:  16, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.007876, mae: 0.122769, mean_q: 0.164512\n",
      "📊 Paso 100,000/5,000,000 (2.0%) - 28.8 pasos/seg - ETA: 47.2h - Memoria: 1294.82 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 174: Recompensa total (clipped): 9.000, Pasos: 502, Mean Reward Calculado: 0.017928 (Recompensa/Pasos)\n",
      "  100281/5000000: episode: 174, duration: 32.411s, episode steps: 502, steps per second:  15, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.008686, mae: 0.145304, mean_q: 0.195177\n",
      "📈 Episodio 175: Recompensa total (clipped): 4.000, Pasos: 292, Mean Reward Calculado: 0.013699 (Recompensa/Pasos)\n",
      "  100573/5000000: episode: 175, duration: 18.420s, episode steps: 292, steps per second:  16, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.011072, mae: 0.158812, mean_q: 0.212269\n",
      "📈 Episodio 176: Recompensa total (clipped): 19.000, Pasos: 805, Mean Reward Calculado: 0.023602 (Recompensa/Pasos)\n",
      "  101378/5000000: episode: 176, duration: 49.648s, episode steps: 805, steps per second:  16, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.009633, mae: 0.160749, mean_q: 0.214108\n",
      "📈 Episodio 177: Recompensa total (clipped): 17.000, Pasos: 809, Mean Reward Calculado: 0.021014 (Recompensa/Pasos)\n",
      "  102187/5000000: episode: 177, duration: 52.515s, episode steps: 809, steps per second:  15, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.008361, mae: 0.156496, mean_q: 0.209018\n",
      "📈 Episodio 178: Recompensa total (clipped): 17.000, Pasos: 615, Mean Reward Calculado: 0.027642 (Recompensa/Pasos)\n",
      "  102802/5000000: episode: 178, duration: 42.291s, episode steps: 615, steps per second:  15, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.007466, mae: 0.155361, mean_q: 0.206380\n",
      "📈 Episodio 179: Recompensa total (clipped): 4.000, Pasos: 300, Mean Reward Calculado: 0.013333 (Recompensa/Pasos)\n",
      "  103102/5000000: episode: 179, duration: 19.841s, episode steps: 300, steps per second:  15, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.007181, mae: 0.156160, mean_q: 0.205602\n",
      "📈 Episodio 180: Recompensa total (clipped): 4.000, Pasos: 444, Mean Reward Calculado: 0.009009 (Recompensa/Pasos)\n",
      "  103546/5000000: episode: 180, duration: 29.691s, episode steps: 444, steps per second:  15, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.008009, mae: 0.156231, mean_q: 0.207142\n",
      "📈 Episodio 181: Recompensa total (clipped): 7.000, Pasos: 470, Mean Reward Calculado: 0.014894 (Recompensa/Pasos)\n",
      "  104016/5000000: episode: 181, duration: 31.559s, episode steps: 470, steps per second:  15, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.008354, mae: 0.153381, mean_q: 0.202108\n",
      "📈 Episodio 182: Recompensa total (clipped): 4.000, Pasos: 375, Mean Reward Calculado: 0.010667 (Recompensa/Pasos)\n",
      "  104391/5000000: episode: 182, duration: 24.210s, episode steps: 375, steps per second:  15, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.008457, mae: 0.160046, mean_q: 0.211516\n",
      "📈 Episodio 183: Recompensa total (clipped): 20.000, Pasos: 691, Mean Reward Calculado: 0.028944 (Recompensa/Pasos)\n",
      "  105082/5000000: episode: 183, duration: 42.255s, episode steps: 691, steps per second:  16, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.007746, mae: 0.151875, mean_q: 0.199798\n",
      "📈 Episodio 184: Recompensa total (clipped): 7.000, Pasos: 375, Mean Reward Calculado: 0.018667 (Recompensa/Pasos)\n",
      "  105457/5000000: episode: 184, duration: 23.390s, episode steps: 375, steps per second:  16, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.008469, mae: 0.150122, mean_q: 0.196122\n",
      "📈 Episodio 185: Recompensa total (clipped): 22.000, Pasos: 995, Mean Reward Calculado: 0.022111 (Recompensa/Pasos)\n",
      "  106452/5000000: episode: 185, duration: 61.973s, episode steps: 995, steps per second:  16, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.006849, mae: 0.152227, mean_q: 0.200329\n",
      "📈 Episodio 186: Recompensa total (clipped): 17.000, Pasos: 755, Mean Reward Calculado: 0.022517 (Recompensa/Pasos)\n",
      "  107207/5000000: episode: 186, duration: 47.117s, episode steps: 755, steps per second:  16, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.007341, mae: 0.150927, mean_q: 0.198472\n",
      "📈 Episodio 187: Recompensa total (clipped): 15.000, Pasos: 507, Mean Reward Calculado: 0.029586 (Recompensa/Pasos)\n",
      "  107714/5000000: episode: 187, duration: 31.808s, episode steps: 507, steps per second:  16, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.009382, mae: 0.157464, mean_q: 0.207993\n",
      "📈 Episodio 188: Recompensa total (clipped): 18.000, Pasos: 869, Mean Reward Calculado: 0.020713 (Recompensa/Pasos)\n",
      "  108583/5000000: episode: 188, duration: 54.607s, episode steps: 869, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.007274, mae: 0.152076, mean_q: 0.201363\n",
      "📈 Episodio 189: Recompensa total (clipped): 7.000, Pasos: 312, Mean Reward Calculado: 0.022436 (Recompensa/Pasos)\n",
      "  108895/5000000: episode: 189, duration: 20.391s, episode steps: 312, steps per second:  15, episode reward:  7.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.006312, mae: 0.152261, mean_q: 0.200473\n",
      "📈 Episodio 190: Recompensa total (clipped): 7.000, Pasos: 432, Mean Reward Calculado: 0.016204 (Recompensa/Pasos)\n",
      "  109327/5000000: episode: 190, duration: 27.455s, episode steps: 432, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.006863, mae: 0.146808, mean_q: 0.191774\n",
      "📈 Episodio 191: Recompensa total (clipped): 12.000, Pasos: 487, Mean Reward Calculado: 0.024641 (Recompensa/Pasos)\n",
      "  109814/5000000: episode: 191, duration: 32.403s, episode steps: 487, steps per second:  15, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.544 [0.000, 5.000],  loss: 0.006598, mae: 0.156172, mean_q: 0.205908\n",
      "📈 Episodio 192: Recompensa total (clipped): 4.000, Pasos: 278, Mean Reward Calculado: 0.014388 (Recompensa/Pasos)\n",
      "  110092/5000000: episode: 192, duration: 18.052s, episode steps: 278, steps per second:  15, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.007138, mae: 0.156419, mean_q: 0.208031\n",
      "📈 Episodio 193: Recompensa total (clipped): 15.000, Pasos: 594, Mean Reward Calculado: 0.025253 (Recompensa/Pasos)\n",
      "  110686/5000000: episode: 193, duration: 37.127s, episode steps: 594, steps per second:  16, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.980 [0.000, 5.000],  loss: 0.007786, mae: 0.178915, mean_q: 0.234494\n",
      "📈 Episodio 194: Recompensa total (clipped): 5.000, Pasos: 374, Mean Reward Calculado: 0.013369 (Recompensa/Pasos)\n",
      "  111060/5000000: episode: 194, duration: 23.622s, episode steps: 374, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.007379, mae: 0.171727, mean_q: 0.223946\n",
      "📈 Episodio 195: Recompensa total (clipped): 11.000, Pasos: 517, Mean Reward Calculado: 0.021277 (Recompensa/Pasos)\n",
      "  111577/5000000: episode: 195, duration: 32.006s, episode steps: 517, steps per second:  16, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.009207, mae: 0.176009, mean_q: 0.228775\n",
      "📈 Episodio 196: Recompensa total (clipped): 7.000, Pasos: 414, Mean Reward Calculado: 0.016908 (Recompensa/Pasos)\n",
      "  111991/5000000: episode: 196, duration: 25.655s, episode steps: 414, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.007714, mae: 0.181070, mean_q: 0.238228\n",
      "📈 Episodio 197: Recompensa total (clipped): 8.000, Pasos: 504, Mean Reward Calculado: 0.015873 (Recompensa/Pasos)\n",
      "  112495/5000000: episode: 197, duration: 30.671s, episode steps: 504, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.009661, mae: 0.180260, mean_q: 0.235834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 198: Recompensa total (clipped): 9.000, Pasos: 503, Mean Reward Calculado: 0.017893 (Recompensa/Pasos)\n",
      "  112998/5000000: episode: 198, duration: 33.881s, episode steps: 503, steps per second:  15, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.942 [0.000, 5.000],  loss: 0.007968, mae: 0.178436, mean_q: 0.233417\n",
      "📈 Episodio 199: Recompensa total (clipped): 7.000, Pasos: 379, Mean Reward Calculado: 0.018470 (Recompensa/Pasos)\n",
      "  113377/5000000: episode: 199, duration: 24.796s, episode steps: 379, steps per second:  15, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.008483, mae: 0.173124, mean_q: 0.225107\n",
      "📈 Episodio 200: Recompensa total (clipped): 9.000, Pasos: 503, Mean Reward Calculado: 0.017893 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 200 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 9.00\n",
      "   Media últimos 100: 11.50 / 20.0\n",
      "   Mejor promedio histórico: 14.69\n",
      "   Estado: 📈 57.5% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "✅ Pesos guardados en checkpoints/DDQN_weights_episode_200.h5f tras el episodio 200\n",
      "  113880/5000000: episode: 200, duration: 34.336s, episode steps: 503, steps per second:  15, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.007720, mae: 0.175411, mean_q: 0.229876\n",
      "📈 Episodio 201: Recompensa total (clipped): 11.000, Pasos: 511, Mean Reward Calculado: 0.021526 (Recompensa/Pasos)\n",
      "  114391/5000000: episode: 201, duration: 32.709s, episode steps: 511, steps per second:  16, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.007884, mae: 0.175447, mean_q: 0.230048\n",
      "📈 Episodio 202: Recompensa total (clipped): 9.000, Pasos: 522, Mean Reward Calculado: 0.017241 (Recompensa/Pasos)\n",
      "  114913/5000000: episode: 202, duration: 31.636s, episode steps: 522, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.007395, mae: 0.165843, mean_q: 0.217652\n",
      "📈 Episodio 203: Recompensa total (clipped): 10.000, Pasos: 741, Mean Reward Calculado: 0.013495 (Recompensa/Pasos)\n",
      "  115654/5000000: episode: 203, duration: 47.252s, episode steps: 741, steps per second:  16, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.007237, mae: 0.176134, mean_q: 0.231611\n",
      "📈 Episodio 204: Recompensa total (clipped): 7.000, Pasos: 520, Mean Reward Calculado: 0.013462 (Recompensa/Pasos)\n",
      "  116174/5000000: episode: 204, duration: 34.588s, episode steps: 520, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.008191, mae: 0.174072, mean_q: 0.227044\n",
      "📈 Episodio 205: Recompensa total (clipped): 4.000, Pasos: 407, Mean Reward Calculado: 0.009828 (Recompensa/Pasos)\n",
      "  116581/5000000: episode: 205, duration: 26.032s, episode steps: 407, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.007789, mae: 0.176186, mean_q: 0.230252\n",
      "📈 Episodio 206: Recompensa total (clipped): 9.000, Pasos: 459, Mean Reward Calculado: 0.019608 (Recompensa/Pasos)\n",
      "  117040/5000000: episode: 206, duration: 30.751s, episode steps: 459, steps per second:  15, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.006653, mae: 0.170822, mean_q: 0.222380\n",
      "📈 Episodio 207: Recompensa total (clipped): 24.000, Pasos: 925, Mean Reward Calculado: 0.025946 (Recompensa/Pasos)\n",
      "  117965/5000000: episode: 207, duration: 60.595s, episode steps: 925, steps per second:  15, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.007794, mae: 0.174548, mean_q: 0.226767\n",
      "📈 Episodio 208: Recompensa total (clipped): 17.000, Pasos: 798, Mean Reward Calculado: 0.021303 (Recompensa/Pasos)\n",
      "  118763/5000000: episode: 208, duration: 50.044s, episode steps: 798, steps per second:  16, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.006434, mae: 0.160956, mean_q: 0.211173\n",
      "📈 Episodio 209: Recompensa total (clipped): 20.000, Pasos: 876, Mean Reward Calculado: 0.022831 (Recompensa/Pasos)\n",
      "  119639/5000000: episode: 209, duration: 56.730s, episode steps: 876, steps per second:  15, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.006746, mae: 0.174376, mean_q: 0.227211\n",
      "📊 Paso 120,000/5,000,000 (2.4%) - 25.3 pasos/seg - ETA: 53.7h - Memoria: 1476.98 MB\n",
      "📈 Episodio 210: Recompensa total (clipped): 9.000, Pasos: 515, Mean Reward Calculado: 0.017476 (Recompensa/Pasos)\n",
      "  120154/5000000: episode: 210, duration: 32.881s, episode steps: 515, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.006383, mae: 0.178412, mean_q: 0.232334\n",
      "📈 Episodio 211: Recompensa total (clipped): 12.000, Pasos: 467, Mean Reward Calculado: 0.025696 (Recompensa/Pasos)\n",
      "  120621/5000000: episode: 211, duration: 29.760s, episode steps: 467, steps per second:  16, episode reward: 12.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.008992, mae: 0.194833, mean_q: 0.250491\n",
      "📈 Episodio 212: Recompensa total (clipped): 7.000, Pasos: 407, Mean Reward Calculado: 0.017199 (Recompensa/Pasos)\n",
      "  121028/5000000: episode: 212, duration: 25.317s, episode steps: 407, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.007253, mae: 0.194859, mean_q: 0.251212\n",
      "📈 Episodio 213: Recompensa total (clipped): 11.000, Pasos: 472, Mean Reward Calculado: 0.023305 (Recompensa/Pasos)\n",
      "  121500/5000000: episode: 213, duration: 29.115s, episode steps: 472, steps per second:  16, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.790 [0.000, 5.000],  loss: 0.007336, mae: 0.198318, mean_q: 0.257159\n",
      "📈 Episodio 214: Recompensa total (clipped): 16.000, Pasos: 655, Mean Reward Calculado: 0.024427 (Recompensa/Pasos)\n",
      "  122155/5000000: episode: 214, duration: 40.633s, episode steps: 655, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.007536, mae: 0.193779, mean_q: 0.251719\n",
      "📈 Episodio 215: Recompensa total (clipped): 17.000, Pasos: 611, Mean Reward Calculado: 0.027823 (Recompensa/Pasos)\n",
      "  122766/5000000: episode: 215, duration: 38.650s, episode steps: 611, steps per second:  16, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.007730, mae: 0.189758, mean_q: 0.248789\n",
      "📈 Episodio 216: Recompensa total (clipped): 14.000, Pasos: 593, Mean Reward Calculado: 0.023609 (Recompensa/Pasos)\n",
      "  123359/5000000: episode: 216, duration: 40.516s, episode steps: 593, steps per second:  15, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.784 [0.000, 5.000],  loss: 0.008175, mae: 0.195617, mean_q: 0.253327\n",
      "📈 Episodio 217: Recompensa total (clipped): 22.000, Pasos: 665, Mean Reward Calculado: 0.033083 (Recompensa/Pasos)\n",
      "  124024/5000000: episode: 217, duration: 45.990s, episode steps: 665, steps per second:  14, episode reward: 22.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.007710, mae: 0.190295, mean_q: 0.245705\n",
      "📈 Episodio 218: Recompensa total (clipped): 15.000, Pasos: 671, Mean Reward Calculado: 0.022355 (Recompensa/Pasos)\n",
      "  124695/5000000: episode: 218, duration: 46.132s, episode steps: 671, steps per second:  15, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.498 [0.000, 5.000],  loss: 0.009134, mae: 0.204434, mean_q: 0.261981\n",
      "📈 Episodio 219: Recompensa total (clipped): 16.000, Pasos: 569, Mean Reward Calculado: 0.028120 (Recompensa/Pasos)\n",
      "  125264/5000000: episode: 219, duration: 39.818s, episode steps: 569, steps per second:  14, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.009198, mae: 0.193327, mean_q: 0.249626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 220: Recompensa total (clipped): 17.000, Pasos: 501, Mean Reward Calculado: 0.033932 (Recompensa/Pasos)\n",
      "  125765/5000000: episode: 220, duration: 34.630s, episode steps: 501, steps per second:  14, episode reward: 17.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.007708, mae: 0.196541, mean_q: 0.253792\n",
      "📈 Episodio 221: Recompensa total (clipped): 13.000, Pasos: 583, Mean Reward Calculado: 0.022298 (Recompensa/Pasos)\n",
      "  126348/5000000: episode: 221, duration: 40.387s, episode steps: 583, steps per second:  14, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.007378, mae: 0.196388, mean_q: 0.252216\n",
      "📈 Episodio 222: Recompensa total (clipped): 6.000, Pasos: 444, Mean Reward Calculado: 0.013514 (Recompensa/Pasos)\n",
      "  126792/5000000: episode: 222, duration: 30.633s, episode steps: 444, steps per second:  14, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.008336, mae: 0.198654, mean_q: 0.254623\n",
      "📈 Episodio 223: Recompensa total (clipped): 15.000, Pasos: 505, Mean Reward Calculado: 0.029703 (Recompensa/Pasos)\n",
      "  127297/5000000: episode: 223, duration: 34.290s, episode steps: 505, steps per second:  15, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007049, mae: 0.192971, mean_q: 0.247918\n",
      "📈 Episodio 224: Recompensa total (clipped): 8.000, Pasos: 455, Mean Reward Calculado: 0.017582 (Recompensa/Pasos)\n",
      "  127752/5000000: episode: 224, duration: 28.579s, episode steps: 455, steps per second:  16, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.132 [0.000, 5.000],  loss: 0.006321, mae: 0.183868, mean_q: 0.237321\n",
      "📈 Episodio 225: Recompensa total (clipped): 19.000, Pasos: 724, Mean Reward Calculado: 0.026243 (Recompensa/Pasos)\n",
      "  128476/5000000: episode: 225, duration: 45.444s, episode steps: 724, steps per second:  16, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.006770, mae: 0.195931, mean_q: 0.253730\n",
      "📈 Episodio 226: Recompensa total (clipped): 9.000, Pasos: 414, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
      "  128890/5000000: episode: 226, duration: 25.176s, episode steps: 414, steps per second:  16, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.006961, mae: 0.191539, mean_q: 0.247930\n",
      "📈 Episodio 227: Recompensa total (clipped): 6.000, Pasos: 355, Mean Reward Calculado: 0.016901 (Recompensa/Pasos)\n",
      "  129245/5000000: episode: 227, duration: 21.861s, episode steps: 355, steps per second:  16, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.007409, mae: 0.193438, mean_q: 0.249978\n",
      "📈 Episodio 228: Recompensa total (clipped): 20.000, Pasos: 552, Mean Reward Calculado: 0.036232 (Recompensa/Pasos)\n",
      "  129797/5000000: episode: 228, duration: 34.244s, episode steps: 552, steps per second:  16, episode reward: 20.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.006376, mae: 0.191199, mean_q: 0.246040\n",
      "📈 Episodio 229: Recompensa total (clipped): 14.000, Pasos: 706, Mean Reward Calculado: 0.019830 (Recompensa/Pasos)\n",
      "  130503/5000000: episode: 229, duration: 43.953s, episode steps: 706, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.009531, mae: 0.218214, mean_q: 0.280771\n",
      "📈 Episodio 230: Recompensa total (clipped): 3.000, Pasos: 403, Mean Reward Calculado: 0.007444 (Recompensa/Pasos)\n",
      "  130906/5000000: episode: 230, duration: 25.706s, episode steps: 403, steps per second:  16, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.603 [0.000, 5.000],  loss: 0.007950, mae: 0.217337, mean_q: 0.282612\n",
      "📈 Episodio 231: Recompensa total (clipped): 14.000, Pasos: 675, Mean Reward Calculado: 0.020741 (Recompensa/Pasos)\n",
      "  131581/5000000: episode: 231, duration: 42.272s, episode steps: 675, steps per second:  16, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.009449, mae: 0.219517, mean_q: 0.284213\n",
      "📈 Episodio 232: Recompensa total (clipped): 7.000, Pasos: 388, Mean Reward Calculado: 0.018041 (Recompensa/Pasos)\n",
      "  131969/5000000: episode: 232, duration: 23.577s, episode steps: 388, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.009338, mae: 0.223060, mean_q: 0.287297\n",
      "📈 Episodio 233: Recompensa total (clipped): 15.000, Pasos: 556, Mean Reward Calculado: 0.026978 (Recompensa/Pasos)\n",
      "  132525/5000000: episode: 233, duration: 36.170s, episode steps: 556, steps per second:  15, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008987, mae: 0.225529, mean_q: 0.292980\n",
      "📈 Episodio 234: Recompensa total (clipped): 8.000, Pasos: 402, Mean Reward Calculado: 0.019900 (Recompensa/Pasos)\n",
      "  132927/5000000: episode: 234, duration: 26.820s, episode steps: 402, steps per second:  15, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007485, mae: 0.212087, mean_q: 0.275815\n",
      "📈 Episodio 235: Recompensa total (clipped): 12.000, Pasos: 921, Mean Reward Calculado: 0.013029 (Recompensa/Pasos)\n",
      "  133848/5000000: episode: 235, duration: 59.490s, episode steps: 921, steps per second:  15, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.008871, mae: 0.221017, mean_q: 0.287815\n",
      "📈 Episodio 236: Recompensa total (clipped): 19.000, Pasos: 781, Mean Reward Calculado: 0.024328 (Recompensa/Pasos)\n",
      "  134629/5000000: episode: 236, duration: 51.338s, episode steps: 781, steps per second:  15, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.007710, mae: 0.214411, mean_q: 0.279689\n",
      "📈 Episodio 237: Recompensa total (clipped): 15.000, Pasos: 607, Mean Reward Calculado: 0.024712 (Recompensa/Pasos)\n",
      "  135236/5000000: episode: 237, duration: 38.521s, episode steps: 607, steps per second:  16, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.007604, mae: 0.213426, mean_q: 0.275875\n",
      "📈 Episodio 238: Recompensa total (clipped): 23.000, Pasos: 824, Mean Reward Calculado: 0.027913 (Recompensa/Pasos)\n",
      "  136060/5000000: episode: 238, duration: 52.656s, episode steps: 824, steps per second:  16, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007709, mae: 0.215366, mean_q: 0.278413\n",
      "📈 Episodio 239: Recompensa total (clipped): 21.000, Pasos: 652, Mean Reward Calculado: 0.032209 (Recompensa/Pasos)\n",
      "  136712/5000000: episode: 239, duration: 41.789s, episode steps: 652, steps per second:  16, episode reward: 21.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.037 [0.000, 5.000],  loss: 0.007864, mae: 0.220680, mean_q: 0.285324\n",
      "📈 Episodio 240: Recompensa total (clipped): 21.000, Pasos: 826, Mean Reward Calculado: 0.025424 (Recompensa/Pasos)\n",
      "  137538/5000000: episode: 240, duration: 52.111s, episode steps: 826, steps per second:  16, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.006409, mae: 0.211680, mean_q: 0.274171\n",
      "📈 Episodio 241: Recompensa total (clipped): 14.000, Pasos: 604, Mean Reward Calculado: 0.023179 (Recompensa/Pasos)\n",
      "  138142/5000000: episode: 241, duration: 39.188s, episode steps: 604, steps per second:  15, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.007409, mae: 0.212555, mean_q: 0.274283\n",
      "📈 Episodio 242: Recompensa total (clipped): 20.000, Pasos: 759, Mean Reward Calculado: 0.026350 (Recompensa/Pasos)\n",
      "  138901/5000000: episode: 242, duration: 46.746s, episode steps: 759, steps per second:  16, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.007591, mae: 0.212435, mean_q: 0.273279\n",
      "📈 Episodio 243: Recompensa total (clipped): 8.000, Pasos: 348, Mean Reward Calculado: 0.022989 (Recompensa/Pasos)\n",
      "  139249/5000000: episode: 243, duration: 22.020s, episode steps: 348, steps per second:  16, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.008079, mae: 0.217353, mean_q: 0.281009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 244: Recompensa total (clipped): 5.000, Pasos: 375, Mean Reward Calculado: 0.013333 (Recompensa/Pasos)\n",
      "  139624/5000000: episode: 244, duration: 23.809s, episode steps: 375, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.006841, mae: 0.208993, mean_q: 0.268830\n",
      "📊 Paso 140,000/5,000,000 (2.8%) - 23.2 pasos/seg - ETA: 58.3h - Memoria: 1656.10 MB\n",
      "📈 Episodio 245: Recompensa total (clipped): 9.000, Pasos: 515, Mean Reward Calculado: 0.017476 (Recompensa/Pasos)\n",
      "  140139/5000000: episode: 245, duration: 32.062s, episode steps: 515, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.008758, mae: 0.222458, mean_q: 0.285956\n",
      "📈 Episodio 246: Recompensa total (clipped): 12.000, Pasos: 556, Mean Reward Calculado: 0.021583 (Recompensa/Pasos)\n",
      "  140695/5000000: episode: 246, duration: 32.840s, episode steps: 556, steps per second:  17, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.009241, mae: 0.255669, mean_q: 0.328031\n",
      "📈 Episodio 247: Recompensa total (clipped): 13.000, Pasos: 453, Mean Reward Calculado: 0.028698 (Recompensa/Pasos)\n",
      "  141148/5000000: episode: 247, duration: 28.148s, episode steps: 453, steps per second:  16, episode reward: 13.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.258 [0.000, 5.000],  loss: 0.008225, mae: 0.245935, mean_q: 0.315187\n",
      "📈 Episodio 248: Recompensa total (clipped): 9.000, Pasos: 286, Mean Reward Calculado: 0.031469 (Recompensa/Pasos)\n",
      "  141434/5000000: episode: 248, duration: 18.166s, episode steps: 286, steps per second:  16, episode reward:  9.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.009806, mae: 0.258699, mean_q: 0.332369\n",
      "📈 Episodio 249: Recompensa total (clipped): 15.000, Pasos: 495, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
      "  141929/5000000: episode: 249, duration: 31.692s, episode steps: 495, steps per second:  16, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.009295, mae: 0.249203, mean_q: 0.321000\n",
      "📈 Episodio 250: Recompensa total (clipped): 8.000, Pasos: 393, Mean Reward Calculado: 0.020356 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 250 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 8.00\n",
      "   Media últimos 100: 11.86 / 20.0\n",
      "   Mejor promedio histórico: 14.69\n",
      "   Estado: 📈 59.3% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "  142322/5000000: episode: 250, duration: 25.557s, episode steps: 393, steps per second:  15, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.008593, mae: 0.249088, mean_q: 0.321326\n",
      "📈 Episodio 251: Recompensa total (clipped): 15.000, Pasos: 497, Mean Reward Calculado: 0.030181 (Recompensa/Pasos)\n",
      "  142819/5000000: episode: 251, duration: 31.055s, episode steps: 497, steps per second:  16, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.007733, mae: 0.252157, mean_q: 0.328370\n",
      "📈 Episodio 252: Recompensa total (clipped): 26.000, Pasos: 883, Mean Reward Calculado: 0.029445 (Recompensa/Pasos)\n",
      "  143702/5000000: episode: 252, duration: 55.775s, episode steps: 883, steps per second:  16, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.008732, mae: 0.248104, mean_q: 0.320316\n",
      "📈 Episodio 253: Recompensa total (clipped): 19.000, Pasos: 625, Mean Reward Calculado: 0.030400 (Recompensa/Pasos)\n",
      "  144327/5000000: episode: 253, duration: 39.581s, episode steps: 625, steps per second:  16, episode reward: 19.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.008265, mae: 0.250150, mean_q: 0.322934\n",
      "📈 Episodio 254: Recompensa total (clipped): 16.000, Pasos: 633, Mean Reward Calculado: 0.025276 (Recompensa/Pasos)\n",
      "  144960/5000000: episode: 254, duration: 39.647s, episode steps: 633, steps per second:  16, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.008384, mae: 0.252597, mean_q: 0.324452\n",
      "📈 Episodio 255: Recompensa total (clipped): 28.000, Pasos: 966, Mean Reward Calculado: 0.028986 (Recompensa/Pasos)\n",
      "  145926/5000000: episode: 255, duration: 60.803s, episode steps: 966, steps per second:  16, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.008269, mae: 0.249917, mean_q: 0.322823\n",
      "📈 Episodio 256: Recompensa total (clipped): 10.000, Pasos: 405, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
      "  146331/5000000: episode: 256, duration: 26.470s, episode steps: 405, steps per second:  15, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.007895, mae: 0.234737, mean_q: 0.301727\n",
      "📈 Episodio 257: Recompensa total (clipped): 12.000, Pasos: 419, Mean Reward Calculado: 0.028640 (Recompensa/Pasos)\n",
      "  146750/5000000: episode: 257, duration: 26.666s, episode steps: 419, steps per second:  16, episode reward: 12.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.008549, mae: 0.247965, mean_q: 0.317407\n",
      "📈 Episodio 258: Recompensa total (clipped): 12.000, Pasos: 579, Mean Reward Calculado: 0.020725 (Recompensa/Pasos)\n",
      "  147329/5000000: episode: 258, duration: 36.554s, episode steps: 579, steps per second:  16, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.007734, mae: 0.244855, mean_q: 0.314077\n",
      "📈 Episodio 259: Recompensa total (clipped): 12.000, Pasos: 528, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
      "  147857/5000000: episode: 259, duration: 32.063s, episode steps: 528, steps per second:  16, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.007497, mae: 0.242126, mean_q: 0.311287\n",
      "📈 Episodio 260: Recompensa total (clipped): 10.000, Pasos: 376, Mean Reward Calculado: 0.026596 (Recompensa/Pasos)\n",
      "  148233/5000000: episode: 260, duration: 23.980s, episode steps: 376, steps per second:  16, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.006054, mae: 0.239309, mean_q: 0.307213\n",
      "📈 Episodio 261: Recompensa total (clipped): 29.000, Pasos: 1024, Mean Reward Calculado: 0.028320 (Recompensa/Pasos)\n",
      "  149257/5000000: episode: 261, duration: 66.291s, episode steps: 1024, steps per second:  15, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.007909, mae: 0.249130, mean_q: 0.319654\n",
      "📈 Episodio 262: Recompensa total (clipped): 15.000, Pasos: 506, Mean Reward Calculado: 0.029644 (Recompensa/Pasos)\n",
      "  149763/5000000: episode: 262, duration: 32.872s, episode steps: 506, steps per second:  15, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.200 [0.000, 5.000],  loss: 0.008764, mae: 0.248066, mean_q: 0.318986\n",
      "📈 Episodio 263: Recompensa total (clipped): 16.000, Pasos: 567, Mean Reward Calculado: 0.028219 (Recompensa/Pasos)\n",
      "  150330/5000000: episode: 263, duration: 37.399s, episode steps: 567, steps per second:  15, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007955, mae: 0.260448, mean_q: 0.336446\n",
      "📈 Episodio 264: Recompensa total (clipped): 12.000, Pasos: 487, Mean Reward Calculado: 0.024641 (Recompensa/Pasos)\n",
      "  150817/5000000: episode: 264, duration: 34.545s, episode steps: 487, steps per second:  14, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.009148, mae: 0.270593, mean_q: 0.347853\n",
      "📈 Episodio 265: Recompensa total (clipped): 13.000, Pasos: 532, Mean Reward Calculado: 0.024436 (Recompensa/Pasos)\n",
      "  151349/5000000: episode: 265, duration: 35.772s, episode steps: 532, steps per second:  15, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.009659, mae: 0.279337, mean_q: 0.358634\n",
      "📈 Episodio 266: Recompensa total (clipped): 11.000, Pasos: 458, Mean Reward Calculado: 0.024017 (Recompensa/Pasos)\n",
      "  151807/5000000: episode: 266, duration: 29.793s, episode steps: 458, steps per second:  15, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.008642, mae: 0.271284, mean_q: 0.348856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 267: Recompensa total (clipped): 6.000, Pasos: 301, Mean Reward Calculado: 0.019934 (Recompensa/Pasos)\n",
      "  152108/5000000: episode: 267, duration: 19.444s, episode steps: 301, steps per second:  15, episode reward:  6.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.505 [0.000, 5.000],  loss: 0.008756, mae: 0.271067, mean_q: 0.349598\n",
      "📈 Episodio 268: Recompensa total (clipped): 13.000, Pasos: 729, Mean Reward Calculado: 0.017833 (Recompensa/Pasos)\n",
      "  152837/5000000: episode: 268, duration: 45.694s, episode steps: 729, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008019, mae: 0.266508, mean_q: 0.343713\n",
      "📈 Episodio 269: Recompensa total (clipped): 21.000, Pasos: 632, Mean Reward Calculado: 0.033228 (Recompensa/Pasos)\n",
      "  153469/5000000: episode: 269, duration: 39.796s, episode steps: 632, steps per second:  16, episode reward: 21.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.007805, mae: 0.275563, mean_q: 0.355204\n",
      "📈 Episodio 270: Recompensa total (clipped): 30.000, Pasos: 1354, Mean Reward Calculado: 0.022157 (Recompensa/Pasos)\n",
      "  154823/5000000: episode: 270, duration: 85.876s, episode steps: 1354, steps per second:  16, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.008806, mae: 0.269383, mean_q: 0.347190\n",
      "📈 Episodio 271: Recompensa total (clipped): 14.000, Pasos: 514, Mean Reward Calculado: 0.027237 (Recompensa/Pasos)\n",
      "  155337/5000000: episode: 271, duration: 32.894s, episode steps: 514, steps per second:  16, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.008859, mae: 0.274085, mean_q: 0.351082\n",
      "📈 Episodio 272: Recompensa total (clipped): 8.000, Pasos: 402, Mean Reward Calculado: 0.019900 (Recompensa/Pasos)\n",
      "  155739/5000000: episode: 272, duration: 25.786s, episode steps: 402, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.008009, mae: 0.263342, mean_q: 0.336924\n",
      "📈 Episodio 273: Recompensa total (clipped): 21.000, Pasos: 716, Mean Reward Calculado: 0.029330 (Recompensa/Pasos)\n",
      "  156455/5000000: episode: 273, duration: 44.705s, episode steps: 716, steps per second:  16, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008631, mae: 0.276514, mean_q: 0.354850\n",
      "📈 Episodio 274: Recompensa total (clipped): 13.000, Pasos: 492, Mean Reward Calculado: 0.026423 (Recompensa/Pasos)\n",
      "  156947/5000000: episode: 274, duration: 30.197s, episode steps: 492, steps per second:  16, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.008930, mae: 0.269611, mean_q: 0.346108\n",
      "📈 Episodio 275: Recompensa total (clipped): 17.000, Pasos: 677, Mean Reward Calculado: 0.025111 (Recompensa/Pasos)\n",
      "  157624/5000000: episode: 275, duration: 45.822s, episode steps: 677, steps per second:  15, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.008317, mae: 0.272574, mean_q: 0.348940\n",
      "📈 Episodio 276: Recompensa total (clipped): 10.000, Pasos: 354, Mean Reward Calculado: 0.028249 (Recompensa/Pasos)\n",
      "  157978/5000000: episode: 276, duration: 21.803s, episode steps: 354, steps per second:  16, episode reward: 10.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.007143, mae: 0.268718, mean_q: 0.344323\n",
      "📈 Episodio 277: Recompensa total (clipped): 12.000, Pasos: 497, Mean Reward Calculado: 0.024145 (Recompensa/Pasos)\n",
      "  158475/5000000: episode: 277, duration: 30.484s, episode steps: 497, steps per second:  16, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.007771, mae: 0.274204, mean_q: 0.353290\n",
      "📈 Episodio 278: Recompensa total (clipped): 16.000, Pasos: 611, Mean Reward Calculado: 0.026187 (Recompensa/Pasos)\n",
      "  159086/5000000: episode: 278, duration: 40.334s, episode steps: 611, steps per second:  15, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.007621, mae: 0.270023, mean_q: 0.344725\n",
      "📈 Episodio 279: Recompensa total (clipped): 16.000, Pasos: 676, Mean Reward Calculado: 0.023669 (Recompensa/Pasos)\n",
      "  159762/5000000: episode: 279, duration: 42.520s, episode steps: 676, steps per second:  16, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.007793, mae: 0.273733, mean_q: 0.349132\n",
      "📊 Paso 160,000/5,000,000 (3.2%) - 21.9 pasos/seg - ETA: 61.5h - Memoria: 1834.73 MB\n",
      "📈 Episodio 280: Recompensa total (clipped): 11.000, Pasos: 413, Mean Reward Calculado: 0.026634 (Recompensa/Pasos)\n",
      "  160175/5000000: episode: 280, duration: 26.068s, episode steps: 413, steps per second:  16, episode reward: 11.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.008652, mae: 0.296479, mean_q: 0.380193\n",
      "📈 Episodio 281: Recompensa total (clipped): 5.000, Pasos: 271, Mean Reward Calculado: 0.018450 (Recompensa/Pasos)\n",
      "  160446/5000000: episode: 281, duration: 18.177s, episode steps: 271, steps per second:  15, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.009896, mae: 0.320875, mean_q: 0.412057\n",
      "📈 Episodio 282: Recompensa total (clipped): 10.000, Pasos: 402, Mean Reward Calculado: 0.024876 (Recompensa/Pasos)\n",
      "  160848/5000000: episode: 282, duration: 26.921s, episode steps: 402, steps per second:  15, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.010018, mae: 0.318813, mean_q: 0.410313\n",
      "📈 Episodio 283: Recompensa total (clipped): 12.000, Pasos: 389, Mean Reward Calculado: 0.030848 (Recompensa/Pasos)\n",
      "  161237/5000000: episode: 283, duration: 26.640s, episode steps: 389, steps per second:  15, episode reward: 12.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.009966, mae: 0.329303, mean_q: 0.420471\n",
      "📈 Episodio 284: Recompensa total (clipped): 5.000, Pasos: 273, Mean Reward Calculado: 0.018315 (Recompensa/Pasos)\n",
      "  161510/5000000: episode: 284, duration: 18.657s, episode steps: 273, steps per second:  15, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.009936, mae: 0.324930, mean_q: 0.415243\n",
      "📈 Episodio 285: Recompensa total (clipped): 7.000, Pasos: 401, Mean Reward Calculado: 0.017456 (Recompensa/Pasos)\n",
      "  161911/5000000: episode: 285, duration: 27.844s, episode steps: 401, steps per second:  14, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.010911, mae: 0.322124, mean_q: 0.412341\n",
      "📈 Episodio 286: Recompensa total (clipped): 16.000, Pasos: 658, Mean Reward Calculado: 0.024316 (Recompensa/Pasos)\n",
      "  162569/5000000: episode: 286, duration: 42.709s, episode steps: 658, steps per second:  15, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.008992, mae: 0.324155, mean_q: 0.414809\n",
      "📈 Episodio 287: Recompensa total (clipped): 5.000, Pasos: 359, Mean Reward Calculado: 0.013928 (Recompensa/Pasos)\n",
      "  162928/5000000: episode: 287, duration: 24.643s, episode steps: 359, steps per second:  15, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.008329, mae: 0.314673, mean_q: 0.402254\n",
      "📈 Episodio 288: Recompensa total (clipped): 25.000, Pasos: 847, Mean Reward Calculado: 0.029516 (Recompensa/Pasos)\n",
      "  163775/5000000: episode: 288, duration: 56.585s, episode steps: 847, steps per second:  15, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.009662, mae: 0.321093, mean_q: 0.411503\n",
      "📈 Episodio 289: Recompensa total (clipped): 21.000, Pasos: 617, Mean Reward Calculado: 0.034036 (Recompensa/Pasos)\n",
      "  164392/5000000: episode: 289, duration: 41.881s, episode steps: 617, steps per second:  15, episode reward: 21.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010249, mae: 0.326231, mean_q: 0.418952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 290: Recompensa total (clipped): 10.000, Pasos: 414, Mean Reward Calculado: 0.024155 (Recompensa/Pasos)\n",
      "  164806/5000000: episode: 290, duration: 27.938s, episode steps: 414, steps per second:  15, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.009586, mae: 0.324793, mean_q: 0.416484\n",
      "📈 Episodio 291: Recompensa total (clipped): 11.000, Pasos: 602, Mean Reward Calculado: 0.018272 (Recompensa/Pasos)\n",
      "  165408/5000000: episode: 291, duration: 40.688s, episode steps: 602, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.008786, mae: 0.322781, mean_q: 0.413321\n",
      "📈 Episodio 292: Recompensa total (clipped): 8.000, Pasos: 361, Mean Reward Calculado: 0.022161 (Recompensa/Pasos)\n",
      "  165769/5000000: episode: 292, duration: 25.711s, episode steps: 361, steps per second:  14, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.008763, mae: 0.322586, mean_q: 0.416374\n",
      "📈 Episodio 293: Recompensa total (clipped): 21.000, Pasos: 707, Mean Reward Calculado: 0.029703 (Recompensa/Pasos)\n",
      "  166476/5000000: episode: 293, duration: 43.568s, episode steps: 707, steps per second:  16, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.008731, mae: 0.319502, mean_q: 0.408669\n",
      "📈 Episodio 294: Recompensa total (clipped): 18.000, Pasos: 541, Mean Reward Calculado: 0.033272 (Recompensa/Pasos)\n",
      "  167017/5000000: episode: 294, duration: 33.259s, episode steps: 541, steps per second:  16, episode reward: 18.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009288, mae: 0.323750, mean_q: 0.414125\n",
      "📈 Episodio 295: Recompensa total (clipped): 10.000, Pasos: 563, Mean Reward Calculado: 0.017762 (Recompensa/Pasos)\n",
      "  167580/5000000: episode: 295, duration: 35.262s, episode steps: 563, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.009150, mae: 0.316797, mean_q: 0.405613\n",
      "📈 Episodio 296: Recompensa total (clipped): 30.000, Pasos: 1051, Mean Reward Calculado: 0.028544 (Recompensa/Pasos)\n",
      "  168631/5000000: episode: 296, duration: 68.656s, episode steps: 1051, steps per second:  15, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.009400, mae: 0.320296, mean_q: 0.409947\n",
      "📈 Episodio 297: Recompensa total (clipped): 16.000, Pasos: 477, Mean Reward Calculado: 0.033543 (Recompensa/Pasos)\n",
      "  169108/5000000: episode: 297, duration: 32.227s, episode steps: 477, steps per second:  15, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.009045, mae: 0.323252, mean_q: 0.416248\n",
      "📈 Episodio 298: Recompensa total (clipped): 9.000, Pasos: 612, Mean Reward Calculado: 0.014706 (Recompensa/Pasos)\n",
      "  169720/5000000: episode: 298, duration: 41.268s, episode steps: 612, steps per second:  15, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.008103, mae: 0.323352, mean_q: 0.412638\n",
      "📈 Episodio 299: Recompensa total (clipped): 13.000, Pasos: 521, Mean Reward Calculado: 0.024952 (Recompensa/Pasos)\n",
      "  170241/5000000: episode: 299, duration: 34.021s, episode steps: 521, steps per second:  15, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.010014, mae: 0.346608, mean_q: 0.441359\n",
      "📈 Episodio 300: Recompensa total (clipped): 11.000, Pasos: 389, Mean Reward Calculado: 0.028278 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 300 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 11.00\n",
      "   Media últimos 100: 13.75 / 20.0\n",
      "   Mejor promedio histórico: 14.69\n",
      "   Estado: 📈 68.8% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "✅ Pesos guardados en checkpoints/DDQN_weights_episode_300.h5f tras el episodio 300\n",
      "  170630/5000000: episode: 300, duration: 24.035s, episode steps: 389, steps per second:  16, episode reward: 11.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.136 [0.000, 5.000],  loss: 0.009975, mae: 0.360283, mean_q: 0.457232\n",
      "📈 Episodio 301: Recompensa total (clipped): 6.000, Pasos: 366, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
      "  170996/5000000: episode: 301, duration: 24.159s, episode steps: 366, steps per second:  15, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011021, mae: 0.358517, mean_q: 0.456541\n",
      "📈 Episodio 302: Recompensa total (clipped): 12.000, Pasos: 548, Mean Reward Calculado: 0.021898 (Recompensa/Pasos)\n",
      "  171544/5000000: episode: 302, duration: 36.132s, episode steps: 548, steps per second:  15, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009791, mae: 0.352235, mean_q: 0.449113\n",
      "📈 Episodio 303: Recompensa total (clipped): 6.000, Pasos: 327, Mean Reward Calculado: 0.018349 (Recompensa/Pasos)\n",
      "  171871/5000000: episode: 303, duration: 21.442s, episode steps: 327, steps per second:  15, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.985 [0.000, 5.000],  loss: 0.010105, mae: 0.347795, mean_q: 0.442914\n",
      "📈 Episodio 304: Recompensa total (clipped): 8.000, Pasos: 520, Mean Reward Calculado: 0.015385 (Recompensa/Pasos)\n",
      "  172391/5000000: episode: 304, duration: 35.528s, episode steps: 520, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.009809, mae: 0.358220, mean_q: 0.458004\n",
      "📈 Episodio 305: Recompensa total (clipped): 19.000, Pasos: 699, Mean Reward Calculado: 0.027182 (Recompensa/Pasos)\n",
      "  173090/5000000: episode: 305, duration: 48.780s, episode steps: 699, steps per second:  14, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.008508, mae: 0.345621, mean_q: 0.442063\n",
      "📈 Episodio 306: Recompensa total (clipped): 24.000, Pasos: 629, Mean Reward Calculado: 0.038156 (Recompensa/Pasos)\n",
      "  173719/5000000: episode: 306, duration: 44.505s, episode steps: 629, steps per second:  14, episode reward: 24.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.159 [0.000, 5.000],  loss: 0.010352, mae: 0.361072, mean_q: 0.461305\n",
      "📈 Episodio 307: Recompensa total (clipped): 17.000, Pasos: 608, Mean Reward Calculado: 0.027961 (Recompensa/Pasos)\n",
      "  174327/5000000: episode: 307, duration: 43.121s, episode steps: 608, steps per second:  14, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.011174, mae: 0.355173, mean_q: 0.454601\n",
      "📈 Episodio 308: Recompensa total (clipped): 12.000, Pasos: 527, Mean Reward Calculado: 0.022770 (Recompensa/Pasos)\n",
      "  174854/5000000: episode: 308, duration: 36.770s, episode steps: 527, steps per second:  14, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.009710, mae: 0.354389, mean_q: 0.456925\n",
      "📈 Episodio 309: Recompensa total (clipped): 17.000, Pasos: 543, Mean Reward Calculado: 0.031308 (Recompensa/Pasos)\n",
      "  175397/5000000: episode: 309, duration: 37.591s, episode steps: 543, steps per second:  14, episode reward: 17.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.011656, mae: 0.362041, mean_q: 0.464042\n",
      "📈 Episodio 310: Recompensa total (clipped): 10.000, Pasos: 548, Mean Reward Calculado: 0.018248 (Recompensa/Pasos)\n",
      "  175945/5000000: episode: 310, duration: 35.013s, episode steps: 548, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.011322, mae: 0.356744, mean_q: 0.455320\n",
      "📈 Episodio 311: Recompensa total (clipped): 20.000, Pasos: 663, Mean Reward Calculado: 0.030166 (Recompensa/Pasos)\n",
      "  176608/5000000: episode: 311, duration: 42.857s, episode steps: 663, steps per second:  15, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.010023, mae: 0.369218, mean_q: 0.470224\n",
      "📈 Episodio 312: Recompensa total (clipped): 20.000, Pasos: 578, Mean Reward Calculado: 0.034602 (Recompensa/Pasos)\n",
      "  177186/5000000: episode: 312, duration: 39.349s, episode steps: 578, steps per second:  15, episode reward: 20.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.009137, mae: 0.354090, mean_q: 0.452503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 313: Recompensa total (clipped): 19.000, Pasos: 690, Mean Reward Calculado: 0.027536 (Recompensa/Pasos)\n",
      "  177876/5000000: episode: 313, duration: 46.880s, episode steps: 690, steps per second:  15, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008347, mae: 0.347880, mean_q: 0.444115\n",
      "📈 Episodio 314: Recompensa total (clipped): 17.000, Pasos: 678, Mean Reward Calculado: 0.025074 (Recompensa/Pasos)\n",
      "  178554/5000000: episode: 314, duration: 43.620s, episode steps: 678, steps per second:  16, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.008847, mae: 0.348212, mean_q: 0.447191\n",
      "📈 Episodio 315: Recompensa total (clipped): 10.000, Pasos: 424, Mean Reward Calculado: 0.023585 (Recompensa/Pasos)\n",
      "  178978/5000000: episode: 315, duration: 28.807s, episode steps: 424, steps per second:  15, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.009155, mae: 0.358974, mean_q: 0.462364\n",
      "📈 Episodio 316: Recompensa total (clipped): 21.000, Pasos: 633, Mean Reward Calculado: 0.033175 (Recompensa/Pasos)\n",
      "  179611/5000000: episode: 316, duration: 45.252s, episode steps: 633, steps per second:  14, episode reward: 21.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.008616, mae: 0.349749, mean_q: 0.446607\n",
      "📊 Paso 180,000/5,000,000 (3.6%) - 20.8 pasos/seg - ETA: 64.4h - Memoria: 2010.79 MB\n",
      "📈 Episodio 317: Recompensa total (clipped): 13.000, Pasos: 510, Mean Reward Calculado: 0.025490 (Recompensa/Pasos)\n",
      "  180121/5000000: episode: 317, duration: 37.044s, episode steps: 510, steps per second:  14, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.011345, mae: 0.370357, mean_q: 0.471821\n",
      "📈 Episodio 318: Recompensa total (clipped): 17.000, Pasos: 602, Mean Reward Calculado: 0.028239 (Recompensa/Pasos)\n",
      "  180723/5000000: episode: 318, duration: 42.968s, episode steps: 602, steps per second:  14, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.158 [0.000, 5.000],  loss: 0.010653, mae: 0.387974, mean_q: 0.495753\n",
      "📈 Episodio 319: Recompensa total (clipped): 8.000, Pasos: 276, Mean Reward Calculado: 0.028986 (Recompensa/Pasos)\n",
      "  180999/5000000: episode: 319, duration: 19.785s, episode steps: 276, steps per second:  14, episode reward:  8.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.993 [0.000, 5.000],  loss: 0.010314, mae: 0.405304, mean_q: 0.516228\n",
      "📈 Episodio 320: Recompensa total (clipped): 19.000, Pasos: 760, Mean Reward Calculado: 0.025000 (Recompensa/Pasos)\n",
      "  181759/5000000: episode: 320, duration: 53.164s, episode steps: 760, steps per second:  14, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.010824, mae: 0.380154, mean_q: 0.488043\n",
      "📈 Episodio 321: Recompensa total (clipped): 13.000, Pasos: 451, Mean Reward Calculado: 0.028825 (Recompensa/Pasos)\n",
      "  182210/5000000: episode: 321, duration: 32.268s, episode steps: 451, steps per second:  14, episode reward: 13.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.010050, mae: 0.377352, mean_q: 0.483193\n",
      "📈 Episodio 322: Recompensa total (clipped): 12.000, Pasos: 509, Mean Reward Calculado: 0.023576 (Recompensa/Pasos)\n",
      "  182719/5000000: episode: 322, duration: 34.699s, episode steps: 509, steps per second:  15, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.010524, mae: 0.379743, mean_q: 0.490592\n",
      "📈 Episodio 323: Recompensa total (clipped): 13.000, Pasos: 460, Mean Reward Calculado: 0.028261 (Recompensa/Pasos)\n",
      "  183179/5000000: episode: 323, duration: 30.493s, episode steps: 460, steps per second:  15, episode reward: 13.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.010425, mae: 0.384531, mean_q: 0.495842\n",
      "📈 Episodio 324: Recompensa total (clipped): 25.000, Pasos: 1076, Mean Reward Calculado: 0.023234 (Recompensa/Pasos)\n",
      "  184255/5000000: episode: 324, duration: 71.448s, episode steps: 1076, steps per second:  15, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.010424, mae: 0.392370, mean_q: 0.503558\n",
      "📈 Episodio 325: Recompensa total (clipped): 9.000, Pasos: 297, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
      "  184552/5000000: episode: 325, duration: 18.801s, episode steps: 297, steps per second:  16, episode reward:  9.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.236 [0.000, 5.000],  loss: 0.009515, mae: 0.374968, mean_q: 0.482373\n",
      "📈 Episodio 326: Recompensa total (clipped): 4.000, Pasos: 298, Mean Reward Calculado: 0.013423 (Recompensa/Pasos)\n",
      "  184850/5000000: episode: 326, duration: 19.325s, episode steps: 298, steps per second:  15, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.010999, mae: 0.387891, mean_q: 0.498596\n",
      "📈 Episodio 327: Recompensa total (clipped): 9.000, Pasos: 410, Mean Reward Calculado: 0.021951 (Recompensa/Pasos)\n",
      "  185260/5000000: episode: 327, duration: 25.830s, episode steps: 410, steps per second:  16, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.011349, mae: 0.383839, mean_q: 0.492050\n",
      "📈 Episodio 328: Recompensa total (clipped): 11.000, Pasos: 603, Mean Reward Calculado: 0.018242 (Recompensa/Pasos)\n",
      "  185863/5000000: episode: 328, duration: 37.617s, episode steps: 603, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.010541, mae: 0.384461, mean_q: 0.493063\n",
      "📈 Episodio 329: Recompensa total (clipped): 12.000, Pasos: 333, Mean Reward Calculado: 0.036036 (Recompensa/Pasos)\n",
      "  186196/5000000: episode: 329, duration: 22.219s, episode steps: 333, steps per second:  15, episode reward: 12.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.010182, mae: 0.383364, mean_q: 0.490654\n",
      "📈 Episodio 330: Recompensa total (clipped): 20.000, Pasos: 721, Mean Reward Calculado: 0.027739 (Recompensa/Pasos)\n",
      "  186917/5000000: episode: 330, duration: 45.815s, episode steps: 721, steps per second:  16, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.010744, mae: 0.395899, mean_q: 0.504644\n",
      "📈 Episodio 331: Recompensa total (clipped): 29.000, Pasos: 850, Mean Reward Calculado: 0.034118 (Recompensa/Pasos)\n",
      "  187767/5000000: episode: 331, duration: 54.468s, episode steps: 850, steps per second:  16, episode reward: 29.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.010134, mae: 0.386747, mean_q: 0.494105\n",
      "📈 Episodio 332: Recompensa total (clipped): 13.000, Pasos: 522, Mean Reward Calculado: 0.024904 (Recompensa/Pasos)\n",
      "  188289/5000000: episode: 332, duration: 33.584s, episode steps: 522, steps per second:  16, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.010565, mae: 0.388420, mean_q: 0.494724\n",
      "📈 Episodio 333: Recompensa total (clipped): 9.000, Pasos: 302, Mean Reward Calculado: 0.029801 (Recompensa/Pasos)\n",
      "  188591/5000000: episode: 333, duration: 18.859s, episode steps: 302, steps per second:  16, episode reward:  9.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.011068, mae: 0.399188, mean_q: 0.511202\n",
      "📈 Episodio 334: Recompensa total (clipped): 18.000, Pasos: 799, Mean Reward Calculado: 0.022528 (Recompensa/Pasos)\n",
      "  189390/5000000: episode: 334, duration: 49.363s, episode steps: 799, steps per second:  16, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.146 [0.000, 5.000],  loss: 0.010572, mae: 0.383541, mean_q: 0.490843\n",
      "📈 Episodio 335: Recompensa total (clipped): 21.000, Pasos: 744, Mean Reward Calculado: 0.028226 (Recompensa/Pasos)\n",
      "  190134/5000000: episode: 335, duration: 46.538s, episode steps: 744, steps per second:  16, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.960 [0.000, 5.000],  loss: 0.010298, mae: 0.388519, mean_q: 0.496514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 336: Recompensa total (clipped): 15.000, Pasos: 500, Mean Reward Calculado: 0.030000 (Recompensa/Pasos)\n",
      "  190634/5000000: episode: 336, duration: 31.274s, episode steps: 500, steps per second:  16, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.011704, mae: 0.417442, mean_q: 0.532511\n",
      "📈 Episodio 337: Recompensa total (clipped): 15.000, Pasos: 540, Mean Reward Calculado: 0.027778 (Recompensa/Pasos)\n",
      "  191174/5000000: episode: 337, duration: 35.424s, episode steps: 540, steps per second:  15, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.202 [0.000, 5.000],  loss: 0.012033, mae: 0.414901, mean_q: 0.529445\n",
      "📈 Episodio 338: Recompensa total (clipped): 17.000, Pasos: 563, Mean Reward Calculado: 0.030195 (Recompensa/Pasos)\n",
      "  191737/5000000: episode: 338, duration: 36.198s, episode steps: 563, steps per second:  16, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.012946, mae: 0.427357, mean_q: 0.542763\n",
      "📈 Episodio 339: Recompensa total (clipped): 6.000, Pasos: 312, Mean Reward Calculado: 0.019231 (Recompensa/Pasos)\n",
      "  192049/5000000: episode: 339, duration: 19.893s, episode steps: 312, steps per second:  16, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010331, mae: 0.430742, mean_q: 0.547625\n",
      "📈 Episodio 340: Recompensa total (clipped): 13.000, Pasos: 398, Mean Reward Calculado: 0.032663 (Recompensa/Pasos)\n",
      "  192447/5000000: episode: 340, duration: 25.221s, episode steps: 398, steps per second:  16, episode reward: 13.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.005 [0.000, 5.000],  loss: 0.010548, mae: 0.417013, mean_q: 0.532116\n",
      "📈 Episodio 341: Recompensa total (clipped): 16.000, Pasos: 634, Mean Reward Calculado: 0.025237 (Recompensa/Pasos)\n",
      "  193081/5000000: episode: 341, duration: 40.312s, episode steps: 634, steps per second:  16, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.207 [0.000, 5.000],  loss: 0.011517, mae: 0.424137, mean_q: 0.538757\n",
      "📈 Episodio 342: Recompensa total (clipped): 18.000, Pasos: 633, Mean Reward Calculado: 0.028436 (Recompensa/Pasos)\n",
      "  193714/5000000: episode: 342, duration: 40.816s, episode steps: 633, steps per second:  16, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.011853, mae: 0.419260, mean_q: 0.532320\n",
      "📈 Episodio 343: Recompensa total (clipped): 11.000, Pasos: 384, Mean Reward Calculado: 0.028646 (Recompensa/Pasos)\n",
      "  194098/5000000: episode: 343, duration: 24.470s, episode steps: 384, steps per second:  16, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011434, mae: 0.425116, mean_q: 0.543815\n",
      "📈 Episodio 344: Recompensa total (clipped): 24.000, Pasos: 700, Mean Reward Calculado: 0.034286 (Recompensa/Pasos)\n",
      "  194798/5000000: episode: 344, duration: 46.307s, episode steps: 700, steps per second:  15, episode reward: 24.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.011466, mae: 0.435925, mean_q: 0.558137\n",
      "📈 Episodio 345: Recompensa total (clipped): 15.000, Pasos: 548, Mean Reward Calculado: 0.027372 (Recompensa/Pasos)\n",
      "  195346/5000000: episode: 345, duration: 35.290s, episode steps: 548, steps per second:  16, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.010744, mae: 0.417176, mean_q: 0.532332\n",
      "📈 Episodio 346: Recompensa total (clipped): 12.000, Pasos: 425, Mean Reward Calculado: 0.028235 (Recompensa/Pasos)\n",
      "  195771/5000000: episode: 346, duration: 26.359s, episode steps: 425, steps per second:  16, episode reward: 12.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.009749, mae: 0.431280, mean_q: 0.549491\n",
      "📈 Episodio 347: Recompensa total (clipped): 20.000, Pasos: 700, Mean Reward Calculado: 0.028571 (Recompensa/Pasos)\n",
      "  196471/5000000: episode: 347, duration: 44.486s, episode steps: 700, steps per second:  16, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.097 [0.000, 5.000],  loss: 0.010490, mae: 0.416080, mean_q: 0.531224\n",
      "📈 Episodio 348: Recompensa total (clipped): 11.000, Pasos: 367, Mean Reward Calculado: 0.029973 (Recompensa/Pasos)\n",
      "  196838/5000000: episode: 348, duration: 23.271s, episode steps: 367, steps per second:  16, episode reward: 11.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.013123, mae: 0.431430, mean_q: 0.548767\n",
      "📈 Episodio 349: Recompensa total (clipped): 15.000, Pasos: 523, Mean Reward Calculado: 0.028681 (Recompensa/Pasos)\n",
      "  197361/5000000: episode: 349, duration: 33.197s, episode steps: 523, steps per second:  16, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.352 [0.000, 5.000],  loss: 0.010673, mae: 0.429694, mean_q: 0.547414\n",
      "📈 Episodio 350: Recompensa total (clipped): 4.000, Pasos: 242, Mean Reward Calculado: 0.016529 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 350 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 4.00\n",
      "   Media últimos 100: 14.52 / 20.0\n",
      "   Mejor promedio histórico: 14.69\n",
      "   Estado: 📈 72.6% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "  197603/5000000: episode: 350, duration: 16.073s, episode steps: 242, steps per second:  15, episode reward:  4.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.012958, mae: 0.409092, mean_q: 0.523519\n",
      "📈 Episodio 351: Recompensa total (clipped): 21.000, Pasos: 669, Mean Reward Calculado: 0.031390 (Recompensa/Pasos)\n",
      "  198272/5000000: episode: 351, duration: 41.796s, episode steps: 669, steps per second:  16, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.010512, mae: 0.414240, mean_q: 0.528207\n",
      "📈 Episodio 352: Recompensa total (clipped): 15.000, Pasos: 500, Mean Reward Calculado: 0.030000 (Recompensa/Pasos)\n",
      "  198772/5000000: episode: 352, duration: 33.077s, episode steps: 500, steps per second:  15, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.010288, mae: 0.421080, mean_q: 0.537620\n",
      "📈 Episodio 353: Recompensa total (clipped): 7.000, Pasos: 258, Mean Reward Calculado: 0.027132 (Recompensa/Pasos)\n",
      "  199030/5000000: episode: 353, duration: 17.025s, episode steps: 258, steps per second:  15, episode reward:  7.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.008572, mae: 0.408324, mean_q: 0.522626\n",
      "📈 Episodio 354: Recompensa total (clipped): 18.000, Pasos: 661, Mean Reward Calculado: 0.027231 (Recompensa/Pasos)\n",
      "  199691/5000000: episode: 354, duration: 42.799s, episode steps: 661, steps per second:  15, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.011285, mae: 0.418399, mean_q: 0.535382\n",
      "📊 Paso 200,000/5,000,000 (4.0%) - 20.1 pasos/seg - ETA: 66.4h - Memoria: 2178.27 MB\n",
      "📈 Episodio 355: Recompensa total (clipped): 7.000, Pasos: 459, Mean Reward Calculado: 0.015251 (Recompensa/Pasos)\n",
      "  200150/5000000: episode: 355, duration: 29.871s, episode steps: 459, steps per second:  15, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.011259, mae: 0.436534, mean_q: 0.557951\n",
      "📈 Episodio 356: Recompensa total (clipped): 16.000, Pasos: 477, Mean Reward Calculado: 0.033543 (Recompensa/Pasos)\n",
      "  200627/5000000: episode: 356, duration: 31.579s, episode steps: 477, steps per second:  15, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.010521, mae: 0.468455, mean_q: 0.597840\n",
      "📈 Episodio 357: Recompensa total (clipped): 5.000, Pasos: 326, Mean Reward Calculado: 0.015337 (Recompensa/Pasos)\n",
      "  200953/5000000: episode: 357, duration: 20.678s, episode steps: 326, steps per second:  16, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.009950, mae: 0.463802, mean_q: 0.590030\n",
      "📈 Episodio 358: Recompensa total (clipped): 13.000, Pasos: 492, Mean Reward Calculado: 0.026423 (Recompensa/Pasos)\n",
      "  201445/5000000: episode: 358, duration: 32.573s, episode steps: 492, steps per second:  15, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.011034, mae: 0.466515, mean_q: 0.596201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 359: Recompensa total (clipped): 20.000, Pasos: 621, Mean Reward Calculado: 0.032206 (Recompensa/Pasos)\n",
      "  202066/5000000: episode: 359, duration: 41.159s, episode steps: 621, steps per second:  15, episode reward: 20.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.012411, mae: 0.479318, mean_q: 0.610449\n",
      "📈 Episodio 360: Recompensa total (clipped): 29.000, Pasos: 772, Mean Reward Calculado: 0.037565 (Recompensa/Pasos)\n",
      "  202838/5000000: episode: 360, duration: 49.561s, episode steps: 772, steps per second:  16, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.010558, mae: 0.466521, mean_q: 0.596116\n",
      "📈 Episodio 361: Recompensa total (clipped): 13.000, Pasos: 469, Mean Reward Calculado: 0.027719 (Recompensa/Pasos)\n",
      "  203307/5000000: episode: 361, duration: 30.295s, episode steps: 469, steps per second:  15, episode reward: 13.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.518 [0.000, 5.000],  loss: 0.013388, mae: 0.471677, mean_q: 0.600747\n",
      "📈 Episodio 362: Recompensa total (clipped): 19.000, Pasos: 704, Mean Reward Calculado: 0.026989 (Recompensa/Pasos)\n",
      "  204011/5000000: episode: 362, duration: 45.372s, episode steps: 704, steps per second:  16, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.010490, mae: 0.460087, mean_q: 0.588892\n",
      "📈 Episodio 363: Recompensa total (clipped): 22.000, Pasos: 545, Mean Reward Calculado: 0.040367 (Recompensa/Pasos)\n",
      "  204556/5000000: episode: 363, duration: 34.627s, episode steps: 545, steps per second:  16, episode reward: 22.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.010912, mae: 0.462535, mean_q: 0.588889\n",
      "📈 Episodio 364: Recompensa total (clipped): 20.000, Pasos: 506, Mean Reward Calculado: 0.039526 (Recompensa/Pasos)\n",
      "  205062/5000000: episode: 364, duration: 32.964s, episode steps: 506, steps per second:  15, episode reward: 20.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.011536, mae: 0.481964, mean_q: 0.613461\n",
      "📈 Episodio 365: Recompensa total (clipped): 16.000, Pasos: 479, Mean Reward Calculado: 0.033403 (Recompensa/Pasos)\n",
      "  205541/5000000: episode: 365, duration: 31.270s, episode steps: 479, steps per second:  15, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.025 [0.000, 5.000],  loss: 0.011199, mae: 0.471309, mean_q: 0.600324\n",
      "📈 Episodio 366: Recompensa total (clipped): 6.000, Pasos: 293, Mean Reward Calculado: 0.020478 (Recompensa/Pasos)\n",
      "  205834/5000000: episode: 366, duration: 19.518s, episode steps: 293, steps per second:  15, episode reward:  6.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.011217, mae: 0.466551, mean_q: 0.593302\n",
      "📈 Episodio 367: Recompensa total (clipped): 18.000, Pasos: 775, Mean Reward Calculado: 0.023226 (Recompensa/Pasos)\n",
      "  206609/5000000: episode: 367, duration: 50.847s, episode steps: 775, steps per second:  15, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.011234, mae: 0.471440, mean_q: 0.599057\n",
      "📈 Episodio 368: Recompensa total (clipped): 27.000, Pasos: 677, Mean Reward Calculado: 0.039882 (Recompensa/Pasos)\n",
      "  207286/5000000: episode: 368, duration: 43.410s, episode steps: 677, steps per second:  16, episode reward: 27.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.011177, mae: 0.466206, mean_q: 0.591688\n",
      "📈 Episodio 369: Recompensa total (clipped): 15.000, Pasos: 573, Mean Reward Calculado: 0.026178 (Recompensa/Pasos)\n",
      "  207859/5000000: episode: 369, duration: 37.616s, episode steps: 573, steps per second:  15, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.009590, mae: 0.452773, mean_q: 0.577040\n",
      "📈 Episodio 370: Recompensa total (clipped): 13.000, Pasos: 442, Mean Reward Calculado: 0.029412 (Recompensa/Pasos)\n",
      "  208301/5000000: episode: 370, duration: 28.362s, episode steps: 442, steps per second:  16, episode reward: 13.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.910 [0.000, 5.000],  loss: 0.009903, mae: 0.452353, mean_q: 0.575699\n",
      "📈 Episodio 371: Recompensa total (clipped): 11.000, Pasos: 325, Mean Reward Calculado: 0.033846 (Recompensa/Pasos)\n",
      "  208626/5000000: episode: 371, duration: 21.294s, episode steps: 325, steps per second:  15, episode reward: 11.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.011949, mae: 0.458176, mean_q: 0.584339\n",
      "📈 Episodio 372: Recompensa total (clipped): 12.000, Pasos: 424, Mean Reward Calculado: 0.028302 (Recompensa/Pasos)\n",
      "  209050/5000000: episode: 372, duration: 28.344s, episode steps: 424, steps per second:  15, episode reward: 12.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.010132, mae: 0.453689, mean_q: 0.577061\n",
      "📈 Episodio 373: Recompensa total (clipped): 15.000, Pasos: 381, Mean Reward Calculado: 0.039370 (Recompensa/Pasos)\n",
      "  209431/5000000: episode: 373, duration: 25.500s, episode steps: 381, steps per second:  15, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.012005, mae: 0.463708, mean_q: 0.590205\n",
      "📈 Episodio 374: Recompensa total (clipped): 12.000, Pasos: 431, Mean Reward Calculado: 0.027842 (Recompensa/Pasos)\n",
      "  209862/5000000: episode: 374, duration: 30.677s, episode steps: 431, steps per second:  14, episode reward: 12.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011156, mae: 0.474938, mean_q: 0.604439\n",
      "📈 Episodio 375: Recompensa total (clipped): 23.000, Pasos: 696, Mean Reward Calculado: 0.033046 (Recompensa/Pasos)\n",
      "  210558/5000000: episode: 375, duration: 47.965s, episode steps: 696, steps per second:  15, episode reward: 23.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.011084, mae: 0.477149, mean_q: 0.609106\n",
      "📈 Episodio 376: Recompensa total (clipped): 18.000, Pasos: 484, Mean Reward Calculado: 0.037190 (Recompensa/Pasos)\n",
      "  211042/5000000: episode: 376, duration: 32.567s, episode steps: 484, steps per second:  15, episode reward: 18.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.010058, mae: 0.485059, mean_q: 0.617983\n",
      "📈 Episodio 377: Recompensa total (clipped): 15.000, Pasos: 509, Mean Reward Calculado: 0.029470 (Recompensa/Pasos)\n",
      "  211551/5000000: episode: 377, duration: 35.386s, episode steps: 509, steps per second:  14, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.011951, mae: 0.486084, mean_q: 0.618898\n",
      "📈 Episodio 378: Recompensa total (clipped): 20.000, Pasos: 662, Mean Reward Calculado: 0.030211 (Recompensa/Pasos)\n",
      "  212213/5000000: episode: 378, duration: 45.873s, episode steps: 662, steps per second:  14, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.010891, mae: 0.497454, mean_q: 0.636486\n",
      "📈 Episodio 379: Recompensa total (clipped): 20.000, Pasos: 649, Mean Reward Calculado: 0.030817 (Recompensa/Pasos)\n",
      "  212862/5000000: episode: 379, duration: 45.293s, episode steps: 649, steps per second:  14, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010691, mae: 0.499704, mean_q: 0.637912\n",
      "📈 Episodio 380: Recompensa total (clipped): 11.000, Pasos: 426, Mean Reward Calculado: 0.025822 (Recompensa/Pasos)\n",
      "  213288/5000000: episode: 380, duration: 29.010s, episode steps: 426, steps per second:  15, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.011368, mae: 0.490544, mean_q: 0.624892\n",
      "📈 Episodio 381: Recompensa total (clipped): 24.000, Pasos: 551, Mean Reward Calculado: 0.043557 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.84 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.8_ep381.h5f\n",
      "  213839/5000000: episode: 381, duration: 37.616s, episode steps: 551, steps per second:  15, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.010951, mae: 0.494200, mean_q: 0.629580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 382: Recompensa total (clipped): 18.000, Pasos: 506, Mean Reward Calculado: 0.035573 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 14.92 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg14.9_ep382.h5f\n",
      "  214345/5000000: episode: 382, duration: 33.350s, episode steps: 506, steps per second:  15, episode reward: 18.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.011578, mae: 0.482503, mean_q: 0.614335\n",
      "📈 Episodio 383: Recompensa total (clipped): 9.000, Pasos: 396, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
      "  214741/5000000: episode: 383, duration: 25.717s, episode steps: 396, steps per second:  15, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011034, mae: 0.490884, mean_q: 0.626238\n",
      "📈 Episodio 384: Recompensa total (clipped): 16.000, Pasos: 526, Mean Reward Calculado: 0.030418 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.00 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.0_ep384.h5f\n",
      "  215267/5000000: episode: 384, duration: 34.830s, episode steps: 526, steps per second:  15, episode reward: 16.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.011232, mae: 0.480202, mean_q: 0.612088\n",
      "📈 Episodio 385: Recompensa total (clipped): 19.000, Pasos: 426, Mean Reward Calculado: 0.044601 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.12 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.1_ep385.h5f\n",
      "  215693/5000000: episode: 385, duration: 28.158s, episode steps: 426, steps per second:  15, episode reward: 19.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.010120, mae: 0.488696, mean_q: 0.621591\n",
      "📈 Episodio 386: Recompensa total (clipped): 22.000, Pasos: 610, Mean Reward Calculado: 0.036066 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.18 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.2_ep386.h5f\n",
      "  216303/5000000: episode: 386, duration: 39.569s, episode steps: 610, steps per second:  15, episode reward: 22.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.010732, mae: 0.479849, mean_q: 0.609126\n",
      "📈 Episodio 387: Recompensa total (clipped): 13.000, Pasos: 352, Mean Reward Calculado: 0.036932 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.26 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.3_ep387.h5f\n",
      "  216655/5000000: episode: 387, duration: 23.482s, episode steps: 352, steps per second:  15, episode reward: 13.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011396, mae: 0.491224, mean_q: 0.627937\n",
      "📈 Episodio 388: Recompensa total (clipped): 13.000, Pasos: 427, Mean Reward Calculado: 0.030445 (Recompensa/Pasos)\n",
      "  217082/5000000: episode: 388, duration: 28.353s, episode steps: 427, steps per second:  15, episode reward: 13.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.011030, mae: 0.496367, mean_q: 0.634596\n",
      "📈 Episodio 389: Recompensa total (clipped): 21.000, Pasos: 596, Mean Reward Calculado: 0.035235 (Recompensa/Pasos)\n",
      "  217678/5000000: episode: 389, duration: 39.130s, episode steps: 596, steps per second:  15, episode reward: 21.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.010832, mae: 0.497427, mean_q: 0.632311\n",
      "📈 Episodio 390: Recompensa total (clipped): 25.000, Pasos: 690, Mean Reward Calculado: 0.036232 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.29 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.3_ep390.h5f\n",
      "  218368/5000000: episode: 390, duration: 45.675s, episode steps: 690, steps per second:  15, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.012149, mae: 0.493599, mean_q: 0.629344\n",
      "📈 Episodio 391: Recompensa total (clipped): 21.000, Pasos: 745, Mean Reward Calculado: 0.028188 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.39 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.4_ep391.h5f\n",
      "  219113/5000000: episode: 391, duration: 47.890s, episode steps: 745, steps per second:  16, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011245, mae: 0.497631, mean_q: 0.634026\n",
      "📈 Episodio 392: Recompensa total (clipped): 11.000, Pasos: 366, Mean Reward Calculado: 0.030055 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.42 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.4_ep392.h5f\n",
      "  219479/5000000: episode: 392, duration: 24.403s, episode steps: 366, steps per second:  15, episode reward: 11.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011596, mae: 0.489923, mean_q: 0.624146\n",
      "📊 Paso 220,000/5,000,000 (4.4%) - 19.5 pasos/seg - ETA: 68.1h - Memoria: 2341.26 MB\n",
      "📈 Episodio 393: Recompensa total (clipped): 21.000, Pasos: 704, Mean Reward Calculado: 0.029830 (Recompensa/Pasos)\n",
      "  220183/5000000: episode: 393, duration: 47.317s, episode steps: 704, steps per second:  15, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.010180, mae: 0.490245, mean_q: 0.623080\n",
      "📈 Episodio 394: Recompensa total (clipped): 16.000, Pasos: 449, Mean Reward Calculado: 0.035635 (Recompensa/Pasos)\n",
      "  220632/5000000: episode: 394, duration: 29.717s, episode steps: 449, steps per second:  15, episode reward: 16.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.010273, mae: 0.513348, mean_q: 0.652418\n",
      "📈 Episodio 395: Recompensa total (clipped): 20.000, Pasos: 687, Mean Reward Calculado: 0.029112 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.50 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.5_ep395.h5f\n",
      "  221319/5000000: episode: 395, duration: 46.707s, episode steps: 687, steps per second:  15, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.009948, mae: 0.500759, mean_q: 0.636381\n",
      "📈 Episodio 396: Recompensa total (clipped): 9.000, Pasos: 410, Mean Reward Calculado: 0.021951 (Recompensa/Pasos)\n",
      "  221729/5000000: episode: 396, duration: 27.698s, episode steps: 410, steps per second:  15, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.010668, mae: 0.499591, mean_q: 0.633983\n",
      "📈 Episodio 397: Recompensa total (clipped): 23.000, Pasos: 694, Mean Reward Calculado: 0.033141 (Recompensa/Pasos)\n",
      "  222423/5000000: episode: 397, duration: 44.896s, episode steps: 694, steps per second:  15, episode reward: 23.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.009985, mae: 0.507327, mean_q: 0.642492\n",
      "📈 Episodio 398: Recompensa total (clipped): 18.000, Pasos: 497, Mean Reward Calculado: 0.036217 (Recompensa/Pasos)\n",
      "  222920/5000000: episode: 398, duration: 30.808s, episode steps: 497, steps per second:  16, episode reward: 18.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.010936, mae: 0.510292, mean_q: 0.647001\n",
      "📈 Episodio 399: Recompensa total (clipped): 11.000, Pasos: 382, Mean Reward Calculado: 0.028796 (Recompensa/Pasos)\n",
      "  223302/5000000: episode: 399, duration: 23.769s, episode steps: 382, steps per second:  16, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.010497, mae: 0.498415, mean_q: 0.631115\n",
      "📈 Episodio 400: Recompensa total (clipped): 15.000, Pasos: 494, Mean Reward Calculado: 0.030364 (Recompensa/Pasos)\n",
      "\n",
      "📊 EPISODIO 400 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 15.00\n",
      "   Media últimos 100: 15.47 / 20.0\n",
      "   Mejor promedio histórico: 15.50\n",
      "   Estado: 📈 77.4% del objetivo\n",
      "   Episodios en objetivo: 0\n",
      "   Episodios consecutivos en objetivo: 0\n",
      "✅ Pesos guardados en checkpoints/DDQN_weights_episode_400.h5f tras el episodio 400\n",
      "  223796/5000000: episode: 400, duration: 32.428s, episode steps: 494, steps per second:  15, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.010722, mae: 0.512080, mean_q: 0.647660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 401: Recompensa total (clipped): 24.000, Pasos: 743, Mean Reward Calculado: 0.032301 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.65 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.7_ep401.h5f\n",
      "  224539/5000000: episode: 401, duration: 49.734s, episode steps: 743, steps per second:  15, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.010255, mae: 0.509078, mean_q: 0.644946\n",
      "📈 Episodio 402: Recompensa total (clipped): 7.000, Pasos: 377, Mean Reward Calculado: 0.018568 (Recompensa/Pasos)\n",
      "  224916/5000000: episode: 402, duration: 24.905s, episode steps: 377, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.010109, mae: 0.501099, mean_q: 0.636467\n",
      "📈 Episodio 403: Recompensa total (clipped): 13.000, Pasos: 357, Mean Reward Calculado: 0.036415 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.67 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.7_ep403.h5f\n",
      "  225273/5000000: episode: 403, duration: 23.570s, episode steps: 357, steps per second:  15, episode reward: 13.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.011481, mae: 0.509242, mean_q: 0.644480\n",
      "📈 Episodio 404: Recompensa total (clipped): 11.000, Pasos: 382, Mean Reward Calculado: 0.028796 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.70 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.7_ep404.h5f\n",
      "  225655/5000000: episode: 404, duration: 26.025s, episode steps: 382, steps per second:  15, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.008848, mae: 0.508172, mean_q: 0.644991\n",
      "📈 Episodio 405: Recompensa total (clipped): 14.000, Pasos: 572, Mean Reward Calculado: 0.024476 (Recompensa/Pasos)\n",
      "  226227/5000000: episode: 405, duration: 38.304s, episode steps: 572, steps per second:  15, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.010082, mae: 0.498801, mean_q: 0.635096\n",
      "📈 Episodio 406: Recompensa total (clipped): 17.000, Pasos: 503, Mean Reward Calculado: 0.033797 (Recompensa/Pasos)\n",
      "  226730/5000000: episode: 406, duration: 34.159s, episode steps: 503, steps per second:  15, episode reward: 17.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.011631, mae: 0.503416, mean_q: 0.639202\n",
      "📈 Episodio 407: Recompensa total (clipped): 20.000, Pasos: 583, Mean Reward Calculado: 0.034305 (Recompensa/Pasos)\n",
      "  227313/5000000: episode: 407, duration: 37.766s, episode steps: 583, steps per second:  15, episode reward: 20.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.175 [0.000, 5.000],  loss: 0.010126, mae: 0.504655, mean_q: 0.640719\n",
      "📈 Episodio 408: Recompensa total (clipped): 20.000, Pasos: 675, Mean Reward Calculado: 0.029630 (Recompensa/Pasos)\n",
      "  227988/5000000: episode: 408, duration: 43.911s, episode steps: 675, steps per second:  15, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.009313, mae: 0.493999, mean_q: 0.626266\n",
      "📈 Episodio 409: Recompensa total (clipped): 11.000, Pasos: 349, Mean Reward Calculado: 0.031519 (Recompensa/Pasos)\n",
      "  228337/5000000: episode: 409, duration: 22.977s, episode steps: 349, steps per second:  15, episode reward: 11.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.010475, mae: 0.505318, mean_q: 0.640469\n",
      "📈 Episodio 410: Recompensa total (clipped): 15.000, Pasos: 474, Mean Reward Calculado: 0.031646 (Recompensa/Pasos)\n",
      "  228811/5000000: episode: 410, duration: 30.947s, episode steps: 474, steps per second:  15, episode reward: 15.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.011295, mae: 0.502686, mean_q: 0.640644\n",
      "📈 Episodio 411: Recompensa total (clipped): 8.000, Pasos: 381, Mean Reward Calculado: 0.020997 (Recompensa/Pasos)\n",
      "  229192/5000000: episode: 411, duration: 24.806s, episode steps: 381, steps per second:  15, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.007916, mae: 0.505066, mean_q: 0.641305\n",
      "📈 Episodio 412: Recompensa total (clipped): 23.000, Pasos: 648, Mean Reward Calculado: 0.035494 (Recompensa/Pasos)\n",
      "  229840/5000000: episode: 412, duration: 41.631s, episode steps: 648, steps per second:  16, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.010732, mae: 0.499225, mean_q: 0.636713\n",
      "📈 Episodio 413: Recompensa total (clipped): 28.000, Pasos: 819, Mean Reward Calculado: 0.034188 (Recompensa/Pasos)\n",
      "  230659/5000000: episode: 413, duration: 53.999s, episode steps: 819, steps per second:  15, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.010293, mae: 0.531298, mean_q: 0.675733\n",
      "📈 Episodio 414: Recompensa total (clipped): 24.000, Pasos: 625, Mean Reward Calculado: 0.038400 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.75 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.8_ep414.h5f\n",
      "  231284/5000000: episode: 414, duration: 40.567s, episode steps: 625, steps per second:  15, episode reward: 24.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011444, mae: 0.538693, mean_q: 0.684444\n",
      "📈 Episodio 415: Recompensa total (clipped): 14.000, Pasos: 497, Mean Reward Calculado: 0.028169 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.79 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.8_ep415.h5f\n",
      "  231781/5000000: episode: 415, duration: 32.111s, episode steps: 497, steps per second:  15, episode reward: 14.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.011593, mae: 0.539288, mean_q: 0.686104\n",
      "📈 Episodio 416: Recompensa total (clipped): 8.000, Pasos: 377, Mean Reward Calculado: 0.021220 (Recompensa/Pasos)\n",
      "  232158/5000000: episode: 416, duration: 24.094s, episode steps: 377, steps per second:  16, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010456, mae: 0.527286, mean_q: 0.672964\n",
      "📈 Episodio 417: Recompensa total (clipped): 23.000, Pasos: 634, Mean Reward Calculado: 0.036278 (Recompensa/Pasos)\n",
      "  232792/5000000: episode: 417, duration: 40.615s, episode steps: 634, steps per second:  16, episode reward: 23.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.011109, mae: 0.529232, mean_q: 0.671864\n",
      "📈 Episodio 418: Recompensa total (clipped): 17.000, Pasos: 444, Mean Reward Calculado: 0.038288 (Recompensa/Pasos)\n",
      "  233236/5000000: episode: 418, duration: 29.976s, episode steps: 444, steps per second:  15, episode reward: 17.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.011403, mae: 0.547051, mean_q: 0.695421\n",
      "📈 Episodio 419: Recompensa total (clipped): 13.000, Pasos: 381, Mean Reward Calculado: 0.034121 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.81 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.8_ep419.h5f\n",
      "  233617/5000000: episode: 419, duration: 25.500s, episode steps: 381, steps per second:  15, episode reward: 13.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.010457, mae: 0.541808, mean_q: 0.688234\n",
      "📈 Episodio 420: Recompensa total (clipped): 10.000, Pasos: 387, Mean Reward Calculado: 0.025840 (Recompensa/Pasos)\n",
      "  234004/5000000: episode: 420, duration: 26.113s, episode steps: 387, steps per second:  15, episode reward: 10.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.011146, mae: 0.533537, mean_q: 0.677653\n",
      "📈 Episodio 421: Recompensa total (clipped): 15.000, Pasos: 490, Mean Reward Calculado: 0.030612 (Recompensa/Pasos)\n",
      "  234494/5000000: episode: 421, duration: 30.019s, episode steps: 490, steps per second:  16, episode reward: 15.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.010666, mae: 0.548194, mean_q: 0.698057\n",
      "📈 Episodio 422: Recompensa total (clipped): 18.000, Pasos: 632, Mean Reward Calculado: 0.028481 (Recompensa/Pasos)\n",
      "  235126/5000000: episode: 422, duration: 40.283s, episode steps: 632, steps per second:  16, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.010561, mae: 0.542901, mean_q: 0.689596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 423: Recompensa total (clipped): 21.000, Pasos: 601, Mean Reward Calculado: 0.034942 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.88 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg15.9_ep423.h5f\n",
      "  235727/5000000: episode: 423, duration: 39.964s, episode steps: 601, steps per second:  15, episode reward: 21.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.010536, mae: 0.532520, mean_q: 0.676791\n",
      "📈 Episodio 424: Recompensa total (clipped): 24.000, Pasos: 744, Mean Reward Calculado: 0.032258 (Recompensa/Pasos)\n",
      "  236471/5000000: episode: 424, duration: 45.678s, episode steps: 744, steps per second:  16, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.011516, mae: 0.544390, mean_q: 0.693243\n",
      "📈 Episodio 425: Recompensa total (clipped): 20.000, Pasos: 608, Mean Reward Calculado: 0.032895 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 15.98 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.0_ep425.h5f\n",
      "  237079/5000000: episode: 425, duration: 38.965s, episode steps: 608, steps per second:  16, episode reward: 20.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.010586, mae: 0.537596, mean_q: 0.681528\n",
      "📈 Episodio 426: Recompensa total (clipped): 11.000, Pasos: 419, Mean Reward Calculado: 0.026253 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 16.05 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.1_ep426.h5f\n",
      "  237498/5000000: episode: 426, duration: 27.407s, episode steps: 419, steps per second:  15, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.012 [0.000, 5.000],  loss: 0.010843, mae: 0.529567, mean_q: 0.674463\n",
      "📈 Episodio 427: Recompensa total (clipped): 14.000, Pasos: 485, Mean Reward Calculado: 0.028866 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 16.10 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.1_ep427.h5f\n",
      "  237983/5000000: episode: 427, duration: 31.746s, episode steps: 485, steps per second:  15, episode reward: 14.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.039 [0.000, 5.000],  loss: 0.009927, mae: 0.535087, mean_q: 0.679944\n",
      "📈 Episodio 428: Recompensa total (clipped): 26.000, Pasos: 833, Mean Reward Calculado: 0.031212 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 16.25 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.2_ep428.h5f\n",
      "  238816/5000000: episode: 428, duration: 51.123s, episode steps: 833, steps per second:  16, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010623, mae: 0.533770, mean_q: 0.679839\n",
      "📈 Episodio 429: Recompensa total (clipped): 19.000, Pasos: 560, Mean Reward Calculado: 0.033929 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 16.32 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.3_ep429.h5f\n",
      "  239376/5000000: episode: 429, duration: 34.238s, episode steps: 560, steps per second:  16, episode reward: 19.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.010189, mae: 0.531634, mean_q: 0.679856\n",
      "📊 Paso 240,000/5,000,000 (4.8%) - 19.1 pasos/seg - ETA: 69.3h - Memoria: 2502.91 MB\n",
      "📈 Episodio 430: Recompensa total (clipped): 24.000, Pasos: 821, Mean Reward Calculado: 0.029233 (Recompensa/Pasos)\n",
      "💾 NUEVO MEJOR PROMEDIO: 16.36 - Guardado: checkpoints/DDQN_better-trains_1750663521/best_model_avg16.4_ep430.h5f\n",
      "  240197/5000000: episode: 430, duration: 49.340s, episode steps: 821, steps per second:  17, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.011739, mae: 0.548685, mean_q: 0.697899\n",
      "📈 Episodio 431: Recompensa total (clipped): 15.000, Pasos: 381, Mean Reward Calculado: 0.039370 (Recompensa/Pasos)\n",
      "  240578/5000000: episode: 431, duration: 22.266s, episode steps: 381, steps per second:  17, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.011927, mae: 0.593931, mean_q: 0.758881\n",
      "📈 Episodio 432: Recompensa total (clipped): 11.000, Pasos: 371, Mean Reward Calculado: 0.029650 (Recompensa/Pasos)\n",
      "  240949/5000000: episode: 432, duration: 21.908s, episode steps: 371, steps per second:  17, episode reward: 11.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013064, mae: 0.577546, mean_q: 0.735129\n",
      "📈 Episodio 433: Recompensa total (clipped): 10.000, Pasos: 268, Mean Reward Calculado: 0.037313 (Recompensa/Pasos)\n",
      "  241217/5000000: episode: 433, duration: 15.981s, episode steps: 268, steps per second:  17, episode reward: 10.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011296, mae: 0.565527, mean_q: 0.719143\n",
      "📈 Episodio 434: Recompensa total (clipped): 18.000, Pasos: 661, Mean Reward Calculado: 0.027231 (Recompensa/Pasos)\n",
      "  241878/5000000: episode: 434, duration: 39.513s, episode steps: 661, steps per second:  17, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.012184, mae: 0.585445, mean_q: 0.744282\n",
      "📈 Episodio 435: Recompensa total (clipped): 32.000, Pasos: 807, Mean Reward Calculado: 0.039653 (Recompensa/Pasos)\n",
      "  242685/5000000: episode: 435, duration: 48.851s, episode steps: 807, steps per second:  17, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011915, mae: 0.589156, mean_q: 0.746791\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True\n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "    episode_render = False\n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    # Optimizar configuración de TensorFlow\n",
    "    optimizar_tensorflow()\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    gc.collect()    \n",
    "\n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    # Ejecutar prueba\n",
    "    print(\"🚀 EJECUTANDO SOLUCIÓN...\")\n",
    "    print(f\"🎯 OBJETIVO: Conseguir media de episode_reward = {TARGET_REWARD} (con clipping)\")    \n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', create_dqn_model, False),\n",
    "        ('DDQN', create_ddqn_models, True),\n",
    "        ('DDQN_REPLAY', create_ddqn_replay_model, False),\n",
    "        ('DUELING_DQN_REPLAY', create_dueling_dqn_replay_model, False)\n",
    "    ]\n",
    "    \n",
    "    for model_name, model_process, training_specific_flag in modelos_a_procesar:\n",
    "        # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "        entrenarSN = training_global and training_specific_flag\n",
    "        model_instance = None\n",
    "        if entrenarSN:\n",
    "            episode_weights = sorted(\n",
    "                [f for f in os.listdir(checkpoint_path) if f.startswith(f\"{model_name}_weights_episode_\")],\n",
    "                key=lambda x: int(re.search(r'episode_(\\d+)', x).group(1)) if re.search(r'episode_(\\d+)', x) else 0\n",
    "            )\n",
    "            if episode_weights:\n",
    "                latest_weights = os.path.join(checkpoint_path, episode_weights[-1])\n",
    "                print(f\"Cargando pesos desde {latest_weights}\")\n",
    "                dqn_auto, success_auto = entrenar_modelo(\n",
    "                    env, model_name, model_process, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                    checkpoint_path=checkpoint_path, input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size,\n",
    "                    warmup_steps=WARMUP_STEPS, target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau,\n",
    "                    epsilon_start=epsilon_start, epsilon_min=epsilon_stop, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS\n",
    "                )\n",
    "                if dqn_auto:\n",
    "                    dqn_auto.load_weights(latest_weights)\n",
    "                    model_instance = dqn_auto\n",
    "            else:\n",
    "                model_instance, success_auto = entrenar_modelo(\n",
    "                    env, model_name, model_process, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                    checkpoint_path=checkpoint_path, input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size,\n",
    "                    warmup_steps=WARMUP_STEPS, target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau,\n",
    "                    epsilon_start=epsilon_start, epsilon_min=epsilon_stop, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS\n",
    "                )\n",
    "            if model_instance:\n",
    "                rewards = evaluar_modelo(model_instance, env, num_episodes=10, render=False, record_video=False)\n",
    "                print(f\"Recompensa promedio para {model_name}: {np.mean(rewards):.2f}\")\n",
    "                trained_models[model_name] = model_instance  \n",
    "                \n",
    "    if model_instance:\n",
    "        print(\"\\n✅ SOLUCIÓN EXITOSA - Entrenamiento completado\")\n",
    "        print(f\"📁 Archivos en: {checkpoint_path}/{model_name}\")\n",
    "\n",
    "        # Evaluación final\n",
    "        print(f\"\\n🎯 EVALUACIÓN FINAL DEL OBJETIVO\")\n",
    "        rewards_eval, objetivo_conseguido = evaluar_modelo(dqn_auto, num_episodes=200)\n",
    "\n",
    "        if objetivo_conseguido:\n",
    "            print(f\"🏆 ¡FELICIDADES EQUIPO! El modelo alcanzó el objetivo de media 20\")\n",
    "        else:\n",
    "            print(f\"📈 El modelo necesita más entrenamiento para alcanzar media 20\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Error en el entrenamiento\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68dzFiUS2U6L"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
