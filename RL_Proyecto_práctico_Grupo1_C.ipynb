{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Actividad - Proyecto pr√°ctico\n",
    "\n",
    "> La actividad se desarrollar√° en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfab√©tico (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah\n",
    "* Alumno 2: Cuesta Cifuentes, Jair\n",
    "* Alumno 3: Gonz√°lez Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalaci√≥n y requisitos previos\n",
    "\n",
    "> Las pr√°cticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderizaci√≥n en gym. Por ello, para obtener estas visualizaciones, se deber√° trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la secci√≥n *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deber√° ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la secci√≥n *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librer√≠as necesarias, siguiendo la secci√≥n *1.4.Instalar librer√≠as necesarias*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el c√≥digo que se presenta comentado en la pr√≥xima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librer√≠as b√°sicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2. Preparar Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "El entorno de Colab est√° preinstalado con una serie de librer√≠as por defecto. Para trabajar en base a las especificaciones del ejercicio se necesitan intalar unas librer√≠as que bajen de versi√≥n las existentes en Colab. Entre ellas tensorflow. El problema de realizar esta acci√≥n es que para que todas las versiones sean consideradas por el entorno hay que reiniciar la sesi√≥n, sino se mantienen dependencias y los import no funcionan. <br>\n",
    "Es decir tras los \"pip install\" hay que hacer un **\"Runtime > Restart runtime\"** o si tienes Colab en espa√±ol: \"Entorno de Ejecuci√≥n/Reiniciar sesion\".<br>\n",
    "En este punto se ha de tener presente que se ha reiniciado y **se han perdido las variables** que se hayan establecido, por ese motivo repetiremos el c√≥digo para identificar si estamos en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Instalar librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "Ignorar los errores que puedan aparecer, son incompatibilidades con librer√≠as avanzadas que no utilizamos ni necesitamos para nuestro c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#Si ya tenemos lasa librer√≠as cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB:  \n",
    "# =========================\n",
    "#  Entorno Colab nativo con todo lo compatible.\n",
    "#  S√≥lo recordar que se debe REINICIAR EL RUNTIME (al acabar)\n",
    "# =========================  \n",
    "  print(\" [INFO] - Instalando paquetes adicionales...\")  \n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Ignorar los errores que aparecen:\")\n",
    "  print(\"   Son incompatibilidades que aparecen con librer√≠a avanzadas\")\n",
    "  print(\"   que no necesitamos ni vamos a utilizar\")  \n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12.1 --quiet\n",
    "  # Instala imageio y sus dependencias para video\n",
    "  %pip install imageio==2.15.0 --quiet\n",
    "  %pip install imageio-ffmpeg\n",
    "  %pip install ffmpeg-python  \n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"INSTALACI√ìN COMPLETADA\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\")\n",
    "  print(\"1. Ve a Runtime > Restart runtime\")\n",
    "  print(\"2. Despu√©s ejecuta las importaciones\")\n",
    "  print(\"=\"*60)  \n",
    "  INSTALL_LOCAL = False\n",
    "  IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB_ENV:\n",
    "# =========================\n",
    "#  Colab con env --> \n",
    "#    no funciona muy bien pues aunque se cree el entorno, Colab sigue\n",
    "#    utilizando el suyo con sus librer√≠a y se necesita usar %%writefile\n",
    "# =========================      \n",
    "  # 1. Instalar virtualenv\n",
    "  !pip install virtualenv --quiet\n",
    "\n",
    "  # 3. Crear el entorno virtual llamado \"miar_rl\"\n",
    "  !virtualenv miar_rl\n",
    "\n",
    "  # 4. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
    "  !./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
    "  !./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
    "  !./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
    "  !./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
    "  !./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
    "\n",
    "  # 5. Librer√≠as adicionales\n",
    "  !./miar_rl/bin/pip install Pillow\n",
    "  !./miar_rl/bin/pip install matplotlib\n",
    "  !./miar_rl/bin/pip install tqdm\n",
    "  INSTALL_LOCAL = False\n",
    "    \n",
    "if INSTALL_LOCAL:    \n",
    "# =========================\n",
    "#  Librer√≠a para trabajar en local, si NO se cargaron las \n",
    "#    librer√≠as desde fichero requirements\n",
    "# =========================        \n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm\n",
    "  # Instala imageio y sus dependencias para video\n",
    "  %pip install imageio==2.15.0 --quiet\n",
    "  %pip install imageio-ffmpeg\n",
    "  %pip install ffmpeg-python  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.3. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE:**<br>\n",
    "Recordar que antes de seguir (si hemos decidido el entorno de Colab nativo - IN_COLAB=True -) \n",
    "* Hay que hacer un <font color='red'>\"Runtime > Restart runtime\"</font> o si tienes Colab en espa√±ol: \"Entorno de Ejecuci√≥n/Reiniciar sesion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si ya tenemos lasa librer√≠as cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCI√ìN!! Modificar ruta relativa a la pr√°ctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_1\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.4. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "3lN7KLe05NSa",
    "outputId": "47c41c84-3bfd-425f-9b8d-6d6aa525afdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.viminfo', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_dqn (1).ipynb', '08MIAR_dqn (2).ipynb', '08MIAR_dqn (3).ipynb', '08MIAR_dqn (4).ipynb', '08MIAR_dqn (5).ipynb', '08MIAR_dqn.ipynb', '08miar_dqn.py', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoint', 'checkpoints', 'Configuraci√≥n local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'diagnosticos', 'Documents', 'Downloads', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'DuelingDQN_3 - copia.csv', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'fffff.py', 'ffmpeg', 'ffmpeg-2025-06-17-git-ee1f79b0fa-essentials_build.7z', 'ffmpeg-2025-06-17-git-ee1f79b0fa-full_build.7z', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'lista.txt', 'menory.txt', 'Men√∫ Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'models', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'output-train-DDQN.csv', 'output-train-DDQN.txt', 'output-train-DuelingDQN.csv', 'output-train-DuelingDQN_OK1.txt', 'output-train-DuelingDQN_OK2.txt', 'Pictures', 'Plantillas', 'ppppp.ipynb', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_pr√°ctico_Grupo1_C.ipynb', 'RL_Proyecto_pr√°ctico_Grupo1_C.py', 'RL_Proyecto_pr√°ctico_Grupo1_C_VERSION_COLAB_V3.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'start.bat', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled221.ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'v3.ipynb', 'Videos', '_RL_Proyecto_pr√°ctico_Grupo1_C_v2.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Cambiar al directorio en Google Drive que deseas usar\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    print(\" [INFO] - Estamos ejecutando en Colab\")\n",
    "    # Montar Google Drive en el punto de montaje\n",
    "    print(\" [INFO] - Colab: montando Google drive en: \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "    # Crear drive_root si no existe\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\n [INFO] - Colab: Asegurando que \", drive_root, \" existe.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Cambiar al directorio\n",
    "    print(\"\\n [INFO] - Colab: Cambiamos el directorio a: \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verificar que estamos en el directorio de trabajo correcto\n",
    "%pwd\n",
    "print(\" [INFO] - Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihTI9TOD43ML"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos ser√° _SpaceInvaders-v0_ y el algoritmo que usaremos ser√° _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito m√≠nimo ser√° alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calcular√° a partir del c√≥digo de test en la √∫ltima celda del notebook.\n",
    "\n",
    "Este proyecto pr√°ctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usar√° en la soluci√≥n\n",
    "2.   Implementar las distintas piezas de la soluci√≥n DQN\n",
    "3.   Justificar la respuesta en relaci√≥n a los resultados obtenidos\n",
    "\n",
    "**R√∫brica**: Se valorar√° la originalidad en la soluci√≥n aportada, as√≠ como la capacidad de discutir los resultados de forma detallada. El requisito m√≠nimo servir√° para aprobar la actividad, bajo premisa de que la discusi√≥n del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuaci√≥n √≥ptima, responder sobre la mejor puntuaci√≥n obtenida.\n",
    "* Para entrenamientos largos, recordad que pod√©is usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los par√°metros adecuadamente (sobre todo los relacionados con el proceso de exploraci√≥n).\n",
    "* Se deber√° entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deber√° de subir la soluci√≥n de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIAR9zQv43MO"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas\n",
    "\n",
    "#### Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4o4-N4T43MO"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2      # Para preprocesamiento de im√°genes si se usa AtariProcessor\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "import json\n",
    "import datetime\n",
    "import IPython\n",
    "import imageio\n",
    "import pandas as pd\n",
    "\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
    "from IPython.core.history import HistoryManager\n",
    "from tensorflow.keras.models import  clone_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "if IN_COLAB:  \n",
    "  from tensorflow.keras.optimizers.legacy import Adam\n",
    "else:\n",
    "  from tensorflow.keras.optimizers import Adam\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puzS2kTzc1Fd"
   },
   "outputs": [],
   "source": [
    "# Necesario para la grabaci√≥n de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "    from gym.wrappers import Monitor \n",
    "except ImportError:\n",
    "    print(\" [WARNING] - gym.wrappers no est√° disponible. La grabaci√≥n de video no funcionar√°.\")\n",
    "    gym.wrappers = None # Asegurar que no d√© error si no se encuentra\n",
    "\n",
    "# Activar precisi√≥n mixta para mayor velocidad\n",
    "print(\" [INFO] - Precisi√≥n mixta activada (mixed_float16)\")\n",
    "# Activar optimizaci√≥n JIT\n",
    "tf.config.optimizer.set_jit(True)\n",
    "print(\" [INFO] - Optimizaci√≥n JIT activada\")    \n",
    "\n",
    "# Hay veces que se produce un error OperationalError('database or disk is full') indica que la \n",
    "# base de datos SQLite que usa Jupyter para guardar el historial de comandos est√° llena. \n",
    "# Esto explica por qu√© no puedes guardar aunque tengas 500GB libres.\n",
    "# Desactivar guardado de historial para esta sesi√≥n\n",
    "ip = IPython.get_ipython()\n",
    "ip.history_manager.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8iRCStcc90p",
    "outputId": "96045a07-febd-4dce-e5c8-bbec04475b33"
   },
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (x cores)\n",
    "def optimizar_tensorflow():\n",
    "    \"\"\"Configura TensorFlow para rendimiento √≥ptimo en CPU/GPU\"\"\"\n",
    "    # Limpiar sesi√≥n previa\n",
    "    gc.collect()\n",
    "\n",
    "    # Optimizaci√≥n de GPU si est√° disponible\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\" [INFO] - GPU optimizada para crecimiento adaptativo de memoria\")\n",
    "        except Exception as e:\n",
    "            print(f\" [INFO] - Error al configurar GPU: {e}\")\n",
    "\n",
    "    # Optimizaci√≥n de CPU\n",
    "    num_cpu_cores = os.cpu_count() or 8  # Fallback a 8 si no se puede detectar\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_cpu_cores // 2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(max(2, num_cpu_cores // 4))\n",
    "\n",
    "    # Modo eager solo si es necesario\n",
    "    # Para entrenamiento, es mejor desactivarlo por rendimiento\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    \n",
    "    # Configuraci√≥n para evitar errores de guardado en Colab\n",
    "    if IN_COLAB:\n",
    "      os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "      os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reducir mensajes de TF\n",
    "      # Configuraci√≥n personalizada para guardado de modelos\n",
    "      tf.keras.backend.set_learning_phase(1)  # Asegurarnos que estamos en modo entrenamiento\n",
    "      # Desactivar guardado as√≠ncrono (la causa m√°s com√∫n del error)\n",
    "      if hasattr(tf.config, 'experimental'):\n",
    "        tf.config.experimental.set_synchronous_execution(True)      \n",
    "    \n",
    "    print(f\" [INFO] - TensorFlow optimizado para {num_cpu_cores} cores CPU\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faNbnMuOdNDP"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WFE0sqPdLsy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crear el entorno\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcjoyH3idqh4",
    "outputId": "85af5e47-6b37-472b-ddbe-ce71bea7eb33"
   },
   "outputs": [],
   "source": [
    "print(\"El tama√±o de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El n√∫mero de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgw7iHVRduGa"
   },
   "source": [
    "#### Definici√≥n Hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TshMrqTjdxja"
   },
   "outputs": [],
   "source": [
    "### HIPERPAR√ÅMETROS DEL MODELO\n",
    "# Hiperpar√°metros optimizados\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "CHANNELS = 1                    # Canal para grises\n",
    "USE_FRAMESTACK = True           # Cambiar a True si quieres detecci√≥n de movimiento\n",
    "WINDOW_LENGTH = 4 if USE_FRAMESTACK else 1   # N√∫mero de fotogramas apilados          # La mayor√≠a de implementaciones usan 4 frames\n",
    "batch_size = 32                 # Tama√±o de batch √≥ptimo\n",
    "gamma = 0.99                    # Factor de descuento (mejor que 0.95 para recompensas a largo plazo)\n",
    "learning_rate = 0.00025         # Tasa de aprendizaje est√°ndar para DQN\n",
    "memory_size = 1000000           # Buffer de memoria grande para mejor estabilidad\n",
    "TARGET_UPDATE_INTERVAL = 10000  # Actualizaci√≥n de red objetivo cada 10,000 pasos\n",
    "WARMUP_STEPS = 50000            # Pasos iniciales para llenar la memoria (experiencia aleatoria)\n",
    "NUM_TRAINING_STEPS = 2000000    # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
    "EPSILON_STEPS = 500000          # Total de pasos de evaluaci√≥n del modelo\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH)   # Dimensiones de cada frame\n",
    "\n",
    "# Single frame shape (height, width, channels)\n",
    "FRAME_SHAPE = (HEIGHT, WIDTH, CHANNELS)  # (84, 84, 1)\n",
    "MODEL_INPUT_SHAPE = (HEIGHT, WIDTH, WINDOW_LENGTH)  # Forma para el modelo (channels_last)\n",
    "SEQ_INPUT_SHAPE = (WINDOW_LENGTH,HEIGHT, WIDTH)  # Forma para el modelo (channels_last)\n",
    "\n",
    "### HIPERPAR√ÅMETROS DE PREPROCESAMIENTO\n",
    "# Definir shape consistente\n",
    "if USE_FRAMESTACK:\n",
    "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
    "else:\n",
    "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
    "\n",
    "state_size = (*INPUT_SHAPE, WINDOW_LENGTH)   # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "input_shape = (*INPUT_SHAPE, WINDOW_LENGTH)  # Para la API de keras-rl\n",
    "action_size = env.action_space.n       # 6 acciones posibles\n",
    "learning_rate =  0.00025               # Alfa (tambi√©n conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST      # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST      # M√°ximo de pasos posibles por episodio\n",
    "total_episodios = 100                # Episodios totales para el entrenamiento\n",
    "max_steps       = 3000               # M√°ximo de pasos posibles por episodio\n",
    "\n",
    "# Par√°metros de exploraci√≥n para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploraci√≥n al inicio\n",
    "epsilon_stop = 0.1             # Probabilidad m√≠nima de exploraci√≥n\n",
    "\n",
    "# Hiperpar√°metros del aprendizaje Q\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "TARGET_REWARD = 20.0\n",
    "\n",
    "### HIPERPAR√ÅMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # N√∫mero de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjFRyr4Ld1Hp"
   },
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tama√±o de la imagen (matriz de 210 x 160 p√≠xeles) y la convertimos En una matriz bidimensional de 80 x 80 p√≠xeles). Tambi√©n convertimos las im√°genes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la informaci√≥n innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06wZVH5c43MP"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona m√©todos para convertir observaciones RGB en\n",
    "    im√°genes en escala de grises, redimensionarlas y normalizarlas, as√≠ como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    M√âTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observaci√≥n RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(INPUT_SHAPE)):\n",
    "        self.input_shape = input_shape\n",
    "        # Precargar una imagen negra para inicializaci√≥n\n",
    "        self.black_frame = np.zeros(input_shape, dtype=np.uint8)\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observaci√≥n convirti√©ndola a escala de grises y redimension√°ndola.\n",
    "\n",
    "        Par√°metros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observaci√≥n cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observaci√≥n no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        # Si la observaci√≥n es None, devolver un marco negro\n",
    "        if observation is None:\n",
    "            return self.black_frame\n",
    "\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        # Crop the screen (remove the part below the player)\n",
    "        # [Up: Down, Left: right]\n",
    "        cropped_img = observation[18:-12, 4:-12]\n",
    "        # Optimizaci√≥n: usar cv2 para redimensionar y convertir a escala de grises (m√°s r√°pido que PIL)\n",
    "        resized = cv2.resize(cropped_img, self.input_shape)\n",
    "        processed_observation = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY) if len(resized.shape) == 3 else resized\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype(np.uint8)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Par√°metros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Normaliza la recompensa al rango [-1, 1].\n",
    "\n",
    "        Par√°metros:\n",
    "        -----------\n",
    "            reward (float): Recompensa original del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada al rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        \"\"\"\n",
    "        Procesa un paso completo del entorno.\n",
    "        \n",
    "        Par√°metros:\n",
    "        -----------\n",
    "            observation: Observaci√≥n del entorno.\n",
    "            reward: Recompensa obtenida.\n",
    "            done: Indicador de fin de episodio.\n",
    "            info: Informaci√≥n adicional del entorno.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "            tuple: (observaci√≥n procesada, recompensa procesada, done, info)\n",
    "        \"\"\"\n",
    "        processed_observation = self.process_observation(observation)\n",
    "        processed_reward = self.process_reward(reward)\n",
    "        return processed_observation, processed_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptwluQRXedZP"
   },
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VwBXOBk43MP",
    "outputId": "88dbba58-92a4-4d75-cf4c-799a04c36cbd"
   },
   "outputs": [],
   "source": [
    "print(\" [INFO] - Numero de acciones disponibles: \" + str(nb_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT6osc3H43MP",
    "outputId": "cbe27690-c40d-49b1-d420-80b7d045ce26"
   },
   "outputs": [],
   "source": [
    "print(\" [INFO] - Formato de las observaciones:\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "7-BoOu_eeiAE",
    "outputId": "5b52561e-70d0-47c1-f55e-c882dda20b66"
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for i in range(22):\n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "vHJaGcAVelRY",
    "outputId": "f2686c2b-a159-4c1f-e857-984352520683"
   },
   "outputs": [],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-yCJoGjf2Fg"
   },
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewKKozUaf-mG"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-asignar arrays con el tama√±o exacto\n",
    "        # Usar uint8 para estados (m√°s eficiente que float32)\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        print(f\" [INFO] - ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
    "        memory_size = (\n",
    "            self.states.nbytes + self.next_states.nbytes +\n",
    "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "        ) / (1024 * 1024)\n",
    "        print(f\" [INFO] - Memoria asignada: {memory_size:.2f} MB\")\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"A√±ade una experiencia al buffer de forma eficiente\"\"\"\n",
    "        # Convertir a uint8 para ahorrar memoria (estados son im√°genes 0-255)\n",
    "        if state.dtype != np.uint8:\n",
    "            state = (state * 255).astype(np.uint8)\n",
    "        if next_state.dtype != np.uint8:\n",
    "            next_state = (next_state * 255).astype(np.uint8)\n",
    "\n",
    "        # Almacenar directamente en el array pre-asignado\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        # Actualizar posici√≥n circular\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
    "\n",
    "        # Generar √≠ndices aleatorios\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "\n",
    "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
    "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
    "        batch_actions = self.actions[indices]\n",
    "        batch_rewards = self.rewards[indices]\n",
    "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
    "        batch_dones = self.dones[indices]\n",
    "\n",
    "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        # No es necesario limpiar los arrays, se sobrescriben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-SaTPRkQoK"
   },
   "source": [
    "#### Clase PerformanceMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente no se usa, si se necesitase mayor detalle de la evoluci√≥n de los entrenamientos se podr√≠a incluir en el Callback antes del entrenamiento. De momento no se usa!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf2A5kDokNdS"
   },
   "outputs": [],
   "source": [
    "# Clase para monitoreo de memoria y rendimiento\n",
    "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path='diagnosticos'):\n",
    "        self.save_path = save_path\n",
    "        self.episode_times = []\n",
    "        self.memory_usage = []\n",
    "        self.current_episode = 0\n",
    "        self.episode_start_time = None\n",
    "        self.episode_start_memory = None\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        self.episode_start_time = time.time()\n",
    "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        self.current_episode = episode\n",
    "        print(f\" [INFO] - Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        end_time = time.time()\n",
    "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        episode_duration = end_time - self.episode_start_time\n",
    "\n",
    "        self.episode_times.append(episode_duration)\n",
    "        self.memory_usage.append(final_memory)\n",
    "\n",
    "        print(f\" [INFO] - Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
    "        print(f\" [INFO] - Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
    "\n",
    "        # Guardar diagn√≥stico cada 5 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            self.save_diagnostics(episode)\n",
    "\n",
    "        # Forzar recolecci√≥n de basura\n",
    "        gc.collect()\n",
    "\n",
    "    def save_diagnostics(self, episode):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.episode_times)\n",
    "        plt.title('Tiempo por episodio')\n",
    "        plt.ylabel('Segundos')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.memory_usage)\n",
    "        plt.title('Uso de memoria')\n",
    "        plt.ylabel('MB')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTgDOJoCgISN"
   },
   "source": [
    "### 1. Implementaci√≥n de la red neuronal\n",
    "\n",
    "#### Definici√≥n de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFAzP0UigPVg"
   },
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ0dGSAUgP0a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP2vNpKnzkTl"
   },
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo DQN usando SOLO Keras est√°ndar. Base com√∫n para redes DQN y DDQN\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la funci√≥n Q en aprendizaje por refuerzo.\n",
    "    Construye un modelo que acepta channels_first y convierte internamente\n",
    "\n",
    "    Esta funci√≥n implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acci√≥n posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activaci√≥n RELU.\n",
    "    Esto evita completamente los problemas de grafos m√∫ltiples\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è Creando modelo DQN est√°ndar: input_shape={input_shape}, actions={nb_actions}\")\n",
    "       \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional est√°ndar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DQN_Model')\n",
    "    memory = None\n",
    "    \n",
    "    print(\"‚úÖ Modelo creado exitosamente\")\n",
    "    print(f\"üìä Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model, memory, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_models(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea modelos para Double DQN (principal y objetivo)\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è Creando modelos DDQN: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Modelo principal\n",
    "    main_model, memory, _ = create_dqn_model(input_shape, nb_actions, memory_size)\n",
    "    main_model._name = 'DDQN_Main_Model'    \n",
    "    \n",
    "    # Modelo objetivo (copia exacta)\n",
    "    target_model = tf.keras.models.clone_model(main_model)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    target_model._name = 'DDQN_Target_Model'\n",
    "    \n",
    "    print(\"‚úÖ Modelos DDQN creados exitosamente\")    \n",
    "    return main_model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_replay_model(input_shape, nb_actions, memory_size):\n",
    "    print(f\"üèóÔ∏è Creando modelos DDQN_replay: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional est√°ndar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DDQN_replay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "    target_model = clone_model(model)  # Create target model for DDQN\n",
    "    target_model.set_weights(model.get_weights())  # Initialize with same weights\n",
    "    target_model._name = 'DDQN_replay_Target_Model'    \n",
    "    \n",
    "    print(\"‚úÖ Modelo creado exitosamente\")\n",
    "    print(f\"üìä Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dueling_dqn_replay_model(input_shape, action_size, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo Dueling DQN con replay.\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è Creando modelo DUELING_DQN_REPLAY: input_shape={input_shape}, actions={action_size}\")\n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)    \n",
    "    x = Permute((2, 3, 1))(inputs)\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    # Value stream\n",
    "    value = Dense(512, activation='relu')(x)\n",
    "    value = Dense(1, activation='linear')(value)\n",
    "    # Advantage stream\n",
    "    advantage = Dense(512, activation='relu')(x)\n",
    "    advantage = Dense(action_size, activation='linear')(advantage)\n",
    "    # Combine streams\n",
    "    outputs = Add()([value, Lambda(lambda a: a - K.mean(a, axis=1, keepdims=True))(advantage)])\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='DuelingDQNReplay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)   \n",
    "    target_model = clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())    \n",
    "    target_model._name = 'DuelingDQNReplay_Target_Model'     \n",
    "   \n",
    "    print(\"‚úÖ Modelo creado exitosamente\")\n",
    "    print(f\"üìä Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0DHvKNshvQo"
   },
   "source": [
    "### 2. Implementaci√≥n de la soluci√≥n DQN\n",
    "\n",
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model, memory, target_model, model_name, checkpoint_dir=\"checkpoints\", suffix=\"lastest\"):\n",
    "    \"\"\"\n",
    "    Carga pesos y estado para componentes separados (modelo, memoria, target_model).\n",
    "    \n",
    "    Par√°metros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetici√≥n\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Tipo del modelo ('DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.)\n",
    "        checkpoint_dir: Directorio donde buscar los checkpoints\n",
    "        suffix: Tipo de checkpoint a cargar (\"lastest\", \"best\", o \"epXXX\")\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (episode, steps, epsilon) - El episodio, pasos y epsilon desde donde continuar\n",
    "               Si no se encuentra el checkpoint, devuelve (0, 0, 0.1)\n",
    "    \"\"\"  \n",
    "    # Valores predeterminados\n",
    "    episode = 0\n",
    "    steps = 0\n",
    "    epsilon = 0.1  # Valor por defecto\n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "      \n",
    "    try:    \n",
    "        # Cargar estado del entrenamiento\n",
    "        if not os.path.exists(state_path):\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ el checkpoint {suffix} para {model_name}\")\n",
    "            return episode, steps, epsilon \n",
    "        else:\n",
    "            print(f\"üìÇ Se carg√≥: {state_path}\")        \n",
    "\n",
    "        with open(state_path, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            \n",
    "        # Extraer informaci√≥n b√°sica\n",
    "        episode = state.get('episode', 0)\n",
    "        steps = state.get('global_steps', 0)\n",
    "        epsilon = state.get('epsilon', 0.1)\n",
    "        \n",
    "        print(f\"üìÇ Cargando checkpoint {suffix} (episodio: {episode}, pasos: {steps}, epsilon: {epsilon})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error al cargar el archivo de estado: {e}\")\n",
    "        \n",
    "    # Cargar modelo principal\n",
    "    if os.path.exists(main_model_path):\n",
    "        try:\n",
    "            model.load_weights(main_model_path)\n",
    "            print(f\"üìÇ Modelo principal cargado: {main_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå [ERROR] - Error al cargar el modelo principal: {e}\")\n",
    "            return 0, 0, 0.1  # Si falla la carga del modelo principal, mejor empezar desde cero\n",
    "    else:\n",
    "        print(f\"‚ùå [ERROR] - No se encontr√≥ el archivo del modelo principal: {main_model_path}\")\n",
    "        return 0, 0, 0.1       \n",
    "\n",
    "    # Cargar modelo target si existe y se proporcion√≥\n",
    "    if target_model is not None:\n",
    "        if os.path.exists(target_model_path):\n",
    "            try:\n",
    "                target_model.load_weights(target_model_path)\n",
    "                print(f\"üìÇ Modelo target cargado: {target_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è [WARNING] - Error al cargar el modelo target: {e}\")\n",
    "                # Si hay error, sincronizar con el principal\n",
    "                print(\"üîÑ Sincronizando red target desde la principal...\")\n",
    "                target_model.set_weights(model.get_weights())\n",
    "        else:\n",
    "            # Si no existe el archivo, sincronizar con el principal\n",
    "            print(\"üîÑ No se encontr√≥ archivo target, sincronizando desde la principal...\")\n",
    "            target_model.set_weights(model.get_weights())\n",
    "          \n",
    "    # Cargar memoria si corresponde al tipo de modelo\n",
    "    if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and memory is not None and os.path.exists(memory_path):\n",
    "        try:              \n",
    "            with open(memory_path, 'rb') as f:\n",
    "                loaded_memory = pickle.load(f)                \n",
    "                \n",
    "            # Transferir contenido no reemplazar objeto\n",
    "            memory_loaded = False\n",
    "            # Estructura SequentialMemory (Keras-RL)\n",
    "            if hasattr(loaded_memory, 'observations') and hasattr(memory, 'observations'):\n",
    "                memory.observations = loaded_memory.observations\n",
    "                memory.actions = loaded_memory.actions\n",
    "                memory.rewards = loaded_memory.rewards\n",
    "                memory.terminals = loaded_memory.terminals\n",
    "                if hasattr(loaded_memory, 'observations_'):\n",
    "                    memory.observations_ = loaded_memory.observations_\n",
    "                memory_loaded = True\n",
    "            elif hasattr(loaded_memory, 'buffer') and hasattr(memory, 'buffer'):\n",
    "                memory.buffer = loaded_memory.buffer\n",
    "                memory.position = loaded_memory.position\n",
    "                if hasattr(loaded_memory, 'size'):\n",
    "                    memory.size = loaded_memory.size\n",
    "                memory_loaded = True\n",
    "\n",
    "            if memory_loaded:\n",
    "                print(f\"üìÇ Memoria cargada correctamente: {memory_path}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Estructura de memoria desconocida, no se pudo cargar\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error al cargar la memoria: {e}\")    \n",
    "    \n",
    "    return episode, steps, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n auxiliar para convertir objetos no serializables a JSON\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convierte tipos numpy a tipos Python nativos para serializaci√≥n JSON\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "        return obj.isoformat()\n",
    "    else:\n",
    "        # Para cualquier otro tipo no reconocido\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(agent, model_name, episode=0, steps=0, \n",
    "                          checkpoint_dir=\"checkpoints\", suffix=\"lastest\", epsilon=0.1):\n",
    "    \"\"\"           \n",
    "    Guarda el modelo, la memoria de repetici√≥n y el estado del entrenamiento.\n",
    "    \n",
    "    Esta funci√≥n guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetici√≥n (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el n√∫mero de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.    \n",
    "\n",
    "    Par√°metros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetici√≥n\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Nombre base del modelo (ej: 'DQN', 'DDQN_REPLAY')\n",
    "        episode: N√∫mero del episodio actual\n",
    "        steps: Pasos globales acumulados\n",
    "        checkpoint_dir: Directorio donde guardar los checkpoints\n",
    "        suffix: Tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\")\n",
    "        \n",
    "        model : tensorflow.keras.Model-  El modelo principal de red neuronal que se desea guardar.\n",
    "        memory : objeto de memoria-      La memoria de repetici√≥n utilizada para almacenar experiencias de entrenamiento.\n",
    "                                         Puede ser SequentialMemory, ReplayBuffer u otra implementaci√≥n compatible.\n",
    "        target_model : tensorflow.keras.Model-         El modelo target utilizado en algoritmos como DDQN. Es una copia del modelo principal\n",
    "            que se actualiza peri√≥dicamente durante el entrenamiento.\n",
    "        model_name : str-                Nombre identificativo del modelo ('DQN', 'DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.).\n",
    "                                         Se utiliza para nombrar los archivos de checkpoint.\n",
    "        episode : int, opcional-         N√∫mero del episodio actual de entrenamiento. Por defecto es 0.\n",
    "        steps : int, opcional-           N√∫mero total de pasos (interacciones con el entorno) realizados. Por defecto es 0.\n",
    "\n",
    "        checkpoint_dir : str, opcional-  Directorio donde se guardar√°n los archivos de checkpoint. Por defecto es \"checkpoints\".\n",
    "\n",
    "        suffix : str, opcional-          Sufijo para identificar el tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\"). Por defecto es \"lastest\".\n",
    "\n",
    "        epsilon : float, opcional-       Valor actual de epsilon para la pol√≠tica epsilon-greedy. Por defecto es 0.1.\n",
    "        force_override: Si es True, sobrescribe incluso los checkpoints 'best'    \n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "    \"\"\"   \n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # Puedes intentar limpiar la memoria de Python no usada expl√≠citamente antes de medir\n",
    "    gc.collect()     \n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "    \n",
    "    # PROTECCI√ìN DE CHECKPOINTS EXISTENTES\n",
    "    # Si es un checkpoint \"best\" y estamos intentando guardarlo con episodio 0\n",
    "    if (\"best\" in suffix or \"lastest\" in suffix) and episode == 0:\n",
    "        # Verificar si ya existe un mejor checkpoint con episodio > 0\n",
    "        state_path = f\"{checkpoint_dir}/{model_name}_best_state.json\"\n",
    "        if os.path.exists(state_path):\n",
    "            try:\n",
    "                with open(state_path, 'r') as f:\n",
    "                    existing_state = json.load(f)\n",
    "                existing_episode = existing_state.get('episode', 0)\n",
    "                \n",
    "                if existing_episode > 0:\n",
    "                    print(f\"üõ°Ô∏è Protegiendo checkpoint '{suffix}' existente (ep: {existing_episode})\")\n",
    "                    print(f\"‚ùå NO se guardar√° un nuevo checkpoint con episodio 0\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error verificando checkpoint existente: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo principal\n",
    "        if hasattr(agent, 'model'):\n",
    "            agent.model.save_weights(main_model_path)\n",
    "            print(f\"üíæ Guardado modelo principal {suffix}: {main_model_path}\")               \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è El agente no tiene el atributo 'model'\")          \n",
    "        \n",
    "        # Guardar pesos del modelo target si existe\n",
    "        if hasattr(agent, 'target_model') and agent.target_model is not None:\n",
    "            agent.target_model.save_weights(target_model_path)      \n",
    "            print(f\"üíæ Guardado modelo target {suffix}: {target_model_path}\")         \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è El agente no tiene el atributo 'target_model' o es None\")            \n",
    "\n",
    "        # Preparar el estado del entrenamiento - Con conversi√≥n a tipos Python nativos\n",
    "        state = {\n",
    "            'episode': int(episode),  \n",
    "            'global_steps': int(steps),  \n",
    "            'epsilon': float(epsilon),        \n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }              \n",
    "        \n",
    "        # Si hay una pol√≠tica espec√≠fica en el agente\n",
    "        if hasattr(agent, 'policy'):\n",
    "            policy_state = {}\n",
    "            \n",
    "            # Intentar guardar el estado de la pol√≠tica\n",
    "            if hasattr(agent.policy, 'eps'):\n",
    "                policy_state['eps'] = float(agent.policy.eps)  # Convertir a float Python\n",
    "        \n",
    "            # Si la pol√≠tica tiene m√°s atributos relevantes\n",
    "            for attr in ['value_max', 'value_min', 'value_test', 'nb_steps']:\n",
    "                if hasattr(agent.policy, attr):\n",
    "                    value = getattr(agent.policy, attr)\n",
    "                    if isinstance(value, (np.integer, np.floating, np.bool_)):\n",
    "                        value = value.item()  # Convierte cualquier tipo numpy a su equivalente Python\n",
    "                    policy_state[attr] = value\n",
    "            \n",
    "            if policy_state:\n",
    "                state['policy'] = policy_state                  \n",
    "               \n",
    "        # Guardar memoria de repetici√≥n para modelos con replay\n",
    "        if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and hasattr(agent, 'memory') and agent.memory is not None:\n",
    "            try:\n",
    "                # Usar compresi√≥n m√°xima\n",
    "                with open(memory_path, 'wb') as f:\n",
    "                    pickle.dump(agent.memory, f)\n",
    "                print(f\"üíæ Memoria {suffix} guardada: {memory_path}\")  \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo guardar la memoria: {e}\")       \n",
    "        \n",
    "        # Guardar el estado\n",
    "        with open(state_path, 'w') as f:\n",
    "            json.dump(state, f, indent=2, default=convert_to_json_serializable)\n",
    "        \n",
    "        print(f\"üíæ Checkpoint {suffix} guardado (ep: {episode}, pasos: {steps})\")  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error guardando checkpoint {suffix}: {e}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para monitorear el progreso del entrenamiento de un agente DQN.\n",
    "\n",
    "    Registra el avance en t√©rminos de pasos completados, porcentaje, velocidad de entrenamiento\n",
    "    (pasos por segundo) y tiempo estimado de finalizaci√≥n (ETA).\n",
    "\n",
    "    Atributos:\n",
    "        total_steps (int): N√∫mero total de pasos de entrenamiento.\n",
    "        print_interval (int): Intervalo de pasos para imprimir el progreso (por defecto: 10,000).\n",
    "        start_time (float): Tiempo de inicio del entrenamiento (en segundos).\n",
    "        last_step (int): √öltimo paso registrado (inicializado en 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, total_steps, print_interval=100):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "\n",
    "        Args:\n",
    "            total_steps (int): N√∫mero total de pasos de entrenamiento.\n",
    "            print_interval (int): Intervalo de pasos para imprimir el progreso.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.print_interval = print_interval\n",
    "        self.step_counter = 0\n",
    "        self.start_time = time.time()\n",
    "        self.episode_rewards = []  # Store clipped episode rewards\n",
    "        self.episode_steps = []\n",
    "        self.current_episode_reward = 0.0  # Track clipped reward for current episode\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al inicio del entrenamiento.\n",
    "\n",
    "        Inicializa el tiempo de inicio y muestra un mensaje de comienzo.\n",
    "\n",
    "        Args:\n",
    "            logs (dict): Diccionario de m√©tricas (no utilizado aqu√≠).\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"üöÄ Entrenamiento iniciado: {self.total_steps:,} pasos\")\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al final de cada paso de entrenamiento.\n",
    "\n",
    "        Calcula y muestra el progreso, incluyendo porcentaje completado, velocidad\n",
    "        (pasos por segundo) y tiempo estimado de finalizaci√≥n (ETA) en horas.\n",
    "\n",
    "        Args:\n",
    "            step (int): N√∫mero del paso actual.\n",
    "            logs (dict): Diccionario de m√©tricas (no utilizado aqu√≠).\n",
    "        \"\"\"\n",
    "        self.step_counter += 1        \n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        clipped_reward = np.clip(raw_reward, -1.0, 1.0)  # Match AtariProcessor clipping\n",
    "        self.current_episode_reward += clipped_reward\n",
    "        if self.step_counter % self.print_interval == 0:\n",
    "            progress = (self.step_counter / self.total_steps) * 100\n",
    "            elapsed_time = (time.time() - self.start_time)\n",
    "            steps_per_sec = self.step_counter / elapsed_time\n",
    "            eta_hours = (self.total_steps - self.step_counter) / steps_per_sec / 3600\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "            print(f\"üìä Paso {self.step_counter:,}/{self.total_steps:,} ({progress:.1f}%) - \"\n",
    "                  f\"{steps_per_sec:.1f} pasos/seg - ETA: {eta_hours:.1f}h - Memoria: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        nb_steps = logs.get('nb_episode_steps', 1)\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.episode_steps.append(nb_steps)\n",
    "        mean_reward = self.current_episode_reward / nb_steps if nb_steps > 0 else 0\n",
    "        print(f\"üìà Episodio {episode+1}: Recompensa total (clipped): {self.current_episode_reward:.3f}, \"\n",
    "              f\"Pasos: {nb_steps}, Mean Reward Calculado: {mean_reward:.6f} (Recompensa/Pasos)\")\n",
    "        # Reset for next episode\n",
    "        self.current_episode_reward = 0.0            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback que monitorea el progreso hacia una media de episode_reward objetivo\n",
    "    e integra con nuestro sistema de checkpoints.\n",
    "    \"\"\"      \n",
    "    def __init__(self, dqn, target_avg_reward=20.0, name_model=None, window_size=100, save_best=True, checkpoint_dir=checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.target_avg_reward = target_avg_reward\n",
    "        self.window_size = window_size\n",
    "        self.save_best = save_best\n",
    "        # Obtener el nombre del modelo sin ruta\n",
    "        self.model = dqn\n",
    "        self.model_name = name_model\n",
    "        self.episode_count = 0\n",
    "        self.episode_rewards = []\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.episodes_at_target = 0\n",
    "        self.consecutive_target_episodes = 0\n",
    "        self.checkpoint_dir =  f\"{checkpoint_path}/{self.model_name}\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)       \n",
    "        \n",
    "        print(f\"üéØ OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
    "        print(f\"üìä Ventana de evaluaci√≥n: {window_size} episodios\")        \n",
    "\n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        self.episode_count += 1\n",
    "        episode_reward = logs.get('episode_reward', 0)\n",
    "        # Convert NumPy types to Python types\n",
    "        if isinstance(episode_reward, np.floating):\n",
    "            episode_reward = episode_reward.item()\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular media m√≥vil\n",
    "        if len(self.episode_rewards) >= self.window_size or self.episode_count % 10 == 0:\n",
    "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
    "            current_avg = np.mean(recent_rewards)\n",
    "            \n",
    "            # Verificar si alcanzamos el objetivo\n",
    "            target_reached = current_avg >= self.target_avg_reward\n",
    "            \n",
    "            if target_reached:\n",
    "                self.episodes_at_target += 1\n",
    "                self.consecutive_target_episodes += 1\n",
    "            else:\n",
    "                self.consecutive_target_episodes = 0\n",
    "            \n",
    "            # Guardar si es el mejor promedio\n",
    "            if current_avg > self.best_avg_reward or self.episode_count % 10 == 0:\n",
    "                self.best_avg_reward = current_avg\n",
    "                if self.save_best and hasattr(self, 'model') and self.episode_count % 50 == 0:\n",
    "                    # Formato del sufijo para el mejor modelo con su promedio\n",
    "                    best_suffix = f\"best_avg{current_avg:.1f}\"                               \n",
    "                    try:\n",
    "                        epsilon = self.model.policy.eps if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'eps') else 0.1\n",
    "                       \n",
    "                        # Usar nuestro sistema de checkpoint para guardar el mejor modelo\n",
    "                        if hasattr(self.model, 'model'): # Por si el agente es un DQNAgent con un .model\n",
    "# AVG                            save_model_checkpoint(   self.model,   self.model_name,\n",
    "# AVG                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "# AVG                                checkpoint_dir=self.checkpoint_dir,   suffix=best_suffix, \n",
    "# AVG                                epsilon=epsilon\n",
    "# AVG                            )                                             \n",
    "                            # Tambi√©n actualizar el checkpoint \"best\" general\n",
    "                            save_model_checkpoint(   self.model,    self.model_name,\n",
    "                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "                                checkpoint_dir=self.checkpoint_dir,   suffix=\"best\", \n",
    "                                epsilon=epsilon\n",
    "                            )\n",
    "                        else:\n",
    "                            # Si no es un agente completo, guardar solo los pesos\n",
    "                            best_filename = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_model.h5\"\n",
    "                            self.model.save_weights(best_filename, overwrite=True)                        \n",
    "                            \n",
    "                        # Guardar m√©tricas en JSON\n",
    "                        metrics = {\n",
    "                            \"episode\": int(self.episode_count),\n",
    "                            \"avg_reward\": float(current_avg),\n",
    "                            \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                            \"timestamp\": int(time.time()),\n",
    "                            \"consecutive_target_episodes\": int(self.consecutive_target_episodes),\n",
    "                            \"episodes_at_target\": int(self.episodes_at_target)\n",
    "                        }\n",
    "                        metrics_file = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_metrics.json\"\n",
    "                        with open(metrics_file, 'w') as f:\n",
    "                            json.dump(metrics, f, indent=2, default=str)\n",
    "                        \n",
    "                        print(f\"üíæ NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado en {self.checkpoint_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è [ERROR] - Error guardando mejor modelo: {e}\")\n",
    "            \n",
    "            # Mostrar progreso cada 50 episodios\n",
    "            if self.episode_count % 50 == 0:\n",
    "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
    "                target_status = \"üéØ OBJETIVO ALCANZADO!\" if target_reached else f\"üìà {progress_pct:.1f}% del objetivo\"\n",
    "                \n",
    "                print(f\"\\nüìä EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
    "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
    "                print(f\"   Media √∫ltimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
    "                print(f\"   Mejor promedio hist√≥rico: {self.best_avg_reward:.2f}\")\n",
    "                print(f\"   Estado: {target_status}\")\n",
    "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
    "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
    "                \n",
    "                if self.consecutive_target_episodes >= 50:\n",
    "                    print(f\"üèÜ ¬°MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
    "        logs = logs or {}\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
    "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
    "            \n",
    "            print(f\"\\nüèÅ RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "            print(f\"   Total episodios: {self.episode_count}\")\n",
    "            print(f\"   Media final √∫ltimos {self.window_size}: {final_avg:.2f}\")\n",
    "            print(f\"   Objetivo ({self.target_avg_reward}): {'‚úÖ ALCANZADO' if objetivo_alcanzado else '‚ùå NO ALCANZADO'}\")\n",
    "            print(f\"   Mejor promedio hist√≥rico: {self.best_avg_reward:.2f}\")\n",
    "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")\n",
    "            \n",
    "            # Save final metrics to JSON\n",
    "            final_metrics = {\n",
    "                \"total_episodes\": int(self.episode_count),\n",
    "                \"final_avg_reward\": float(final_avg),\n",
    "                \"target_reached\": bool(objetivo_alcanzado),\n",
    "                \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                \"episodes_at_target\": int(self.episodes_at_target),\n",
    "                \"consecutive_target_episodes\": int(self.consecutive_target_episodes)  \n",
    "            }\n",
    "            final_log_path = f\"{self.checkpoint_dir}/final_metrics.json\"\n",
    "            try:\n",
    "                with open(final_log_path, 'w') as f:\n",
    "                    json.dump(final_metrics, f, indent=2, default=str)                                    \n",
    "                print(f\"üíæ M√©tricas finales guardadas en: {final_log_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error al guardar m√©tricas finales: {e}\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFileLogger(FileLogger):\n",
    "    def __init__(self, filepath, interval=100):\n",
    "        super().__init__(filepath, interval)\n",
    "        self.step = 0  \n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.logs = {}        \n",
    "    \n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.step += 1  \n",
    "        if self.step % self.interval == 0:\n",
    "            episode_logs = {}\n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        self.current_episode_reward += np.clip(raw_reward, -1.0, 1.0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        metrics = logs.copy()\n",
    "        metrics['episode_reward'] = self.current_episode_reward\n",
    "        metrics['mean_reward_step'] = self.current_episode_reward / metrics.get('nb_episode_steps', 1)\n",
    "        metrics = {k: float(v) if isinstance(v, np.floating) else v for k, v in metrics.items()}\n",
    "        self.metrics[int(episode)] = metrics\n",
    "        if self.step % self.interval == 0:\n",
    "            with open(self.filepath, 'w') as f:\n",
    "                json.dump(self.metrics, f, indent=2, default=str)                \n",
    "        self.current_episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback para guardar checkpoints epis√≥dicamente usando nuestras funciones personalizadas.\n",
    "    \"\"\"    \n",
    "    def __init__(self, dqnet, checkpoint_path, save_freq=100, model_name='DQN'):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "        \n",
    "        Parametros:\n",
    "            dqnet: El agente DQN a guardar\n",
    "            checkpoint_path: Ruta donde guardar los checkpoints\n",
    "            save_freq: Frecuencia de episodios para guardar\n",
    "            model_name: Nombre base para los archivos de checkpoint\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_path = f\"{checkpoint_path}/{self.model_name}\"\n",
    "        self.episode_counter = 0\n",
    "        self.dqnet = dqnet\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        \"\"\"\n",
    "        Guarda checkpoint al final de ciertos episodios.\n",
    "        \"\"\"        \n",
    "        self.episode_counter += 1\n",
    "        if self.episode_counter % self.save_freq == 0:\n",
    "            try:       \n",
    "                epsilon = self.dqnet.policy.eps if hasattr(self.dqnet, 'policy') and hasattr(self.dqnet.policy, 'eps') else 0.1\n",
    "                # Tambi√©n actualizar el checkpoint \"lastest\"\n",
    "                save_model_checkpoint(\n",
    "                    self.dqnet, \n",
    "                    self.model_name,\n",
    "                    episode=self.episode_counter,\n",
    "                    steps=self.dqnet.step, \n",
    "                    checkpoint_dir=self.checkpoint_path, \n",
    "                    suffix=\"lastest\",\n",
    "                    epsilon=epsilon\n",
    "                )                \n",
    "                print(f\"‚úÖ Checkpoint guardado para episodio {self.episode_counter}\")        \n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå [ERROR] - Error al guardar checkpoint para episodio {self.episode_counter}: {str(e)}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgbzJyUjmTzs"
   },
   "source": [
    "#### **ENTRENAMIENTO** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(env,  model_name,  model, memory, target_model, model_instance=False, \n",
    "        start_episode=0, start_steps=0,\n",
    "        batch_size=batch_size, learning_rate=learning_rate, checkpoint_path='checkpoints',\n",
    "        input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size, warmup_steps=WARMUP_STEPS,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau, epsilon_start=epsilon_start,\n",
    "        epsilon_min=0.1, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS, target_reward=TARGET_REWARD,\n",
    "        enable_double_dqn = False):     \n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN con el entorno proporcionado\n",
    "    \n",
    "    Par√°metros:\n",
    "    -----------\n",
    "        env: El entorno de Gym\n",
    "        model_name: Nombre identificador del modelo ('DQN', 'DDQN', etc.)\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetici√≥n\n",
    "        target_model: El modelo target (puede ser None para DQN)        \n",
    "        model_instance: Flag que determina si existe modelo cargado (si False, se crea uno nuevo)\n",
    "        checkpoint_path: Ruta donde guardar checkpoints\n",
    "        start_episode: Episodio desde donde continuar el entrenamiento\n",
    "        start_steps: Pasos desde donde continuar el entrenamiento\n",
    "        batch_size: Tama√±o del lote para el entrenamiento\n",
    "        learning_rate: Tasa de aprendizaje del optimizador\n",
    "        input_shape: Forma de la entrada para el modelo\n",
    "        memoria_tamano: Tama√±o de la memoria de repetici√≥n\n",
    "        warmup_steps: Pasos de calentamiento antes del entrenamiento\n",
    "        target_update_interval: Intervalo para actualizar la red objetivo\n",
    "        target_update_tau: Factor de actualizaci√≥n suave para la red objetivo\n",
    "        epsilon_start: Valor inicial de epsilon para exploraci√≥n\n",
    "        epsilon_min: Valor m√≠nimo de epsilon para exploraci√≥n\n",
    "        epsilon_steps: N√∫mero de pasos para decrementar epsilon\n",
    "        num_steps: N√∫mero total de pasos de entrenamiento\n",
    "        target_reward: Recompensa objetivo para considerar resuelto el entorno\n",
    "        enable_double_dqn: Si es True, usa DDQN; si no, usa DQN est√°ndar\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (agente_entrenado, √©xito) - Modelo entrenado y booleano indicando √©xito    \n",
    "    \"\"\"\n",
    "    # Nombre del modelo para logs y checkpoints\n",
    "    name_model = model_name.upper()    \n",
    "    print(f\"ü§ñ {'Continuando' if model_instance else 'Creando'} entrenamiento para {name_model}...\")\n",
    "        \n",
    "    # Inicializar dqn desde el principio para evitar referencia antes de asignaci√≥n\n",
    "    dqn = None      \n",
    "    save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"\n",
    "    # Inicializar callbacks al principio para asegurarnos de que siempre est√© definido\n",
    "    callbacks = []\n",
    "    # Verificar target_model solo si estamos usando DDQN\n",
    "    if enable_double_dqn and target_model is None:\n",
    "        raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "            \n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()   \n",
    "    try:  \n",
    "        # Verificar que tenemos un modelo v√°lido\n",
    "        if model is None:\n",
    "            raise ValueError(\"El modelo principal no puede ser None.\")              \n",
    "        # Verificar target_model solo si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is None:\n",
    "            raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "\n",
    "        # Verificar si la memoria ya se cre√≥ o necesitamos crearla\n",
    "        if memory is None:\n",
    "            print(\"Creando nueva memoria de experiencia...\")\n",
    "            memory = SequentialMemory(limit=memoria_tamano, window_length=WINDOW_LENGTH)\n",
    "        \n",
    "        # Pol√≠tica de exploraci√≥n\n",
    "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                     attr='eps',\n",
    "                                     value_max=epsilon_start, \n",
    "                                     value_min=epsilon_min, \n",
    "                                     value_test=.05,\n",
    "                                     nb_steps=epsilon_steps)  \n",
    "        # Crear agente DQN\n",
    "        dqn = DQNAgent(\n",
    "            model=model,\n",
    "            nb_actions=env.action_space.n,\n",
    "            memory=memory,\n",
    "            processor=processor,\n",
    "            nb_steps_warmup=warmup_steps,\n",
    "            target_model_update=target_update_interval if enable_double_dqn else 10000,\n",
    "            enable_double_dqn=enable_double_dqn,\n",
    "            policy=policy,\n",
    "            gamma=0.99,\n",
    "            train_interval=4,\n",
    "            delta_clip=1.0,\n",
    "            batch_size=batch_size\n",
    "        )                \n",
    "        # Despu√©s de crear el agente, reemplazar el target_model si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is not None:\n",
    "            dqn.target_model = target_model            \n",
    "\n",
    "        # Compilar el agente\n",
    "        optimizer = Adam(learning_rate=learning_rate)  \n",
    "        dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al crear o compilar el agente: {str(e)}\")\n",
    "        return None, False\n",
    "        \n",
    "    # Verificar que tenemos un agente v√°lido antes de continuar\n",
    "    if dqn is None:\n",
    "        print(f\"‚ùå Error: No se pudo inicializar el agente DQN\")\n",
    "        return None, False        \n",
    "    try:\n",
    "        # DEBUG --------------------\n",
    "        # callbacks = [\n",
    "        #     log_filename = f'{checkpoint_path}/{name_model}_log.json'\n",
    "        #     progress_callback,\n",
    "        #     ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
    "        #     CustomFileLogger(log_filename, interval=1000),\n",
    "        #     PerformanceMonitor(save_path='diagnosticos')\n",
    "        # ]\n",
    "        # Callbacks optimizados para el objetivo\n",
    "        callbacks = [\n",
    "            SimpleProgressCallback(num_steps, print_interval=20000),         \n",
    "            TargetRewardTracker(dqn,target_avg_reward=target_reward, name_model=name_model, window_size=100, \n",
    "                                save_best=True, checkpoint_dir=checkpoint_path),\n",
    "            EpisodeCheckpointCallback(dqn, checkpoint_path=checkpoint_path, save_freq=100, model_name=name_model)    \n",
    "        ]         \n",
    "          \n",
    "         # Ajustar el n√∫mero de pasos restantes si estamos continuando el entrenamiento\n",
    "        adjusted_steps = max(0, num_steps - start_steps)\n",
    "        if start_steps > 0:\n",
    "            print(f\"Continuando desde el paso {start_steps} (quedan {adjusted_steps} pasos)\")\n",
    "\n",
    "        print(f\"Iniciando entrenamiento de {name_model} por {adjusted_steps} pasos...\")\n",
    "        start_time = time.time()       \n",
    "        # Fit del agente al entorno\n",
    "        history = dqn.fit(env, nb_steps=adjusted_steps, callbacks=callbacks, verbose=2)\n",
    "       \n",
    "        training_time = (time.time() - start_time) / 60\n",
    "        print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        # Verificar correctamente las claves disponibles y usar la adecuada\n",
    "        if hasattr(history, 'history'):\n",
    "            if 'episode' in history.history and history.history['episode']:\n",
    "                episode = start_episode + history.history['episode'][-1]\n",
    "            elif 'nb_episode' in history.history and history.history['nb_episode']:\n",
    "                episode = start_episode + history.history['nb_episode'][-1]\n",
    "            else:\n",
    "                # Intentar obtener el episodio del agente directamente\n",
    "                episode = getattr(dqn, 'episode', start_episode)\n",
    "        else:\n",
    "            episode = start_episode        \n",
    "        steps = start_steps + adjusted_steps\n",
    "        epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1          \n",
    "        try:\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"lastest\", \n",
    "                                epsilon=epsilon)\n",
    "                \n",
    "            # Tambi√©n guardar como \"best\" si no hay un checkpoint \"best\" previo\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                    checkpoint_dir=save_checkpoint_path, suffix=\"best\", \n",
    "                                    epsilon=epsilon)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error al guardar el modelo: {str(e)}\")\n",
    "                                                        \n",
    "        return dqn, True    \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido por el usuario\") \n",
    "        # Guardar pesos de emergencia\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            episode = start_episode  # No podemos saber el episodio exacto despu√©s de la interrupci√≥n\n",
    "            steps = start_steps + dqn.step if hasattr(dqn, 'step') else start_steps\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"emergency\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"‚úÖ Modelo guardado en estado de emergencia\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error al guardar el modelo de emergencia: {str(e)}\")\n",
    "        return dqn, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå [ERROR] - Error durante el entrenamiento: {str(e)}\")\n",
    "        # Intentar guardar en estado de error\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            save_model_checkpoint(dqn, name_model, episode=start_episode, \n",
    "                                steps=start_steps + (dqn.step if hasattr(dqn, 'step') else 0),\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"error_recovery\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"‚úÖ Modelo guardado en estado de recuperaci√≥n de error\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Error al guardar modelo de recuperaci√≥n: {str(e2)}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVALUACION** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GZzkrK72yj1"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para evaluar el modelo\n",
    "def evaluar_modelo(agent, model_name, env, num_episodes=200, render=True, record_video=False):\n",
    "    \"\"\"\n",
    "    Eval√∫a un modelo DQN o DDQN y si el modelo alcanza el objetivo de media\n",
    "\n",
    "    Par√°metros:\n",
    "    -----------\n",
    "        agent: Agente DQN entrenado\n",
    "        model_name: Nobre del agente\n",
    "        env: Entorno de gym\n",
    "        num_episodes: N√∫mero de episodios para evaluar\n",
    "        render: Si se debe mostrar la visualizaci√≥n\n",
    "        record_video: Si se debe grabar video\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        Lista de recompensas por episodio\n",
    "    \"\"\"\n",
    "    print(f\"üéØ EVALUANDO MODELO {model_name}\")\n",
    "    print(f\"üìä Evaluando por {num_episodes} episodios...\")\n",
    "    rewards = []\n",
    "    rewards_clip = []    \n",
    "    \n",
    "    # Setup for frame stacking\n",
    "    window_length = 4  # As specified in your model input shape\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            total_reward_clip = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the frame buffer with the initial observation\n",
    "            if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                processed_obs = agent.processor.process_observation(observation)\n",
    "            else:\n",
    "                processed_obs = observation\n",
    "                \n",
    "            # Create a frame stack buffer of the right shape\n",
    "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
    "            \n",
    "            # Fill buffer with the first observation\n",
    "            for i in range(window_length):\n",
    "                frame_buffer[i] = processed_obs\n",
    "            \n",
    "            while not done and step < 2000:\n",
    "                try:\n",
    "                    # Prepare input in the format expected by the model: (batch, channels=window_length, h, w)\n",
    "                    # Format directly to channels_first, correcting for the specific shape expected\n",
    "                    input_data = np.expand_dims(frame_buffer, axis=0)  # Add batch dimension\n",
    "                    \n",
    "                    # Get Q-values directly from the model\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        q_values = agent.model.predict(input_data)\n",
    "                        \n",
    "                        # Handle the output format for Dueling DQN\n",
    "                        if isinstance(q_values, list):\n",
    "                            q_values = q_values[0]  # Take first output for value function\n",
    "                      \n",
    "                        action = np.argmax(q_values)\n",
    "                    else:\n",
    "                        # Fallback to random action\n",
    "                        print(f\"‚ö†Ô∏è No se encuentra ning√∫n modelo: {model_name}, utilizando una acci√≥n aleatoria\")\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    # Execute action\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    # Render if enabled\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    \n",
    "                    # Process new observation\n",
    "                    if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                        processed_obs = agent.processor.process_observation(observation)\n",
    "                    else:\n",
    "                        processed_obs = observation\n",
    "                    \n",
    "                    # Update frame buffer - shift frames\n",
    "                    for i in range(window_length-1):\n",
    "                        frame_buffer[i] = frame_buffer[i+1]\n",
    "                    frame_buffer[window_length-1] = processed_obs\n",
    "                    \n",
    "                    # Aplica clipping de la recompensa entre -1 y 1\n",
    "                    reward_clip = np.clip(reward, -1.0, 1.0)\n",
    "                    # Update counters\n",
    "                    total_reward += reward\n",
    "                    total_reward_clip += reward_clip\n",
    "                    step += 1\n",
    "                    \n",
    "                except Exception as step_error:\n",
    "                    print(f\"‚ö†Ô∏è Error en el paso {step}: {step_error}\")\n",
    "                    break\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "            rewards_clip.append(total_reward_clip)            \n",
    "            print(f\"   Episodio {episode + 1}/{num_episodes}: reward (clip) = {total_reward_clip:.1f}: reward (real) = {total_reward:.1f}\")\n",
    "        \n",
    "        # An√°lisis final\n",
    "        if len(rewards_clip) > 0:\n",
    "            avg_reward = np.mean(rewards_clip)\n",
    "            std_reward = np.std(rewards_clip)\n",
    "            max_reward = np.max(rewards_clip)\n",
    "            min_reward = np.min(rewards_clip)\n",
    "            \n",
    "            objetivo_alcanzado = avg_reward >= TARGET_REWARD\n",
    "            \n",
    "            print(f\"\\nüìä RESULTADOS DE EVALUACI√ìN:\")\n",
    "            print(f\"   Media: {avg_reward:.2f} {'‚úÖ' if objetivo_alcanzado else '‚ùå'}\")\n",
    "            print(f\"   Desviaci√≥n: ¬±{std_reward:.2f}\")\n",
    "            print(f\"   M√°ximo: {max_reward:.2f}\")\n",
    "            print(f\"   M√≠nimo: {min_reward:.2f}\")\n",
    "            print(f\"   Episodios sobre {TARGET_REWARD}: {sum(1 for r in rewards_clip if r >= TARGET_REWARD)} / {len(rewards_clip)}\")\n",
    "            \n",
    "            if objetivo_alcanzado:\n",
    "                print(f\"üèÜ ¬°OBJETIVO ALCANZADO! El modelo tiene una media de {avg_reward:.2f}\")\n",
    "            else:\n",
    "                print(f\"üìà Progreso: {(avg_reward/TARGET_REWARD)*100:.1f}% del objetivo\")\n",
    "        else:\n",
    "            print(\"‚ùå No se completaron episodios correctamente\")\n",
    "            avg_reward = float('nan')\n",
    "            objetivo_alcanzado = False\n",
    "        \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()            \n",
    "        return rewards_clip, objetivo_alcanzado        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [ERROR] - Error durante evaluaci√≥n: {e}\")    \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()           \n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabar_video_del_modelo(model, model_name, env, num_episodes=1, video_dir=\"checkpoints/videos\", fps=30, max_steps=10000):\n",
    "    \"\"\"\n",
    "    Graba un video del comportamiento del modelo en el entorno.\n",
    "    Los frames se capturan manualmente y se guardan directamente en checkpoints/videos.\n",
    "    \n",
    "    Par√°metros:\n",
    "    -----------\n",
    "    model: El agente entrenado (DQN, DDQN, etc.) con un m√©todo .predict() y un .processor.\n",
    "    model_name: Nombre del modelo para identificar el archivo de video.\n",
    "    env: El entorno de Gym. Debe ser capaz de renderizar en 'rgb_array'.\n",
    "         Aseg√∫rate de que este entorno NO tiene wrappers de FrameStack si tu agente\n",
    "         ya maneja el apilamiento de frames con su procesador.\n",
    "    num_episodes: N√∫mero de episodios a grabar.\n",
    "    fps: Frames por segundo para el video.\n",
    "    max_steps: N√∫mero m√°ximo de pasos a ejecutar por episodio.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    str: Ruta al archivo de video grabado, o None si hubo un error.\n",
    "    \"\"\"\n",
    "    # Crear directorio para videos\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Nombre del archivo MP4 directamente en el directorio de videos\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    video_path = os.path.join(video_dir, f\"{model_name}_{timestamp}.mp4\")\n",
    "    \n",
    "    frames = [] # Lista para almacenar los frames RGB capturados\n",
    "    \n",
    "    # Obtener window_length del agente (para el frame stacking de la l√≥gica del agente)\n",
    "    window_length = 4 # Valor por defecto\n",
    "    if hasattr(model, 'processor') and hasattr(model.processor, 'window_length'):\n",
    "        # Ajuste si el processor est√° anidado, como en keras-rl\n",
    "        if hasattr(model.processor, 'processor') and hasattr(model.processor.processor, 'window_length'):\n",
    "            window_length = model.processor.processor.window_length\n",
    "        else:\n",
    "            window_length = model.processor.window_length\n",
    "\n",
    "    try:\n",
    "        print(f\"üìπ Grabando video del modelo {model_name} en: {video_path}...\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Inicializar episodio\n",
    "            observation_raw = env.reset() \n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            # Preprocesar la observaci√≥n inicial para el agente y llenar el buffer\n",
    "            processed_obs = model.processor.process_observation(observation_raw)\n",
    "            \n",
    "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
    "            for k in range(window_length):\n",
    "                frame_buffer[k] = processed_obs\n",
    "            \n",
    "            while not done and steps < max_steps:\n",
    "                # Capturar frame RGB para el video\n",
    "                # env.render() DEBE devolver un array RGB (altura, anchura, 3)\n",
    "                current_frame_rgb = env.render(mode='rgb_array')\n",
    "                \n",
    "                if current_frame_rgb is not None:\n",
    "                    frames.append(current_frame_rgb)\n",
    "                else:\n",
    "                    print(\"Advertencia: env.render() devolvi√≥ None. No se pudieron capturar frames para el video.\")\n",
    "                    # Si no se capturan frames, el video no se generar√°.\n",
    "                    # Puedes optar por romper el bucle o continuar, pero el video estar√° vac√≠o.\n",
    "                    \n",
    "                # Preparar la entrada para el modelo del agente\n",
    "                input_data_for_model = np.expand_dims(frame_buffer, axis=0)\n",
    "                \n",
    "                # Obtener acci√≥n del modelo\n",
    "                action = None\n",
    "                if hasattr(model, 'model'): # Asumo que tu agente tiene un atributo 'model' que es un modelo Keras\n",
    "                    try:\n",
    "                        q_values = model.model.predict(input_data_for_model, verbose=0)\n",
    "                        if isinstance(q_values, list):\n",
    "                            q_values = q_values[0]\n",
    "                        action = np.argmax(q_values)\n",
    "                    except Exception as predict_error:\n",
    "                        print(f\"‚ö†Ô∏è Error al predecir la acci√≥n: {predict_error}. Usando acci√≥n aleatoria.\")\n",
    "                        action = env.action_space.sample()\n",
    "                else: # Fallback a acci√≥n aleatoria\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                # Ejecutar acci√≥n en el entorno\n",
    "                observation_next_raw, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Preprocesar la nueva observaci√≥n para actualizar el buffer de frames\n",
    "                processed_next_obs = model.processor.process_observation(observation_next_raw)              \n",
    "                # Actualizar el buffer de frames\n",
    "                frame_buffer[:window_length-1] = frame_buffer[1:]\n",
    "                frame_buffer[window_length-1] = processed_next_obs\n",
    "\n",
    "            print(f\"  Episodio {episode+1}: Recompensa Total = {total_reward} (Pasos: {steps})\")\n",
    "        \n",
    "        # Guardar video con imageio\n",
    "        if frames:\n",
    "            print(f\"üíæ Guardando video con {len(frames)} frames en {video_path}...\")\n",
    "            # Usa fps aqu√≠ para controlar la velocidad del video\n",
    "            imageio.mimsave(video_path, frames, fps=fps) \n",
    "            print(f\"‚úÖ Video guardado en: {video_path}\")\n",
    "        else:\n",
    "            print(\"‚ùå No se generaron frames para el video. El archivo MP4 no se crear√°.\")\n",
    "            video_path = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al grabar el video: {e}\")\n",
    "        video_path = None\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar entorno SIEMPRE al final\n",
    "        env.close() \n",
    "            \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬°¬°¬°¬°¬°¬°¬° **EJECUCION - MAIN** !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True\n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabaci√≥n de video final)\n",
    "    episode_render = False\n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    # Optimizar configuraci√≥n de TensorFlow\n",
    "    optimizar_tensorflow()\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    gc.collect()    \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    # Ejecutar prueba\n",
    "    print(\"üöÄ EJECUTANDO SOLUCI√ìN...\")\n",
    "    print(f\"üéØ OBJETIVO: Conseguir media de episode_reward = {TARGET_REWARD} (con clipping)\")    \n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', create_dqn_model, False),\n",
    "        ('DDQN', create_ddqn_models, False),\n",
    "        ('DDQN_REPLAY', create_ddqn_replay_model, False),\n",
    "        ('DUELING_DQN_REPLAY', create_dueling_dqn_replay_model, True)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for model_name, model_process, training_specific_flag in modelos_a_procesar:\n",
    "            # La bandera de entrenamiento final es la global AND la espec√≠fica del modelo\n",
    "            # Verificar el tipo de modelo y establecer enable_double_dqn correctamente\n",
    "            enable_double_dqn = model_name in ['DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']            \n",
    "            entrenarSN = training_global and training_specific_flag\n",
    "            model_instance = False    \n",
    "            agent = None\n",
    "            save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"            \n",
    "            if entrenarSN:\n",
    "                # Crear una nueva sesi√≥n para cada modelo\n",
    "                tf.keras.backend.clear_session()            \n",
    "                # Intentar cargar un modelo previamente guardado (independientemente de si entrenaremos o no)\n",
    "                try:        \n",
    "                    # Crear una instancia del modelo - crear primero la arquitectura antes de poder cargar los pesos en ella\n",
    "                    if enable_double_dqn:\n",
    "                        model, memory, target_model = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                    else:\n",
    "                        model, memory, _ = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                        target_model = None    \n",
    "\n",
    "                    # Intentar cargar el mejor checkpoint (o lastest si best no existe)\n",
    "                    start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                 checkpoint_dir=save_checkpoint_path, suffix=\"best\")\n",
    "                    if start_episode == 0:\n",
    "                        # Si no encontr√≥ el mejor, intentar con el √∫ltimo guardado\n",
    "                        start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                     checkpoint_dir=save_checkpoint_path, suffix=\"lastest\")\n",
    "                    if start_episode > 0:\n",
    "                        model_instance = True\n",
    "                        print(f\"‚úÖ Modelo {model_name} cargado exitosamente desde el episodio {start_episode}\")\n",
    "                        # Si se debe entrenar, continuar desde donde qued√≥\n",
    "                        if entrenarSN:\n",
    "                            print(f\"‚è© Continuando entrenamiento de {model_name} desde episodio {start_episode+1}\")\n",
    "                            # Establecer par√°metros para continuar el entrenamiento\n",
    "                            epsilon_actual = epsilon\n",
    "\n",
    "                            # Entrenar el modelo desde donde qued√≥\n",
    "                            agent, success = entrenar_modelo(\n",
    "                                env=env,\n",
    "                                model_name=model_name,\n",
    "                                model=model, \n",
    "                                memory=memory, \n",
    "                                target_model=target_model,\n",
    "                                model_instance=model_instance,\n",
    "                                checkpoint_path=checkpoint_path,\n",
    "                                start_episode=start_episode+1,\n",
    "                                start_steps=global_steps,\n",
    "                                epsilon_start=epsilon_actual,  # Usar el epsilon guardado\n",
    "                                epsilon_min=epsilon_stop,\n",
    "                                epsilon_steps=EPSILON_STEPS,\n",
    "                                num_steps=NUM_TRAINING_STEPS,\n",
    "                                warmup_steps=WARMUP_STEPS,\n",
    "                                target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                                target_update_tau=tau,\n",
    "                                enable_double_dqn = enable_double_dqn\n",
    "                            )                                                           \n",
    "                    else:\n",
    "                        print(f\"üÜï Creando y entrenando un nuevo modelo {model_name}\")\n",
    "                        model_instance = False   \n",
    "                        # Entrenar el modelo desde cero\n",
    "                        agent, success = entrenar_modelo(\n",
    "                            env=env,\n",
    "                            model_name=model_name,\n",
    "                            model=model, \n",
    "                            memory=memory, \n",
    "                            target_model=target_model,                        \n",
    "                            model_instance=model_instance,\n",
    "                            checkpoint_path=checkpoint_path,\n",
    "                            start_episode=0,  # A√±adido: Especificar episodio inicial\n",
    "                            start_steps=0,    # A√±adido: Especificar pasos iniciales\n",
    "                            epsilon_start=epsilon_start,\n",
    "                            epsilon_min=epsilon_stop,\n",
    "                            epsilon_steps=EPSILON_STEPS,\n",
    "                            num_steps=NUM_TRAINING_STEPS,\n",
    "                            warmup_steps=WARMUP_STEPS,\n",
    "                            target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                            target_update_tau=tau,\n",
    "                            enable_double_dqn = enable_double_dqn\n",
    "                        )                \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå [ERROR] - Error al cargar o entrenar modelo {model_name}: {e}\")\n",
    "\n",
    "            # Si tenemos un modelo v√°lido (cargado o entrenado), evaluarlo y guardarlo en el diccionario\n",
    "            # Evaluaci√≥n r√°pida con 10 episodios para:\n",
    "            # - Comprobar el rendimiento b√°sico del modelo\n",
    "            # - Decidir si vale la pena guardarlo como \"best\"\n",
    "            # - Mostrar un feedback r√°pido sobre su desempe√±o\n",
    "            if agent:\n",
    "                rewards, _ = evaluar_modelo(agent, model_name, env, num_episodes=10, render=False, record_video=False)\n",
    "                avg_reward = np.mean(rewards)\n",
    "                trained_models[model_name] = agent                       \n",
    "                print(f\"üìä Recompensa promedio para {model_name}: {avg_reward:.2f}\")\n",
    "                # Guardar el modelo como \"best\" si supera umbral de evaluaci√≥n\n",
    "                if avg_reward >= TARGET_REWARD * 0.8:  # 80% del objetivo como umbral m√≠nimo\n",
    "                    save_model_checkpoint(agent, model_name, episode=start_episode, steps=global_steps,  # Corregido: Usar el episodio y pasos actuales\n",
    "                                         checkpoint_dir=save_checkpoint_path, suffix=\"best_reward\", \n",
    "                                         epsilon=epsilon_start)\n",
    "                    print(f\"üèÜ Modelo {model_name} guardado como 'best_reward' con recompensa {avg_reward:.2f}\")        \n",
    "\n",
    "        # ------------------------------------------------------    \n",
    "        # B√∫squeda del mejor modelo entre todos los entrenados/cargados: comparar su rendimiento\n",
    "        # con el mismo n√∫mero de episodios (10) para mantener una comparaci√≥n justa\n",
    "        # Ayuda a determinar cu√°l es el mejor modelo para la evaluaci√≥n final\n",
    "        best_model_name = None\n",
    "        best_reward = -float('inf')    \n",
    "        for name, model in trained_models.items():\n",
    "            rewards, objetivo_conseguido = evaluar_modelo(model, model_name, env, num_episodes=10, render=False, record_video=False)\n",
    "            avg_reward = np.mean(rewards)\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                best_model_name = name        \n",
    "\n",
    "        if best_model_name:\n",
    "            print(\"\\n‚úÖ SOLUCI√ìN EXITOSA - Entrenamiento completado\")        \n",
    "            print(f\"\\nü•á El mejor modelo es {best_model_name} con recompensa promedio {best_reward:.2f}\")\n",
    "            best_model = trained_models[best_model_name]\n",
    "\n",
    "            # Evaluaci√≥n final y m√°s exhaustiva del mejor modelo: Se hace con muchos m√°s episodios (200) \n",
    "            #   para obtener resultados estad√≠sticamente m√°s significativos. Es la evaluaci√≥n definitiva \n",
    "            #   para determinar si se cumple el objetivo --> conclusi√≥n final del proceso        \n",
    "            #   print(f\"\\nüéØ EVALUACI√ìN FINAL DEL OBJETIVO\")\n",
    "            #   rewards_eval, objetivo_conseguido = evaluar_modelo(best_model, env, num_episodes=200)                \n",
    "\n",
    "            # Grabar video del mejor modelo\n",
    "            video_path = grabar_video_del_modelo(best_model, best_model_name, env)\n",
    "            if video_path:\n",
    "                print(f\"üé¨ Se ha grabado una demostraci√≥n del modelo en: {video_path}\")        \n",
    "\n",
    "            if objetivo_conseguido:\n",
    "                print(f\"üèÜ ¬°FELICIDADES EQUIPO! El modelo alcanz√≥ el objetivo de media {TARGET_REWARD}\")\n",
    "            else:\n",
    "                print(f\"üìà El modelo necesita m√°s entrenamiento para alcanzar media {TARGET_REWARD}\")\n",
    "        else:\n",
    "            print(\"‚ùå [ERROR] - No se pudo entrenar ning√∫n modelo correctamente\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è Entrenamiento interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Error inesperado: {e}\")\n",
    "    finally:\n",
    "        # Asegurar que siempre se cierra el entorno\n",
    "        env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque de Ejecuci√≥n Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **PARTE 4** - *An√°lisis del entrenamiento*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Justificaci√≥n de los par√°metros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelos_a_procesar = ['DQN','DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68dzFiUS2U6L"
   },
   "outputs": [],
   "source": [
    "file_ok = 'output-train-DDQN'\n",
    "file = 'output-train-DDQN'\n",
    "ruta_del_archivo = f'{file}.txt'\n",
    "ruta_archivo_csv = f'{file_ok}.csv'\n",
    "\n",
    "def parse_datos_episodio_flexible_desde_archivo(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Analiza el contenido de un archivo dado para extraer campos de datos espec√≠ficos para cada episodio,\n",
    "    estrictamente comprobando el formato de la segunda l√≠nea para evitar datos \"basura\".\n",
    "    Adem√°s, calcula la \"Recompensa acumulada media\" de forma progresiva.\n",
    "\n",
    "    Argumentos:\n",
    "        ruta_archivo (str): La ruta al archivo de texto de entrada que contiene la informaci√≥n del episodio.\n",
    "\n",
    "    Retorna:\n",
    "        pandas.DataFrame: Un DataFrame con las columnas especificadas para cada episodio,\n",
    "                          incluyendo la \"Recompensa acumulada media\",\n",
    "                          o None si el archivo no se puede leer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intenta abrir y leer el archivo\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "            texto = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontr√≥ el archivo '{ruta_archivo}'. Por favor, verifica la ruta.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error al leer el archivo: {e}\")\n",
    "        return None\n",
    "\n",
    "    datos = [] # Lista para almacenar los diccionarios de cada episodio\n",
    "    lineas = texto.strip().split('\\n') # Divide el texto en l√≠neas y elimina espacios en blanco al inicio/final\n",
    "    i = 0 # √çndice para recorrer las l√≠neas\n",
    "\n",
    "    while i < len(lineas):\n",
    "        linea1 = lineas[i]\n",
    "        \n",
    "        # Expresi√≥n regular para la primera l√≠nea del episodio (Ej: üìà Episodio X: Recompensa total...)\n",
    "        # Permanece estricta ya que es el marcador de inicio de un episodio.\n",
    "        coincidencia1 = re.match(\n",
    "            r\"üìà Episodio (\\d+): Recompensa total \\(clipped\\): ([\\d.]+), Pasos: (\\d+), Mean Reward Calculado: ([\\d.]+)\",\n",
    "            linea1\n",
    "        )\n",
    "\n",
    "        if coincidencia1:\n",
    "            episodio = int(coincidencia1.group(1))\n",
    "            # Si la primera l√≠nea coincide, extrae los datos principales del episodio\n",
    "            datos_episodio = {\n",
    "                'Episodio': episodio,\n",
    "                'Recompensa total': None,\n",
    "                'Pasos': int(coincidencia1.group(3)),\n",
    "                'Mean Reward Calculado': float(coincidencia1.group(4))\n",
    "            }\n",
    "\n",
    "            # Inicializa las columnas de la segunda l√≠nea a None por si no se encuentran\n",
    "            datos_episodio['duracion (s)'] = None\n",
    "            datos_episodio['steps per second'] = None\n",
    "            datos_episodio['Recompensa Episodio'] = None\n",
    "            datos_episodio['mean action'] = None\n",
    "            datos_episodio['loss'] = None\n",
    "\n",
    "\n",
    "            segunda_linea_encontrada = False\n",
    "            j = i + 1 # Inicia la b√∫squeda de la segunda l√≠nea a partir de la siguiente\n",
    "\n",
    "            # Bucle interno para buscar la l√≠nea de m√©tricas detalladas (la \"segunda l√≠nea\")\n",
    "            while j < len(lineas):\n",
    "                potencial_linea2 = lineas[j]\n",
    "                \n",
    "                # Criterio clave REVISADO: Buscamos \"episode: <numero_episodio_actual>,\" en la l√≠nea.\n",
    "                # Esto es m√°s robusto ante variaciones de espacios o \"basura\" antes de las m√©tricas.\n",
    "                numero_episodio_actual = coincidencia1.group(1) # Obtenemos el n√∫mero de episodio de la l√≠nea 1\n",
    "                if re.search(rf\"episode:\\s*{re.escape(numero_episodio_actual)},\", potencial_linea2):\n",
    "                    \n",
    "                    # Si esta l√≠nea contiene el n√∫mero de episodio al que corresponde,\n",
    "                    # es muy probable que sea nuestra l√≠nea de m√©tricas detalladas.\n",
    "                    # Extrae las m√©tricas individuales de forma flexible.\n",
    "                    duracion_coincidencia = re.search(r\"duration:\\s*([\\d.]+)s\", potencial_linea2)\n",
    "                    pasos_por_segundo_coincidencia = re.search(r\"steps\\s*(?:per\\s*)?second:\\s*(\\d+)\", potencial_linea2)\n",
    "                    recompensa_episodio_coincidencia = re.search(r\"episode\\s*reward:\\s*([\\d.]+)\", potencial_linea2)\n",
    "                    accion_media_coincidencia = re.search(r\"mean\\s*action:\\s*([\\d.]+)\", potencial_linea2)\n",
    "                    perdida_coincidencia = re.search(r\"loss:\\s*([-\\d.]+)\", potencial_linea2) # Maneja '--'\n",
    "\n",
    "                    datos_episodio['duracion (s)'] = float(duracion_coincidencia.group(1)) if duracion_coincidencia else 0\n",
    "                    datos_episodio['steps per second'] = int(pasos_por_segundo_coincidencia.group(1)) if pasos_por_segundo_coincidencia else None\n",
    "                    datos_episodio['Recompensa Episodio'] = float(recompensa_episodio_coincidencia.group(1)) if recompensa_episodio_coincidencia else None\n",
    "                    datos_episodio['mean action'] = float(accion_media_coincidencia.group(1)) if accion_media_coincidencia else None\n",
    "                    \n",
    "                    # Manejo especial para 'loss', que puede ser '--'\n",
    "                    if perdida_coincidencia and perdida_coincidencia.group(1) != '--':\n",
    "                        datos_episodio['loss'] = float(perdida_coincidencia.group(1))\n",
    "                    else:\n",
    "                        datos_episodio['loss'] = None\n",
    "                    \n",
    "                    segunda_linea_encontrada = True\n",
    "                    i = j # Actualiza el √≠ndice principal 'i' a la posici√≥n de esta segunda l√≠nea\n",
    "                    break # Sale del bucle interno, ya encontramos nuestra l√≠nea2\n",
    "\n",
    "                # Si encontramos el inicio de un NUEVO episodio, significa que la l√≠nea de m√©tricas\n",
    "                # para el episodio actual no existe o no tiene el formato esperado.\n",
    "                elif re.match(r\"üìà Episodio \\d+:\", potencial_linea2):\n",
    "                    break # Salimos del bucle interno sin encontrar la segunda l√≠nea\n",
    "                \n",
    "                j += 1 # Avanza a la siguiente l√≠nea para buscar la segunda l√≠nea\n",
    "\n",
    "            datos.append(datos_episodio) # A√±ade los datos del episodio (completos o con Nones)\n",
    "            i += 1 # Incrementa el √≠ndice principal 'i'. El bucle principal continuar√° desde aqu√≠.\n",
    "                   # Si se encontr√≥ la l√≠nea2, 'i' ya se actualiz√≥ a 'j' y este i+=1 lo mueve a j+1.\n",
    "                   # Si no se encontr√≥ la l√≠nea2, 'i' sigue en su valor original y este i+=1 lo mueve a i+1.\n",
    "        else:\n",
    "            i += 1 # Si la l√≠nea actual no es el inicio de un episodio, simplemente avanza a la siguiente l√≠nea\n",
    "                   # (esto ignora las l√≠neas de \"basura\" que no son ni inicio de episodio ni la l√≠nea de m√©tricas esperada).\n",
    "\n",
    "    df = pd.DataFrame(datos)    \n",
    "    # Calcular el promedio de los √∫ltimos 100 episodios (media m√≥vil)\n",
    "    # min_periods=1 asegura que la media se calcule incluso si hay menos de 100 episodios al principio\n",
    "    df['Recompensa total'] = round(df['Recompensa Episodio'].rolling(window=100, min_periods=1).mean(), 2)  \n",
    "    \n",
    "    # --- C√ÅLCULO DE TIEMPO ACUMULADO ---\n",
    "    # Convertir 'duration (s)' a tipo num√©rico, forzando errores a NaN\n",
    "    df['duration (s)'] = pd.to_numeric(df['duracion (s)'], errors='coerce')\n",
    "    # Rellenar cualquier NaN en duration (s) con 0 para el c√°lculo acumulado si es apropiado\n",
    "    df['duration (s)'] = df['duration (s)'].fillna(0)\n",
    "    # Calcular el tiempo acumulado sumando las duraciones individuales\n",
    "    df['Tiempo acumulado'] = df['duration (s)'].cumsum()    \n",
    "    # --- FIN C√ÅLCULO DE TIEMPO ACUMULADO ---        \n",
    "    \n",
    "    return df\n",
    "\n",
    "# ---\n",
    "# Aseg√∫rate de que el fichero est√© en la misma carpeta, o proporciona la ruta completa\n",
    "df_desde_archivo = parse_datos_episodio_flexible_desde_archivo(ruta_del_archivo)\n",
    "# Opcional: guardar el DataFrame en un archivo CSV\n",
    "df_desde_archivo.to_csv(f'{file_ok}.csv', index=False)\n",
    "\n",
    "if df_desde_archivo is not None:\n",
    "    print(\"Primeras 10 filas del DataFrame (con Recompensa acumulada media):\")\n",
    "    print(df_desde_archivo.head) \n",
    "    print(duracion)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_csv = f'{file_ok}.csv'\n",
    "# Lee el CSV. Pandas es muy bueno infiriendo el formato por defecto.\n",
    "dt_total = pd.read_csv(ruta_archivo_csv, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la figura y los ejes para el gr√°fico\n",
    "plt.figure(figsize=(12, 7)) # Aumenta el tama√±o para mejor visualizaci√≥n\n",
    "\n",
    "# Dibuja la recompensa por cada episodio\n",
    "plt.plot(dt_total['Episodio'], dt_total['Recompensa Episodio'], \n",
    "         label='Recompensa/Episodio', \n",
    "         alpha=0.7, # Hazla un poco transparente para que la media se vea bien\n",
    "         color='skyblue')\n",
    "\n",
    "# Dibuja la recompensa acumulada media sobre el mismo gr√°fico\n",
    "plt.plot(dt_total['Episodio'], dt_total['Recompensa total'], \n",
    "         label='Recompensa Media (100)', \n",
    "         color='red', \n",
    "         linewidth=2) # Haz la l√≠nea m√°s gruesa para que destaque\n",
    "\n",
    "# A√±ade t√≠tulo y etiquetas\n",
    "plt.title('Recompensa por Episodio y Recompensa Acumulada Media durante el Entrenamiento')\n",
    "plt.xlabel('N√∫mero de Episodio')\n",
    "plt.ylabel('Recompensa')\n",
    "\n",
    "# A√±ade una leyenda para identificar cada l√≠nea\n",
    "plt.legend()\n",
    "\n",
    "# A√±ade una cuadr√≠cula para facilitar la lectura\n",
    "plt.grid(True)\n",
    "\n",
    "# Muestra el gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥mo trazar la p√©rdida (si est√° disponible)\n",
    "#if 'loss' in dt_total:\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dt_total['loss'])\n",
    "plt.title('P√©rdida durante el Entrenamiento')\n",
    "plt.xlabel('Paso de Entrenamiento') # O Episodio, dependiendo de c√≥mo lo registre keras-rl\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "dt_total[\"Pasos\"].astype(int).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Tiempo acumulado')\n",
    "tiempo_acumulado_horas = dt_total['Tiempo acumulado'] / 3600\n",
    "plt.xlabel('Paso de Entrenamiento') # O Episodio, dependiendo de c√≥mo lo registre keras-rl\n",
    "plt.ylabel('Horas ejecuci√≥n')\n",
    "tiempo_acumulado_horas.astype(float).plot()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
