{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah\n",
    "* Alumno 2: Cuesta Cifuentes, Jair\n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2. Preparar Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "El entorno de Colab está preinstalado con una serie de librerías por defecto. Para trabajar en base a las especificaciones del ejercicio se necesitan intalar unas librerías que bajen de versión las existentes en Colab. Entre ellas tensorflow. El problema de realizar esta acción es que para que todas las versiones sean consideradas por el entorno hay que reiniciar la sesión, sino se mantienen dependencias y los import no funcionan. <br>\n",
    "Es decir tras los \"pip install\" hay que hacer un **\"Runtime > Restart runtime\"** o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\".<br>\n",
    "En este punto se ha de tener presente que se ha reiniciado y **se han perdido las variables** que se hayan establecido, por ese motivo repetiremos el código para identificar si estamos en Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**IMPORTANTE:**</font><br>\n",
    "Ignorar los errores que puedan aparecer, son incompatibilidades con librerías avanzadas que no utilizamos ni necesitamos para nuestro código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB:  \n",
    "# =========================\n",
    "#  Entorno Colab nativo con todo lo compatible.\n",
    "#  Sólo recordar que se debe REINICIAR EL RUNTIME (al acabar)\n",
    "# =========================  \n",
    "  print(\" [INFO] - Instalando paquetes adicionales...\")  \n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Ignorar los errores que aparecen:\")\n",
    "  print(\"   Son incompatibilidades que aparecen con librería avanzadas\")\n",
    "  print(\"   que no necesitamos ni vamos a utilizar\")  \n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12.1 --quiet\n",
    "  # Instala imageio y sus dependencias para video\n",
    "  %pip install imageio==2.15.0 --quiet\n",
    "  %pip install imageio-ffmpeg\n",
    "  %pip install ffmpeg-python  \n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"INSTALACIÓN COMPLETADA\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\")\n",
    "  print(\"1. Ve a Runtime > Restart runtime\")\n",
    "  print(\"2. Después ejecuta las importaciones\")\n",
    "  print(\"=\"*60)  \n",
    "  INSTALL_LOCAL = False\n",
    "  IN_COLAB_ENV = False\n",
    "\n",
    "if IN_COLAB_ENV:\n",
    "# =========================\n",
    "#  Colab con env --> \n",
    "#    no funciona muy bien pues aunque se cree el entorno, Colab sigue\n",
    "#    utilizando el suyo con sus librería y se necesita usar %%writefile\n",
    "# =========================      \n",
    "  # 1. Instalar virtualenv\n",
    "  !pip install virtualenv --quiet\n",
    "\n",
    "  # 3. Crear el entorno virtual llamado \"miar_rl\"\n",
    "  !virtualenv miar_rl\n",
    "\n",
    "  # 4. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
    "  !./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
    "  !./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
    "  !./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
    "  !./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
    "  !./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
    "\n",
    "  # 5. Librerías adicionales\n",
    "  !./miar_rl/bin/pip install Pillow\n",
    "  !./miar_rl/bin/pip install matplotlib\n",
    "  !./miar_rl/bin/pip install tqdm\n",
    "  INSTALL_LOCAL = False\n",
    "    \n",
    "if INSTALL_LOCAL:    \n",
    "# =========================\n",
    "#  Librería para trabajar en local, si NO se cargaron las \n",
    "#    librerías desde fichero requirements\n",
    "# =========================        \n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm\n",
    "  # Instala imageio y sus dependencias para video\n",
    "  %pip install imageio==2.15.0 --quiet\n",
    "  %pip install imageio-ffmpeg\n",
    "  %pip install ffmpeg-python  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.3. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE:**<br>\n",
    "Recordar que antes de seguir (si hemos decidido el entorno de Colab nativo - IN_COLAB=True -) \n",
    "* Hay que hacer un <font color='red'>\"Runtime > Restart runtime\"</font> o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
    "INSTALL_LOCAL = False\n",
    "#Si quremos trabajar con el entorno nativo --> false\n",
    "IN_COLAB_ENV = False\n",
    "\n",
    "# Verificamos si estamos en Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_1\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.4. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "3lN7KLe05NSa",
    "outputId": "47c41c84-3bfd-425f-9b8d-6d6aa525afdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.viminfo', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_dqn (1).ipynb', '08MIAR_dqn (2).ipynb', '08MIAR_dqn (3).ipynb', '08MIAR_dqn (4).ipynb', '08MIAR_dqn (5).ipynb', '08MIAR_dqn.ipynb', '08miar_dqn.py', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoint', 'checkpoints', 'Configuración local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'diagnosticos', 'Documents', 'Downloads', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'DuelingDQN_3 - copia.csv', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'fffff.py', 'ffmpeg', 'ffmpeg-2025-06-17-git-ee1f79b0fa-essentials_build.7z', 'ffmpeg-2025-06-17-git-ee1f79b0fa-full_build.7z', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'lista.txt', 'menory.txt', 'Menú Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'models', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'output-train-DDQN.csv', 'output-train-DDQN.txt', 'output-train-DuelingDQN.csv', 'output-train-DuelingDQN_OK1.txt', 'output-train-DuelingDQN_OK2.txt', 'Pictures', 'Plantillas', 'ppppp.ipynb', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_práctico_Grupo1_C.ipynb', 'RL_Proyecto_práctico_Grupo1_C.py', 'RL_Proyecto_práctico_Grupo1_C_VERSION_COLAB_V3.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'start.bat', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled221.ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'v3.ipynb', 'Videos', '_RL_Proyecto_práctico_Grupo1_C_v2.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Cambiar al directorio en Google Drive que deseas usar\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    print(\" [INFO] - Estamos ejecutando en Colab\")\n",
    "    # Montar Google Drive en el punto de montaje\n",
    "    print(\" [INFO] - Colab: montando Google drive en: \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "    # Crear drive_root si no existe\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\n [INFO] - Colab: Asegurando que \", drive_root, \" existe.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Cambiar al directorio\n",
    "    print(\"\\n [INFO] - Colab: Cambiamos el directorio a: \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verificar que estamos en el directorio de trabajo correcto\n",
    "%pwd\n",
    "print(\" [INFO] - Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihTI9TOD43ML"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIAR9zQv43MO"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas\n",
    "\n",
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K4o4-N4T43MO"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2      # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "import json\n",
    "import datetime\n",
    "import IPython\n",
    "import imageio\n",
    "import pandas as pd\n",
    "\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
    "from IPython.core.history import HistoryManager\n",
    "from tensorflow.keras.models import  clone_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "if IN_COLAB:  \n",
    "  from tensorflow.keras.optimizers.legacy import Adam\n",
    "else:\n",
    "  from tensorflow.keras.optimizers import Adam\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "puzS2kTzc1Fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Precisión mixta activada (mixed_float16)\n",
      " [INFO] - Optimización JIT activada\n"
     ]
    }
   ],
   "source": [
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "    from gym.wrappers import Monitor \n",
    "except ImportError:\n",
    "    print(\" [WARNING] - gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n",
    "\n",
    "# Activar precisión mixta para mayor velocidad\n",
    "print(\" [INFO] - Precisión mixta activada (mixed_float16)\")\n",
    "# Activar optimización JIT\n",
    "tf.config.optimizer.set_jit(True)\n",
    "print(\" [INFO] - Optimización JIT activada\")    \n",
    "\n",
    "# Hay veces que se produce un error OperationalError('database or disk is full') indica que la \n",
    "# base de datos SQLite que usa Jupyter para guardar el historial de comandos está llena. \n",
    "# Esto explica por qué no puedes guardar aunque tengas 500GB libres.\n",
    "# Desactivar guardado de historial para esta sesión\n",
    "ip = IPython.get_ipython()\n",
    "ip.history_manager.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8iRCStcc90p",
    "outputId": "96045a07-febd-4dce-e5c8-bbec04475b33"
   },
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (x cores)\n",
    "def optimizar_tensorflow():\n",
    "    \"\"\"Configura TensorFlow para rendimiento óptimo en CPU/GPU\"\"\"\n",
    "    # Limpiar sesión previa\n",
    "    gc.collect()\n",
    "\n",
    "    # Optimización de GPU si está disponible\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\" [INFO] - GPU optimizada para crecimiento adaptativo de memoria\")\n",
    "        except Exception as e:\n",
    "            print(f\" [INFO] - Error al configurar GPU: {e}\")\n",
    "\n",
    "    # Optimización de CPU\n",
    "    num_cpu_cores = os.cpu_count() or 8  # Fallback a 8 si no se puede detectar\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
    "\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(num_cpu_cores // 2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(max(2, num_cpu_cores // 4))\n",
    "\n",
    "    # Modo eager solo si es necesario\n",
    "    # Para entrenamiento, es mejor desactivarlo por rendimiento\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    \n",
    "    # Configuración para evitar errores de guardado en Colab\n",
    "    if IN_COLAB:\n",
    "      os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "      os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reducir mensajes de TF\n",
    "      # Configuración personalizada para guardado de modelos\n",
    "      tf.keras.backend.set_learning_phase(1)  # Asegurarnos que estamos en modo entrenamiento\n",
    "      # Desactivar guardado asíncrono (la causa más común del error)\n",
    "      if hasattr(tf.config, 'experimental'):\n",
    "        tf.config.experimental.set_synchronous_execution(True)      \n",
    "    \n",
    "    print(f\" [INFO] - TensorFlow optimizado para {num_cpu_cores} cores CPU\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faNbnMuOdNDP"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7WFE0sqPdLsy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Crear el entorno\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcjoyH3idqh4",
    "outputId": "85af5e47-6b37-472b-ddbe-ce71bea7eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgw7iHVRduGa"
   },
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TshMrqTjdxja"
   },
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "# Hiperparámetros optimizados\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "CHANNELS = 1                    # Canal para grises\n",
    "USE_FRAMESTACK = True           # Cambiar a True si quieres detección de movimiento\n",
    "WINDOW_LENGTH = 4 if USE_FRAMESTACK else 1   # Número de fotogramas apilados          # La mayoría de implementaciones usan 4 frames\n",
    "batch_size = 32                 # Tamaño de batch óptimo\n",
    "gamma = 0.99                    # Factor de descuento (mejor que 0.95 para recompensas a largo plazo)\n",
    "learning_rate = 0.00025         # Tasa de aprendizaje estándar para DQN\n",
    "memory_size = 1000000           # Buffer de memoria grande para mejor estabilidad\n",
    "TARGET_UPDATE_INTERVAL = 10000  # Actualización de red objetivo cada 10,000 pasos\n",
    "WARMUP_STEPS = 50000            # Pasos iniciales para llenar la memoria (experiencia aleatoria)\n",
    "NUM_TRAINING_STEPS = 2000000    # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
    "EPSILON_STEPS = 500000          # Total de pasos de evaluación del modelo\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH)   # Dimensiones de cada frame\n",
    "\n",
    "# Single frame shape (height, width, channels)\n",
    "FRAME_SHAPE = (HEIGHT, WIDTH, CHANNELS)  # (84, 84, 1)\n",
    "MODEL_INPUT_SHAPE = (HEIGHT, WIDTH, WINDOW_LENGTH)  # Forma para el modelo (channels_last)\n",
    "SEQ_INPUT_SHAPE = (WINDOW_LENGTH,HEIGHT, WIDTH)  # Forma para el modelo (channels_last)\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "# Definir shape consistente\n",
    "if USE_FRAMESTACK:\n",
    "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
    "else:\n",
    "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
    "\n",
    "state_size = (*INPUT_SHAPE, WINDOW_LENGTH)   # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "input_shape = (*INPUT_SHAPE, WINDOW_LENGTH)  # Para la API de keras-rl\n",
    "action_size = env.action_space.n       # 6 acciones posibles\n",
    "learning_rate =  0.00025               # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST      # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST      # Máximo de pasos posibles por episodio\n",
    "total_episodios = 100                # Episodios totales para el entrenamiento\n",
    "max_steps       = 3000               # Máximo de pasos posibles por episodio\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.1             # Probabilidad mínima de exploración\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "TARGET_REWARD = 20.0\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjFRyr4Ld1Hp"
   },
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "06wZVH5c43MP"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(INPUT_SHAPE)):\n",
    "        self.input_shape = input_shape\n",
    "        # Precargar una imagen negra para inicialización\n",
    "        self.black_frame = np.zeros(input_shape, dtype=np.uint8)\n",
    "\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        # Si la observación es None, devolver un marco negro\n",
    "        if observation is None:\n",
    "            return self.black_frame\n",
    "\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        # Crop the screen (remove the part below the player)\n",
    "        # [Up: Down, Left: right]\n",
    "        cropped_img = observation[18:-12, 4:-12]\n",
    "        # Optimización: usar cv2 para redimensionar y convertir a escala de grises (más rápido que PIL)\n",
    "        resized = cv2.resize(cropped_img, self.input_shape)\n",
    "        processed_observation = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY) if len(resized.shape) == 3 else resized\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype(np.uint8)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Normaliza la recompensa al rango [-1, 1].\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            reward (float): Recompensa original del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada al rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        \"\"\"\n",
    "        Procesa un paso completo del entorno.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation: Observación del entorno.\n",
    "            reward: Recompensa obtenida.\n",
    "            done: Indicador de fin de episodio.\n",
    "            info: Información adicional del entorno.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "            tuple: (observación procesada, recompensa procesada, done, info)\n",
    "        \"\"\"\n",
    "        processed_observation = self.process_observation(observation)\n",
    "        processed_reward = self.process_reward(reward)\n",
    "        return processed_observation, processed_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptwluQRXedZP"
   },
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VwBXOBk43MP",
    "outputId": "88dbba58-92a4-4d75-cf4c-799a04c36cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Numero de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "print(\" [INFO] - Numero de acciones disponibles: \" + str(nb_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT6osc3H43MP",
    "outputId": "cbe27690-c40d-49b1-d420-80b7d045ce26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - Formato de las observaciones:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" [INFO] - Formato de las observaciones:\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "7-BoOu_eeiAE",
    "outputId": "5b52561e-70d0-47c1-f55e-c882dda20b66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for i in range(22):\n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "vHJaGcAVelRY",
    "outputId": "f2686c2b-a159-4c1f-e857-984352520683"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-yCJoGjf2Fg"
   },
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ewKKozUaf-mG"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-asignar arrays con el tamaño exacto\n",
    "        # Usar uint8 para estados (más eficiente que float32)\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        print(f\" [INFO] - ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
    "        memory_size = (\n",
    "            self.states.nbytes + self.next_states.nbytes +\n",
    "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "        ) / (1024 * 1024)\n",
    "        print(f\" [INFO] - Memoria asignada: {memory_size:.2f} MB\")\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Añade una experiencia al buffer de forma eficiente\"\"\"\n",
    "        # Convertir a uint8 para ahorrar memoria (estados son imágenes 0-255)\n",
    "        if state.dtype != np.uint8:\n",
    "            state = (state * 255).astype(np.uint8)\n",
    "        if next_state.dtype != np.uint8:\n",
    "            next_state = (next_state * 255).astype(np.uint8)\n",
    "\n",
    "        # Almacenar directamente en el array pre-asignado\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        # Actualizar posición circular\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
    "\n",
    "        # Generar índices aleatorios\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "\n",
    "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
    "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
    "        batch_actions = self.actions[indices]\n",
    "        batch_rewards = self.rewards[indices]\n",
    "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
    "        batch_dones = self.dones[indices]\n",
    "\n",
    "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        # No es necesario limpiar los arrays, se sobrescriben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-SaTPRkQoK"
   },
   "source": [
    "#### Clase PerformanceMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente no se usa, si se necesitase mayor detalle de la evolución de los entrenamientos se podría incluir en el Callback antes del entrenamiento. De momento no se usa!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Wf2A5kDokNdS"
   },
   "outputs": [],
   "source": [
    "# Clase para monitoreo de memoria y rendimiento\n",
    "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path='diagnosticos'):\n",
    "        self.save_path = save_path\n",
    "        self.episode_times = []\n",
    "        self.memory_usage = []\n",
    "        self.current_episode = 0\n",
    "        self.episode_start_time = None\n",
    "        self.episode_start_memory = None\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        self.episode_start_time = time.time()\n",
    "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        self.current_episode = episode\n",
    "        print(f\" [INFO] - Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        end_time = time.time()\n",
    "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        episode_duration = end_time - self.episode_start_time\n",
    "\n",
    "        self.episode_times.append(episode_duration)\n",
    "        self.memory_usage.append(final_memory)\n",
    "\n",
    "        print(f\" [INFO] - Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
    "        print(f\" [INFO] - Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
    "\n",
    "        # Guardar diagnóstico cada 5 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            self.save_diagnostics(episode)\n",
    "\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()\n",
    "\n",
    "    def save_diagnostics(self, episode):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.episode_times)\n",
    "        plt.title('Tiempo por episodio')\n",
    "        plt.ylabel('Segundos')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.memory_usage)\n",
    "        plt.title('Uso de memoria')\n",
    "        plt.ylabel('MB')\n",
    "        plt.xlabel('Episodio')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTgDOJoCgISN"
   },
   "source": [
    "### 1. Implementación de la red neuronal\n",
    "\n",
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFAzP0UigPVg"
   },
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kP2vNpKnzkTl"
   },
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo DQN usando SOLO Keras estándar. Base común para redes DQN y DDQN\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "    Construye un modelo que acepta channels_first y convierte internamente\n",
    "\n",
    "    Esta función implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación RELU.\n",
    "    Esto evita completamente los problemas de grafos múltiples\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DQN estándar: input_shape={input_shape}, actions={nb_actions}\")\n",
    "       \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DQN_Model')\n",
    "    memory = None\n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model, memory, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_models(input_shape, nb_actions, memory_size):\n",
    "    \"\"\"\n",
    "    Crea modelos para Double DQN (principal y objetivo)\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelos DDQN: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Modelo principal\n",
    "    main_model, _, _ = create_dqn_model(input_shape, nb_actions, memory_size)\n",
    "    main_model._name = 'DDQN_Main_Model'    \n",
    "    \n",
    "    # Modelo objetivo (copia exacta)\n",
    "    target_model = tf.keras.models.clone_model(main_model)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    target_model._name = 'DDQN_Target_Model'\n",
    "    #Pequeño buffer para que entrene paso a paso\n",
    "    memory = SequentialMemory(limit=10000, window_length=WINDOW_LENGTH)\n",
    "    \n",
    "    print(\"✅ Modelos DDQN creados exitosamente\")    \n",
    "    return main_model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddqn_replay_model(input_shape, nb_actions, memory_size):\n",
    "    print(f\"🏗️ Creando modelos DDQN_replay: input_shape={input_shape}, actions={nb_actions}\")\n",
    "    \n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
    "\n",
    "    # Convertir a channels_last usando Permute\n",
    "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
    "    # Red convolucional estándar\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='dense1')(x)\n",
    "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DDQN_replay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "    target_model = clone_model(model)  # Create target model for DDQN\n",
    "    target_model.set_weights(model.get_weights())  # Initialize with same weights\n",
    "    target_model._name = 'DDQN_replay_Target_Model'    \n",
    "    \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dueling_dqn_replay_model(input_shape, action_size, memory_size):\n",
    "    \"\"\"\n",
    "    Crea un modelo Dueling DQN con replay.\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape={input_shape}, actions={action_size}\")\n",
    "    # Input en formato channels_first (como viene de SequentialMemory)\n",
    "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)    \n",
    "    x = Permute((2, 3, 1))(inputs)\n",
    "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', input_shape=input_shape)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    # Value stream\n",
    "    value = Dense(512, activation='relu')(x)\n",
    "    value = Dense(1, activation='linear')(value)\n",
    "    # Advantage stream\n",
    "    advantage = Dense(512, activation='relu')(x)\n",
    "    advantage = Dense(action_size, activation='linear')(advantage)\n",
    "    # Combine streams\n",
    "    outputs = Add()([value, Lambda(lambda a: a - K.mean(a, axis=1, keepdims=True))(advantage)])\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='DuelingDQNReplay_Main_Model')\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)   \n",
    "    target_model = clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())    \n",
    "    target_model._name = 'DuelingDQNReplay_Target_Model'     \n",
    "   \n",
    "    print(\"✅ Modelo creado exitosamente\")\n",
    "    print(f\"📊 Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    return model, memory, target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0DHvKNshvQo"
   },
   "source": [
    "### 2. Implementación de la solución DQN\n",
    "\n",
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model, memory, target_model, model_name, checkpoint_dir=\"checkpoints\", suffix=\"lastest\"):\n",
    "    \"\"\"\n",
    "    Carga pesos y estado para componentes separados (modelo, memoria, target_model).\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Tipo del modelo ('DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.)\n",
    "        checkpoint_dir: Directorio donde buscar los checkpoints\n",
    "        suffix: Tipo de checkpoint a cargar (\"lastest\", \"best\", o \"epXXX\")\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (episode, steps, epsilon) - El episodio, pasos y epsilon desde donde continuar\n",
    "               Si no se encuentra el checkpoint, devuelve (0, 0, 0.1)\n",
    "    \"\"\"  \n",
    "    # Valores predeterminados\n",
    "    episode = 0\n",
    "    steps = 0\n",
    "    epsilon = 0.1  # Valor por defecto\n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "      \n",
    "    try:    \n",
    "        # Cargar estado del entrenamiento\n",
    "        if not os.path.exists(state_path):\n",
    "            print(f\"⚠️ No se encontró el checkpoint {suffix} para {model_name}\")\n",
    "            return episode, steps, epsilon \n",
    "        else:\n",
    "            print(f\"📂 Se cargó: {state_path}\")        \n",
    "\n",
    "        with open(state_path, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            \n",
    "        # Extraer información básica\n",
    "        episode = state.get('episode', 0)\n",
    "        steps = state.get('global_steps', 0)\n",
    "        epsilon = state.get('epsilon', 0.1)\n",
    "        \n",
    "        print(f\"📂 Cargando checkpoint {suffix} (episodio: {episode}, pasos: {steps}, epsilon: {epsilon})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error al cargar el archivo de estado: {e}\")\n",
    "        \n",
    "    # Cargar modelo principal\n",
    "    if os.path.exists(main_model_path):\n",
    "        try:\n",
    "            model.load_weights(main_model_path)\n",
    "            print(f\"📂 Modelo principal cargado: {main_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ [ERROR] - Error al cargar el modelo principal: {e}\")\n",
    "            return 0, 0, 0.1  # Si falla la carga del modelo principal, mejor empezar desde cero\n",
    "    else:\n",
    "        print(f\"❌ [ERROR] - No se encontró el archivo del modelo principal: {main_model_path}\")\n",
    "        return 0, 0, 0.1       \n",
    "\n",
    "    # Cargar modelo target si existe y se proporcionó\n",
    "    if target_model is not None:\n",
    "        if os.path.exists(target_model_path):\n",
    "            try:\n",
    "                target_model.load_weights(target_model_path)\n",
    "                print(f\"📂 Modelo target cargado: {target_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ [WARNING] - Error al cargar el modelo target: {e}\")\n",
    "                # Si hay error, sincronizar con el principal\n",
    "                print(\"🔄 Sincronizando red target desde la principal...\")\n",
    "                target_model.set_weights(model.get_weights())\n",
    "        else:\n",
    "            # Si no existe el archivo, sincronizar con el principal\n",
    "            print(\"🔄 No se encontró archivo target, sincronizando desde la principal...\")\n",
    "            target_model.set_weights(model.get_weights())\n",
    "          \n",
    "    # Cargar memoria si corresponde al tipo de modelo\n",
    "    if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and memory is not None and os.path.exists(memory_path):\n",
    "        try:              \n",
    "            with open(memory_path, 'rb') as f:\n",
    "                loaded_memory = pickle.load(f)                \n",
    "                \n",
    "            # Transferir contenido no reemplazar objeto\n",
    "            memory_loaded = False\n",
    "            # Estructura SequentialMemory (Keras-RL)\n",
    "            if hasattr(loaded_memory, 'observations') and hasattr(memory, 'observations'):\n",
    "                memory.observations = loaded_memory.observations\n",
    "                memory.actions = loaded_memory.actions\n",
    "                memory.rewards = loaded_memory.rewards\n",
    "                memory.terminals = loaded_memory.terminals\n",
    "                if hasattr(loaded_memory, 'observations_'):\n",
    "                    memory.observations_ = loaded_memory.observations_\n",
    "                memory_loaded = True\n",
    "            elif hasattr(loaded_memory, 'buffer') and hasattr(memory, 'buffer'):\n",
    "                memory.buffer = loaded_memory.buffer\n",
    "                memory.position = loaded_memory.position\n",
    "                if hasattr(loaded_memory, 'size'):\n",
    "                    memory.size = loaded_memory.size\n",
    "                memory_loaded = True\n",
    "\n",
    "            if memory_loaded:\n",
    "                print(f\"📂 Memoria cargada correctamente: {memory_path}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Estructura de memoria desconocida, no se pudo cargar\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al cargar la memoria: {e}\")    \n",
    "    \n",
    "    return episode, steps, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para convertir objetos no serializables a JSON\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convierte tipos numpy a tipos Python nativos para serialización JSON\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "        return obj.isoformat()\n",
    "    else:\n",
    "        # Para cualquier otro tipo no reconocido\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(agent, model_name, episode=0, steps=0, \n",
    "                          checkpoint_dir=\"checkpoints\", suffix=\"lastest\", epsilon=0.1):\n",
    "    \"\"\"           \n",
    "    Guarda el modelo, la memoria de repetición y el estado del entrenamiento.\n",
    "    \n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.    \n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None)\n",
    "        model_name: Nombre base del modelo (ej: 'DQN', 'DDQN_REPLAY')\n",
    "        episode: Número del episodio actual\n",
    "        steps: Pasos globales acumulados\n",
    "        checkpoint_dir: Directorio donde guardar los checkpoints\n",
    "        suffix: Tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\")\n",
    "        \n",
    "        model : tensorflow.keras.Model-  El modelo principal de red neuronal que se desea guardar.\n",
    "        memory : objeto de memoria-      La memoria de repetición utilizada para almacenar experiencias de entrenamiento.\n",
    "                                         Puede ser SequentialMemory, ReplayBuffer u otra implementación compatible.\n",
    "        target_model : tensorflow.keras.Model-         El modelo target utilizado en algoritmos como DDQN. Es una copia del modelo principal\n",
    "            que se actualiza periódicamente durante el entrenamiento.\n",
    "        model_name : str-                Nombre identificativo del modelo ('DQN', 'DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.).\n",
    "                                         Se utiliza para nombrar los archivos de checkpoint.\n",
    "        episode : int, opcional-         Número del episodio actual de entrenamiento. Por defecto es 0.\n",
    "        steps : int, opcional-           Número total de pasos (interacciones con el entorno) realizados. Por defecto es 0.\n",
    "\n",
    "        checkpoint_dir : str, opcional-  Directorio donde se guardarán los archivos de checkpoint. Por defecto es \"checkpoints\".\n",
    "\n",
    "        suffix : str, opcional-          Sufijo para identificar el tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\"). Por defecto es \"lastest\".\n",
    "\n",
    "        epsilon : float, opcional-       Valor actual de epsilon para la política epsilon-greedy. Por defecto es 0.1.\n",
    "        force_override: Si es True, sobrescribe incluso los checkpoints 'best'    \n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "    \"\"\"   \n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # Puedes intentar limpiar la memoria de Python no usada explícitamente antes de medir\n",
    "    gc.collect()     \n",
    "    \n",
    "    # Definir las rutas de los archivos\n",
    "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
    "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
    "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
    "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"    \n",
    "    \n",
    "    # PROTECCIÓN DE CHECKPOINTS EXISTENTES\n",
    "    # Si es un checkpoint \"best\" y estamos intentando guardarlo con episodio 0\n",
    "    if (\"best\" in suffix or \"lastest\" in suffix) and episode == 0:\n",
    "        # Verificar si ya existe un mejor checkpoint con episodio > 0\n",
    "        state_path = f\"{checkpoint_dir}/{model_name}_best_state.json\"\n",
    "        if os.path.exists(state_path):\n",
    "            try:\n",
    "                with open(state_path, 'r') as f:\n",
    "                    existing_state = json.load(f)\n",
    "                existing_episode = existing_state.get('episode', 0)\n",
    "                \n",
    "                if existing_episode > 0:\n",
    "                    print(f\"🛡️ Protegiendo checkpoint '{suffix}' existente (ep: {existing_episode})\")\n",
    "                    print(f\"❌ NO se guardará un nuevo checkpoint con episodio 0\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error verificando checkpoint existente: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo principal\n",
    "        if hasattr(agent, 'model'):\n",
    "            agent.model.save_weights(main_model_path)\n",
    "            print(f\"💾 Guardado modelo principal {suffix}: {main_model_path}\")               \n",
    "        else:\n",
    "            print(f\"⚠️ El agente no tiene el atributo 'model'\")          \n",
    "        \n",
    "        # Guardar pesos del modelo target si existe\n",
    "        if hasattr(agent, 'target_model') and agent.target_model is not None:\n",
    "            agent.target_model.save_weights(target_model_path)      \n",
    "            print(f\"💾 Guardado modelo target {suffix}: {target_model_path}\")         \n",
    "        else:\n",
    "            print(f\"⚠️ El agente no tiene el atributo 'target_model' o es None\")            \n",
    "\n",
    "        # Preparar el estado del entrenamiento - Con conversión a tipos Python nativos\n",
    "        state = {\n",
    "            'episode': int(episode),  \n",
    "            'global_steps': int(steps),  \n",
    "            'epsilon': float(epsilon),        \n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }              \n",
    "        \n",
    "        # Si hay una política específica en el agente\n",
    "        if hasattr(agent, 'policy'):\n",
    "            policy_state = {}\n",
    "            \n",
    "            # Intentar guardar el estado de la política\n",
    "            if hasattr(agent.policy, 'eps'):\n",
    "                policy_state['eps'] = float(agent.policy.eps)  # Convertir a float Python\n",
    "        \n",
    "            # Si la política tiene más atributos relevantes\n",
    "            for attr in ['value_max', 'value_min', 'value_test', 'nb_steps']:\n",
    "                if hasattr(agent.policy, attr):\n",
    "                    value = getattr(agent.policy, attr)\n",
    "                    if isinstance(value, (np.integer, np.floating, np.bool_)):\n",
    "                        value = value.item()  # Convierte cualquier tipo numpy a su equivalente Python\n",
    "                    policy_state[attr] = value\n",
    "            \n",
    "            if policy_state:\n",
    "                state['policy'] = policy_state                  \n",
    "               \n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and hasattr(agent, 'memory') and agent.memory is not None:\n",
    "            try:\n",
    "                # Usar compresión máxima\n",
    "                with open(memory_path, 'wb') as f:\n",
    "                    pickle.dump(agent.memory, f)\n",
    "                print(f\"💾 Memoria {suffix} guardada: {memory_path}\")  \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ No se pudo guardar la memoria: {e}\")       \n",
    "        \n",
    "        # Guardar el estado\n",
    "        with open(state_path, 'w') as f:\n",
    "            json.dump(state, f, indent=2, default=convert_to_json_serializable)\n",
    "        \n",
    "        print(f\"💾 Checkpoint {suffix} guardado (ep: {episode}, pasos: {steps})\")  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint {suffix}: {e}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para monitorear el progreso del entrenamiento de un agente DQN.\n",
    "\n",
    "    Registra el avance en términos de pasos completados, porcentaje, velocidad de entrenamiento\n",
    "    (pasos por segundo) y tiempo estimado de finalización (ETA).\n",
    "\n",
    "    Atributos:\n",
    "        total_steps (int): Número total de pasos de entrenamiento.\n",
    "        print_interval (int): Intervalo de pasos para imprimir el progreso (por defecto: 10,000).\n",
    "        start_time (float): Tiempo de inicio del entrenamiento (en segundos).\n",
    "        last_step (int): Último paso registrado (inicializado en 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, total_steps, print_interval=100):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "\n",
    "        Args:\n",
    "            total_steps (int): Número total de pasos de entrenamiento.\n",
    "            print_interval (int): Intervalo de pasos para imprimir el progreso.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.print_interval = print_interval\n",
    "        self.step_counter = 0\n",
    "        self.start_time = time.time()\n",
    "        self.episode_rewards = []  # Store clipped episode rewards\n",
    "        self.episode_steps = []\n",
    "        self.current_episode_reward = 0.0  # Track clipped reward for current episode\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al inicio del entrenamiento.\n",
    "\n",
    "        Inicializa el tiempo de inicio y muestra un mensaje de comienzo.\n",
    "\n",
    "        Args:\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"🚀 Entrenamiento iniciado: {self.total_steps:,} pasos\")\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        \"\"\"\n",
    "        Se ejecuta al final de cada paso de entrenamiento.\n",
    "\n",
    "        Calcula y muestra el progreso, incluyendo porcentaje completado, velocidad\n",
    "        (pasos por segundo) y tiempo estimado de finalización (ETA) en horas.\n",
    "\n",
    "        Args:\n",
    "            step (int): Número del paso actual.\n",
    "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        self.step_counter += 1        \n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        clipped_reward = np.clip(raw_reward, -1.0, 1.0)  # Match AtariProcessor clipping\n",
    "        self.current_episode_reward += clipped_reward\n",
    "        if self.step_counter % self.print_interval == 0:\n",
    "            progress = (self.step_counter / self.total_steps) * 100\n",
    "            elapsed_time = (time.time() - self.start_time)\n",
    "            steps_per_sec = self.step_counter / elapsed_time\n",
    "            eta_hours = (self.total_steps - self.step_counter) / steps_per_sec / 3600\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "            print(f\"📊 Paso {self.step_counter:,}/{self.total_steps:,} ({progress:.1f}%) - \"\n",
    "                  f\"{steps_per_sec:.1f} pasos/seg - ETA: {eta_hours:.1f}h - Memoria: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        nb_steps = logs.get('nb_episode_steps', 1)\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.episode_steps.append(nb_steps)\n",
    "        mean_reward = self.current_episode_reward / nb_steps if nb_steps > 0 else 0\n",
    "        print(f\"📈 Episodio {episode+1}: Recompensa total (clipped): {self.current_episode_reward:.3f}, \"\n",
    "              f\"Pasos: {nb_steps}, Mean Reward Calculado: {mean_reward:.6f} (Recompensa/Pasos)\")\n",
    "        # Reset for next episode\n",
    "        self.current_episode_reward = 0.0            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback que monitorea el progreso hacia una media de episode_reward objetivo\n",
    "    e integra con nuestro sistema de checkpoints.\n",
    "    \"\"\"      \n",
    "    def __init__(self, dqn, target_avg_reward=20.0, name_model=None, window_size=100, save_best=True, checkpoint_dir=checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.target_avg_reward = target_avg_reward\n",
    "        self.window_size = window_size\n",
    "        self.save_best = save_best\n",
    "        # Obtener el nombre del modelo sin ruta\n",
    "        self.model = dqn\n",
    "        self.model_name = name_model\n",
    "        self.episode_count = 0\n",
    "        self.episode_rewards = []\n",
    "        self.best_avg_reward = float('-inf')\n",
    "        self.episodes_at_target = 0\n",
    "        self.consecutive_target_episodes = 0\n",
    "        self.checkpoint_dir =  f\"{checkpoint_path}/{self.model_name}\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)       \n",
    "        \n",
    "        print(f\"🎯 OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
    "        print(f\"📊 Ventana de evaluación: {window_size} episodios\")        \n",
    "\n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()        \n",
    "        \n",
    "        self.episode_count += 1\n",
    "        episode_reward = logs.get('episode_reward', 0)\n",
    "        # Convert NumPy types to Python types\n",
    "        if isinstance(episode_reward, np.floating):\n",
    "            episode_reward = episode_reward.item()\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular media móvil\n",
    "        if len(self.episode_rewards) >= self.window_size or self.episode_count % 10 == 0:\n",
    "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
    "            current_avg = np.mean(recent_rewards)\n",
    "            \n",
    "            # Verificar si alcanzamos el objetivo\n",
    "            target_reached = current_avg >= self.target_avg_reward\n",
    "            \n",
    "            if target_reached:\n",
    "                self.episodes_at_target += 1\n",
    "                self.consecutive_target_episodes += 1\n",
    "            else:\n",
    "                self.consecutive_target_episodes = 0\n",
    "            \n",
    "            # Guardar si es el mejor promedio\n",
    "            if current_avg > self.best_avg_reward or self.episode_count % 10 == 0:\n",
    "                self.best_avg_reward = current_avg\n",
    "                if self.save_best and hasattr(self, 'model') and self.episode_count % 50 == 0:\n",
    "                    # Formato del sufijo para el mejor modelo con su promedio\n",
    "                    best_suffix = f\"best_avg{current_avg:.1f}\"                               \n",
    "                    try:\n",
    "                        epsilon = self.model.policy.eps if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'eps') else 0.1\n",
    "                       \n",
    "                        # Usar nuestro sistema de checkpoint para guardar el mejor modelo\n",
    "                        if hasattr(self.model, 'model'): # Por si el agente es un DQNAgent con un .model\n",
    "# AVG                            save_model_checkpoint(   self.model,   self.model_name,\n",
    "# AVG                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "# AVG                                checkpoint_dir=self.checkpoint_dir,   suffix=best_suffix, \n",
    "# AVG                                epsilon=epsilon\n",
    "# AVG                            )                                             \n",
    "                            # También actualizar el checkpoint \"best\" general\n",
    "                            save_model_checkpoint(   self.model,    self.model_name,\n",
    "                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
    "                                checkpoint_dir=self.checkpoint_dir,   suffix=\"best\", \n",
    "                                epsilon=epsilon\n",
    "                            )\n",
    "                        else:\n",
    "                            # Si no es un agente completo, guardar solo los pesos\n",
    "                            best_filename = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_model.h5\"\n",
    "                            self.model.save_weights(best_filename, overwrite=True)                        \n",
    "                            \n",
    "                        # Guardar métricas en JSON\n",
    "                        metrics = {\n",
    "                            \"episode\": int(self.episode_count),\n",
    "                            \"avg_reward\": float(current_avg),\n",
    "                            \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                            \"timestamp\": int(time.time()),\n",
    "                            \"consecutive_target_episodes\": int(self.consecutive_target_episodes),\n",
    "                            \"episodes_at_target\": int(self.episodes_at_target)\n",
    "                        }\n",
    "                        metrics_file = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_metrics.json\"\n",
    "                        with open(metrics_file, 'w') as f:\n",
    "                            json.dump(metrics, f, indent=2, default=str)\n",
    "                        \n",
    "                        print(f\"💾 NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado en {self.checkpoint_dir}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ [ERROR] - Error guardando mejor modelo: {e}\")\n",
    "            \n",
    "            # Mostrar progreso cada 50 episodios\n",
    "            if self.episode_count % 50 == 0:\n",
    "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
    "                target_status = \"🎯 OBJETIVO ALCANZADO!\" if target_reached else f\"📈 {progress_pct:.1f}% del objetivo\"\n",
    "                \n",
    "                print(f\"\\n📊 EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
    "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
    "                print(f\"   Media últimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
    "                print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "                print(f\"   Estado: {target_status}\")\n",
    "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
    "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
    "                \n",
    "                if self.consecutive_target_episodes >= 50:\n",
    "                    print(f\"🏆 ¡MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
    "        logs = logs or {}\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
    "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
    "            \n",
    "            print(f\"\\n🏁 RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "            print(f\"   Total episodios: {self.episode_count}\")\n",
    "            print(f\"   Media final últimos {self.window_size}: {final_avg:.2f}\")\n",
    "            print(f\"   Objetivo ({self.target_avg_reward}): {'✅ ALCANZADO' if objetivo_alcanzado else '❌ NO ALCANZADO'}\")\n",
    "            print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
    "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")\n",
    "            \n",
    "            # Save final metrics to JSON\n",
    "            final_metrics = {\n",
    "                \"total_episodes\": int(self.episode_count),\n",
    "                \"final_avg_reward\": float(final_avg),\n",
    "                \"target_reached\": bool(objetivo_alcanzado),\n",
    "                \"best_avg_reward\": float(self.best_avg_reward),\n",
    "                \"episodes_at_target\": int(self.episodes_at_target),\n",
    "                \"consecutive_target_episodes\": int(self.consecutive_target_episodes)  \n",
    "            }\n",
    "            final_log_path = f\"{self.checkpoint_dir}/final_metrics.json\"\n",
    "            try:\n",
    "                with open(final_log_path, 'w') as f:\n",
    "                    json.dump(final_metrics, f, indent=2, default=str)                                    \n",
    "                print(f\"💾 Métricas finales guardadas en: {final_log_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error al guardar métricas finales: {e}\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFileLogger(FileLogger):\n",
    "    def __init__(self, filepath, interval=100):\n",
    "        super().__init__(filepath, interval)\n",
    "        self.step = 0  \n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.logs = {}        \n",
    "    \n",
    "    def on_step_end(self, step, logs={}):\n",
    "        self.step += 1  \n",
    "        if self.step % self.interval == 0:\n",
    "            episode_logs = {}\n",
    "        raw_reward = logs.get('reward', 0.0)\n",
    "        self.current_episode_reward += np.clip(raw_reward, -1.0, 1.0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        metrics = logs.copy()\n",
    "        metrics['episode_reward'] = self.current_episode_reward\n",
    "        metrics['mean_reward_step'] = self.current_episode_reward / metrics.get('nb_episode_steps', 1)\n",
    "        metrics = {k: float(v) if isinstance(v, np.floating) else v for k, v in metrics.items()}\n",
    "        self.metrics[int(episode)] = metrics\n",
    "        if self.step % self.interval == 0:\n",
    "            with open(self.filepath, 'w') as f:\n",
    "                json.dump(self.metrics, f, indent=2, default=str)                \n",
    "        self.current_episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback para guardar checkpoints episódicamente usando nuestras funciones personalizadas.\n",
    "    \"\"\"    \n",
    "    def __init__(self, dqnet, checkpoint_path, save_freq=100, model_name='DQN'):\n",
    "        \"\"\"\n",
    "        Inicializa el callback.\n",
    "        \n",
    "        Parametros:\n",
    "            dqnet: El agente DQN a guardar\n",
    "            checkpoint_path: Ruta donde guardar los checkpoints\n",
    "            save_freq: Frecuencia de episodios para guardar\n",
    "            model_name: Nombre base para los archivos de checkpoint\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.model_name = model_name\n",
    "        self.checkpoint_path = f\"{checkpoint_path}/{self.model_name}\"\n",
    "        self.episode_counter = 0\n",
    "        self.dqnet = dqnet\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        \"\"\"\n",
    "        Guarda checkpoint al final de ciertos episodios.\n",
    "        \"\"\"        \n",
    "        self.episode_counter += 1\n",
    "        if self.episode_counter % self.save_freq == 0:\n",
    "            try:       \n",
    "                epsilon = self.dqnet.policy.eps if hasattr(self.dqnet, 'policy') and hasattr(self.dqnet.policy, 'eps') else 0.1\n",
    "                # También actualizar el checkpoint \"lastest\"\n",
    "                save_model_checkpoint(\n",
    "                    self.dqnet, \n",
    "                    self.model_name,\n",
    "                    episode=self.episode_counter,\n",
    "                    steps=self.dqnet.step, \n",
    "                    checkpoint_dir=self.checkpoint_path, \n",
    "                    suffix=\"lastest\",\n",
    "                    epsilon=epsilon\n",
    "                )                \n",
    "                print(f\"✅ Checkpoint guardado para episodio {self.episode_counter}\")        \n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ [ERROR] - Error al guardar checkpoint para episodio {self.episode_counter}: {str(e)}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgbzJyUjmTzs"
   },
   "source": [
    "#### **ENTRENAMIENTO** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(env,  model_name,  model, memory, target_model, model_instance=False, \n",
    "        start_episode=0, start_steps=0,\n",
    "        batch_size=batch_size, learning_rate=learning_rate, checkpoint_path='checkpoints',\n",
    "        input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size, warmup_steps=WARMUP_STEPS,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau, epsilon_start=epsilon_start,\n",
    "        epsilon_min=0.1, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS, target_reward=TARGET_REWARD,\n",
    "        enable_double_dqn = False):     \n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN con el entorno proporcionado\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        env: El entorno de Gym\n",
    "        model_name: Nombre identificador del modelo ('DQN', 'DDQN', etc.)\n",
    "        model: El modelo principal\n",
    "        memory: La memoria de repetición\n",
    "        target_model: El modelo target (puede ser None para DQN)        \n",
    "        model_instance: Flag que determina si existe modelo cargado (si False, se crea uno nuevo)\n",
    "        checkpoint_path: Ruta donde guardar checkpoints\n",
    "        start_episode: Episodio desde donde continuar el entrenamiento\n",
    "        start_steps: Pasos desde donde continuar el entrenamiento\n",
    "        batch_size: Tamaño del lote para el entrenamiento\n",
    "        learning_rate: Tasa de aprendizaje del optimizador\n",
    "        input_shape: Forma de la entrada para el modelo\n",
    "        memoria_tamano: Tamaño de la memoria de repetición\n",
    "        warmup_steps: Pasos de calentamiento antes del entrenamiento\n",
    "        target_update_interval: Intervalo para actualizar la red objetivo\n",
    "        target_update_tau: Factor de actualización suave para la red objetivo\n",
    "        epsilon_start: Valor inicial de epsilon para exploración\n",
    "        epsilon_min: Valor mínimo de epsilon para exploración\n",
    "        epsilon_steps: Número de pasos para decrementar epsilon\n",
    "        num_steps: Número total de pasos de entrenamiento\n",
    "        target_reward: Recompensa objetivo para considerar resuelto el entorno\n",
    "        enable_double_dqn: Si es True, usa DDQN; si no, usa DQN estándar\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "        tuple: (agente_entrenado, éxito) - Modelo entrenado y booleano indicando éxito    \n",
    "    \"\"\"\n",
    "    # Nombre del modelo para logs y checkpoints\n",
    "    name_model = model_name.upper()    \n",
    "    print(f\"🤖 {'Continuando' if model_instance else 'Creando'} entrenamiento para {name_model}...\")\n",
    "        \n",
    "    # Inicializar dqn desde el principio para evitar referencia antes de asignación\n",
    "    dqn = None      \n",
    "    save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"\n",
    "    # Inicializar callbacks al principio para asegurarnos de que siempre esté definido\n",
    "    callbacks = []\n",
    "    # Verificar target_model solo si estamos usando DDQN\n",
    "    if enable_double_dqn and target_model is None:\n",
    "        raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "            \n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()   \n",
    "    try:  \n",
    "        # Verificar que tenemos un modelo válido\n",
    "        if model is None:\n",
    "            raise ValueError(\"El modelo principal no puede ser None.\")              \n",
    "        # Verificar target_model solo si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is None:\n",
    "            raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
    "\n",
    "        # Verificar si la memoria ya se creó o necesitamos crearla\n",
    "        if memory is None:\n",
    "             memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
    "         # Para DDQN ejecutar paso a paso, así vermos diferencia con DDQN_REPLAY\n",
    "        if model_name in ['DDQN']:\n",
    "            train_interval_param = 1\n",
    "            warmup_steps_param = 0\n",
    "        else:\n",
    "            train_interval_param = WINDOW_LENGTH\n",
    "            warmup_steps_param = warmup_steps\n",
    "        \n",
    "        # Política de exploración\n",
    "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                     attr='eps',\n",
    "                                     value_max=epsilon_start, \n",
    "                                     value_min=epsilon_min, \n",
    "                                     value_test=.05,\n",
    "                                     nb_steps=epsilon_steps)  \n",
    "        # Crear agente DQN\n",
    "        dqn = DQNAgent(\n",
    "            model=model,\n",
    "            nb_actions=env.action_space.n,\n",
    "            memory=memory,\n",
    "            processor=processor,\n",
    "            nb_steps_warmup=warmup_steps_param,\n",
    "            target_model_update=target_update_interval if enable_double_dqn else 10000,\n",
    "            enable_double_dqn=enable_double_dqn,\n",
    "            policy=policy,\n",
    "            gamma=0.99,\n",
    "            train_interval=train_interval_param,\n",
    "            delta_clip=1.0,\n",
    "            batch_size=batch_size\n",
    "        )                \n",
    "        # Después de crear el agente, reemplazar el target_model si estamos usando DDQN\n",
    "        if enable_double_dqn and target_model is not None:\n",
    "            dqn.target_model = target_model            \n",
    "\n",
    "        # Compilar el agente\n",
    "        optimizer = Adam(learning_rate=learning_rate)  \n",
    "        dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al crear o compilar el agente: {str(e)}\")\n",
    "        return None, False\n",
    "        \n",
    "    # Verificar que tenemos un agente válido antes de continuar\n",
    "    if dqn is None:\n",
    "        print(f\"❌ Error: No se pudo inicializar el agente DQN\")\n",
    "        return None, False        \n",
    "    try:\n",
    "        # DEBUG --------------------\n",
    "        # callbacks = [\n",
    "        #     log_filename = f'{checkpoint_path}/{name_model}_log.json'\n",
    "        #     progress_callback,\n",
    "        #     ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
    "        #     CustomFileLogger(log_filename, interval=1000),\n",
    "        #     PerformanceMonitor(save_path='diagnosticos')\n",
    "        # ]\n",
    "        # Callbacks optimizados para el objetivo\n",
    "        callbacks = [\n",
    "            SimpleProgressCallback(num_steps, print_interval=20000),         \n",
    "            TargetRewardTracker(dqn,target_avg_reward=target_reward, name_model=name_model, window_size=100, \n",
    "                                save_best=True, checkpoint_dir=checkpoint_path),\n",
    "            EpisodeCheckpointCallback(dqn, checkpoint_path=checkpoint_path, save_freq=1000, model_name=name_model)    \n",
    "        ]         \n",
    "          \n",
    "         # Ajustar el número de pasos restantes si estamos continuando el entrenamiento\n",
    "        adjusted_steps = max(0, num_steps - start_steps)\n",
    "        if start_steps > 0:\n",
    "            print(f\"Continuando desde el paso {start_steps} (quedan {adjusted_steps} pasos)\")\n",
    "\n",
    "        print(f\"Iniciando entrenamiento de {name_model} por {adjusted_steps} pasos...\")\n",
    "        start_time = time.time()       \n",
    "        # Fit del agente al entorno\n",
    "        history = dqn.fit(env, nb_steps=adjusted_steps, callbacks=callbacks, verbose=2)\n",
    "       \n",
    "        training_time = (time.time() - start_time) / 60\n",
    "        print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n",
    "        \n",
    "        # Guardar checkpoint final\n",
    "        # Verificar correctamente las claves disponibles y usar la adecuada\n",
    "        if hasattr(history, 'history'):\n",
    "            if 'episode' in history.history and history.history['episode']:\n",
    "                episode = start_episode + history.history['episode'][-1]\n",
    "            elif 'nb_episode' in history.history and history.history['nb_episode']:\n",
    "                episode = start_episode + history.history['nb_episode'][-1]\n",
    "            else:\n",
    "                # Intentar obtener el episodio del agente directamente\n",
    "                episode = getattr(dqn, 'episode', start_episode)\n",
    "        else:\n",
    "            episode = start_episode        \n",
    "        steps = start_steps + adjusted_steps\n",
    "        epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1          \n",
    "        try:\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"lastest\", \n",
    "                                epsilon=epsilon)\n",
    "                \n",
    "            # También guardar como \"best\" si no hay un checkpoint \"best\" previo\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                    checkpoint_dir=save_checkpoint_path, suffix=\"best\", \n",
    "                                    epsilon=epsilon)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al guardar el modelo: {str(e)}\")\n",
    "                                                        \n",
    "        return dqn, True    \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido por el usuario\") \n",
    "        # Guardar pesos de emergencia\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            episode = start_episode  # No podemos saber el episodio exacto después de la interrupción\n",
    "            steps = start_steps + dqn.step if hasattr(dqn, 'step') else start_steps\n",
    "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"emergency\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"✅ Modelo guardado en estado de emergencia\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error al guardar el modelo de emergencia: {str(e)}\")\n",
    "        return dqn, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ [ERROR] - Error durante el entrenamiento: {str(e)}\")\n",
    "        # Intentar guardar en estado de error\n",
    "        try:\n",
    "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
    "            save_model_checkpoint(dqn, name_model, episode=start_episode, \n",
    "                                steps=start_steps + (dqn.step if hasattr(dqn, 'step') else 0),\n",
    "                                checkpoint_dir=save_checkpoint_path, suffix=\"error_recovery\", \n",
    "                                epsilon=epsilon)\n",
    "            print(\"✅ Modelo guardado en estado de recuperación de error\")\n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ Error al guardar modelo de recuperación: {str(e2)}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVALUACION** ***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6GZzkrK72yj1"
   },
   "outputs": [],
   "source": [
    "# Función para evaluar el modelo\n",
    "def evaluar_modelo(agent, model_name, env, num_episodes=200, render=True, record_video=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo DQN o DDQN y si el modelo alcanza el objetivo de media\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        agent: Agente DQN entrenado\n",
    "        model_name: Nobre del agente\n",
    "        env: Entorno de gym\n",
    "        num_episodes: Número de episodios para evaluar\n",
    "        render: Si se debe mostrar la visualización\n",
    "        record_video: Si se debe grabar video\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        Lista de recompensas por episodio\n",
    "    \"\"\"\n",
    "    print(f\"🎯 EVALUANDO MODELO {model_name}\")\n",
    "    print(f\"📊 Evaluando por {num_episodes} episodios...\")\n",
    "    rewards = []\n",
    "    rewards_clip = []    \n",
    "    \n",
    "    # Setup for frame stacking\n",
    "    window_length = 4  # As specified in your model input shape\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            total_reward_clip = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the frame buffer with the initial observation\n",
    "            if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                processed_obs = agent.processor.process_observation(observation)\n",
    "            else:\n",
    "                processed_obs = observation\n",
    "                \n",
    "            # Create a frame stack buffer of the right shape\n",
    "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
    "            \n",
    "            # Fill buffer with the first observation\n",
    "            for i in range(window_length):\n",
    "                frame_buffer[i] = processed_obs\n",
    "            \n",
    "            while not done and step < 2000:\n",
    "                try:\n",
    "                    # Prepare input in the format expected by the model: (batch, channels=window_length, h, w)\n",
    "                    # Format directly to channels_first, correcting for the specific shape expected\n",
    "                    input_data = np.expand_dims(frame_buffer, axis=0)  # Add batch dimension\n",
    "                    \n",
    "                    # Get Q-values directly from the model\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        q_values = agent.model.predict(input_data)\n",
    "                        \n",
    "                        # Handle the output format for Dueling DQN\n",
    "                        if isinstance(q_values, list):\n",
    "                            q_values = q_values[0]  # Take first output for value function\n",
    "                      \n",
    "                        action = np.argmax(q_values)\n",
    "                    else:\n",
    "                        # Fallback to random action\n",
    "                        print(f\"⚠️ No se encuentra ningún modelo: {model_name}, utilizando una acción aleatoria\")\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    # Execute action\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    # Render if enabled\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    \n",
    "                    # Process new observation\n",
    "                    if hasattr(agent, 'processor') and agent.processor is not None:\n",
    "                        processed_obs = agent.processor.process_observation(observation)\n",
    "                    else:\n",
    "                        processed_obs = observation\n",
    "                    \n",
    "                    # Update frame buffer - shift frames\n",
    "                    for i in range(window_length-1):\n",
    "                        frame_buffer[i] = frame_buffer[i+1]\n",
    "                    frame_buffer[window_length-1] = processed_obs\n",
    "                    \n",
    "                    # Aplica clipping de la recompensa entre -1 y 1\n",
    "                    reward_clip = np.clip(reward, -1.0, 1.0)\n",
    "                    # Update counters\n",
    "                    total_reward += reward\n",
    "                    total_reward_clip += reward_clip\n",
    "                    step += 1\n",
    "                    \n",
    "                except Exception as step_error:\n",
    "                    print(f\"⚠️ Error en el paso {step}: {step_error}\")\n",
    "                    break\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "            rewards_clip.append(total_reward_clip)            \n",
    "            print(f\"   Episodio {episode + 1}/{num_episodes}: reward (clip) = {total_reward_clip:.1f}: reward (real) = {total_reward:.1f}\")\n",
    "        \n",
    "        # Análisis final\n",
    "        if len(rewards_clip) > 0:\n",
    "            avg_reward = np.mean(rewards_clip)\n",
    "            std_reward = np.std(rewards_clip)\n",
    "            max_reward = np.max(rewards_clip)\n",
    "            min_reward = np.min(rewards_clip)\n",
    "            \n",
    "            objetivo_alcanzado = avg_reward >= TARGET_REWARD\n",
    "            \n",
    "            print(f\"\\n📊 RESULTADOS DE EVALUACIÓN:\")\n",
    "            print(f\"   Media: {avg_reward:.2f} {'✅' if objetivo_alcanzado else '❌'}\")\n",
    "            print(f\"   Desviación: ±{std_reward:.2f}\")\n",
    "            print(f\"   Máximo: {max_reward:.2f}\")\n",
    "            print(f\"   Mínimo: {min_reward:.2f}\")\n",
    "            print(f\"   Episodios sobre {TARGET_REWARD}: {sum(1 for r in rewards_clip if r >= TARGET_REWARD)} / {len(rewards_clip)}\")\n",
    "            \n",
    "            if objetivo_alcanzado:\n",
    "                print(f\"🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de {avg_reward:.2f}\")\n",
    "            else:\n",
    "                print(f\"📈 Progreso: {(avg_reward/TARGET_REWARD)*100:.1f}% del objetivo\")\n",
    "        else:\n",
    "            print(\"❌ No se completaron episodios correctamente\")\n",
    "            avg_reward = float('nan')\n",
    "            objetivo_alcanzado = False\n",
    "        \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()            \n",
    "        return rewards_clip, objetivo_alcanzado        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ [ERROR] - Error durante evaluación: {e}\")    \n",
    "        # Limpiar entorno si es necesario\n",
    "        if hasattr(env, 'close'):\n",
    "            env.close()           \n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabar_video_del_modelo(model, model_name, env, num_episodes=1, video_dir=\"checkpoints/videos\", fps=30, max_steps=10000):\n",
    "    \"\"\"\n",
    "    Graba un video del comportamiento del modelo en el entorno.\n",
    "    Los frames se capturan manualmente y se guardan directamente en checkpoints/videos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    model: El agente entrenado (DQN, DDQN, etc.) con un método .predict() y un .processor.\n",
    "    model_name: Nombre del modelo para identificar el archivo de video.\n",
    "    env: El entorno de Gym. Debe ser capaz de renderizar en 'rgb_array'.\n",
    "         Asegúrate de que este entorno NO tiene wrappers de FrameStack si tu agente\n",
    "         ya maneja el apilamiento de frames con su procesador.\n",
    "    num_episodes: Número de episodios a grabar.\n",
    "    fps: Frames por segundo para el video.\n",
    "    max_steps: Número máximo de pasos a ejecutar por episodio.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    str: Ruta al archivo de video grabado, o None si hubo un error.\n",
    "    \"\"\"\n",
    "    # Crear directorio para videos\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Nombre del archivo MP4 directamente en el directorio de videos\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    video_path = os.path.join(video_dir, f\"{model_name}_{timestamp}.mp4\")\n",
    "    \n",
    "    frames = [] # Lista para almacenar los frames RGB capturados\n",
    "    \n",
    "    # Obtener window_length del agente (para el frame stacking de la lógica del agente)\n",
    "    window_length = 4 # Valor por defecto\n",
    "    if hasattr(model, 'processor') and hasattr(model.processor, 'window_length'):\n",
    "        # Ajuste si el processor está anidado, como en keras-rl\n",
    "        if hasattr(model.processor, 'processor') and hasattr(model.processor.processor, 'window_length'):\n",
    "            window_length = model.processor.processor.window_length\n",
    "        else:\n",
    "            window_length = model.processor.window_length\n",
    "\n",
    "    try:\n",
    "        print(f\"📹 Grabando video del modelo {model_name} en: {video_path}...\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Inicializar episodio\n",
    "            observation_raw = env.reset() \n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            # Preprocesar la observación inicial para el agente y llenar el buffer\n",
    "            processed_obs = model.processor.process_observation(observation_raw)\n",
    "            \n",
    "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
    "            for k in range(window_length):\n",
    "                frame_buffer[k] = processed_obs\n",
    "            \n",
    "            while not done and steps < max_steps:\n",
    "                # Capturar frame RGB para el video\n",
    "                # env.render() DEBE devolver un array RGB (altura, anchura, 3)\n",
    "                current_frame_rgb = env.render(mode='rgb_array')\n",
    "                \n",
    "                if current_frame_rgb is not None:\n",
    "                    frames.append(current_frame_rgb)\n",
    "                else:\n",
    "                    print(\"Advertencia: env.render() devolvió None. No se pudieron capturar frames para el video.\")\n",
    "                    # Si no se capturan frames, el video no se generará.\n",
    "                    # Puedes optar por romper el bucle o continuar, pero el video estará vacío.\n",
    "                    \n",
    "                # Preparar la entrada para el modelo del agente\n",
    "                input_data_for_model = np.expand_dims(frame_buffer, axis=0)\n",
    "                \n",
    "                # Obtener acción del modelo\n",
    "                action = None\n",
    "                if hasattr(model, 'model'): # Asumo que tu agente tiene un atributo 'model' que es un modelo Keras\n",
    "                    try:\n",
    "                        q_values = model.model.predict(input_data_for_model, verbose=0)\n",
    "                        if isinstance(q_values, list):\n",
    "                            q_values = q_values[0]\n",
    "                        action = np.argmax(q_values)\n",
    "                    except Exception as predict_error:\n",
    "                        print(f\"⚠️ Error al predecir la acción: {predict_error}. Usando acción aleatoria.\")\n",
    "                        action = env.action_space.sample()\n",
    "                else: # Fallback a acción aleatoria\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                # Ejecutar acción en el entorno\n",
    "                observation_next_raw, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Preprocesar la nueva observación para actualizar el buffer de frames\n",
    "                processed_next_obs = model.processor.process_observation(observation_next_raw)              \n",
    "                # Actualizar el buffer de frames\n",
    "                frame_buffer[:window_length-1] = frame_buffer[1:]\n",
    "                frame_buffer[window_length-1] = processed_next_obs\n",
    "\n",
    "            print(f\"  Episodio {episode+1}: Recompensa Total = {total_reward} (Pasos: {steps})\")\n",
    "        \n",
    "        # Guardar video con imageio\n",
    "        if frames:\n",
    "            print(f\"💾 Guardando video con {len(frames)} frames en {video_path}...\")\n",
    "            # Usa fps aquí para controlar la velocidad del video\n",
    "            imageio.mimsave(video_path, frames, fps=fps) \n",
    "            print(f\"✅ Video guardado en: {video_path}\")\n",
    "        else:\n",
    "            print(\"❌ No se generaron frames para el video. El archivo MP4 no se creará.\")\n",
    "            video_path = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al grabar el video: {e}\")\n",
    "        video_path = None\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar entorno SIEMPRE al final\n",
    "        env.close() \n",
    "            \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¡¡¡¡¡¡¡ **EJECUCION - MAIN** !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True\n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "    episode_render = False\n",
    "    # Asegurar que existe el directorio\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    # Optimizar configuración de TensorFlow\n",
    "    optimizar_tensorflow()\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    gc.collect()    \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    # Ejecutar prueba\n",
    "    print(\"🚀 EJECUTANDO SOLUCIÓN...\")\n",
    "    print(f\"🎯 OBJETIVO: Conseguir media de episode_reward = {TARGET_REWARD} (con clipping)\")    \n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', create_dqn_model, False),\n",
    "        ('DDQN', create_ddqn_models, False),\n",
    "        ('DDQN_REPLAY', create_ddqn_replay_model, False),\n",
    "        ('DUELING_DQN_REPLAY', create_dueling_dqn_replay_model, True)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for model_name, model_process, training_specific_flag in modelos_a_procesar:\n",
    "            # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "            # Verificar el tipo de modelo y establecer enable_double_dqn correctamente\n",
    "            enable_double_dqn = model_name in ['DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']            \n",
    "            entrenarSN = training_global and training_specific_flag\n",
    "            model_instance = False    \n",
    "            agent = None\n",
    "            save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"            \n",
    "            if entrenarSN:\n",
    "                # Crear una nueva sesión para cada modelo\n",
    "                tf.keras.backend.clear_session()            \n",
    "                # Intentar cargar un modelo previamente guardado (independientemente de si entrenaremos o no)\n",
    "                try:        \n",
    "                    # Crear una instancia del modelo - crear primero la arquitectura antes de poder cargar los pesos en ella\n",
    "                    if enable_double_dqn:\n",
    "                        model, memory, target_model = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                    else:\n",
    "                        model, memory, _ = model_process(input_shape, env.action_space.n, memory_size)\n",
    "                        target_model = None    \n",
    "\n",
    "                    # Intentar cargar el mejor checkpoint (o lastest si best no existe)\n",
    "                    start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                 checkpoint_dir=save_checkpoint_path, suffix=\"best\")\n",
    "                    if start_episode == 0:\n",
    "                        # Si no encontró el mejor, intentar con el último guardado\n",
    "                        start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name, \n",
    "                                                                                     checkpoint_dir=save_checkpoint_path, suffix=\"lastest\")\n",
    "                    if start_episode > 0:\n",
    "                        model_instance = True\n",
    "                        print(f\"✅ Modelo {model_name} cargado exitosamente desde el episodio {start_episode}\")\n",
    "                        # Si se debe entrenar, continuar desde donde quedó\n",
    "                        if entrenarSN:\n",
    "                            print(f\"⏩ Continuando entrenamiento de {model_name} desde episodio {start_episode+1}\")\n",
    "                            # Establecer parámetros para continuar el entrenamiento\n",
    "                            epsilon_actual = epsilon\n",
    "\n",
    "                            # Entrenar el modelo desde donde quedó\n",
    "                            agent, success = entrenar_modelo(\n",
    "                                env=env,\n",
    "                                model_name=model_name,\n",
    "                                model=model, \n",
    "                                memory=memory, \n",
    "                                target_model=target_model,\n",
    "                                model_instance=model_instance,\n",
    "                                checkpoint_path=checkpoint_path,\n",
    "                                start_episode=start_episode+1,\n",
    "                                start_steps=global_steps,\n",
    "                                epsilon_start=epsilon_actual,  # Usar el epsilon guardado\n",
    "                                epsilon_min=epsilon_stop,\n",
    "                                epsilon_steps=EPSILON_STEPS,\n",
    "                                num_steps=NUM_TRAINING_STEPS,\n",
    "                                warmup_steps=WARMUP_STEPS,\n",
    "                                target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                                target_update_tau=tau,\n",
    "                                enable_double_dqn = enable_double_dqn\n",
    "                            )                                                           \n",
    "                    else:\n",
    "                        print(f\"🆕 Creando y entrenando un nuevo modelo {model_name}\")\n",
    "                        model_instance = False   \n",
    "                        # Entrenar el modelo desde cero\n",
    "                        agent, success = entrenar_modelo(\n",
    "                            env=env,\n",
    "                            model_name=model_name,\n",
    "                            model=model, \n",
    "                            memory=memory, \n",
    "                            target_model=target_model,                        \n",
    "                            model_instance=model_instance,\n",
    "                            checkpoint_path=checkpoint_path,\n",
    "                            start_episode=0,  # Añadido: Especificar episodio inicial\n",
    "                            start_steps=0,    # Añadido: Especificar pasos iniciales\n",
    "                            epsilon_start=epsilon_start,\n",
    "                            epsilon_min=epsilon_stop,\n",
    "                            epsilon_steps=EPSILON_STEPS,\n",
    "                            num_steps=NUM_TRAINING_STEPS,\n",
    "                            warmup_steps=WARMUP_STEPS,\n",
    "                            target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "                            target_update_tau=tau,\n",
    "                            enable_double_dqn = enable_double_dqn\n",
    "                        )                \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ [ERROR] - Error al cargar o entrenar modelo {model_name}: {e}\")\n",
    "\n",
    "            # Si tenemos un modelo válido (cargado o entrenado), evaluarlo y guardarlo en el diccionario\n",
    "            # Evaluación rápida con 10 episodios para:\n",
    "            # - Comprobar el rendimiento básico del modelo\n",
    "            # - Decidir si vale la pena guardarlo como \"best\"\n",
    "            # - Mostrar un feedback rápido sobre su desempeño\n",
    "            if agent:\n",
    "                rewards, _ = evaluar_modelo(agent, model_name, env, num_episodes=10, render=False, record_video=False)\n",
    "                avg_reward = np.mean(rewards)\n",
    "                trained_models[model_name] = agent                       \n",
    "                print(f\"📊 Recompensa promedio para {model_name}: {avg_reward:.2f}\")\n",
    "                # Guardar el modelo como \"best\" si supera umbral de evaluación\n",
    "                if avg_reward >= TARGET_REWARD * 0.8:  # 80% del objetivo como umbral mínimo\n",
    "                    save_model_checkpoint(agent, model_name, episode=start_episode, steps=global_steps,  # Corregido: Usar el episodio y pasos actuales\n",
    "                                         checkpoint_dir=save_checkpoint_path, suffix=\"best_reward\", \n",
    "                                         epsilon=epsilon_start)\n",
    "                    print(f\"🏆 Modelo {model_name} guardado como 'best_reward' con recompensa {avg_reward:.2f}\")        \n",
    "\n",
    "        # ------------------------------------------------------    \n",
    "        # Búsqueda del mejor modelo entre todos los entrenados/cargados: comparar su rendimiento\n",
    "        # con el mismo número de episodios (10) para mantener una comparación justa\n",
    "        # Ayuda a determinar cuál es el mejor modelo para la evaluación final\n",
    "        best_model_name = None\n",
    "        best_reward = -float('inf')    \n",
    "        for name, model in trained_models.items():\n",
    "            rewards, objetivo_conseguido = evaluar_modelo(model, model_name, env, num_episodes=10, render=False, record_video=False)\n",
    "            avg_reward = np.mean(rewards)\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                best_model_name = name        \n",
    "\n",
    "        if best_model_name:\n",
    "            print(\"\\n✅ SOLUCIÓN EXITOSA - Entrenamiento completado\")        \n",
    "            print(f\"\\n🥇 El mejor modelo es {best_model_name} con recompensa promedio {best_reward:.2f}\")\n",
    "            best_model = trained_models[best_model_name]\n",
    "\n",
    "            # Evaluación final y más exhaustiva del mejor modelo: Se hace con muchos más episodios (200) \n",
    "            #   para obtener resultados estadísticamente más significativos. Es la evaluación definitiva \n",
    "            #   para determinar si se cumple el objetivo --> conclusión final del proceso        \n",
    "            #   print(f\"\\n🎯 EVALUACIÓN FINAL DEL OBJETIVO\")\n",
    "            #   rewards_eval, objetivo_conseguido = evaluar_modelo(best_model, env, num_episodes=200)                \n",
    "\n",
    "            # Grabar video del mejor modelo\n",
    "            video_path = grabar_video_del_modelo(best_model, best_model_name, env)\n",
    "            if video_path:\n",
    "                print(f\"🎬 Se ha grabado una demostración del modelo en: {video_path}\")        \n",
    "\n",
    "            if objetivo_conseguido:\n",
    "                print(f\"🏆 ¡FELICIDADES EQUIPO! El modelo alcanzó el objetivo de media {TARGET_REWARD}\")\n",
    "            else:\n",
    "                print(f\"📈 El modelo necesita más entrenamiento para alcanzar media {TARGET_REWARD}\")\n",
    "        else:\n",
    "            print(\"❌ [ERROR] - No se pudo entrenar ningún modelo correctamente\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️ Entrenamiento interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n❌ Error inesperado: {e}\")\n",
    "    finally:\n",
    "        # Asegurar que siempre se cierra el entorno\n",
    "        env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] - TensorFlow optimizado para 20 cores CPU\n",
      "🚀 EJECUTANDO SOLUCIÓN...\n",
      "🎯 OBJETIVO: Conseguir media de episode_reward = 20.0 (con clipping)\n",
      "🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape=(84, 84, 4), actions=6\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "✅ Modelo creado exitosamente\n",
      "📊 Resumen del modelo:\n",
      "Model: \"DuelingDQNReplay_Main_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_channels_first (InputLay  [(None, 4, 84, 84)]  0          []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 84, 84, 4)    0           ['input_channels_first[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 20, 20, 32)   8224        ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 9, 9, 64)     32832       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 7, 64)     36928       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          1606144     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          1606144     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            3078        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 6)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6)            0           ['dense_1[0][0]',                \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,293,863\n",
      "Trainable params: 3,293,863\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "📂 Se cargó: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_state.json\n",
      "📂 Cargando checkpoint best (episodio: 2050, pasos: 1174811, epsilon: 0.1)\n",
      "📂 Modelo principal cargado: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "📂 Modelo target cargado: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "📂 Memoria cargada correctamente: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "✅ Modelo DUELING_DQN_REPLAY cargado exitosamente desde el episodio 2050\n",
      "⏩ Continuando entrenamiento de DUELING_DQN_REPLAY desde episodio 2051\n",
      "🤖 Continuando entrenamiento para DUELING_DQN_REPLAY...\n",
      "🎯 OBJETIVO: Media de episode_reward = 20.0\n",
      "📊 Ventana de evaluación: 100 episodios\n",
      "Continuando desde el paso 1174811 (quedan 825189 pasos)\n",
      "Iniciando entrenamiento de DUELING_DQN_REPLAY por 825189 pasos...\n",
      "🚀 Entrenamiento iniciado: 2,000,000 pasos\n",
      "Training for 825189 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1: Recompensa total (clipped): 21.000, Pasos: 665, Mean Reward Calculado: 0.031579 (Recompensa/Pasos)\n",
      "    665/825189: episode: 1, duration: 25.863s, episode steps: 665, steps per second:  26, episode reward: 21.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.036588, mae: 3.374380, mean_q: 4.070583, mean_eps: 0.100000\n",
      "📈 Episodio 2: Recompensa total (clipped): 32.000, Pasos: 832, Mean Reward Calculado: 0.038462 (Recompensa/Pasos)\n",
      "   1497/825189: episode: 2, duration: 31.774s, episode steps: 832, steps per second:  26, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.036046, mae: 3.358141, mean_q: 4.053008, mean_eps: 0.100000\n",
      "📈 Episodio 3: Recompensa total (clipped): 24.000, Pasos: 545, Mean Reward Calculado: 0.044037 (Recompensa/Pasos)\n",
      "   2042/825189: episode: 3, duration: 21.122s, episode steps: 545, steps per second:  26, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.141 [0.000, 5.000],  loss: 0.042410, mae: 3.375717, mean_q: 4.073449, mean_eps: 0.100000\n",
      "📈 Episodio 4: Recompensa total (clipped): 12.000, Pasos: 400, Mean Reward Calculado: 0.030000 (Recompensa/Pasos)\n",
      "   2442/825189: episode: 4, duration: 15.283s, episode steps: 400, steps per second:  26, episode reward: 12.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.036125, mae: 3.362041, mean_q: 4.058698, mean_eps: 0.100000\n",
      "📈 Episodio 5: Recompensa total (clipped): 18.000, Pasos: 407, Mean Reward Calculado: 0.044226 (Recompensa/Pasos)\n",
      "   2849/825189: episode: 5, duration: 15.670s, episode steps: 407, steps per second:  26, episode reward: 18.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.342 [0.000, 5.000],  loss: 0.036511, mae: 3.358671, mean_q: 4.052806, mean_eps: 0.100000\n",
      "📈 Episodio 6: Recompensa total (clipped): 30.000, Pasos: 673, Mean Reward Calculado: 0.044577 (Recompensa/Pasos)\n",
      "   3522/825189: episode: 6, duration: 25.927s, episode steps: 673, steps per second:  26, episode reward: 30.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.042370, mae: 3.377890, mean_q: 4.074844, mean_eps: 0.100000\n",
      "📈 Episodio 7: Recompensa total (clipped): 56.000, Pasos: 1254, Mean Reward Calculado: 0.044657 (Recompensa/Pasos)\n",
      "   4776/825189: episode: 7, duration: 48.219s, episode steps: 1254, steps per second:  26, episode reward: 56.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.033343, mae: 3.352243, mean_q: 4.046118, mean_eps: 0.100000\n",
      "📈 Episodio 8: Recompensa total (clipped): 24.000, Pasos: 514, Mean Reward Calculado: 0.046693 (Recompensa/Pasos)\n",
      "   5290/825189: episode: 8, duration: 19.781s, episode steps: 514, steps per second:  26, episode reward: 24.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.049173, mae: 3.359404, mean_q: 4.050908, mean_eps: 0.100000\n",
      "📈 Episodio 9: Recompensa total (clipped): 29.000, Pasos: 627, Mean Reward Calculado: 0.046252 (Recompensa/Pasos)\n",
      "   5917/825189: episode: 9, duration: 24.130s, episode steps: 627, steps per second:  26, episode reward: 29.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.923 [0.000, 5.000],  loss: 0.033931, mae: 3.384053, mean_q: 4.083869, mean_eps: 0.100000\n",
      "📈 Episodio 10: Recompensa total (clipped): 35.000, Pasos: 1007, Mean Reward Calculado: 0.034757 (Recompensa/Pasos)\n",
      "   6924/825189: episode: 10, duration: 38.210s, episode steps: 1007, steps per second:  26, episode reward: 35.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.036334, mae: 3.352424, mean_q: 4.045361, mean_eps: 0.100000\n",
      "📈 Episodio 11: Recompensa total (clipped): 28.000, Pasos: 724, Mean Reward Calculado: 0.038674 (Recompensa/Pasos)\n",
      "   7648/825189: episode: 11, duration: 27.719s, episode steps: 724, steps per second:  26, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.038037, mae: 3.374658, mean_q: 4.069871, mean_eps: 0.100000\n",
      "📈 Episodio 12: Recompensa total (clipped): 17.000, Pasos: 524, Mean Reward Calculado: 0.032443 (Recompensa/Pasos)\n",
      "   8172/825189: episode: 12, duration: 20.163s, episode steps: 524, steps per second:  26, episode reward: 17.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.043449, mae: 3.349447, mean_q: 4.039660, mean_eps: 0.100000\n",
      "📈 Episodio 13: Recompensa total (clipped): 28.000, Pasos: 745, Mean Reward Calculado: 0.037584 (Recompensa/Pasos)\n",
      "   8917/825189: episode: 13, duration: 28.419s, episode steps: 745, steps per second:  26, episode reward: 28.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.041573, mae: 3.367615, mean_q: 4.065897, mean_eps: 0.100000\n",
      "📈 Episodio 14: Recompensa total (clipped): 32.000, Pasos: 874, Mean Reward Calculado: 0.036613 (Recompensa/Pasos)\n",
      "   9791/825189: episode: 14, duration: 33.258s, episode steps: 874, steps per second:  26, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.040888, mae: 3.337609, mean_q: 4.026751, mean_eps: 0.100000\n",
      "📈 Episodio 15: Recompensa total (clipped): 29.000, Pasos: 710, Mean Reward Calculado: 0.040845 (Recompensa/Pasos)\n",
      "  10501/825189: episode: 15, duration: 27.311s, episode steps: 710, steps per second:  26, episode reward: 29.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.038501, mae: 3.389262, mean_q: 4.090951, mean_eps: 0.100000\n",
      "📈 Episodio 16: Recompensa total (clipped): 15.000, Pasos: 380, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      "  10881/825189: episode: 16, duration: 14.599s, episode steps: 380, steps per second:  26, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.037449, mae: 3.400670, mean_q: 4.104284, mean_eps: 0.100000\n",
      "📈 Episodio 17: Recompensa total (clipped): 17.000, Pasos: 438, Mean Reward Calculado: 0.038813 (Recompensa/Pasos)\n",
      "  11319/825189: episode: 17, duration: 16.880s, episode steps: 438, steps per second:  26, episode reward: 17.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.046495, mae: 3.353252, mean_q: 4.041834, mean_eps: 0.100000\n",
      "📈 Episodio 18: Recompensa total (clipped): 15.000, Pasos: 484, Mean Reward Calculado: 0.030992 (Recompensa/Pasos)\n",
      "  11803/825189: episode: 18, duration: 18.634s, episode steps: 484, steps per second:  26, episode reward: 15.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.043315, mae: 3.317341, mean_q: 3.999076, mean_eps: 0.100000\n",
      "📈 Episodio 19: Recompensa total (clipped): 26.000, Pasos: 687, Mean Reward Calculado: 0.037846 (Recompensa/Pasos)\n",
      "  12490/825189: episode: 19, duration: 26.381s, episode steps: 687, steps per second:  26, episode reward: 26.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.039162, mae: 3.344497, mean_q: 4.032710, mean_eps: 0.100000\n",
      "📈 Episodio 20: Recompensa total (clipped): 32.000, Pasos: 753, Mean Reward Calculado: 0.042497 (Recompensa/Pasos)\n",
      "  13243/825189: episode: 20, duration: 28.603s, episode steps: 753, steps per second:  26, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.951 [0.000, 5.000],  loss: 0.036631, mae: 3.367250, mean_q: 4.060059, mean_eps: 0.100000\n",
      "📈 Episodio 21: Recompensa total (clipped): 28.000, Pasos: 817, Mean Reward Calculado: 0.034272 (Recompensa/Pasos)\n",
      "  14060/825189: episode: 21, duration: 31.331s, episode steps: 817, steps per second:  26, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.038971, mae: 3.401535, mean_q: 4.106259, mean_eps: 0.100000\n",
      "📈 Episodio 22: Recompensa total (clipped): 23.000, Pasos: 631, Mean Reward Calculado: 0.036450 (Recompensa/Pasos)\n",
      "  14691/825189: episode: 22, duration: 24.098s, episode steps: 631, steps per second:  26, episode reward: 23.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.038124, mae: 3.402942, mean_q: 4.106096, mean_eps: 0.100000\n",
      "📈 Episodio 23: Recompensa total (clipped): 30.000, Pasos: 751, Mean Reward Calculado: 0.039947 (Recompensa/Pasos)\n",
      "  15442/825189: episode: 23, duration: 28.886s, episode steps: 751, steps per second:  26, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.035320, mae: 3.332983, mean_q: 4.023953, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 24: Recompensa total (clipped): 28.000, Pasos: 614, Mean Reward Calculado: 0.045603 (Recompensa/Pasos)\n",
      "  16056/825189: episode: 24, duration: 23.507s, episode steps: 614, steps per second:  26, episode reward: 28.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.036970, mae: 3.338754, mean_q: 4.030988, mean_eps: 0.100000\n",
      "📈 Episodio 25: Recompensa total (clipped): 20.000, Pasos: 545, Mean Reward Calculado: 0.036697 (Recompensa/Pasos)\n",
      "  16601/825189: episode: 25, duration: 21.166s, episode steps: 545, steps per second:  26, episode reward: 20.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.050592, mae: 3.379930, mean_q: 4.078167, mean_eps: 0.100000\n",
      "📈 Episodio 26: Recompensa total (clipped): 11.000, Pasos: 345, Mean Reward Calculado: 0.031884 (Recompensa/Pasos)\n",
      "  16946/825189: episode: 26, duration: 13.241s, episode steps: 345, steps per second:  26, episode reward: 11.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.038465, mae: 3.425043, mean_q: 4.138082, mean_eps: 0.100000\n",
      "📈 Episodio 27: Recompensa total (clipped): 32.000, Pasos: 956, Mean Reward Calculado: 0.033473 (Recompensa/Pasos)\n",
      "  17902/825189: episode: 27, duration: 36.860s, episode steps: 956, steps per second:  26, episode reward: 32.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.035094, mae: 3.340803, mean_q: 4.032479, mean_eps: 0.100000\n",
      "📈 Episodio 28: Recompensa total (clipped): 14.000, Pasos: 365, Mean Reward Calculado: 0.038356 (Recompensa/Pasos)\n",
      "  18267/825189: episode: 28, duration: 13.906s, episode steps: 365, steps per second:  26, episode reward: 14.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.041316, mae: 3.348629, mean_q: 4.039858, mean_eps: 0.100000\n",
      "📈 Episodio 29: Recompensa total (clipped): 18.000, Pasos: 436, Mean Reward Calculado: 0.041284 (Recompensa/Pasos)\n",
      "  18703/825189: episode: 29, duration: 16.765s, episode steps: 436, steps per second:  26, episode reward: 18.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.035609, mae: 3.367721, mean_q: 4.066951, mean_eps: 0.100000\n",
      "📈 Episodio 30: Recompensa total (clipped): 24.000, Pasos: 639, Mean Reward Calculado: 0.037559 (Recompensa/Pasos)\n",
      "  19342/825189: episode: 30, duration: 24.631s, episode steps: 639, steps per second:  26, episode reward: 24.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.045112, mae: 3.381005, mean_q: 4.079458, mean_eps: 0.100000\n",
      "📊 Paso 20,000/2,000,000 (1.0%) - 26.1 pasos/seg - ETA: 21.1h - Memoria: 8013.07 MB\n",
      "📈 Episodio 31: Recompensa total (clipped): 31.000, Pasos: 846, Mean Reward Calculado: 0.036643 (Recompensa/Pasos)\n",
      "  20188/825189: episode: 31, duration: 32.434s, episode steps: 846, steps per second:  26, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.040411, mae: 3.400010, mean_q: 4.102810, mean_eps: 0.100000\n",
      "📈 Episodio 32: Recompensa total (clipped): 28.000, Pasos: 674, Mean Reward Calculado: 0.041543 (Recompensa/Pasos)\n",
      "  20862/825189: episode: 32, duration: 25.799s, episode steps: 674, steps per second:  26, episode reward: 28.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.029782, mae: 3.374935, mean_q: 4.076508, mean_eps: 0.100000\n",
      "📈 Episodio 33: Recompensa total (clipped): 22.000, Pasos: 453, Mean Reward Calculado: 0.048565 (Recompensa/Pasos)\n",
      "  21315/825189: episode: 33, duration: 17.467s, episode steps: 453, steps per second:  26, episode reward: 22.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.036963, mae: 3.385949, mean_q: 4.084566, mean_eps: 0.100000\n",
      "📈 Episodio 34: Recompensa total (clipped): 22.000, Pasos: 624, Mean Reward Calculado: 0.035256 (Recompensa/Pasos)\n",
      "  21939/825189: episode: 34, duration: 24.088s, episode steps: 624, steps per second:  26, episode reward: 22.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.034779, mae: 3.402400, mean_q: 4.109016, mean_eps: 0.100000\n",
      "📈 Episodio 35: Recompensa total (clipped): 25.000, Pasos: 771, Mean Reward Calculado: 0.032425 (Recompensa/Pasos)\n",
      "  22710/825189: episode: 35, duration: 29.506s, episode steps: 771, steps per second:  26, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.041279, mae: 3.418971, mean_q: 4.125860, mean_eps: 0.100000\n",
      "📈 Episodio 36: Recompensa total (clipped): 22.000, Pasos: 586, Mean Reward Calculado: 0.037543 (Recompensa/Pasos)\n",
      "  23296/825189: episode: 36, duration: 22.617s, episode steps: 586, steps per second:  26, episode reward: 22.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.157 [0.000, 5.000],  loss: 0.036613, mae: 3.407400, mean_q: 4.112749, mean_eps: 0.100000\n",
      "📈 Episodio 37: Recompensa total (clipped): 37.000, Pasos: 1150, Mean Reward Calculado: 0.032174 (Recompensa/Pasos)\n",
      "  24446/825189: episode: 37, duration: 44.951s, episode steps: 1150, steps per second:  26, episode reward: 37.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.040500, mae: 3.411742, mean_q: 4.118279, mean_eps: 0.100000\n",
      "📈 Episodio 38: Recompensa total (clipped): 29.000, Pasos: 771, Mean Reward Calculado: 0.037613 (Recompensa/Pasos)\n",
      "  25217/825189: episode: 38, duration: 29.545s, episode steps: 771, steps per second:  26, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.043154, mae: 3.377425, mean_q: 4.072783, mean_eps: 0.100000\n",
      "📈 Episodio 39: Recompensa total (clipped): 21.000, Pasos: 703, Mean Reward Calculado: 0.029872 (Recompensa/Pasos)\n",
      "  25920/825189: episode: 39, duration: 27.040s, episode steps: 703, steps per second:  26, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.121 [0.000, 5.000],  loss: 0.035451, mae: 3.363024, mean_q: 4.060007, mean_eps: 0.100000\n",
      "📈 Episodio 40: Recompensa total (clipped): 31.000, Pasos: 885, Mean Reward Calculado: 0.035028 (Recompensa/Pasos)\n",
      "  26805/825189: episode: 40, duration: 34.062s, episode steps: 885, steps per second:  26, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.049 [0.000, 5.000],  loss: 0.038278, mae: 3.397237, mean_q: 4.101608, mean_eps: 0.100000\n",
      "📈 Episodio 41: Recompensa total (clipped): 12.000, Pasos: 379, Mean Reward Calculado: 0.031662 (Recompensa/Pasos)\n",
      "  27184/825189: episode: 41, duration: 14.459s, episode steps: 379, steps per second:  26, episode reward: 12.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.035742, mae: 3.381053, mean_q: 4.085064, mean_eps: 0.100000\n",
      "📈 Episodio 42: Recompensa total (clipped): 29.000, Pasos: 748, Mean Reward Calculado: 0.038770 (Recompensa/Pasos)\n",
      "  27932/825189: episode: 42, duration: 28.922s, episode steps: 748, steps per second:  26, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.037399, mae: 3.348588, mean_q: 4.044619, mean_eps: 0.100000\n",
      "📈 Episodio 43: Recompensa total (clipped): 27.000, Pasos: 637, Mean Reward Calculado: 0.042386 (Recompensa/Pasos)\n",
      "  28569/825189: episode: 43, duration: 24.272s, episode steps: 637, steps per second:  26, episode reward: 27.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.370 [0.000, 5.000],  loss: 0.042354, mae: 3.374181, mean_q: 4.073345, mean_eps: 0.100000\n",
      "📈 Episodio 44: Recompensa total (clipped): 14.000, Pasos: 386, Mean Reward Calculado: 0.036269 (Recompensa/Pasos)\n",
      "  28955/825189: episode: 44, duration: 15.025s, episode steps: 386, steps per second:  26, episode reward: 14.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.040246, mae: 3.396432, mean_q: 4.101915, mean_eps: 0.100000\n",
      "📈 Episodio 45: Recompensa total (clipped): 24.000, Pasos: 554, Mean Reward Calculado: 0.043321 (Recompensa/Pasos)\n",
      "  29509/825189: episode: 45, duration: 21.515s, episode steps: 554, steps per second:  26, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.045299, mae: 3.413807, mean_q: 4.117304, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 46: Recompensa total (clipped): 18.000, Pasos: 384, Mean Reward Calculado: 0.046875 (Recompensa/Pasos)\n",
      "  29893/825189: episode: 46, duration: 14.952s, episode steps: 384, steps per second:  26, episode reward: 18.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 3.130 [0.000, 5.000],  loss: 0.039234, mae: 3.366718, mean_q: 4.061421, mean_eps: 0.100000\n",
      "📈 Episodio 47: Recompensa total (clipped): 26.000, Pasos: 589, Mean Reward Calculado: 0.044143 (Recompensa/Pasos)\n",
      "  30482/825189: episode: 47, duration: 22.415s, episode steps: 589, steps per second:  26, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.044210, mae: 3.359262, mean_q: 4.054599, mean_eps: 0.100000\n",
      "📈 Episodio 48: Recompensa total (clipped): 28.000, Pasos: 684, Mean Reward Calculado: 0.040936 (Recompensa/Pasos)\n",
      "  31166/825189: episode: 48, duration: 26.090s, episode steps: 684, steps per second:  26, episode reward: 28.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.043120, mae: 3.367822, mean_q: 4.065210, mean_eps: 0.100000\n",
      "📈 Episodio 49: Recompensa total (clipped): 33.000, Pasos: 764, Mean Reward Calculado: 0.043194 (Recompensa/Pasos)\n",
      "  31930/825189: episode: 49, duration: 29.342s, episode steps: 764, steps per second:  26, episode reward: 33.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.038446, mae: 3.409060, mean_q: 4.115669, mean_eps: 0.100000\n",
      "📈 Episodio 50: Recompensa total (clipped): 19.000, Pasos: 426, Mean Reward Calculado: 0.044601 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 50, pasos: 32356)\n",
      "💾 NUEVO MEJOR PROMEDIO: 24.92 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 50 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 19.00\n",
      "   Media últimos 100: 24.92 / 20.0\n",
      "   Mejor promedio histórico: 24.92\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 5\n",
      "   Episodios consecutivos en objetivo: 5\n",
      "  32356/825189: episode: 50, duration: 34.988s, episode steps: 426, steps per second:  12, episode reward: 19.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.041077, mae: 3.378413, mean_q: 4.076731, mean_eps: 0.100000\n",
      "📈 Episodio 51: Recompensa total (clipped): 33.000, Pasos: 973, Mean Reward Calculado: 0.033916 (Recompensa/Pasos)\n",
      "  33329/825189: episode: 51, duration: 38.522s, episode steps: 973, steps per second:  25, episode reward: 33.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.039877, mae: 3.424366, mean_q: 4.133406, mean_eps: 0.100000\n",
      "📈 Episodio 52: Recompensa total (clipped): 24.000, Pasos: 636, Mean Reward Calculado: 0.037736 (Recompensa/Pasos)\n",
      "  33965/825189: episode: 52, duration: 24.872s, episode steps: 636, steps per second:  26, episode reward: 24.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.044023, mae: 3.372074, mean_q: 4.068086, mean_eps: 0.100000\n",
      "📈 Episodio 53: Recompensa total (clipped): 28.000, Pasos: 625, Mean Reward Calculado: 0.044800 (Recompensa/Pasos)\n",
      "  34590/825189: episode: 53, duration: 24.678s, episode steps: 625, steps per second:  25, episode reward: 28.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.041854, mae: 3.408201, mean_q: 4.113977, mean_eps: 0.100000\n",
      "📈 Episodio 54: Recompensa total (clipped): 25.000, Pasos: 582, Mean Reward Calculado: 0.042955 (Recompensa/Pasos)\n",
      "  35172/825189: episode: 54, duration: 23.142s, episode steps: 582, steps per second:  25, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.039654, mae: 3.375789, mean_q: 4.074679, mean_eps: 0.100000\n",
      "📈 Episodio 55: Recompensa total (clipped): 34.000, Pasos: 784, Mean Reward Calculado: 0.043367 (Recompensa/Pasos)\n",
      "  35956/825189: episode: 55, duration: 31.421s, episode steps: 784, steps per second:  25, episode reward: 34.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.035523, mae: 3.382080, mean_q: 4.085709, mean_eps: 0.100000\n",
      "📈 Episodio 56: Recompensa total (clipped): 21.000, Pasos: 456, Mean Reward Calculado: 0.046053 (Recompensa/Pasos)\n",
      "  36412/825189: episode: 56, duration: 18.207s, episode steps: 456, steps per second:  25, episode reward: 21.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.040785, mae: 3.394099, mean_q: 4.097862, mean_eps: 0.100000\n",
      "📈 Episodio 57: Recompensa total (clipped): 22.000, Pasos: 499, Mean Reward Calculado: 0.044088 (Recompensa/Pasos)\n",
      "  36911/825189: episode: 57, duration: 19.714s, episode steps: 499, steps per second:  25, episode reward: 22.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.040884, mae: 3.383156, mean_q: 4.084901, mean_eps: 0.100000\n",
      "📈 Episodio 58: Recompensa total (clipped): 17.000, Pasos: 485, Mean Reward Calculado: 0.035052 (Recompensa/Pasos)\n",
      "  37396/825189: episode: 58, duration: 19.527s, episode steps: 485, steps per second:  25, episode reward: 17.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.044427, mae: 3.357657, mean_q: 4.050290, mean_eps: 0.100000\n",
      "📈 Episodio 59: Recompensa total (clipped): 25.000, Pasos: 584, Mean Reward Calculado: 0.042808 (Recompensa/Pasos)\n",
      "  37980/825189: episode: 59, duration: 23.135s, episode steps: 584, steps per second:  25, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.514 [0.000, 5.000],  loss: 0.047143, mae: 3.412108, mean_q: 4.121693, mean_eps: 0.100000\n",
      "📈 Episodio 60: Recompensa total (clipped): 14.000, Pasos: 362, Mean Reward Calculado: 0.038674 (Recompensa/Pasos)\n",
      "  38342/825189: episode: 60, duration: 14.458s, episode steps: 362, steps per second:  25, episode reward: 14.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.204 [0.000, 5.000],  loss: 0.040158, mae: 3.406403, mean_q: 4.113715, mean_eps: 0.100000\n",
      "📈 Episodio 61: Recompensa total (clipped): 23.000, Pasos: 474, Mean Reward Calculado: 0.048523 (Recompensa/Pasos)\n",
      "  38816/825189: episode: 61, duration: 19.023s, episode steps: 474, steps per second:  25, episode reward: 23.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 3.099 [0.000, 5.000],  loss: 0.039341, mae: 3.386347, mean_q: 4.087660, mean_eps: 0.100000\n",
      "📈 Episodio 62: Recompensa total (clipped): 27.000, Pasos: 730, Mean Reward Calculado: 0.036986 (Recompensa/Pasos)\n",
      "  39546/825189: episode: 62, duration: 28.813s, episode steps: 730, steps per second:  25, episode reward: 27.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.047205, mae: 3.402245, mean_q: 4.103641, mean_eps: 0.100000\n",
      "📊 Paso 40,000/2,000,000 (2.0%) - 25.6 pasos/seg - ETA: 21.3h - Memoria: 8210.00 MB\n",
      "📈 Episodio 63: Recompensa total (clipped): 27.000, Pasos: 712, Mean Reward Calculado: 0.037921 (Recompensa/Pasos)\n",
      "  40258/825189: episode: 63, duration: 28.208s, episode steps: 712, steps per second:  25, episode reward: 27.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.032046, mae: 3.385655, mean_q: 4.088339, mean_eps: 0.100000\n",
      "📈 Episodio 64: Recompensa total (clipped): 27.000, Pasos: 637, Mean Reward Calculado: 0.042386 (Recompensa/Pasos)\n",
      "  40895/825189: episode: 64, duration: 25.473s, episode steps: 637, steps per second:  25, episode reward: 27.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.040660, mae: 3.390182, mean_q: 4.091771, mean_eps: 0.100000\n",
      "📈 Episodio 65: Recompensa total (clipped): 25.000, Pasos: 607, Mean Reward Calculado: 0.041186 (Recompensa/Pasos)\n",
      "  41502/825189: episode: 65, duration: 23.989s, episode steps: 607, steps per second:  25, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.035859, mae: 3.367448, mean_q: 4.067578, mean_eps: 0.100000\n",
      "📈 Episodio 66: Recompensa total (clipped): 34.000, Pasos: 980, Mean Reward Calculado: 0.034694 (Recompensa/Pasos)\n",
      "  42482/825189: episode: 66, duration: 38.474s, episode steps: 980, steps per second:  25, episode reward: 34.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.037951, mae: 3.336538, mean_q: 4.031155, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 67: Recompensa total (clipped): 8.000, Pasos: 324, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
      "  42806/825189: episode: 67, duration: 12.770s, episode steps: 324, steps per second:  25, episode reward:  8.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.036627, mae: 3.330912, mean_q: 4.020980, mean_eps: 0.100000\n",
      "📈 Episodio 68: Recompensa total (clipped): 26.000, Pasos: 659, Mean Reward Calculado: 0.039454 (Recompensa/Pasos)\n",
      "  43465/825189: episode: 68, duration: 25.967s, episode steps: 659, steps per second:  25, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.037576, mae: 3.347708, mean_q: 4.046751, mean_eps: 0.100000\n",
      "📈 Episodio 69: Recompensa total (clipped): 31.000, Pasos: 741, Mean Reward Calculado: 0.041835 (Recompensa/Pasos)\n",
      "  44206/825189: episode: 69, duration: 28.440s, episode steps: 741, steps per second:  26, episode reward: 31.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.042525, mae: 3.380990, mean_q: 4.082496, mean_eps: 0.100000\n",
      "📈 Episodio 70: Recompensa total (clipped): 33.000, Pasos: 879, Mean Reward Calculado: 0.037543 (Recompensa/Pasos)\n",
      "  45085/825189: episode: 70, duration: 35.827s, episode steps: 879, steps per second:  25, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.037470, mae: 3.343093, mean_q: 4.038250, mean_eps: 0.100000\n",
      "📈 Episodio 71: Recompensa total (clipped): 16.000, Pasos: 534, Mean Reward Calculado: 0.029963 (Recompensa/Pasos)\n",
      "  45619/825189: episode: 71, duration: 21.815s, episode steps: 534, steps per second:  24, episode reward: 16.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.035926, mae: 3.333212, mean_q: 4.025277, mean_eps: 0.100000\n",
      "📈 Episodio 72: Recompensa total (clipped): 18.000, Pasos: 530, Mean Reward Calculado: 0.033962 (Recompensa/Pasos)\n",
      "  46149/825189: episode: 72, duration: 22.289s, episode steps: 530, steps per second:  24, episode reward: 18.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.043833, mae: 3.371805, mean_q: 4.068676, mean_eps: 0.100000\n",
      "📈 Episodio 73: Recompensa total (clipped): 34.000, Pasos: 809, Mean Reward Calculado: 0.042027 (Recompensa/Pasos)\n",
      "  46958/825189: episode: 73, duration: 33.293s, episode steps: 809, steps per second:  24, episode reward: 34.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.039739, mae: 3.364414, mean_q: 4.064271, mean_eps: 0.100000\n",
      "📈 Episodio 74: Recompensa total (clipped): 12.000, Pasos: 367, Mean Reward Calculado: 0.032698 (Recompensa/Pasos)\n",
      "  47325/825189: episode: 74, duration: 15.272s, episode steps: 367, steps per second:  24, episode reward: 12.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.371 [0.000, 5.000],  loss: 0.042360, mae: 3.320846, mean_q: 4.008547, mean_eps: 0.100000\n",
      "📈 Episodio 75: Recompensa total (clipped): 32.000, Pasos: 854, Mean Reward Calculado: 0.037471 (Recompensa/Pasos)\n",
      "  48179/825189: episode: 75, duration: 35.122s, episode steps: 854, steps per second:  24, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.040490, mae: 3.359503, mean_q: 4.056886, mean_eps: 0.100000\n",
      "📈 Episodio 76: Recompensa total (clipped): 28.000, Pasos: 715, Mean Reward Calculado: 0.039161 (Recompensa/Pasos)\n",
      "  48894/825189: episode: 76, duration: 29.555s, episode steps: 715, steps per second:  24, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.038462, mae: 3.344377, mean_q: 4.040923, mean_eps: 0.100000\n",
      "📈 Episodio 77: Recompensa total (clipped): 22.000, Pasos: 529, Mean Reward Calculado: 0.041588 (Recompensa/Pasos)\n",
      "  49423/825189: episode: 77, duration: 21.812s, episode steps: 529, steps per second:  24, episode reward: 22.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.037927, mae: 3.382035, mean_q: 4.082324, mean_eps: 0.100000\n",
      "📈 Episodio 78: Recompensa total (clipped): 29.000, Pasos: 621, Mean Reward Calculado: 0.046699 (Recompensa/Pasos)\n",
      "  50044/825189: episode: 78, duration: 25.461s, episode steps: 621, steps per second:  24, episode reward: 29.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.039017, mae: 3.348650, mean_q: 4.042024, mean_eps: 0.100000\n",
      "📈 Episodio 79: Recompensa total (clipped): 31.000, Pasos: 966, Mean Reward Calculado: 0.032091 (Recompensa/Pasos)\n",
      "  51010/825189: episode: 79, duration: 39.680s, episode steps: 966, steps per second:  24, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.040283, mae: 3.378298, mean_q: 4.077937, mean_eps: 0.100000\n",
      "📈 Episodio 80: Recompensa total (clipped): 20.000, Pasos: 595, Mean Reward Calculado: 0.033613 (Recompensa/Pasos)\n",
      "  51605/825189: episode: 80, duration: 24.676s, episode steps: 595, steps per second:  24, episode reward: 20.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.897 [0.000, 5.000],  loss: 0.041356, mae: 3.347147, mean_q: 4.041982, mean_eps: 0.100000\n",
      "📈 Episodio 81: Recompensa total (clipped): 18.000, Pasos: 567, Mean Reward Calculado: 0.031746 (Recompensa/Pasos)\n",
      "  52172/825189: episode: 81, duration: 22.932s, episode steps: 567, steps per second:  25, episode reward: 18.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.034565, mae: 3.316776, mean_q: 4.004510, mean_eps: 0.100000\n",
      "📈 Episodio 82: Recompensa total (clipped): 35.000, Pasos: 1024, Mean Reward Calculado: 0.034180 (Recompensa/Pasos)\n",
      "  53196/825189: episode: 82, duration: 40.568s, episode steps: 1024, steps per second:  25, episode reward: 35.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.040244, mae: 3.345358, mean_q: 4.034046, mean_eps: 0.100000\n",
      "📈 Episodio 83: Recompensa total (clipped): 32.000, Pasos: 834, Mean Reward Calculado: 0.038369 (Recompensa/Pasos)\n",
      "  54030/825189: episode: 83, duration: 33.053s, episode steps: 834, steps per second:  25, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.044302, mae: 3.356189, mean_q: 4.049944, mean_eps: 0.100000\n",
      "📈 Episodio 84: Recompensa total (clipped): 31.000, Pasos: 776, Mean Reward Calculado: 0.039948 (Recompensa/Pasos)\n",
      "  54806/825189: episode: 84, duration: 30.691s, episode steps: 776, steps per second:  25, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.204 [0.000, 5.000],  loss: 0.038493, mae: 3.364104, mean_q: 4.059581, mean_eps: 0.100000\n",
      "📈 Episodio 85: Recompensa total (clipped): 28.000, Pasos: 633, Mean Reward Calculado: 0.044234 (Recompensa/Pasos)\n",
      "  55439/825189: episode: 85, duration: 24.730s, episode steps: 633, steps per second:  26, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.032516, mae: 3.342310, mean_q: 4.034801, mean_eps: 0.100000\n",
      "📈 Episodio 86: Recompensa total (clipped): 15.000, Pasos: 451, Mean Reward Calculado: 0.033259 (Recompensa/Pasos)\n",
      "  55890/825189: episode: 86, duration: 17.751s, episode steps: 451, steps per second:  25, episode reward: 15.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.993 [0.000, 5.000],  loss: 0.043811, mae: 3.379267, mean_q: 4.078813, mean_eps: 0.100000\n",
      "📈 Episodio 87: Recompensa total (clipped): 30.000, Pasos: 742, Mean Reward Calculado: 0.040431 (Recompensa/Pasos)\n",
      "  56632/825189: episode: 87, duration: 28.696s, episode steps: 742, steps per second:  26, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.033729, mae: 3.330540, mean_q: 4.017740, mean_eps: 0.100000\n",
      "📈 Episodio 88: Recompensa total (clipped): 32.000, Pasos: 948, Mean Reward Calculado: 0.033755 (Recompensa/Pasos)\n",
      "  57580/825189: episode: 88, duration: 37.285s, episode steps: 948, steps per second:  25, episode reward: 32.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.037752, mae: 3.330704, mean_q: 4.019897, mean_eps: 0.100000\n",
      "📈 Episodio 89: Recompensa total (clipped): 30.000, Pasos: 1009, Mean Reward Calculado: 0.029732 (Recompensa/Pasos)\n",
      "  58589/825189: episode: 89, duration: 39.888s, episode steps: 1009, steps per second:  25, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.038955, mae: 3.351856, mean_q: 4.044661, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 90: Recompensa total (clipped): 26.000, Pasos: 639, Mean Reward Calculado: 0.040689 (Recompensa/Pasos)\n",
      "  59228/825189: episode: 90, duration: 25.219s, episode steps: 639, steps per second:  25, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.127 [0.000, 5.000],  loss: 0.038757, mae: 3.364199, mean_q: 4.059415, mean_eps: 0.100000\n",
      "📈 Episodio 91: Recompensa total (clipped): 21.000, Pasos: 560, Mean Reward Calculado: 0.037500 (Recompensa/Pasos)\n",
      "  59788/825189: episode: 91, duration: 22.624s, episode steps: 560, steps per second:  25, episode reward: 21.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.036690, mae: 3.359174, mean_q: 4.052849, mean_eps: 0.100000\n",
      "📊 Paso 60,000/2,000,000 (3.0%) - 25.3 pasos/seg - ETA: 21.3h - Memoria: 8210.26 MB\n",
      "📈 Episodio 92: Recompensa total (clipped): 14.000, Pasos: 359, Mean Reward Calculado: 0.038997 (Recompensa/Pasos)\n",
      "  60147/825189: episode: 92, duration: 14.234s, episode steps: 359, steps per second:  25, episode reward: 14.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.037867, mae: 3.337218, mean_q: 4.022732, mean_eps: 0.100000\n",
      "📈 Episodio 93: Recompensa total (clipped): 34.000, Pasos: 883, Mean Reward Calculado: 0.038505 (Recompensa/Pasos)\n",
      "  61030/825189: episode: 93, duration: 35.168s, episode steps: 883, steps per second:  25, episode reward: 34.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.037408, mae: 3.379642, mean_q: 4.078907, mean_eps: 0.100000\n",
      "📈 Episodio 94: Recompensa total (clipped): 32.000, Pasos: 681, Mean Reward Calculado: 0.046990 (Recompensa/Pasos)\n",
      "  61711/825189: episode: 94, duration: 26.800s, episode steps: 681, steps per second:  25, episode reward: 32.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.039237, mae: 3.357807, mean_q: 4.053133, mean_eps: 0.100000\n",
      "📈 Episodio 95: Recompensa total (clipped): 32.000, Pasos: 899, Mean Reward Calculado: 0.035595 (Recompensa/Pasos)\n",
      "  62610/825189: episode: 95, duration: 35.330s, episode steps: 899, steps per second:  25, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.039613, mae: 3.347127, mean_q: 4.038721, mean_eps: 0.100000\n",
      "📈 Episodio 96: Recompensa total (clipped): 19.000, Pasos: 433, Mean Reward Calculado: 0.043880 (Recompensa/Pasos)\n",
      "  63043/825189: episode: 96, duration: 17.015s, episode steps: 433, steps per second:  25, episode reward: 19.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.040348, mae: 3.386594, mean_q: 4.086333, mean_eps: 0.100000\n",
      "📈 Episodio 97: Recompensa total (clipped): 25.000, Pasos: 604, Mean Reward Calculado: 0.041391 (Recompensa/Pasos)\n",
      "  63647/825189: episode: 97, duration: 23.625s, episode steps: 604, steps per second:  26, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.034495, mae: 3.348102, mean_q: 4.041252, mean_eps: 0.100000\n",
      "📈 Episodio 98: Recompensa total (clipped): 24.000, Pasos: 662, Mean Reward Calculado: 0.036254 (Recompensa/Pasos)\n",
      "  64309/825189: episode: 98, duration: 26.281s, episode steps: 662, steps per second:  25, episode reward: 24.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.041362, mae: 3.333165, mean_q: 4.020177, mean_eps: 0.100000\n",
      "📈 Episodio 99: Recompensa total (clipped): 22.000, Pasos: 683, Mean Reward Calculado: 0.032211 (Recompensa/Pasos)\n",
      "  64992/825189: episode: 99, duration: 27.049s, episode steps: 683, steps per second:  25, episode reward: 22.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.307 [0.000, 5.000],  loss: 0.037280, mae: 3.332243, mean_q: 4.024411, mean_eps: 0.100000\n",
      "📈 Episodio 100: Recompensa total (clipped): 18.000, Pasos: 458, Mean Reward Calculado: 0.039301 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 100, pasos: 65450)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.10 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 100 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 18.00\n",
      "   Media últimos 100: 25.10 / 20.0\n",
      "   Mejor promedio histórico: 25.10\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 10\n",
      "   Episodios consecutivos en objetivo: 10\n",
      "  65450/825189: episode: 100, duration: 45.193s, episode steps: 458, steps per second:  10, episode reward: 18.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.406 [0.000, 5.000],  loss: 0.030728, mae: 3.362718, mean_q: 4.062426, mean_eps: 0.100000\n",
      "📈 Episodio 101: Recompensa total (clipped): 31.000, Pasos: 828, Mean Reward Calculado: 0.037440 (Recompensa/Pasos)\n",
      "  66278/825189: episode: 101, duration: 33.102s, episode steps: 828, steps per second:  25, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.035019, mae: 3.357906, mean_q: 4.056520, mean_eps: 0.100000\n",
      "📈 Episodio 102: Recompensa total (clipped): 16.000, Pasos: 449, Mean Reward Calculado: 0.035635 (Recompensa/Pasos)\n",
      "  66727/825189: episode: 102, duration: 17.907s, episode steps: 449, steps per second:  25, episode reward: 16.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.037431, mae: 3.373804, mean_q: 4.072519, mean_eps: 0.100000\n",
      "📈 Episodio 103: Recompensa total (clipped): 20.000, Pasos: 568, Mean Reward Calculado: 0.035211 (Recompensa/Pasos)\n",
      "  67295/825189: episode: 103, duration: 22.592s, episode steps: 568, steps per second:  25, episode reward: 20.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.035667, mae: 3.353779, mean_q: 4.050070, mean_eps: 0.100000\n",
      "📈 Episodio 104: Recompensa total (clipped): 20.000, Pasos: 419, Mean Reward Calculado: 0.047733 (Recompensa/Pasos)\n",
      "  67714/825189: episode: 104, duration: 16.605s, episode steps: 419, steps per second:  25, episode reward: 20.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.045782, mae: 3.393564, mean_q: 4.094328, mean_eps: 0.100000\n",
      "📈 Episodio 105: Recompensa total (clipped): 26.000, Pasos: 792, Mean Reward Calculado: 0.032828 (Recompensa/Pasos)\n",
      "  68506/825189: episode: 105, duration: 31.446s, episode steps: 792, steps per second:  25, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.041492, mae: 3.393486, mean_q: 4.093982, mean_eps: 0.100000\n",
      "📈 Episodio 106: Recompensa total (clipped): 31.000, Pasos: 845, Mean Reward Calculado: 0.036686 (Recompensa/Pasos)\n",
      "  69351/825189: episode: 106, duration: 33.611s, episode steps: 845, steps per second:  25, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.033518, mae: 3.351051, mean_q: 4.043957, mean_eps: 0.100000\n",
      "📈 Episodio 107: Recompensa total (clipped): 16.000, Pasos: 481, Mean Reward Calculado: 0.033264 (Recompensa/Pasos)\n",
      "  69832/825189: episode: 107, duration: 19.182s, episode steps: 481, steps per second:  25, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.333 [0.000, 5.000],  loss: 0.039297, mae: 3.353690, mean_q: 4.047614, mean_eps: 0.100000\n",
      "📈 Episodio 108: Recompensa total (clipped): 27.000, Pasos: 531, Mean Reward Calculado: 0.050847 (Recompensa/Pasos)\n",
      "  70363/825189: episode: 108, duration: 21.292s, episode steps: 531, steps per second:  25, episode reward: 27.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 3.200 [0.000, 5.000],  loss: 0.034044, mae: 3.379860, mean_q: 4.084018, mean_eps: 0.100000\n",
      "📈 Episodio 109: Recompensa total (clipped): 24.000, Pasos: 600, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      "  70963/825189: episode: 109, duration: 23.683s, episode steps: 600, steps per second:  25, episode reward: 24.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.037201, mae: 3.394177, mean_q: 4.097930, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 110: Recompensa total (clipped): 21.000, Pasos: 548, Mean Reward Calculado: 0.038321 (Recompensa/Pasos)\n",
      "  71511/825189: episode: 110, duration: 21.997s, episode steps: 548, steps per second:  25, episode reward: 21.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.038796, mae: 3.369747, mean_q: 4.066734, mean_eps: 0.100000\n",
      "📈 Episodio 111: Recompensa total (clipped): 26.000, Pasos: 672, Mean Reward Calculado: 0.038690 (Recompensa/Pasos)\n",
      "  72183/825189: episode: 111, duration: 26.842s, episode steps: 672, steps per second:  25, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.094 [0.000, 5.000],  loss: 0.040719, mae: 3.356035, mean_q: 4.049785, mean_eps: 0.100000\n",
      "📈 Episodio 112: Recompensa total (clipped): 21.000, Pasos: 458, Mean Reward Calculado: 0.045852 (Recompensa/Pasos)\n",
      "  72641/825189: episode: 112, duration: 18.068s, episode steps: 458, steps per second:  25, episode reward: 21.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.038775, mae: 3.396123, mean_q: 4.098727, mean_eps: 0.100000\n",
      "📈 Episodio 113: Recompensa total (clipped): 15.000, Pasos: 456, Mean Reward Calculado: 0.032895 (Recompensa/Pasos)\n",
      "  73097/825189: episode: 113, duration: 18.197s, episode steps: 456, steps per second:  25, episode reward: 15.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.038871, mae: 3.412599, mean_q: 4.121961, mean_eps: 0.100000\n",
      "📈 Episodio 114: Recompensa total (clipped): 33.000, Pasos: 729, Mean Reward Calculado: 0.045267 (Recompensa/Pasos)\n",
      "  73826/825189: episode: 114, duration: 29.041s, episode steps: 729, steps per second:  25, episode reward: 33.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.034703, mae: 3.358249, mean_q: 4.056415, mean_eps: 0.100000\n",
      "📈 Episodio 115: Recompensa total (clipped): 27.000, Pasos: 808, Mean Reward Calculado: 0.033416 (Recompensa/Pasos)\n",
      "  74634/825189: episode: 115, duration: 31.905s, episode steps: 808, steps per second:  25, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.036360, mae: 3.388705, mean_q: 4.093265, mean_eps: 0.100000\n",
      "📈 Episodio 116: Recompensa total (clipped): 35.000, Pasos: 803, Mean Reward Calculado: 0.043587 (Recompensa/Pasos)\n",
      "  75437/825189: episode: 116, duration: 32.029s, episode steps: 803, steps per second:  25, episode reward: 35.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.041427, mae: 3.364173, mean_q: 4.061614, mean_eps: 0.100000\n",
      "📈 Episodio 117: Recompensa total (clipped): 28.000, Pasos: 634, Mean Reward Calculado: 0.044164 (Recompensa/Pasos)\n",
      "  76071/825189: episode: 117, duration: 24.861s, episode steps: 634, steps per second:  26, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.032 [0.000, 5.000],  loss: 0.035853, mae: 3.401174, mean_q: 4.109723, mean_eps: 0.100000\n",
      "📈 Episodio 118: Recompensa total (clipped): 19.000, Pasos: 606, Mean Reward Calculado: 0.031353 (Recompensa/Pasos)\n",
      "  76677/825189: episode: 118, duration: 24.019s, episode steps: 606, steps per second:  25, episode reward: 19.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.037746, mae: 3.348919, mean_q: 4.045579, mean_eps: 0.100000\n",
      "📈 Episodio 119: Recompensa total (clipped): 31.000, Pasos: 793, Mean Reward Calculado: 0.039092 (Recompensa/Pasos)\n",
      "  77470/825189: episode: 119, duration: 31.775s, episode steps: 793, steps per second:  25, episode reward: 31.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.202 [0.000, 5.000],  loss: 0.037848, mae: 3.360504, mean_q: 4.057208, mean_eps: 0.100000\n",
      "📈 Episodio 120: Recompensa total (clipped): 30.000, Pasos: 938, Mean Reward Calculado: 0.031983 (Recompensa/Pasos)\n",
      "  78408/825189: episode: 120, duration: 37.603s, episode steps: 938, steps per second:  25, episode reward: 30.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.037165, mae: 3.363896, mean_q: 4.060431, mean_eps: 0.100000\n",
      "📈 Episodio 121: Recompensa total (clipped): 22.000, Pasos: 536, Mean Reward Calculado: 0.041045 (Recompensa/Pasos)\n",
      "  78944/825189: episode: 121, duration: 21.520s, episode steps: 536, steps per second:  25, episode reward: 22.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.032 [0.000, 5.000],  loss: 0.041294, mae: 3.425684, mean_q: 4.132872, mean_eps: 0.100000\n",
      "📈 Episodio 122: Recompensa total (clipped): 22.000, Pasos: 549, Mean Reward Calculado: 0.040073 (Recompensa/Pasos)\n",
      "  79493/825189: episode: 122, duration: 21.684s, episode steps: 549, steps per second:  25, episode reward: 22.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.040207, mae: 3.365441, mean_q: 4.061346, mean_eps: 0.100000\n",
      "📊 Paso 80,000/2,000,000 (4.0%) - 25.1 pasos/seg - ETA: 21.3h - Memoria: 8240.18 MB\n",
      "📈 Episodio 123: Recompensa total (clipped): 26.000, Pasos: 764, Mean Reward Calculado: 0.034031 (Recompensa/Pasos)\n",
      "  80257/825189: episode: 123, duration: 30.358s, episode steps: 764, steps per second:  25, episode reward: 26.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.039762, mae: 3.373741, mean_q: 4.071315, mean_eps: 0.100000\n",
      "📈 Episodio 124: Recompensa total (clipped): 19.000, Pasos: 482, Mean Reward Calculado: 0.039419 (Recompensa/Pasos)\n",
      "  80739/825189: episode: 124, duration: 18.839s, episode steps: 482, steps per second:  26, episode reward: 19.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.045733, mae: 3.392533, mean_q: 4.095107, mean_eps: 0.100000\n",
      "📈 Episodio 125: Recompensa total (clipped): 31.000, Pasos: 672, Mean Reward Calculado: 0.046131 (Recompensa/Pasos)\n",
      "  81411/825189: episode: 125, duration: 26.359s, episode steps: 672, steps per second:  25, episode reward: 31.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.038880, mae: 3.415951, mean_q: 4.123576, mean_eps: 0.100000\n",
      "📈 Episodio 126: Recompensa total (clipped): 23.000, Pasos: 517, Mean Reward Calculado: 0.044487 (Recompensa/Pasos)\n",
      "  81928/825189: episode: 126, duration: 20.712s, episode steps: 517, steps per second:  25, episode reward: 23.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.137 [0.000, 5.000],  loss: 0.037434, mae: 3.386426, mean_q: 4.088661, mean_eps: 0.100000\n",
      "📈 Episodio 127: Recompensa total (clipped): 21.000, Pasos: 702, Mean Reward Calculado: 0.029915 (Recompensa/Pasos)\n",
      "  82630/825189: episode: 127, duration: 27.862s, episode steps: 702, steps per second:  25, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.039107, mae: 3.390862, mean_q: 4.094434, mean_eps: 0.100000\n",
      "📈 Episodio 128: Recompensa total (clipped): 23.000, Pasos: 630, Mean Reward Calculado: 0.036508 (Recompensa/Pasos)\n",
      "  83260/825189: episode: 128, duration: 24.908s, episode steps: 630, steps per second:  25, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.038284, mae: 3.379612, mean_q: 4.076172, mean_eps: 0.100000\n",
      "📈 Episodio 129: Recompensa total (clipped): 16.000, Pasos: 483, Mean Reward Calculado: 0.033126 (Recompensa/Pasos)\n",
      "  83743/825189: episode: 129, duration: 19.263s, episode steps: 483, steps per second:  25, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.039297, mae: 3.367141, mean_q: 4.060231, mean_eps: 0.100000\n",
      "📈 Episodio 130: Recompensa total (clipped): 40.000, Pasos: 1214, Mean Reward Calculado: 0.032949 (Recompensa/Pasos)\n",
      "  84957/825189: episode: 130, duration: 48.596s, episode steps: 1214, steps per second:  25, episode reward: 40.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.036264, mae: 3.403604, mean_q: 4.104875, mean_eps: 0.100000\n",
      "📈 Episodio 131: Recompensa total (clipped): 31.000, Pasos: 774, Mean Reward Calculado: 0.040052 (Recompensa/Pasos)\n",
      "  85731/825189: episode: 131, duration: 30.302s, episode steps: 774, steps per second:  26, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.037424, mae: 3.404431, mean_q: 4.104161, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 132: Recompensa total (clipped): 34.000, Pasos: 914, Mean Reward Calculado: 0.037199 (Recompensa/Pasos)\n",
      "  86645/825189: episode: 132, duration: 36.402s, episode steps: 914, steps per second:  25, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.041689, mae: 3.385363, mean_q: 4.079312, mean_eps: 0.100000\n",
      "📈 Episodio 133: Recompensa total (clipped): 32.000, Pasos: 893, Mean Reward Calculado: 0.035834 (Recompensa/Pasos)\n",
      "  87538/825189: episode: 133, duration: 34.801s, episode steps: 893, steps per second:  26, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.042135, mae: 3.396817, mean_q: 4.097190, mean_eps: 0.100000\n",
      "📈 Episodio 134: Recompensa total (clipped): 33.000, Pasos: 1257, Mean Reward Calculado: 0.026253 (Recompensa/Pasos)\n",
      "  88795/825189: episode: 134, duration: 49.455s, episode steps: 1257, steps per second:  25, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.040337, mae: 3.388045, mean_q: 4.084647, mean_eps: 0.100000\n",
      "📈 Episodio 135: Recompensa total (clipped): 16.000, Pasos: 383, Mean Reward Calculado: 0.041775 (Recompensa/Pasos)\n",
      "  89178/825189: episode: 135, duration: 15.533s, episode steps: 383, steps per second:  25, episode reward: 16.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.036793, mae: 3.426227, mean_q: 4.131046, mean_eps: 0.100000\n",
      "📈 Episodio 136: Recompensa total (clipped): 35.000, Pasos: 1228, Mean Reward Calculado: 0.028502 (Recompensa/Pasos)\n",
      "  90406/825189: episode: 136, duration: 48.940s, episode steps: 1228, steps per second:  25, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.034263, mae: 3.409466, mean_q: 4.114283, mean_eps: 0.100000\n",
      "📈 Episodio 137: Recompensa total (clipped): 23.000, Pasos: 554, Mean Reward Calculado: 0.041516 (Recompensa/Pasos)\n",
      "  90960/825189: episode: 137, duration: 22.061s, episode steps: 554, steps per second:  25, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.035930, mae: 3.428521, mean_q: 4.135852, mean_eps: 0.100000\n",
      "📈 Episodio 138: Recompensa total (clipped): 24.000, Pasos: 551, Mean Reward Calculado: 0.043557 (Recompensa/Pasos)\n",
      "  91511/825189: episode: 138, duration: 21.822s, episode steps: 551, steps per second:  25, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.037449, mae: 3.394475, mean_q: 4.093511, mean_eps: 0.100000\n",
      "📈 Episodio 139: Recompensa total (clipped): 26.000, Pasos: 640, Mean Reward Calculado: 0.040625 (Recompensa/Pasos)\n",
      "  92151/825189: episode: 139, duration: 25.103s, episode steps: 640, steps per second:  25, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.037140, mae: 3.396275, mean_q: 4.098681, mean_eps: 0.100000\n",
      "📈 Episodio 140: Recompensa total (clipped): 26.000, Pasos: 634, Mean Reward Calculado: 0.041009 (Recompensa/Pasos)\n",
      "  92785/825189: episode: 140, duration: 25.076s, episode steps: 634, steps per second:  25, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.034348, mae: 3.430911, mean_q: 4.137875, mean_eps: 0.100000\n",
      "📈 Episodio 141: Recompensa total (clipped): 27.000, Pasos: 710, Mean Reward Calculado: 0.038028 (Recompensa/Pasos)\n",
      "  93495/825189: episode: 141, duration: 27.966s, episode steps: 710, steps per second:  25, episode reward: 27.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.901 [0.000, 5.000],  loss: 0.038960, mae: 3.423257, mean_q: 4.128297, mean_eps: 0.100000\n",
      "📈 Episodio 142: Recompensa total (clipped): 29.000, Pasos: 1066, Mean Reward Calculado: 0.027205 (Recompensa/Pasos)\n",
      "  94561/825189: episode: 142, duration: 42.052s, episode steps: 1066, steps per second:  25, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.036738, mae: 3.424081, mean_q: 4.129286, mean_eps: 0.100000\n",
      "📈 Episodio 143: Recompensa total (clipped): 27.000, Pasos: 658, Mean Reward Calculado: 0.041033 (Recompensa/Pasos)\n",
      "  95219/825189: episode: 143, duration: 25.921s, episode steps: 658, steps per second:  25, episode reward: 27.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.040504, mae: 3.423108, mean_q: 4.127295, mean_eps: 0.100000\n",
      "📈 Episodio 144: Recompensa total (clipped): 33.000, Pasos: 933, Mean Reward Calculado: 0.035370 (Recompensa/Pasos)\n",
      "  96152/825189: episode: 144, duration: 37.139s, episode steps: 933, steps per second:  25, episode reward: 33.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.038352, mae: 3.418914, mean_q: 4.124998, mean_eps: 0.100000\n",
      "📈 Episodio 145: Recompensa total (clipped): 17.000, Pasos: 433, Mean Reward Calculado: 0.039261 (Recompensa/Pasos)\n",
      "  96585/825189: episode: 145, duration: 17.009s, episode steps: 433, steps per second:  25, episode reward: 17.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.038234, mae: 3.435245, mean_q: 4.144936, mean_eps: 0.100000\n",
      "📈 Episodio 146: Recompensa total (clipped): 29.000, Pasos: 679, Mean Reward Calculado: 0.042710 (Recompensa/Pasos)\n",
      "  97264/825189: episode: 146, duration: 26.690s, episode steps: 679, steps per second:  25, episode reward: 29.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.951 [0.000, 5.000],  loss: 0.039041, mae: 3.402330, mean_q: 4.105840, mean_eps: 0.100000\n",
      "📈 Episodio 147: Recompensa total (clipped): 32.000, Pasos: 879, Mean Reward Calculado: 0.036405 (Recompensa/Pasos)\n",
      "  98143/825189: episode: 147, duration: 34.737s, episode steps: 879, steps per second:  25, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.314 [0.000, 5.000],  loss: 0.037215, mae: 3.396060, mean_q: 4.098011, mean_eps: 0.100000\n",
      "📈 Episodio 148: Recompensa total (clipped): 30.000, Pasos: 748, Mean Reward Calculado: 0.040107 (Recompensa/Pasos)\n",
      "  98891/825189: episode: 148, duration: 29.330s, episode steps: 748, steps per second:  26, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.033 [0.000, 5.000],  loss: 0.036072, mae: 3.428539, mean_q: 4.137862, mean_eps: 0.100000\n",
      "📈 Episodio 149: Recompensa total (clipped): 21.000, Pasos: 566, Mean Reward Calculado: 0.037102 (Recompensa/Pasos)\n",
      "  99457/825189: episode: 149, duration: 22.404s, episode steps: 566, steps per second:  25, episode reward: 21.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.292 [0.000, 5.000],  loss: 0.033473, mae: 3.412044, mean_q: 4.117547, mean_eps: 0.100000\n",
      "📊 Paso 100,000/2,000,000 (5.0%) - 25.1 pasos/seg - ETA: 21.0h - Memoria: 8240.32 MB\n",
      "📈 Episodio 150: Recompensa total (clipped): 25.000, Pasos: 617, Mean Reward Calculado: 0.040519 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 150, pasos: 100074)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.54 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 150 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 25.00\n",
      "   Media últimos 100: 25.54 / 20.0\n",
      "   Mejor promedio histórico: 25.54\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 60\n",
      "   Episodios consecutivos en objetivo: 60\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 60 episodios consecutivos\n",
      " 100074/825189: episode: 150, duration: 51.895s, episode steps: 617, steps per second:  12, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.063 [0.000, 5.000],  loss: 0.034827, mae: 3.395455, mean_q: 4.100326, mean_eps: 0.100000\n",
      "📈 Episodio 151: Recompensa total (clipped): 21.000, Pasos: 470, Mean Reward Calculado: 0.044681 (Recompensa/Pasos)\n",
      " 100544/825189: episode: 151, duration: 18.599s, episode steps: 470, steps per second:  25, episode reward: 21.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.033780, mae: 3.472685, mean_q: 4.194655, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 152: Recompensa total (clipped): 26.000, Pasos: 617, Mean Reward Calculado: 0.042139 (Recompensa/Pasos)\n",
      " 101161/825189: episode: 152, duration: 24.840s, episode steps: 617, steps per second:  25, episode reward: 26.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.041633, mae: 3.396094, mean_q: 4.096778, mean_eps: 0.100000\n",
      "📈 Episodio 153: Recompensa total (clipped): 21.000, Pasos: 500, Mean Reward Calculado: 0.042000 (Recompensa/Pasos)\n",
      " 101661/825189: episode: 153, duration: 20.204s, episode steps: 500, steps per second:  25, episode reward: 21.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.035799, mae: 3.368980, mean_q: 4.063621, mean_eps: 0.100000\n",
      "📈 Episodio 154: Recompensa total (clipped): 26.000, Pasos: 904, Mean Reward Calculado: 0.028761 (Recompensa/Pasos)\n",
      " 102565/825189: episode: 154, duration: 36.439s, episode steps: 904, steps per second:  25, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.044270, mae: 3.431324, mean_q: 4.137700, mean_eps: 0.100000\n",
      "📈 Episodio 155: Recompensa total (clipped): 25.000, Pasos: 691, Mean Reward Calculado: 0.036179 (Recompensa/Pasos)\n",
      " 103256/825189: episode: 155, duration: 27.673s, episode steps: 691, steps per second:  25, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.036302, mae: 3.404792, mean_q: 4.105548, mean_eps: 0.100000\n",
      "📈 Episodio 156: Recompensa total (clipped): 30.000, Pasos: 858, Mean Reward Calculado: 0.034965 (Recompensa/Pasos)\n",
      " 104114/825189: episode: 156, duration: 34.558s, episode steps: 858, steps per second:  25, episode reward: 30.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.038377, mae: 3.408464, mean_q: 4.110735, mean_eps: 0.100000\n",
      "📈 Episodio 157: Recompensa total (clipped): 35.000, Pasos: 886, Mean Reward Calculado: 0.039503 (Recompensa/Pasos)\n",
      " 105000/825189: episode: 157, duration: 35.363s, episode steps: 886, steps per second:  25, episode reward: 35.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.037356, mae: 3.425131, mean_q: 4.129178, mean_eps: 0.100000\n",
      "📈 Episodio 158: Recompensa total (clipped): 18.000, Pasos: 471, Mean Reward Calculado: 0.038217 (Recompensa/Pasos)\n",
      " 105471/825189: episode: 158, duration: 19.013s, episode steps: 471, steps per second:  25, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.043216, mae: 3.424228, mean_q: 4.126137, mean_eps: 0.100000\n",
      "📈 Episodio 159: Recompensa total (clipped): 24.000, Pasos: 615, Mean Reward Calculado: 0.039024 (Recompensa/Pasos)\n",
      " 106086/825189: episode: 159, duration: 24.736s, episode steps: 615, steps per second:  25, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.038929, mae: 3.425842, mean_q: 4.131988, mean_eps: 0.100000\n",
      "📈 Episodio 160: Recompensa total (clipped): 21.000, Pasos: 549, Mean Reward Calculado: 0.038251 (Recompensa/Pasos)\n",
      " 106635/825189: episode: 160, duration: 22.123s, episode steps: 549, steps per second:  25, episode reward: 21.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.098 [0.000, 5.000],  loss: 0.047729, mae: 3.423737, mean_q: 4.129040, mean_eps: 0.100000\n",
      "📈 Episodio 161: Recompensa total (clipped): 29.000, Pasos: 736, Mean Reward Calculado: 0.039402 (Recompensa/Pasos)\n",
      " 107371/825189: episode: 161, duration: 29.283s, episode steps: 736, steps per second:  25, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.042658, mae: 3.419422, mean_q: 4.122043, mean_eps: 0.100000\n",
      "📈 Episodio 162: Recompensa total (clipped): 26.000, Pasos: 643, Mean Reward Calculado: 0.040435 (Recompensa/Pasos)\n",
      " 108014/825189: episode: 162, duration: 26.064s, episode steps: 643, steps per second:  25, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.041778, mae: 3.415785, mean_q: 4.115291, mean_eps: 0.100000\n",
      "📈 Episodio 163: Recompensa total (clipped): 13.000, Pasos: 311, Mean Reward Calculado: 0.041801 (Recompensa/Pasos)\n",
      " 108325/825189: episode: 163, duration: 12.529s, episode steps: 311, steps per second:  25, episode reward: 13.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.033714, mae: 3.444843, mean_q: 4.154184, mean_eps: 0.100000\n",
      "📈 Episodio 164: Recompensa total (clipped): 18.000, Pasos: 577, Mean Reward Calculado: 0.031196 (Recompensa/Pasos)\n",
      " 108902/825189: episode: 164, duration: 23.116s, episode steps: 577, steps per second:  25, episode reward: 18.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.040625, mae: 3.435083, mean_q: 4.142468, mean_eps: 0.100000\n",
      "📈 Episodio 165: Recompensa total (clipped): 29.000, Pasos: 650, Mean Reward Calculado: 0.044615 (Recompensa/Pasos)\n",
      " 109552/825189: episode: 165, duration: 26.512s, episode steps: 650, steps per second:  25, episode reward: 29.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.045 [0.000, 5.000],  loss: 0.038298, mae: 3.417605, mean_q: 4.122435, mean_eps: 0.100000\n",
      "📈 Episodio 166: Recompensa total (clipped): 30.000, Pasos: 842, Mean Reward Calculado: 0.035629 (Recompensa/Pasos)\n",
      " 110394/825189: episode: 166, duration: 33.485s, episode steps: 842, steps per second:  25, episode reward: 30.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.037235, mae: 3.446603, mean_q: 4.157788, mean_eps: 0.100000\n",
      "📈 Episodio 167: Recompensa total (clipped): 25.000, Pasos: 735, Mean Reward Calculado: 0.034014 (Recompensa/Pasos)\n",
      " 111129/825189: episode: 167, duration: 30.014s, episode steps: 735, steps per second:  24, episode reward: 25.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.035340, mae: 3.404378, mean_q: 4.108907, mean_eps: 0.100000\n",
      "📈 Episodio 168: Recompensa total (clipped): 26.000, Pasos: 589, Mean Reward Calculado: 0.044143 (Recompensa/Pasos)\n",
      " 111718/825189: episode: 168, duration: 23.577s, episode steps: 589, steps per second:  25, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.036111, mae: 3.402780, mean_q: 4.106091, mean_eps: 0.100000\n",
      "📈 Episodio 169: Recompensa total (clipped): 25.000, Pasos: 692, Mean Reward Calculado: 0.036127 (Recompensa/Pasos)\n",
      " 112410/825189: episode: 169, duration: 27.726s, episode steps: 692, steps per second:  25, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.033836, mae: 3.419008, mean_q: 4.124620, mean_eps: 0.100000\n",
      "📈 Episodio 170: Recompensa total (clipped): 25.000, Pasos: 691, Mean Reward Calculado: 0.036179 (Recompensa/Pasos)\n",
      " 113101/825189: episode: 170, duration: 28.166s, episode steps: 691, steps per second:  25, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.039017, mae: 3.414381, mean_q: 4.118227, mean_eps: 0.100000\n",
      "📈 Episodio 171: Recompensa total (clipped): 28.000, Pasos: 691, Mean Reward Calculado: 0.040521 (Recompensa/Pasos)\n",
      " 113792/825189: episode: 171, duration: 27.396s, episode steps: 691, steps per second:  25, episode reward: 28.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.037587, mae: 3.406791, mean_q: 4.109150, mean_eps: 0.100000\n",
      "📈 Episodio 172: Recompensa total (clipped): 31.000, Pasos: 673, Mean Reward Calculado: 0.046062 (Recompensa/Pasos)\n",
      " 114465/825189: episode: 172, duration: 27.035s, episode steps: 673, steps per second:  25, episode reward: 31.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.036616, mae: 3.430249, mean_q: 4.139456, mean_eps: 0.100000\n",
      "📈 Episodio 173: Recompensa total (clipped): 28.000, Pasos: 563, Mean Reward Calculado: 0.049734 (Recompensa/Pasos)\n",
      " 115028/825189: episode: 173, duration: 22.703s, episode steps: 563, steps per second:  25, episode reward: 28.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.035003, mae: 3.448002, mean_q: 4.158706, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 174: Recompensa total (clipped): 21.000, Pasos: 579, Mean Reward Calculado: 0.036269 (Recompensa/Pasos)\n",
      " 115607/825189: episode: 174, duration: 23.184s, episode steps: 579, steps per second:  25, episode reward: 21.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.041032, mae: 3.396124, mean_q: 4.095501, mean_eps: 0.100000\n",
      "📈 Episodio 175: Recompensa total (clipped): 22.000, Pasos: 541, Mean Reward Calculado: 0.040665 (Recompensa/Pasos)\n",
      " 116148/825189: episode: 175, duration: 21.857s, episode steps: 541, steps per second:  25, episode reward: 22.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.510 [0.000, 5.000],  loss: 0.037315, mae: 3.404749, mean_q: 4.106828, mean_eps: 0.100000\n",
      "📈 Episodio 176: Recompensa total (clipped): 34.000, Pasos: 1184, Mean Reward Calculado: 0.028716 (Recompensa/Pasos)\n",
      " 117332/825189: episode: 176, duration: 47.100s, episode steps: 1184, steps per second:  25, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.036168, mae: 3.429807, mean_q: 4.137456, mean_eps: 0.100000\n",
      "📈 Episodio 177: Recompensa total (clipped): 32.000, Pasos: 703, Mean Reward Calculado: 0.045519 (Recompensa/Pasos)\n",
      " 118035/825189: episode: 177, duration: 28.083s, episode steps: 703, steps per second:  25, episode reward: 32.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.037587, mae: 3.420408, mean_q: 4.127471, mean_eps: 0.100000\n",
      "📈 Episodio 178: Recompensa total (clipped): 26.000, Pasos: 698, Mean Reward Calculado: 0.037249 (Recompensa/Pasos)\n",
      " 118733/825189: episode: 178, duration: 28.330s, episode steps: 698, steps per second:  25, episode reward: 26.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.037752, mae: 3.431132, mean_q: 4.138245, mean_eps: 0.100000\n",
      "📈 Episodio 179: Recompensa total (clipped): 35.000, Pasos: 878, Mean Reward Calculado: 0.039863 (Recompensa/Pasos)\n",
      " 119611/825189: episode: 179, duration: 34.857s, episode steps: 878, steps per second:  25, episode reward: 35.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.036834, mae: 3.389359, mean_q: 4.086020, mean_eps: 0.100000\n",
      "📊 Paso 120,000/2,000,000 (6.0%) - 24.9 pasos/seg - ETA: 20.9h - Memoria: 8358.30 MB\n",
      "📈 Episodio 180: Recompensa total (clipped): 24.000, Pasos: 704, Mean Reward Calculado: 0.034091 (Recompensa/Pasos)\n",
      " 120315/825189: episode: 180, duration: 28.155s, episode steps: 704, steps per second:  25, episode reward: 24.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.036992, mae: 3.428537, mean_q: 4.135228, mean_eps: 0.100000\n",
      "📈 Episodio 181: Recompensa total (clipped): 31.000, Pasos: 976, Mean Reward Calculado: 0.031762 (Recompensa/Pasos)\n",
      " 121291/825189: episode: 181, duration: 38.961s, episode steps: 976, steps per second:  25, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.037140, mae: 3.463509, mean_q: 4.178702, mean_eps: 0.100000\n",
      "📈 Episodio 182: Recompensa total (clipped): 33.000, Pasos: 706, Mean Reward Calculado: 0.046742 (Recompensa/Pasos)\n",
      " 121997/825189: episode: 182, duration: 28.130s, episode steps: 706, steps per second:  25, episode reward: 33.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.033882, mae: 3.453254, mean_q: 4.166976, mean_eps: 0.100000\n",
      "📈 Episodio 183: Recompensa total (clipped): 20.000, Pasos: 509, Mean Reward Calculado: 0.039293 (Recompensa/Pasos)\n",
      " 122506/825189: episode: 183, duration: 20.161s, episode steps: 509, steps per second:  25, episode reward: 20.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.041003, mae: 3.436621, mean_q: 4.143165, mean_eps: 0.100000\n",
      "📈 Episodio 184: Recompensa total (clipped): 26.000, Pasos: 673, Mean Reward Calculado: 0.038633 (Recompensa/Pasos)\n",
      " 123179/825189: episode: 184, duration: 27.030s, episode steps: 673, steps per second:  25, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.033680, mae: 3.470701, mean_q: 4.187408, mean_eps: 0.100000\n",
      "📈 Episodio 185: Recompensa total (clipped): 28.000, Pasos: 651, Mean Reward Calculado: 0.043011 (Recompensa/Pasos)\n",
      " 123830/825189: episode: 185, duration: 26.333s, episode steps: 651, steps per second:  25, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.033603, mae: 3.511388, mean_q: 4.235111, mean_eps: 0.100000\n",
      "📈 Episodio 186: Recompensa total (clipped): 32.000, Pasos: 933, Mean Reward Calculado: 0.034298 (Recompensa/Pasos)\n",
      " 124763/825189: episode: 186, duration: 37.432s, episode steps: 933, steps per second:  25, episode reward: 32.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.076 [0.000, 5.000],  loss: 0.037916, mae: 3.435814, mean_q: 4.143618, mean_eps: 0.100000\n",
      "📈 Episodio 187: Recompensa total (clipped): 32.000, Pasos: 999, Mean Reward Calculado: 0.032032 (Recompensa/Pasos)\n",
      " 125762/825189: episode: 187, duration: 40.577s, episode steps: 999, steps per second:  25, episode reward: 32.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.035908, mae: 3.462807, mean_q: 4.176423, mean_eps: 0.100000\n",
      "📈 Episodio 188: Recompensa total (clipped): 29.000, Pasos: 635, Mean Reward Calculado: 0.045669 (Recompensa/Pasos)\n",
      " 126397/825189: episode: 188, duration: 25.519s, episode steps: 635, steps per second:  25, episode reward: 29.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 3.280 [0.000, 5.000],  loss: 0.039036, mae: 3.438528, mean_q: 4.145600, mean_eps: 0.100000\n",
      "📈 Episodio 189: Recompensa total (clipped): 16.000, Pasos: 511, Mean Reward Calculado: 0.031311 (Recompensa/Pasos)\n",
      " 126908/825189: episode: 189, duration: 20.533s, episode steps: 511, steps per second:  25, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.440 [0.000, 5.000],  loss: 0.029939, mae: 3.443150, mean_q: 4.153391, mean_eps: 0.100000\n",
      "📈 Episodio 190: Recompensa total (clipped): 21.000, Pasos: 575, Mean Reward Calculado: 0.036522 (Recompensa/Pasos)\n",
      " 127483/825189: episode: 190, duration: 23.212s, episode steps: 575, steps per second:  25, episode reward: 21.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.960 [0.000, 5.000],  loss: 0.038536, mae: 3.423560, mean_q: 4.128695, mean_eps: 0.100000\n",
      "📈 Episodio 191: Recompensa total (clipped): 25.000, Pasos: 689, Mean Reward Calculado: 0.036284 (Recompensa/Pasos)\n",
      " 128172/825189: episode: 191, duration: 27.717s, episode steps: 689, steps per second:  25, episode reward: 25.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.038768, mae: 3.498576, mean_q: 4.216258, mean_eps: 0.100000\n",
      "📈 Episodio 192: Recompensa total (clipped): 16.000, Pasos: 496, Mean Reward Calculado: 0.032258 (Recompensa/Pasos)\n",
      " 128668/825189: episode: 192, duration: 20.356s, episode steps: 496, steps per second:  24, episode reward: 16.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.042479, mae: 3.436488, mean_q: 4.141842, mean_eps: 0.100000\n",
      "📈 Episodio 193: Recompensa total (clipped): 27.000, Pasos: 621, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 129289/825189: episode: 193, duration: 25.317s, episode steps: 621, steps per second:  25, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.036099, mae: 3.418344, mean_q: 4.120728, mean_eps: 0.100000\n",
      "📈 Episodio 194: Recompensa total (clipped): 32.000, Pasos: 1052, Mean Reward Calculado: 0.030418 (Recompensa/Pasos)\n",
      " 130341/825189: episode: 194, duration: 42.561s, episode steps: 1052, steps per second:  25, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.032783, mae: 3.504477, mean_q: 4.229765, mean_eps: 0.100000\n",
      "📈 Episodio 195: Recompensa total (clipped): 19.000, Pasos: 513, Mean Reward Calculado: 0.037037 (Recompensa/Pasos)\n",
      " 130854/825189: episode: 195, duration: 20.594s, episode steps: 513, steps per second:  25, episode reward: 19.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.031162, mae: 3.444173, mean_q: 4.156264, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 196: Recompensa total (clipped): 13.000, Pasos: 489, Mean Reward Calculado: 0.026585 (Recompensa/Pasos)\n",
      " 131343/825189: episode: 196, duration: 19.768s, episode steps: 489, steps per second:  25, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.044746, mae: 3.455336, mean_q: 4.165210, mean_eps: 0.100000\n",
      "📈 Episodio 197: Recompensa total (clipped): 27.000, Pasos: 628, Mean Reward Calculado: 0.042994 (Recompensa/Pasos)\n",
      " 131971/825189: episode: 197, duration: 25.204s, episode steps: 628, steps per second:  25, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.030647, mae: 3.438151, mean_q: 4.147733, mean_eps: 0.100000\n",
      "📈 Episodio 198: Recompensa total (clipped): 21.000, Pasos: 643, Mean Reward Calculado: 0.032659 (Recompensa/Pasos)\n",
      " 132614/825189: episode: 198, duration: 25.829s, episode steps: 643, steps per second:  25, episode reward: 21.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.038818, mae: 3.447600, mean_q: 4.154962, mean_eps: 0.100000\n",
      "📈 Episodio 199: Recompensa total (clipped): 23.000, Pasos: 577, Mean Reward Calculado: 0.039861 (Recompensa/Pasos)\n",
      " 133191/825189: episode: 199, duration: 23.169s, episode steps: 577, steps per second:  25, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.039726, mae: 3.493304, mean_q: 4.210556, mean_eps: 0.100000\n",
      "📈 Episodio 200: Recompensa total (clipped): 24.000, Pasos: 686, Mean Reward Calculado: 0.034985 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 200, pasos: 133877)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.59 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 200 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 24.00\n",
      "   Media últimos 100: 25.59 / 20.0\n",
      "   Mejor promedio histórico: 25.59\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 110\n",
      "   Episodios consecutivos en objetivo: 110\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 110 episodios consecutivos\n",
      " 133877/825189: episode: 200, duration: 59.305s, episode steps: 686, steps per second:  12, episode reward: 24.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.034547, mae: 3.447467, mean_q: 4.154853, mean_eps: 0.100000\n",
      "📈 Episodio 201: Recompensa total (clipped): 15.000, Pasos: 371, Mean Reward Calculado: 0.040431 (Recompensa/Pasos)\n",
      " 134248/825189: episode: 201, duration: 15.231s, episode steps: 371, steps per second:  24, episode reward: 15.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.038613, mae: 3.493197, mean_q: 4.211727, mean_eps: 0.100000\n",
      "📈 Episodio 202: Recompensa total (clipped): 27.000, Pasos: 701, Mean Reward Calculado: 0.038516 (Recompensa/Pasos)\n",
      " 134949/825189: episode: 202, duration: 28.784s, episode steps: 701, steps per second:  24, episode reward: 27.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.039388, mae: 3.480378, mean_q: 4.197775, mean_eps: 0.100000\n",
      "📈 Episodio 203: Recompensa total (clipped): 25.000, Pasos: 613, Mean Reward Calculado: 0.040783 (Recompensa/Pasos)\n",
      " 135562/825189: episode: 203, duration: 24.745s, episode steps: 613, steps per second:  25, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.040317, mae: 3.436433, mean_q: 4.142420, mean_eps: 0.100000\n",
      "📈 Episodio 204: Recompensa total (clipped): 14.000, Pasos: 420, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      " 135982/825189: episode: 204, duration: 17.188s, episode steps: 420, steps per second:  24, episode reward: 14.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.340 [0.000, 5.000],  loss: 0.036552, mae: 3.467169, mean_q: 4.177066, mean_eps: 0.100000\n",
      "📈 Episodio 205: Recompensa total (clipped): 33.000, Pasos: 716, Mean Reward Calculado: 0.046089 (Recompensa/Pasos)\n",
      " 136698/825189: episode: 205, duration: 28.886s, episode steps: 716, steps per second:  25, episode reward: 33.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.038649, mae: 3.463113, mean_q: 4.174329, mean_eps: 0.100000\n",
      "📈 Episodio 206: Recompensa total (clipped): 20.000, Pasos: 517, Mean Reward Calculado: 0.038685 (Recompensa/Pasos)\n",
      " 137215/825189: episode: 206, duration: 20.900s, episode steps: 517, steps per second:  25, episode reward: 20.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.045520, mae: 3.464691, mean_q: 4.174588, mean_eps: 0.100000\n",
      "📈 Episodio 207: Recompensa total (clipped): 35.000, Pasos: 980, Mean Reward Calculado: 0.035714 (Recompensa/Pasos)\n",
      " 138195/825189: episode: 207, duration: 39.451s, episode steps: 980, steps per second:  25, episode reward: 35.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.036177, mae: 3.465338, mean_q: 4.176160, mean_eps: 0.100000\n",
      "📈 Episodio 208: Recompensa total (clipped): 32.000, Pasos: 837, Mean Reward Calculado: 0.038232 (Recompensa/Pasos)\n",
      " 139032/825189: episode: 208, duration: 33.860s, episode steps: 837, steps per second:  25, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.032174, mae: 3.449485, mean_q: 4.158610, mean_eps: 0.100000\n",
      "📈 Episodio 209: Recompensa total (clipped): 33.000, Pasos: 856, Mean Reward Calculado: 0.038551 (Recompensa/Pasos)\n",
      " 139888/825189: episode: 209, duration: 34.807s, episode steps: 856, steps per second:  25, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.033628, mae: 3.465108, mean_q: 4.178767, mean_eps: 0.100000\n",
      "📊 Paso 140,000/2,000,000 (7.0%) - 24.8 pasos/seg - ETA: 20.8h - Memoria: 8374.90 MB\n",
      "📈 Episodio 210: Recompensa total (clipped): 24.000, Pasos: 715, Mean Reward Calculado: 0.033566 (Recompensa/Pasos)\n",
      " 140603/825189: episode: 210, duration: 28.780s, episode steps: 715, steps per second:  25, episode reward: 24.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.040517, mae: 3.449395, mean_q: 4.159585, mean_eps: 0.100000\n",
      "📈 Episodio 211: Recompensa total (clipped): 23.000, Pasos: 896, Mean Reward Calculado: 0.025670 (Recompensa/Pasos)\n",
      " 141499/825189: episode: 211, duration: 36.381s, episode steps: 896, steps per second:  25, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.042 [0.000, 5.000],  loss: 0.035838, mae: 3.433516, mean_q: 4.141499, mean_eps: 0.100000\n",
      "📈 Episodio 212: Recompensa total (clipped): 21.000, Pasos: 597, Mean Reward Calculado: 0.035176 (Recompensa/Pasos)\n",
      " 142096/825189: episode: 212, duration: 24.108s, episode steps: 597, steps per second:  25, episode reward: 21.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.040086, mae: 3.426432, mean_q: 4.132812, mean_eps: 0.100000\n",
      "📈 Episodio 213: Recompensa total (clipped): 35.000, Pasos: 876, Mean Reward Calculado: 0.039954 (Recompensa/Pasos)\n",
      " 142972/825189: episode: 213, duration: 35.445s, episode steps: 876, steps per second:  25, episode reward: 35.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.376 [0.000, 5.000],  loss: 0.037032, mae: 3.439606, mean_q: 4.147001, mean_eps: 0.100000\n",
      "📈 Episodio 214: Recompensa total (clipped): 33.000, Pasos: 807, Mean Reward Calculado: 0.040892 (Recompensa/Pasos)\n",
      " 143779/825189: episode: 214, duration: 32.620s, episode steps: 807, steps per second:  25, episode reward: 33.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.072 [0.000, 5.000],  loss: 0.033451, mae: 3.459602, mean_q: 4.173427, mean_eps: 0.100000\n",
      "📈 Episodio 215: Recompensa total (clipped): 14.000, Pasos: 382, Mean Reward Calculado: 0.036649 (Recompensa/Pasos)\n",
      " 144161/825189: episode: 215, duration: 15.546s, episode steps: 382, steps per second:  25, episode reward: 14.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.390 [0.000, 5.000],  loss: 0.044441, mae: 3.495695, mean_q: 4.211262, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 216: Recompensa total (clipped): 23.000, Pasos: 536, Mean Reward Calculado: 0.042910 (Recompensa/Pasos)\n",
      " 144697/825189: episode: 216, duration: 21.670s, episode steps: 536, steps per second:  25, episode reward: 23.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.979 [0.000, 5.000],  loss: 0.035424, mae: 3.447675, mean_q: 4.156955, mean_eps: 0.100000\n",
      "📈 Episodio 217: Recompensa total (clipped): 35.000, Pasos: 798, Mean Reward Calculado: 0.043860 (Recompensa/Pasos)\n",
      " 145495/825189: episode: 217, duration: 32.027s, episode steps: 798, steps per second:  25, episode reward: 35.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.971 [0.000, 5.000],  loss: 0.042577, mae: 3.439209, mean_q: 4.143508, mean_eps: 0.100000\n",
      "📈 Episodio 218: Recompensa total (clipped): 23.000, Pasos: 551, Mean Reward Calculado: 0.041742 (Recompensa/Pasos)\n",
      " 146046/825189: episode: 218, duration: 22.286s, episode steps: 551, steps per second:  25, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.046771, mae: 3.473132, mean_q: 4.183782, mean_eps: 0.100000\n",
      "📈 Episodio 219: Recompensa total (clipped): 27.000, Pasos: 692, Mean Reward Calculado: 0.039017 (Recompensa/Pasos)\n",
      " 146738/825189: episode: 219, duration: 27.851s, episode steps: 692, steps per second:  25, episode reward: 27.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.037564, mae: 3.434024, mean_q: 4.139525, mean_eps: 0.100000\n",
      "📈 Episodio 220: Recompensa total (clipped): 25.000, Pasos: 619, Mean Reward Calculado: 0.040388 (Recompensa/Pasos)\n",
      " 147357/825189: episode: 220, duration: 25.297s, episode steps: 619, steps per second:  24, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.033561, mae: 3.439901, mean_q: 4.152311, mean_eps: 0.100000\n",
      "📈 Episodio 221: Recompensa total (clipped): 25.000, Pasos: 621, Mean Reward Calculado: 0.040258 (Recompensa/Pasos)\n",
      " 147978/825189: episode: 221, duration: 25.501s, episode steps: 621, steps per second:  24, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.034379, mae: 3.431137, mean_q: 4.139382, mean_eps: 0.100000\n",
      "📈 Episodio 222: Recompensa total (clipped): 36.000, Pasos: 1051, Mean Reward Calculado: 0.034253 (Recompensa/Pasos)\n",
      " 149029/825189: episode: 222, duration: 42.538s, episode steps: 1051, steps per second:  25, episode reward: 36.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.039916, mae: 3.427965, mean_q: 4.134506, mean_eps: 0.100000\n",
      "📈 Episodio 223: Recompensa total (clipped): 34.000, Pasos: 833, Mean Reward Calculado: 0.040816 (Recompensa/Pasos)\n",
      " 149862/825189: episode: 223, duration: 34.083s, episode steps: 833, steps per second:  24, episode reward: 34.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.035927, mae: 3.441356, mean_q: 4.149973, mean_eps: 0.100000\n",
      "📈 Episodio 224: Recompensa total (clipped): 29.000, Pasos: 791, Mean Reward Calculado: 0.036662 (Recompensa/Pasos)\n",
      " 150653/825189: episode: 224, duration: 31.981s, episode steps: 791, steps per second:  25, episode reward: 29.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.044831, mae: 3.441622, mean_q: 4.147792, mean_eps: 0.100000\n",
      "📈 Episodio 225: Recompensa total (clipped): 19.000, Pasos: 459, Mean Reward Calculado: 0.041394 (Recompensa/Pasos)\n",
      " 151112/825189: episode: 225, duration: 18.635s, episode steps: 459, steps per second:  25, episode reward: 19.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.105 [0.000, 5.000],  loss: 0.034875, mae: 3.468352, mean_q: 4.182468, mean_eps: 0.100000\n",
      "📈 Episodio 226: Recompensa total (clipped): 19.000, Pasos: 430, Mean Reward Calculado: 0.044186 (Recompensa/Pasos)\n",
      " 151542/825189: episode: 226, duration: 17.591s, episode steps: 430, steps per second:  24, episode reward: 19.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.039986, mae: 3.454461, mean_q: 4.163220, mean_eps: 0.100000\n",
      "📈 Episodio 227: Recompensa total (clipped): 25.000, Pasos: 561, Mean Reward Calculado: 0.044563 (Recompensa/Pasos)\n",
      " 152103/825189: episode: 227, duration: 22.642s, episode steps: 561, steps per second:  25, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.037610, mae: 3.452193, mean_q: 4.163634, mean_eps: 0.100000\n",
      "📈 Episodio 228: Recompensa total (clipped): 23.000, Pasos: 630, Mean Reward Calculado: 0.036508 (Recompensa/Pasos)\n",
      " 152733/825189: episode: 228, duration: 25.636s, episode steps: 630, steps per second:  25, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.298 [0.000, 5.000],  loss: 0.031996, mae: 3.478024, mean_q: 4.195646, mean_eps: 0.100000\n",
      "📈 Episodio 229: Recompensa total (clipped): 24.000, Pasos: 502, Mean Reward Calculado: 0.047809 (Recompensa/Pasos)\n",
      " 153235/825189: episode: 229, duration: 20.436s, episode steps: 502, steps per second:  25, episode reward: 24.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 3.056 [0.000, 5.000],  loss: 0.034992, mae: 3.472327, mean_q: 4.188487, mean_eps: 0.100000\n",
      "📈 Episodio 230: Recompensa total (clipped): 25.000, Pasos: 654, Mean Reward Calculado: 0.038226 (Recompensa/Pasos)\n",
      " 153889/825189: episode: 230, duration: 27.032s, episode steps: 654, steps per second:  24, episode reward: 25.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.034062, mae: 3.438337, mean_q: 4.145875, mean_eps: 0.100000\n",
      "📈 Episodio 231: Recompensa total (clipped): 32.000, Pasos: 812, Mean Reward Calculado: 0.039409 (Recompensa/Pasos)\n",
      " 154701/825189: episode: 231, duration: 33.082s, episode steps: 812, steps per second:  25, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.038637, mae: 3.455189, mean_q: 4.166500, mean_eps: 0.100000\n",
      "📈 Episodio 232: Recompensa total (clipped): 19.000, Pasos: 453, Mean Reward Calculado: 0.041943 (Recompensa/Pasos)\n",
      " 155154/825189: episode: 232, duration: 18.669s, episode steps: 453, steps per second:  24, episode reward: 19.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.041675, mae: 3.480408, mean_q: 4.195691, mean_eps: 0.100000\n",
      "📈 Episodio 233: Recompensa total (clipped): 33.000, Pasos: 845, Mean Reward Calculado: 0.039053 (Recompensa/Pasos)\n",
      " 155999/825189: episode: 233, duration: 34.239s, episode steps: 845, steps per second:  25, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.036173, mae: 3.444830, mean_q: 4.153407, mean_eps: 0.100000\n",
      "📈 Episodio 234: Recompensa total (clipped): 23.000, Pasos: 603, Mean Reward Calculado: 0.038143 (Recompensa/Pasos)\n",
      " 156602/825189: episode: 234, duration: 24.281s, episode steps: 603, steps per second:  25, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.038820, mae: 3.477439, mean_q: 4.194430, mean_eps: 0.100000\n",
      "📈 Episodio 235: Recompensa total (clipped): 25.000, Pasos: 647, Mean Reward Calculado: 0.038640 (Recompensa/Pasos)\n",
      " 157249/825189: episode: 235, duration: 26.056s, episode steps: 647, steps per second:  25, episode reward: 25.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.040941, mae: 3.456906, mean_q: 4.170317, mean_eps: 0.100000\n",
      "📈 Episodio 236: Recompensa total (clipped): 18.000, Pasos: 629, Mean Reward Calculado: 0.028617 (Recompensa/Pasos)\n",
      " 157878/825189: episode: 236, duration: 25.730s, episode steps: 629, steps per second:  24, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.032627, mae: 3.500834, mean_q: 4.226084, mean_eps: 0.100000\n",
      "📈 Episodio 237: Recompensa total (clipped): 17.000, Pasos: 419, Mean Reward Calculado: 0.040573 (Recompensa/Pasos)\n",
      " 158297/825189: episode: 237, duration: 16.983s, episode steps: 419, steps per second:  25, episode reward: 17.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.048009, mae: 3.460838, mean_q: 4.172282, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 238: Recompensa total (clipped): 23.000, Pasos: 553, Mean Reward Calculado: 0.041591 (Recompensa/Pasos)\n",
      " 158850/825189: episode: 238, duration: 22.547s, episode steps: 553, steps per second:  25, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.033344, mae: 3.489592, mean_q: 4.213807, mean_eps: 0.100000\n",
      "📈 Episodio 239: Recompensa total (clipped): 12.000, Pasos: 403, Mean Reward Calculado: 0.029777 (Recompensa/Pasos)\n",
      " 159253/825189: episode: 239, duration: 16.284s, episode steps: 403, steps per second:  25, episode reward: 12.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.033169, mae: 3.464030, mean_q: 4.182159, mean_eps: 0.100000\n",
      "📈 Episodio 240: Recompensa total (clipped): 16.000, Pasos: 468, Mean Reward Calculado: 0.034188 (Recompensa/Pasos)\n",
      " 159721/825189: episode: 240, duration: 19.222s, episode steps: 468, steps per second:  24, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.039186, mae: 3.416400, mean_q: 4.120763, mean_eps: 0.100000\n",
      "📊 Paso 160,000/2,000,000 (8.0%) - 24.8 pasos/seg - ETA: 20.6h - Memoria: 8375.00 MB\n",
      "📈 Episodio 241: Recompensa total (clipped): 22.000, Pasos: 662, Mean Reward Calculado: 0.033233 (Recompensa/Pasos)\n",
      " 160383/825189: episode: 241, duration: 26.831s, episode steps: 662, steps per second:  25, episode reward: 22.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.034831, mae: 3.459328, mean_q: 4.172139, mean_eps: 0.100000\n",
      "📈 Episodio 242: Recompensa total (clipped): 20.000, Pasos: 589, Mean Reward Calculado: 0.033956 (Recompensa/Pasos)\n",
      " 160972/825189: episode: 242, duration: 23.988s, episode steps: 589, steps per second:  25, episode reward: 20.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.039625, mae: 3.432563, mean_q: 4.136836, mean_eps: 0.100000\n",
      "📈 Episodio 243: Recompensa total (clipped): 19.000, Pasos: 472, Mean Reward Calculado: 0.040254 (Recompensa/Pasos)\n",
      " 161444/825189: episode: 243, duration: 19.271s, episode steps: 472, steps per second:  24, episode reward: 19.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.045623, mae: 3.519947, mean_q: 4.241072, mean_eps: 0.100000\n",
      "📈 Episodio 244: Recompensa total (clipped): 29.000, Pasos: 742, Mean Reward Calculado: 0.039084 (Recompensa/Pasos)\n",
      " 162186/825189: episode: 244, duration: 29.873s, episode steps: 742, steps per second:  25, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.033423, mae: 3.449921, mean_q: 4.157853, mean_eps: 0.100000\n",
      "📈 Episodio 245: Recompensa total (clipped): 26.000, Pasos: 552, Mean Reward Calculado: 0.047101 (Recompensa/Pasos)\n",
      " 162738/825189: episode: 245, duration: 22.158s, episode steps: 552, steps per second:  25, episode reward: 26.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.037799, mae: 3.453070, mean_q: 4.165904, mean_eps: 0.100000\n",
      "📈 Episodio 246: Recompensa total (clipped): 33.000, Pasos: 825, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 163563/825189: episode: 246, duration: 33.256s, episode steps: 825, steps per second:  25, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.034255, mae: 3.441859, mean_q: 4.153455, mean_eps: 0.100000\n",
      "📈 Episodio 247: Recompensa total (clipped): 27.000, Pasos: 620, Mean Reward Calculado: 0.043548 (Recompensa/Pasos)\n",
      " 164183/825189: episode: 247, duration: 24.891s, episode steps: 620, steps per second:  25, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.036018, mae: 3.429362, mean_q: 4.135769, mean_eps: 0.100000\n",
      "📈 Episodio 248: Recompensa total (clipped): 28.000, Pasos: 645, Mean Reward Calculado: 0.043411 (Recompensa/Pasos)\n",
      " 164828/825189: episode: 248, duration: 26.282s, episode steps: 645, steps per second:  25, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.038748, mae: 3.434161, mean_q: 4.142435, mean_eps: 0.100000\n",
      "📈 Episodio 249: Recompensa total (clipped): 13.000, Pasos: 405, Mean Reward Calculado: 0.032099 (Recompensa/Pasos)\n",
      " 165233/825189: episode: 249, duration: 16.577s, episode steps: 405, steps per second:  24, episode reward: 13.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.032231, mae: 3.447462, mean_q: 4.159292, mean_eps: 0.100000\n",
      "📈 Episodio 250: Recompensa total (clipped): 27.000, Pasos: 583, Mean Reward Calculado: 0.046312 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 250, pasos: 165816)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.07 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 250 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 27.00\n",
      "   Media últimos 100: 25.07 / 20.0\n",
      "   Mejor promedio histórico: 25.07\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 160\n",
      "   Episodios consecutivos en objetivo: 160\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 160 episodios consecutivos\n",
      " 165816/825189: episode: 250, duration: 56.309s, episode steps: 583, steps per second:  10, episode reward: 27.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.032809, mae: 3.465757, mean_q: 4.180939, mean_eps: 0.100000\n",
      "📈 Episodio 251: Recompensa total (clipped): 23.000, Pasos: 625, Mean Reward Calculado: 0.036800 (Recompensa/Pasos)\n",
      " 166441/825189: episode: 251, duration: 25.359s, episode steps: 625, steps per second:  25, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.035831, mae: 3.444785, mean_q: 4.153558, mean_eps: 0.100000\n",
      "📈 Episodio 252: Recompensa total (clipped): 19.000, Pasos: 447, Mean Reward Calculado: 0.042506 (Recompensa/Pasos)\n",
      " 166888/825189: episode: 252, duration: 18.282s, episode steps: 447, steps per second:  24, episode reward: 19.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.040591, mae: 3.470926, mean_q: 4.182465, mean_eps: 0.100000\n",
      "📈 Episodio 253: Recompensa total (clipped): 27.000, Pasos: 608, Mean Reward Calculado: 0.044408 (Recompensa/Pasos)\n",
      " 167496/825189: episode: 253, duration: 24.949s, episode steps: 608, steps per second:  24, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.039099, mae: 3.483041, mean_q: 4.197817, mean_eps: 0.100000\n",
      "📈 Episodio 254: Recompensa total (clipped): 32.000, Pasos: 846, Mean Reward Calculado: 0.037825 (Recompensa/Pasos)\n",
      " 168342/825189: episode: 254, duration: 34.485s, episode steps: 846, steps per second:  25, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.033709, mae: 3.460189, mean_q: 4.172228, mean_eps: 0.100000\n",
      "📈 Episodio 255: Recompensa total (clipped): 28.000, Pasos: 664, Mean Reward Calculado: 0.042169 (Recompensa/Pasos)\n",
      " 169006/825189: episode: 255, duration: 26.699s, episode steps: 664, steps per second:  25, episode reward: 28.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.037135, mae: 3.481189, mean_q: 4.197947, mean_eps: 0.100000\n",
      "📈 Episodio 256: Recompensa total (clipped): 39.000, Pasos: 972, Mean Reward Calculado: 0.040123 (Recompensa/Pasos)\n",
      " 169978/825189: episode: 256, duration: 39.401s, episode steps: 972, steps per second:  25, episode reward: 39.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.041695, mae: 3.460364, mean_q: 4.172533, mean_eps: 0.100000\n",
      "📈 Episodio 257: Recompensa total (clipped): 32.000, Pasos: 827, Mean Reward Calculado: 0.038694 (Recompensa/Pasos)\n",
      " 170805/825189: episode: 257, duration: 33.696s, episode steps: 827, steps per second:  25, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.037057, mae: 3.481748, mean_q: 4.202370, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 258: Recompensa total (clipped): 33.000, Pasos: 783, Mean Reward Calculado: 0.042146 (Recompensa/Pasos)\n",
      " 171588/825189: episode: 258, duration: 31.581s, episode steps: 783, steps per second:  25, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.039000, mae: 3.488006, mean_q: 4.208183, mean_eps: 0.100000\n",
      "📈 Episodio 259: Recompensa total (clipped): 27.000, Pasos: 566, Mean Reward Calculado: 0.047703 (Recompensa/Pasos)\n",
      " 172154/825189: episode: 259, duration: 23.404s, episode steps: 566, steps per second:  24, episode reward: 27.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.036582, mae: 3.475937, mean_q: 4.193142, mean_eps: 0.100000\n",
      "📈 Episodio 260: Recompensa total (clipped): 32.000, Pasos: 860, Mean Reward Calculado: 0.037209 (Recompensa/Pasos)\n",
      " 173014/825189: episode: 260, duration: 34.715s, episode steps: 860, steps per second:  25, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.037793, mae: 3.456643, mean_q: 4.167836, mean_eps: 0.100000\n",
      "📈 Episodio 261: Recompensa total (clipped): 28.000, Pasos: 666, Mean Reward Calculado: 0.042042 (Recompensa/Pasos)\n",
      " 173680/825189: episode: 261, duration: 27.392s, episode steps: 666, steps per second:  24, episode reward: 28.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.037943, mae: 3.494414, mean_q: 4.213329, mean_eps: 0.100000\n",
      "📈 Episodio 262: Recompensa total (clipped): 31.000, Pasos: 930, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      " 174610/825189: episode: 262, duration: 37.645s, episode steps: 930, steps per second:  25, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.033512, mae: 3.455311, mean_q: 4.166199, mean_eps: 0.100000\n",
      "📈 Episodio 263: Recompensa total (clipped): 33.000, Pasos: 1012, Mean Reward Calculado: 0.032609 (Recompensa/Pasos)\n",
      " 175622/825189: episode: 263, duration: 40.847s, episode steps: 1012, steps per second:  25, episode reward: 33.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.037398, mae: 3.488491, mean_q: 4.206641, mean_eps: 0.100000\n",
      "📈 Episodio 264: Recompensa total (clipped): 32.000, Pasos: 948, Mean Reward Calculado: 0.033755 (Recompensa/Pasos)\n",
      " 176570/825189: episode: 264, duration: 38.533s, episode steps: 948, steps per second:  25, episode reward: 32.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.040412, mae: 3.455595, mean_q: 4.167710, mean_eps: 0.100000\n",
      "📈 Episodio 265: Recompensa total (clipped): 25.000, Pasos: 706, Mean Reward Calculado: 0.035411 (Recompensa/Pasos)\n",
      " 177276/825189: episode: 265, duration: 29.234s, episode steps: 706, steps per second:  24, episode reward: 25.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.042904, mae: 3.493674, mean_q: 4.210691, mean_eps: 0.100000\n",
      "📈 Episodio 266: Recompensa total (clipped): 32.000, Pasos: 662, Mean Reward Calculado: 0.048338 (Recompensa/Pasos)\n",
      " 177938/825189: episode: 266, duration: 26.831s, episode steps: 662, steps per second:  25, episode reward: 32.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.042559, mae: 3.486468, mean_q: 4.202308, mean_eps: 0.100000\n",
      "📈 Episodio 267: Recompensa total (clipped): 24.000, Pasos: 597, Mean Reward Calculado: 0.040201 (Recompensa/Pasos)\n",
      " 178535/825189: episode: 267, duration: 24.111s, episode steps: 597, steps per second:  25, episode reward: 24.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.039299, mae: 3.478742, mean_q: 4.196926, mean_eps: 0.100000\n",
      "📈 Episodio 268: Recompensa total (clipped): 31.000, Pasos: 987, Mean Reward Calculado: 0.031408 (Recompensa/Pasos)\n",
      " 179522/825189: episode: 268, duration: 40.391s, episode steps: 987, steps per second:  24, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.038645, mae: 3.477132, mean_q: 4.193310, mean_eps: 0.100000\n",
      "📊 Paso 180,000/2,000,000 (9.0%) - 24.6 pasos/seg - ETA: 20.5h - Memoria: 8356.39 MB\n",
      "📈 Episodio 269: Recompensa total (clipped): 30.000, Pasos: 754, Mean Reward Calculado: 0.039788 (Recompensa/Pasos)\n",
      " 180276/825189: episode: 269, duration: 30.906s, episode steps: 754, steps per second:  24, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.037503, mae: 3.516797, mean_q: 4.242520, mean_eps: 0.100000\n",
      "📈 Episodio 270: Recompensa total (clipped): 33.000, Pasos: 1038, Mean Reward Calculado: 0.031792 (Recompensa/Pasos)\n",
      " 181314/825189: episode: 270, duration: 42.819s, episode steps: 1038, steps per second:  24, episode reward: 33.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.036538, mae: 3.509351, mean_q: 4.232706, mean_eps: 0.100000\n",
      "📈 Episodio 271: Recompensa total (clipped): 29.000, Pasos: 733, Mean Reward Calculado: 0.039563 (Recompensa/Pasos)\n",
      " 182047/825189: episode: 271, duration: 30.025s, episode steps: 733, steps per second:  24, episode reward: 29.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.033144, mae: 3.464337, mean_q: 4.176085, mean_eps: 0.100000\n",
      "📈 Episodio 272: Recompensa total (clipped): 32.000, Pasos: 841, Mean Reward Calculado: 0.038050 (Recompensa/Pasos)\n",
      " 182888/825189: episode: 272, duration: 34.653s, episode steps: 841, steps per second:  24, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.035160, mae: 3.510998, mean_q: 4.235613, mean_eps: 0.100000\n",
      "📈 Episodio 273: Recompensa total (clipped): 20.000, Pasos: 551, Mean Reward Calculado: 0.036298 (Recompensa/Pasos)\n",
      " 183439/825189: episode: 273, duration: 22.743s, episode steps: 551, steps per second:  24, episode reward: 20.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.227 [0.000, 5.000],  loss: 0.038613, mae: 3.462633, mean_q: 4.173789, mean_eps: 0.100000\n",
      "📈 Episodio 274: Recompensa total (clipped): 17.000, Pasos: 410, Mean Reward Calculado: 0.041463 (Recompensa/Pasos)\n",
      " 183849/825189: episode: 274, duration: 17.159s, episode steps: 410, steps per second:  24, episode reward: 17.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.033644, mae: 3.481751, mean_q: 4.198611, mean_eps: 0.100000\n",
      "📈 Episodio 275: Recompensa total (clipped): 20.000, Pasos: 462, Mean Reward Calculado: 0.043290 (Recompensa/Pasos)\n",
      " 184311/825189: episode: 275, duration: 19.003s, episode steps: 462, steps per second:  24, episode reward: 20.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.043668, mae: 3.470747, mean_q: 4.182494, mean_eps: 0.100000\n",
      "📈 Episodio 276: Recompensa total (clipped): 30.000, Pasos: 732, Mean Reward Calculado: 0.040984 (Recompensa/Pasos)\n",
      " 185043/825189: episode: 276, duration: 29.611s, episode steps: 732, steps per second:  25, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.039650, mae: 3.484191, mean_q: 4.201470, mean_eps: 0.100000\n",
      "📈 Episodio 277: Recompensa total (clipped): 27.000, Pasos: 654, Mean Reward Calculado: 0.041284 (Recompensa/Pasos)\n",
      " 185697/825189: episode: 277, duration: 26.938s, episode steps: 654, steps per second:  24, episode reward: 27.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.037799, mae: 3.513877, mean_q: 4.239061, mean_eps: 0.100000\n",
      "📈 Episodio 278: Recompensa total (clipped): 25.000, Pasos: 562, Mean Reward Calculado: 0.044484 (Recompensa/Pasos)\n",
      " 186259/825189: episode: 278, duration: 23.033s, episode steps: 562, steps per second:  24, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.040009, mae: 3.463322, mean_q: 4.171617, mean_eps: 0.100000\n",
      "📈 Episodio 279: Recompensa total (clipped): 24.000, Pasos: 645, Mean Reward Calculado: 0.037209 (Recompensa/Pasos)\n",
      " 186904/825189: episode: 279, duration: 26.388s, episode steps: 645, steps per second:  24, episode reward: 24.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.033149, mae: 3.492903, mean_q: 4.211465, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 280: Recompensa total (clipped): 17.000, Pasos: 469, Mean Reward Calculado: 0.036247 (Recompensa/Pasos)\n",
      " 187373/825189: episode: 280, duration: 19.521s, episode steps: 469, steps per second:  24, episode reward: 17.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.443 [0.000, 5.000],  loss: 0.041225, mae: 3.502913, mean_q: 4.225317, mean_eps: 0.100000\n",
      "📈 Episodio 281: Recompensa total (clipped): 32.000, Pasos: 878, Mean Reward Calculado: 0.036446 (Recompensa/Pasos)\n",
      " 188251/825189: episode: 281, duration: 35.810s, episode steps: 878, steps per second:  25, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.037245, mae: 3.489257, mean_q: 4.210221, mean_eps: 0.100000\n",
      "📈 Episodio 282: Recompensa total (clipped): 25.000, Pasos: 705, Mean Reward Calculado: 0.035461 (Recompensa/Pasos)\n",
      " 188956/825189: episode: 282, duration: 28.799s, episode steps: 705, steps per second:  24, episode reward: 25.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.458 [0.000, 5.000],  loss: 0.039141, mae: 3.499465, mean_q: 4.223382, mean_eps: 0.100000\n",
      "📈 Episodio 283: Recompensa total (clipped): 24.000, Pasos: 543, Mean Reward Calculado: 0.044199 (Recompensa/Pasos)\n",
      " 189499/825189: episode: 283, duration: 22.317s, episode steps: 543, steps per second:  24, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.365 [0.000, 5.000],  loss: 0.034547, mae: 3.493268, mean_q: 4.213507, mean_eps: 0.100000\n",
      "📈 Episodio 284: Recompensa total (clipped): 26.000, Pasos: 590, Mean Reward Calculado: 0.044068 (Recompensa/Pasos)\n",
      " 190089/825189: episode: 284, duration: 24.358s, episode steps: 590, steps per second:  24, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.507 [0.000, 5.000],  loss: 0.038527, mae: 3.500812, mean_q: 4.223013, mean_eps: 0.100000\n",
      "📈 Episodio 285: Recompensa total (clipped): 21.000, Pasos: 604, Mean Reward Calculado: 0.034768 (Recompensa/Pasos)\n",
      " 190693/825189: episode: 285, duration: 24.793s, episode steps: 604, steps per second:  24, episode reward: 21.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.243 [0.000, 5.000],  loss: 0.035481, mae: 3.459419, mean_q: 4.175396, mean_eps: 0.100000\n",
      "📈 Episodio 286: Recompensa total (clipped): 25.000, Pasos: 595, Mean Reward Calculado: 0.042017 (Recompensa/Pasos)\n",
      " 191288/825189: episode: 286, duration: 24.630s, episode steps: 595, steps per second:  24, episode reward: 25.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.035598, mae: 3.472568, mean_q: 4.189955, mean_eps: 0.100000\n",
      "📈 Episodio 287: Recompensa total (clipped): 29.000, Pasos: 767, Mean Reward Calculado: 0.037810 (Recompensa/Pasos)\n",
      " 192055/825189: episode: 287, duration: 31.461s, episode steps: 767, steps per second:  24, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.040510, mae: 3.511884, mean_q: 4.233990, mean_eps: 0.100000\n",
      "📈 Episodio 288: Recompensa total (clipped): 23.000, Pasos: 524, Mean Reward Calculado: 0.043893 (Recompensa/Pasos)\n",
      " 192579/825189: episode: 288, duration: 21.369s, episode steps: 524, steps per second:  25, episode reward: 23.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.033474, mae: 3.505190, mean_q: 4.226709, mean_eps: 0.100000\n",
      "📈 Episodio 289: Recompensa total (clipped): 28.000, Pasos: 803, Mean Reward Calculado: 0.034869 (Recompensa/Pasos)\n",
      " 193382/825189: episode: 289, duration: 33.031s, episode steps: 803, steps per second:  24, episode reward: 28.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.030982, mae: 3.465854, mean_q: 4.178274, mean_eps: 0.100000\n",
      "📈 Episodio 290: Recompensa total (clipped): 23.000, Pasos: 543, Mean Reward Calculado: 0.042357 (Recompensa/Pasos)\n",
      " 193925/825189: episode: 290, duration: 22.150s, episode steps: 543, steps per second:  25, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.041428, mae: 3.476523, mean_q: 4.189580, mean_eps: 0.100000\n",
      "📈 Episodio 291: Recompensa total (clipped): 26.000, Pasos: 581, Mean Reward Calculado: 0.044750 (Recompensa/Pasos)\n",
      " 194506/825189: episode: 291, duration: 23.755s, episode steps: 581, steps per second:  24, episode reward: 26.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.033112, mae: 3.506242, mean_q: 4.224874, mean_eps: 0.100000\n",
      "📈 Episodio 292: Recompensa total (clipped): 35.000, Pasos: 833, Mean Reward Calculado: 0.042017 (Recompensa/Pasos)\n",
      " 195339/825189: episode: 292, duration: 34.060s, episode steps: 833, steps per second:  24, episode reward: 35.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.038482, mae: 3.487734, mean_q: 4.201597, mean_eps: 0.100000\n",
      "📈 Episodio 293: Recompensa total (clipped): 13.000, Pasos: 394, Mean Reward Calculado: 0.032995 (Recompensa/Pasos)\n",
      " 195733/825189: episode: 293, duration: 16.379s, episode steps: 394, steps per second:  24, episode reward: 13.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.036592, mae: 3.512446, mean_q: 4.235700, mean_eps: 0.100000\n",
      "📈 Episodio 294: Recompensa total (clipped): 24.000, Pasos: 564, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 196297/825189: episode: 294, duration: 22.904s, episode steps: 564, steps per second:  25, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.860 [0.000, 5.000],  loss: 0.039138, mae: 3.487579, mean_q: 4.202020, mean_eps: 0.100000\n",
      "📈 Episodio 295: Recompensa total (clipped): 32.000, Pasos: 1046, Mean Reward Calculado: 0.030593 (Recompensa/Pasos)\n",
      " 197343/825189: episode: 295, duration: 41.958s, episode steps: 1046, steps per second:  25, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.037455, mae: 3.460155, mean_q: 4.171085, mean_eps: 0.100000\n",
      "📈 Episodio 296: Recompensa total (clipped): 12.000, Pasos: 345, Mean Reward Calculado: 0.034783 (Recompensa/Pasos)\n",
      " 197688/825189: episode: 296, duration: 14.061s, episode steps: 345, steps per second:  25, episode reward: 12.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.036791, mae: 3.471105, mean_q: 4.183483, mean_eps: 0.100000\n",
      "📈 Episodio 297: Recompensa total (clipped): 12.000, Pasos: 372, Mean Reward Calculado: 0.032258 (Recompensa/Pasos)\n",
      " 198060/825189: episode: 297, duration: 15.329s, episode steps: 372, steps per second:  24, episode reward: 12.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.028728, mae: 3.494516, mean_q: 4.214064, mean_eps: 0.100000\n",
      "📈 Episodio 298: Recompensa total (clipped): 22.000, Pasos: 581, Mean Reward Calculado: 0.037866 (Recompensa/Pasos)\n",
      " 198641/825189: episode: 298, duration: 23.754s, episode steps: 581, steps per second:  24, episode reward: 22.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.042268, mae: 3.498005, mean_q: 4.213349, mean_eps: 0.100000\n",
      "📈 Episodio 299: Recompensa total (clipped): 31.000, Pasos: 903, Mean Reward Calculado: 0.034330 (Recompensa/Pasos)\n",
      " 199544/825189: episode: 299, duration: 36.857s, episode steps: 903, steps per second:  24, episode reward: 31.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.036799, mae: 3.513399, mean_q: 4.237602, mean_eps: 0.100000\n",
      "📊 Paso 200,000/2,000,000 (10.0%) - 24.6 pasos/seg - ETA: 20.3h - Memoria: 8356.61 MB\n",
      "📈 Episodio 300: Recompensa total (clipped): 24.000, Pasos: 576, Mean Reward Calculado: 0.041667 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 300, pasos: 200120)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.57 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 300 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 24.00\n",
      "   Media últimos 100: 25.57 / 20.0\n",
      "   Mejor promedio histórico: 25.57\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 210\n",
      "   Episodios consecutivos en objetivo: 210\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 210 episodios consecutivos\n",
      " 200120/825189: episode: 300, duration: 60.169s, episode steps: 576, steps per second:  10, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.033790, mae: 3.453167, mean_q: 4.161181, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 301: Recompensa total (clipped): 14.000, Pasos: 373, Mean Reward Calculado: 0.037534 (Recompensa/Pasos)\n",
      " 200493/825189: episode: 301, duration: 15.426s, episode steps: 373, steps per second:  24, episode reward: 14.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.038829, mae: 3.495378, mean_q: 4.215466, mean_eps: 0.100000\n",
      "📈 Episodio 302: Recompensa total (clipped): 19.000, Pasos: 477, Mean Reward Calculado: 0.039832 (Recompensa/Pasos)\n",
      " 200970/825189: episode: 302, duration: 19.744s, episode steps: 477, steps per second:  24, episode reward: 19.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.030548, mae: 3.512298, mean_q: 4.233767, mean_eps: 0.100000\n",
      "📈 Episodio 303: Recompensa total (clipped): 33.000, Pasos: 827, Mean Reward Calculado: 0.039903 (Recompensa/Pasos)\n",
      " 201797/825189: episode: 303, duration: 33.704s, episode steps: 827, steps per second:  25, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.034374, mae: 3.520355, mean_q: 4.242147, mean_eps: 0.100000\n",
      "📈 Episodio 304: Recompensa total (clipped): 34.000, Pasos: 1049, Mean Reward Calculado: 0.032412 (Recompensa/Pasos)\n",
      " 202846/825189: episode: 304, duration: 43.054s, episode steps: 1049, steps per second:  24, episode reward: 34.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.033085, mae: 3.515296, mean_q: 4.236030, mean_eps: 0.100000\n",
      "📈 Episodio 305: Recompensa total (clipped): 29.000, Pasos: 606, Mean Reward Calculado: 0.047855 (Recompensa/Pasos)\n",
      " 203452/825189: episode: 305, duration: 25.076s, episode steps: 606, steps per second:  24, episode reward: 29.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.036398, mae: 3.459660, mean_q: 4.168571, mean_eps: 0.100000\n",
      "📈 Episodio 306: Recompensa total (clipped): 16.000, Pasos: 485, Mean Reward Calculado: 0.032990 (Recompensa/Pasos)\n",
      " 203937/825189: episode: 306, duration: 20.090s, episode steps: 485, steps per second:  24, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.038284, mae: 3.454046, mean_q: 4.158415, mean_eps: 0.100000\n",
      "📈 Episodio 307: Recompensa total (clipped): 24.000, Pasos: 575, Mean Reward Calculado: 0.041739 (Recompensa/Pasos)\n",
      " 204512/825189: episode: 307, duration: 23.989s, episode steps: 575, steps per second:  24, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.034772, mae: 3.488900, mean_q: 4.204225, mean_eps: 0.100000\n",
      "📈 Episodio 308: Recompensa total (clipped): 15.000, Pasos: 393, Mean Reward Calculado: 0.038168 (Recompensa/Pasos)\n",
      " 204905/825189: episode: 308, duration: 16.612s, episode steps: 393, steps per second:  24, episode reward: 15.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.038177, mae: 3.518158, mean_q: 4.236406, mean_eps: 0.100000\n",
      "📈 Episodio 309: Recompensa total (clipped): 29.000, Pasos: 736, Mean Reward Calculado: 0.039402 (Recompensa/Pasos)\n",
      " 205641/825189: episode: 309, duration: 30.214s, episode steps: 736, steps per second:  24, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.035153, mae: 3.482110, mean_q: 4.195931, mean_eps: 0.100000\n",
      "📈 Episodio 310: Recompensa total (clipped): 34.000, Pasos: 997, Mean Reward Calculado: 0.034102 (Recompensa/Pasos)\n",
      " 206638/825189: episode: 310, duration: 41.094s, episode steps: 997, steps per second:  24, episode reward: 34.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.036940, mae: 3.484722, mean_q: 4.201643, mean_eps: 0.100000\n",
      "📈 Episodio 311: Recompensa total (clipped): 25.000, Pasos: 560, Mean Reward Calculado: 0.044643 (Recompensa/Pasos)\n",
      " 207198/825189: episode: 311, duration: 23.309s, episode steps: 560, steps per second:  24, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.039 [0.000, 5.000],  loss: 0.036785, mae: 3.517290, mean_q: 4.238965, mean_eps: 0.100000\n",
      "📈 Episodio 312: Recompensa total (clipped): 12.000, Pasos: 375, Mean Reward Calculado: 0.032000 (Recompensa/Pasos)\n",
      " 207573/825189: episode: 312, duration: 15.511s, episode steps: 375, steps per second:  24, episode reward: 12.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.037525, mae: 3.504556, mean_q: 4.223850, mean_eps: 0.100000\n",
      "📈 Episodio 313: Recompensa total (clipped): 31.000, Pasos: 670, Mean Reward Calculado: 0.046269 (Recompensa/Pasos)\n",
      " 208243/825189: episode: 313, duration: 27.783s, episode steps: 670, steps per second:  24, episode reward: 31.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.034801, mae: 3.536262, mean_q: 4.258541, mean_eps: 0.100000\n",
      "📈 Episodio 314: Recompensa total (clipped): 22.000, Pasos: 486, Mean Reward Calculado: 0.045267 (Recompensa/Pasos)\n",
      " 208729/825189: episode: 314, duration: 20.145s, episode steps: 486, steps per second:  24, episode reward: 22.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.040601, mae: 3.544694, mean_q: 4.268984, mean_eps: 0.100000\n",
      "📈 Episodio 315: Recompensa total (clipped): 35.000, Pasos: 941, Mean Reward Calculado: 0.037194 (Recompensa/Pasos)\n",
      " 209670/825189: episode: 315, duration: 38.799s, episode steps: 941, steps per second:  24, episode reward: 35.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.034714, mae: 3.492070, mean_q: 4.208938, mean_eps: 0.100000\n",
      "📈 Episodio 316: Recompensa total (clipped): 27.000, Pasos: 628, Mean Reward Calculado: 0.042994 (Recompensa/Pasos)\n",
      " 210298/825189: episode: 316, duration: 26.542s, episode steps: 628, steps per second:  24, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.036324, mae: 3.463772, mean_q: 4.177205, mean_eps: 0.100000\n",
      "📈 Episodio 317: Recompensa total (clipped): 16.000, Pasos: 408, Mean Reward Calculado: 0.039216 (Recompensa/Pasos)\n",
      " 210706/825189: episode: 317, duration: 16.976s, episode steps: 408, steps per second:  24, episode reward: 16.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.027567, mae: 3.502753, mean_q: 4.226676, mean_eps: 0.100000\n",
      "📈 Episodio 318: Recompensa total (clipped): 21.000, Pasos: 523, Mean Reward Calculado: 0.040153 (Recompensa/Pasos)\n",
      " 211229/825189: episode: 318, duration: 21.860s, episode steps: 523, steps per second:  24, episode reward: 21.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.035100, mae: 3.518648, mean_q: 4.240942, mean_eps: 0.100000\n",
      "📈 Episodio 319: Recompensa total (clipped): 14.000, Pasos: 352, Mean Reward Calculado: 0.039773 (Recompensa/Pasos)\n",
      " 211581/825189: episode: 319, duration: 14.581s, episode steps: 352, steps per second:  24, episode reward: 14.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.030122, mae: 3.469459, mean_q: 4.183621, mean_eps: 0.100000\n",
      "📈 Episodio 320: Recompensa total (clipped): 30.000, Pasos: 711, Mean Reward Calculado: 0.042194 (Recompensa/Pasos)\n",
      " 212292/825189: episode: 320, duration: 29.561s, episode steps: 711, steps per second:  24, episode reward: 30.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.036554, mae: 3.499590, mean_q: 4.222136, mean_eps: 0.100000\n",
      "📈 Episodio 321: Recompensa total (clipped): 23.000, Pasos: 491, Mean Reward Calculado: 0.046843 (Recompensa/Pasos)\n",
      " 212783/825189: episode: 321, duration: 20.449s, episode steps: 491, steps per second:  24, episode reward: 23.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.039936, mae: 3.548446, mean_q: 4.276505, mean_eps: 0.100000\n",
      "📈 Episodio 322: Recompensa total (clipped): 29.000, Pasos: 684, Mean Reward Calculado: 0.042398 (Recompensa/Pasos)\n",
      " 213467/825189: episode: 322, duration: 27.941s, episode steps: 684, steps per second:  24, episode reward: 29.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.032918, mae: 3.517222, mean_q: 4.241380, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 323: Recompensa total (clipped): 33.000, Pasos: 868, Mean Reward Calculado: 0.038018 (Recompensa/Pasos)\n",
      " 214335/825189: episode: 323, duration: 36.151s, episode steps: 868, steps per second:  24, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.034592, mae: 3.492434, mean_q: 4.207021, mean_eps: 0.100000\n",
      "📈 Episodio 324: Recompensa total (clipped): 21.000, Pasos: 576, Mean Reward Calculado: 0.036458 (Recompensa/Pasos)\n",
      " 214911/825189: episode: 324, duration: 23.574s, episode steps: 576, steps per second:  24, episode reward: 21.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.032465, mae: 3.537253, mean_q: 4.265710, mean_eps: 0.100000\n",
      "📈 Episodio 325: Recompensa total (clipped): 23.000, Pasos: 559, Mean Reward Calculado: 0.041145 (Recompensa/Pasos)\n",
      " 215470/825189: episode: 325, duration: 23.208s, episode steps: 559, steps per second:  24, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.035300, mae: 3.506833, mean_q: 4.225582, mean_eps: 0.100000\n",
      "📈 Episodio 326: Recompensa total (clipped): 33.000, Pasos: 860, Mean Reward Calculado: 0.038372 (Recompensa/Pasos)\n",
      " 216330/825189: episode: 326, duration: 35.714s, episode steps: 860, steps per second:  24, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.039069, mae: 3.520109, mean_q: 4.239886, mean_eps: 0.100000\n",
      "📈 Episodio 327: Recompensa total (clipped): 27.000, Pasos: 566, Mean Reward Calculado: 0.047703 (Recompensa/Pasos)\n",
      " 216896/825189: episode: 327, duration: 23.390s, episode steps: 566, steps per second:  24, episode reward: 27.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.040472, mae: 3.478271, mean_q: 4.189795, mean_eps: 0.100000\n",
      "📈 Episodio 328: Recompensa total (clipped): 24.000, Pasos: 588, Mean Reward Calculado: 0.040816 (Recompensa/Pasos)\n",
      " 217484/825189: episode: 328, duration: 24.589s, episode steps: 588, steps per second:  24, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.036411, mae: 3.511537, mean_q: 4.233490, mean_eps: 0.100000\n",
      "📈 Episodio 329: Recompensa total (clipped): 34.000, Pasos: 808, Mean Reward Calculado: 0.042079 (Recompensa/Pasos)\n",
      " 218292/825189: episode: 329, duration: 33.535s, episode steps: 808, steps per second:  24, episode reward: 34.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.037175, mae: 3.479371, mean_q: 4.193362, mean_eps: 0.100000\n",
      "📈 Episodio 330: Recompensa total (clipped): 34.000, Pasos: 818, Mean Reward Calculado: 0.041565 (Recompensa/Pasos)\n",
      " 219110/825189: episode: 330, duration: 33.568s, episode steps: 818, steps per second:  24, episode reward: 34.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.034078, mae: 3.499766, mean_q: 4.218148, mean_eps: 0.100000\n",
      "📈 Episodio 331: Recompensa total (clipped): 24.000, Pasos: 681, Mean Reward Calculado: 0.035242 (Recompensa/Pasos)\n",
      " 219791/825189: episode: 331, duration: 28.063s, episode steps: 681, steps per second:  24, episode reward: 24.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.034497, mae: 3.487155, mean_q: 4.202711, mean_eps: 0.100000\n",
      "📊 Paso 220,000/2,000,000 (11.0%) - 24.5 pasos/seg - ETA: 20.2h - Memoria: 8514.30 MB\n",
      "📈 Episodio 332: Recompensa total (clipped): 24.000, Pasos: 622, Mean Reward Calculado: 0.038585 (Recompensa/Pasos)\n",
      " 220413/825189: episode: 332, duration: 25.552s, episode steps: 622, steps per second:  24, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.035895, mae: 3.488352, mean_q: 4.203854, mean_eps: 0.100000\n",
      "📈 Episodio 333: Recompensa total (clipped): 29.000, Pasos: 750, Mean Reward Calculado: 0.038667 (Recompensa/Pasos)\n",
      " 221163/825189: episode: 333, duration: 30.711s, episode steps: 750, steps per second:  24, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.120 [0.000, 5.000],  loss: 0.031207, mae: 3.515737, mean_q: 4.238546, mean_eps: 0.100000\n",
      "📈 Episodio 334: Recompensa total (clipped): 30.000, Pasos: 831, Mean Reward Calculado: 0.036101 (Recompensa/Pasos)\n",
      " 221994/825189: episode: 334, duration: 34.204s, episode steps: 831, steps per second:  24, episode reward: 30.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.478 [0.000, 5.000],  loss: 0.035922, mae: 3.519456, mean_q: 4.239212, mean_eps: 0.100000\n",
      "📈 Episodio 335: Recompensa total (clipped): 18.000, Pasos: 413, Mean Reward Calculado: 0.043584 (Recompensa/Pasos)\n",
      " 222407/825189: episode: 335, duration: 17.133s, episode steps: 413, steps per second:  24, episode reward: 18.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.993 [0.000, 5.000],  loss: 0.033955, mae: 3.531760, mean_q: 4.255083, mean_eps: 0.100000\n",
      "📈 Episodio 336: Recompensa total (clipped): 33.000, Pasos: 874, Mean Reward Calculado: 0.037757 (Recompensa/Pasos)\n",
      " 223281/825189: episode: 336, duration: 36.452s, episode steps: 874, steps per second:  24, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.037381, mae: 3.511739, mean_q: 4.230996, mean_eps: 0.100000\n",
      "📈 Episodio 337: Recompensa total (clipped): 10.000, Pasos: 308, Mean Reward Calculado: 0.032468 (Recompensa/Pasos)\n",
      " 223589/825189: episode: 337, duration: 12.772s, episode steps: 308, steps per second:  24, episode reward: 10.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.033011, mae: 3.536136, mean_q: 4.260218, mean_eps: 0.100000\n",
      "📈 Episodio 338: Recompensa total (clipped): 19.000, Pasos: 422, Mean Reward Calculado: 0.045024 (Recompensa/Pasos)\n",
      " 224011/825189: episode: 338, duration: 17.106s, episode steps: 422, steps per second:  25, episode reward: 19.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.039745, mae: 3.460404, mean_q: 4.167506, mean_eps: 0.100000\n",
      "📈 Episodio 339: Recompensa total (clipped): 29.000, Pasos: 667, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 224678/825189: episode: 339, duration: 27.809s, episode steps: 667, steps per second:  24, episode reward: 29.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.031997, mae: 3.485143, mean_q: 4.200476, mean_eps: 0.100000\n",
      "📈 Episodio 340: Recompensa total (clipped): 32.000, Pasos: 846, Mean Reward Calculado: 0.037825 (Recompensa/Pasos)\n",
      " 225524/825189: episode: 340, duration: 34.654s, episode steps: 846, steps per second:  24, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.037332, mae: 3.527748, mean_q: 4.249701, mean_eps: 0.100000\n",
      "📈 Episodio 341: Recompensa total (clipped): 33.000, Pasos: 890, Mean Reward Calculado: 0.037079 (Recompensa/Pasos)\n",
      " 226414/825189: episode: 341, duration: 36.743s, episode steps: 890, steps per second:  24, episode reward: 33.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.037773, mae: 3.509471, mean_q: 4.229521, mean_eps: 0.100000\n",
      "📈 Episodio 342: Recompensa total (clipped): 24.000, Pasos: 529, Mean Reward Calculado: 0.045369 (Recompensa/Pasos)\n",
      " 226943/825189: episode: 342, duration: 21.903s, episode steps: 529, steps per second:  24, episode reward: 24.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.039492, mae: 3.516739, mean_q: 4.234834, mean_eps: 0.100000\n",
      "📈 Episodio 343: Recompensa total (clipped): 32.000, Pasos: 794, Mean Reward Calculado: 0.040302 (Recompensa/Pasos)\n",
      " 227737/825189: episode: 343, duration: 33.075s, episode steps: 794, steps per second:  24, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.036942, mae: 3.523651, mean_q: 4.243243, mean_eps: 0.100000\n",
      "📈 Episodio 344: Recompensa total (clipped): 19.000, Pasos: 485, Mean Reward Calculado: 0.039175 (Recompensa/Pasos)\n",
      " 228222/825189: episode: 344, duration: 20.203s, episode steps: 485, steps per second:  24, episode reward: 19.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.040886, mae: 3.531881, mean_q: 4.252424, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 345: Recompensa total (clipped): 14.000, Pasos: 362, Mean Reward Calculado: 0.038674 (Recompensa/Pasos)\n",
      " 228584/825189: episode: 345, duration: 14.809s, episode steps: 362, steps per second:  24, episode reward: 14.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.039505, mae: 3.483335, mean_q: 4.194940, mean_eps: 0.100000\n",
      "📈 Episodio 346: Recompensa total (clipped): 29.000, Pasos: 624, Mean Reward Calculado: 0.046474 (Recompensa/Pasos)\n",
      " 229208/825189: episode: 346, duration: 26.025s, episode steps: 624, steps per second:  24, episode reward: 29.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.035064, mae: 3.538358, mean_q: 4.260784, mean_eps: 0.100000\n",
      "📈 Episodio 347: Recompensa total (clipped): 34.000, Pasos: 915, Mean Reward Calculado: 0.037158 (Recompensa/Pasos)\n",
      " 230123/825189: episode: 347, duration: 37.596s, episode steps: 915, steps per second:  24, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.037004, mae: 3.522981, mean_q: 4.242756, mean_eps: 0.100000\n",
      "📈 Episodio 348: Recompensa total (clipped): 23.000, Pasos: 503, Mean Reward Calculado: 0.045726 (Recompensa/Pasos)\n",
      " 230626/825189: episode: 348, duration: 20.548s, episode steps: 503, steps per second:  24, episode reward: 23.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.036102, mae: 3.528112, mean_q: 4.248756, mean_eps: 0.100000\n",
      "📈 Episodio 349: Recompensa total (clipped): 18.000, Pasos: 440, Mean Reward Calculado: 0.040909 (Recompensa/Pasos)\n",
      " 231066/825189: episode: 349, duration: 18.075s, episode steps: 440, steps per second:  24, episode reward: 18.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.036955, mae: 3.514168, mean_q: 4.233614, mean_eps: 0.100000\n",
      "📈 Episodio 350: Recompensa total (clipped): 11.000, Pasos: 321, Mean Reward Calculado: 0.034268 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 350, pasos: 231387)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.65 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 350 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 11.00\n",
      "   Media últimos 100: 25.65 / 20.0\n",
      "   Mejor promedio histórico: 25.65\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 260\n",
      "   Episodios consecutivos en objetivo: 260\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 260 episodios consecutivos\n",
      " 231387/825189: episode: 350, duration: 48.456s, episode steps: 321, steps per second:   7, episode reward: 11.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.036221, mae: 3.558015, mean_q: 4.286989, mean_eps: 0.100000\n",
      "📈 Episodio 351: Recompensa total (clipped): 15.000, Pasos: 462, Mean Reward Calculado: 0.032468 (Recompensa/Pasos)\n",
      " 231849/825189: episode: 351, duration: 19.341s, episode steps: 462, steps per second:  24, episode reward: 15.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.029046, mae: 3.526079, mean_q: 4.247861, mean_eps: 0.100000\n",
      "📈 Episodio 352: Recompensa total (clipped): 22.000, Pasos: 557, Mean Reward Calculado: 0.039497 (Recompensa/Pasos)\n",
      " 232406/825189: episode: 352, duration: 23.153s, episode steps: 557, steps per second:  24, episode reward: 22.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.041069, mae: 3.577098, mean_q: 4.308783, mean_eps: 0.100000\n",
      "📈 Episodio 353: Recompensa total (clipped): 23.000, Pasos: 557, Mean Reward Calculado: 0.041293 (Recompensa/Pasos)\n",
      " 232963/825189: episode: 353, duration: 22.888s, episode steps: 557, steps per second:  24, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.941 [0.000, 5.000],  loss: 0.035520, mae: 3.552117, mean_q: 4.278555, mean_eps: 0.100000\n",
      "📈 Episodio 354: Recompensa total (clipped): 25.000, Pasos: 582, Mean Reward Calculado: 0.042955 (Recompensa/Pasos)\n",
      " 233545/825189: episode: 354, duration: 24.371s, episode steps: 582, steps per second:  24, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.273 [0.000, 5.000],  loss: 0.038696, mae: 3.556200, mean_q: 4.283265, mean_eps: 0.100000\n",
      "📈 Episodio 355: Recompensa total (clipped): 28.000, Pasos: 706, Mean Reward Calculado: 0.039660 (Recompensa/Pasos)\n",
      " 234251/825189: episode: 355, duration: 29.624s, episode steps: 706, steps per second:  24, episode reward: 28.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.036917, mae: 3.501113, mean_q: 4.216798, mean_eps: 0.100000\n",
      "📈 Episodio 356: Recompensa total (clipped): 25.000, Pasos: 661, Mean Reward Calculado: 0.037821 (Recompensa/Pasos)\n",
      " 234912/825189: episode: 356, duration: 27.472s, episode steps: 661, steps per second:  24, episode reward: 25.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.034115, mae: 3.547284, mean_q: 4.277634, mean_eps: 0.100000\n",
      "📈 Episodio 357: Recompensa total (clipped): 23.000, Pasos: 537, Mean Reward Calculado: 0.042831 (Recompensa/Pasos)\n",
      " 235449/825189: episode: 357, duration: 22.509s, episode steps: 537, steps per second:  24, episode reward: 23.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.035730, mae: 3.538971, mean_q: 4.265395, mean_eps: 0.100000\n",
      "📈 Episodio 358: Recompensa total (clipped): 22.000, Pasos: 527, Mean Reward Calculado: 0.041746 (Recompensa/Pasos)\n",
      " 235976/825189: episode: 358, duration: 22.021s, episode steps: 527, steps per second:  24, episode reward: 22.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.271 [0.000, 5.000],  loss: 0.034222, mae: 3.550858, mean_q: 4.279169, mean_eps: 0.100000\n",
      "📈 Episodio 359: Recompensa total (clipped): 12.000, Pasos: 294, Mean Reward Calculado: 0.040816 (Recompensa/Pasos)\n",
      " 236270/825189: episode: 359, duration: 12.507s, episode steps: 294, steps per second:  24, episode reward: 12.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.047749, mae: 3.525887, mean_q: 4.240135, mean_eps: 0.100000\n",
      "📈 Episodio 360: Recompensa total (clipped): 13.000, Pasos: 487, Mean Reward Calculado: 0.026694 (Recompensa/Pasos)\n",
      " 236757/825189: episode: 360, duration: 20.398s, episode steps: 487, steps per second:  24, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.040303, mae: 3.566775, mean_q: 4.296557, mean_eps: 0.100000\n",
      "📈 Episodio 361: Recompensa total (clipped): 16.000, Pasos: 384, Mean Reward Calculado: 0.041667 (Recompensa/Pasos)\n",
      " 237141/825189: episode: 361, duration: 15.898s, episode steps: 384, steps per second:  24, episode reward: 16.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.032750, mae: 3.518378, mean_q: 4.242126, mean_eps: 0.100000\n",
      "📈 Episodio 362: Recompensa total (clipped): 32.000, Pasos: 792, Mean Reward Calculado: 0.040404 (Recompensa/Pasos)\n",
      " 237933/825189: episode: 362, duration: 32.977s, episode steps: 792, steps per second:  24, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.036826, mae: 3.552043, mean_q: 4.278775, mean_eps: 0.100000\n",
      "📈 Episodio 363: Recompensa total (clipped): 24.000, Pasos: 787, Mean Reward Calculado: 0.030496 (Recompensa/Pasos)\n",
      " 238720/825189: episode: 363, duration: 33.397s, episode steps: 787, steps per second:  24, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.043395, mae: 3.516239, mean_q: 4.233927, mean_eps: 0.100000\n",
      "📈 Episodio 364: Recompensa total (clipped): 30.000, Pasos: 791, Mean Reward Calculado: 0.037927 (Recompensa/Pasos)\n",
      " 239511/825189: episode: 364, duration: 33.027s, episode steps: 791, steps per second:  24, episode reward: 30.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.039121, mae: 3.523937, mean_q: 4.242464, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Paso 240,000/2,000,000 (12.0%) - 24.4 pasos/seg - ETA: 20.1h - Memoria: 8631.97 MB\n",
      "📈 Episodio 365: Recompensa total (clipped): 24.000, Pasos: 538, Mean Reward Calculado: 0.044610 (Recompensa/Pasos)\n",
      " 240049/825189: episode: 365, duration: 22.301s, episode steps: 538, steps per second:  24, episode reward: 24.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.034788, mae: 3.546272, mean_q: 4.270998, mean_eps: 0.100000\n",
      "📈 Episodio 366: Recompensa total (clipped): 34.000, Pasos: 890, Mean Reward Calculado: 0.038202 (Recompensa/Pasos)\n",
      " 240939/825189: episode: 366, duration: 36.796s, episode steps: 890, steps per second:  24, episode reward: 34.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.335 [0.000, 5.000],  loss: 0.033837, mae: 3.555528, mean_q: 4.283810, mean_eps: 0.100000\n",
      "📈 Episodio 367: Recompensa total (clipped): 16.000, Pasos: 455, Mean Reward Calculado: 0.035165 (Recompensa/Pasos)\n",
      " 241394/825189: episode: 367, duration: 19.045s, episode steps: 455, steps per second:  24, episode reward: 16.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.413 [0.000, 5.000],  loss: 0.035393, mae: 3.585387, mean_q: 4.319720, mean_eps: 0.100000\n",
      "📈 Episodio 368: Recompensa total (clipped): 24.000, Pasos: 669, Mean Reward Calculado: 0.035874 (Recompensa/Pasos)\n",
      " 242063/825189: episode: 368, duration: 28.126s, episode steps: 669, steps per second:  24, episode reward: 24.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.041619, mae: 3.551428, mean_q: 4.275048, mean_eps: 0.100000\n",
      "📈 Episodio 369: Recompensa total (clipped): 16.000, Pasos: 475, Mean Reward Calculado: 0.033684 (Recompensa/Pasos)\n",
      " 242538/825189: episode: 369, duration: 19.757s, episode steps: 475, steps per second:  24, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.039859, mae: 3.569226, mean_q: 4.297576, mean_eps: 0.100000\n",
      "📈 Episodio 370: Recompensa total (clipped): 10.000, Pasos: 349, Mean Reward Calculado: 0.028653 (Recompensa/Pasos)\n",
      " 242887/825189: episode: 370, duration: 14.479s, episode steps: 349, steps per second:  24, episode reward: 10.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.112 [0.000, 5.000],  loss: 0.033135, mae: 3.532511, mean_q: 4.254426, mean_eps: 0.100000\n",
      "📈 Episodio 371: Recompensa total (clipped): 10.000, Pasos: 289, Mean Reward Calculado: 0.034602 (Recompensa/Pasos)\n",
      " 243176/825189: episode: 371, duration: 12.221s, episode steps: 289, steps per second:  24, episode reward: 10.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.033767, mae: 3.557141, mean_q: 4.284429, mean_eps: 0.100000\n",
      "📈 Episodio 372: Recompensa total (clipped): 19.000, Pasos: 546, Mean Reward Calculado: 0.034799 (Recompensa/Pasos)\n",
      " 243722/825189: episode: 372, duration: 22.753s, episode steps: 546, steps per second:  24, episode reward: 19.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.037211, mae: 3.554031, mean_q: 4.280734, mean_eps: 0.100000\n",
      "📈 Episodio 373: Recompensa total (clipped): 13.000, Pasos: 496, Mean Reward Calculado: 0.026210 (Recompensa/Pasos)\n",
      " 244218/825189: episode: 373, duration: 20.696s, episode steps: 496, steps per second:  24, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.042319, mae: 3.562735, mean_q: 4.288638, mean_eps: 0.100000\n",
      "📈 Episodio 374: Recompensa total (clipped): 25.000, Pasos: 624, Mean Reward Calculado: 0.040064 (Recompensa/Pasos)\n",
      " 244842/825189: episode: 374, duration: 25.775s, episode steps: 624, steps per second:  24, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.031375, mae: 3.570445, mean_q: 4.300500, mean_eps: 0.100000\n",
      "📈 Episodio 375: Recompensa total (clipped): 15.000, Pasos: 387, Mean Reward Calculado: 0.038760 (Recompensa/Pasos)\n",
      " 245229/825189: episode: 375, duration: 16.046s, episode steps: 387, steps per second:  24, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.111 [0.000, 5.000],  loss: 0.034798, mae: 3.569630, mean_q: 4.297413, mean_eps: 0.100000\n",
      "📈 Episodio 376: Recompensa total (clipped): 14.000, Pasos: 475, Mean Reward Calculado: 0.029474 (Recompensa/Pasos)\n",
      " 245704/825189: episode: 376, duration: 19.828s, episode steps: 475, steps per second:  24, episode reward: 14.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.033717, mae: 3.586508, mean_q: 4.321698, mean_eps: 0.100000\n",
      "📈 Episodio 377: Recompensa total (clipped): 20.000, Pasos: 526, Mean Reward Calculado: 0.038023 (Recompensa/Pasos)\n",
      " 246230/825189: episode: 377, duration: 21.990s, episode steps: 526, steps per second:  24, episode reward: 20.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.278 [0.000, 5.000],  loss: 0.040331, mae: 3.551823, mean_q: 4.276269, mean_eps: 0.100000\n",
      "📈 Episodio 378: Recompensa total (clipped): 13.000, Pasos: 519, Mean Reward Calculado: 0.025048 (Recompensa/Pasos)\n",
      " 246749/825189: episode: 378, duration: 21.477s, episode steps: 519, steps per second:  24, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.040116, mae: 3.550017, mean_q: 4.275859, mean_eps: 0.100000\n",
      "📈 Episodio 379: Recompensa total (clipped): 33.000, Pasos: 782, Mean Reward Calculado: 0.042199 (Recompensa/Pasos)\n",
      " 247531/825189: episode: 379, duration: 31.955s, episode steps: 782, steps per second:  24, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.035152, mae: 3.588643, mean_q: 4.322050, mean_eps: 0.100000\n",
      "📈 Episodio 380: Recompensa total (clipped): 24.000, Pasos: 608, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 248139/825189: episode: 380, duration: 25.021s, episode steps: 608, steps per second:  24, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.038750, mae: 3.566069, mean_q: 4.294258, mean_eps: 0.100000\n",
      "📈 Episodio 381: Recompensa total (clipped): 32.000, Pasos: 805, Mean Reward Calculado: 0.039752 (Recompensa/Pasos)\n",
      " 248944/825189: episode: 381, duration: 33.324s, episode steps: 805, steps per second:  24, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.038287, mae: 3.549330, mean_q: 4.272414, mean_eps: 0.100000\n",
      "📈 Episodio 382: Recompensa total (clipped): 33.000, Pasos: 853, Mean Reward Calculado: 0.038687 (Recompensa/Pasos)\n",
      " 249797/825189: episode: 382, duration: 35.698s, episode steps: 853, steps per second:  24, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.038826, mae: 3.541715, mean_q: 4.263104, mean_eps: 0.100000\n",
      "📈 Episodio 383: Recompensa total (clipped): 24.000, Pasos: 583, Mean Reward Calculado: 0.041166 (Recompensa/Pasos)\n",
      " 250380/825189: episode: 383, duration: 24.520s, episode steps: 583, steps per second:  24, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.035829, mae: 3.582045, mean_q: 4.315886, mean_eps: 0.100000\n",
      "📈 Episodio 384: Recompensa total (clipped): 20.000, Pasos: 502, Mean Reward Calculado: 0.039841 (Recompensa/Pasos)\n",
      " 250882/825189: episode: 384, duration: 21.095s, episode steps: 502, steps per second:  24, episode reward: 20.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.032220, mae: 3.616107, mean_q: 4.359362, mean_eps: 0.100000\n",
      "📈 Episodio 385: Recompensa total (clipped): 32.000, Pasos: 858, Mean Reward Calculado: 0.037296 (Recompensa/Pasos)\n",
      " 251740/825189: episode: 385, duration: 35.636s, episode steps: 858, steps per second:  24, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.036974, mae: 3.558232, mean_q: 4.286703, mean_eps: 0.100000\n",
      "📈 Episodio 386: Recompensa total (clipped): 18.000, Pasos: 407, Mean Reward Calculado: 0.044226 (Recompensa/Pasos)\n",
      " 252147/825189: episode: 386, duration: 16.812s, episode steps: 407, steps per second:  24, episode reward: 18.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.033718, mae: 3.577831, mean_q: 4.310040, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 387: Recompensa total (clipped): 28.000, Pasos: 947, Mean Reward Calculado: 0.029567 (Recompensa/Pasos)\n",
      " 253094/825189: episode: 387, duration: 39.205s, episode steps: 947, steps per second:  24, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.033968, mae: 3.569597, mean_q: 4.301514, mean_eps: 0.100000\n",
      "📈 Episodio 388: Recompensa total (clipped): 17.000, Pasos: 483, Mean Reward Calculado: 0.035197 (Recompensa/Pasos)\n",
      " 253577/825189: episode: 388, duration: 20.167s, episode steps: 483, steps per second:  24, episode reward: 17.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.032841, mae: 3.603839, mean_q: 4.344624, mean_eps: 0.100000\n",
      "📈 Episodio 389: Recompensa total (clipped): 23.000, Pasos: 615, Mean Reward Calculado: 0.037398 (Recompensa/Pasos)\n",
      " 254192/825189: episode: 389, duration: 25.486s, episode steps: 615, steps per second:  24, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.036875, mae: 3.583100, mean_q: 4.316268, mean_eps: 0.100000\n",
      "📈 Episodio 390: Recompensa total (clipped): 9.000, Pasos: 257, Mean Reward Calculado: 0.035019 (Recompensa/Pasos)\n",
      " 254449/825189: episode: 390, duration: 10.835s, episode steps: 257, steps per second:  24, episode reward:  9.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.506 [0.000, 5.000],  loss: 0.038924, mae: 3.571296, mean_q: 4.298208, mean_eps: 0.100000\n",
      "📈 Episodio 391: Recompensa total (clipped): 27.000, Pasos: 715, Mean Reward Calculado: 0.037762 (Recompensa/Pasos)\n",
      " 255164/825189: episode: 391, duration: 29.670s, episode steps: 715, steps per second:  24, episode reward: 27.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.038760, mae: 3.606667, mean_q: 4.342710, mean_eps: 0.100000\n",
      "📈 Episodio 392: Recompensa total (clipped): 18.000, Pasos: 449, Mean Reward Calculado: 0.040089 (Recompensa/Pasos)\n",
      " 255613/825189: episode: 392, duration: 18.816s, episode steps: 449, steps per second:  24, episode reward: 18.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.032878, mae: 3.593961, mean_q: 4.329422, mean_eps: 0.100000\n",
      "📈 Episodio 393: Recompensa total (clipped): 29.000, Pasos: 557, Mean Reward Calculado: 0.052065 (Recompensa/Pasos)\n",
      " 256170/825189: episode: 393, duration: 23.260s, episode steps: 557, steps per second:  24, episode reward: 29.000, mean reward:  0.052 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.037928, mae: 3.564387, mean_q: 4.292672, mean_eps: 0.100000\n",
      "📈 Episodio 394: Recompensa total (clipped): 22.000, Pasos: 582, Mean Reward Calculado: 0.037801 (Recompensa/Pasos)\n",
      " 256752/825189: episode: 394, duration: 24.732s, episode steps: 582, steps per second:  24, episode reward: 22.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.031592, mae: 3.576962, mean_q: 4.311179, mean_eps: 0.100000\n",
      "📈 Episodio 395: Recompensa total (clipped): 33.000, Pasos: 861, Mean Reward Calculado: 0.038328 (Recompensa/Pasos)\n",
      " 257613/825189: episode: 395, duration: 36.348s, episode steps: 861, steps per second:  24, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.034325, mae: 3.581397, mean_q: 4.314072, mean_eps: 0.100000\n",
      "📈 Episodio 396: Recompensa total (clipped): 31.000, Pasos: 875, Mean Reward Calculado: 0.035429 (Recompensa/Pasos)\n",
      " 258488/825189: episode: 396, duration: 36.810s, episode steps: 875, steps per second:  24, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.038699, mae: 3.596598, mean_q: 4.329955, mean_eps: 0.100000\n",
      "📈 Episodio 397: Recompensa total (clipped): 29.000, Pasos: 644, Mean Reward Calculado: 0.045031 (Recompensa/Pasos)\n",
      " 259132/825189: episode: 397, duration: 26.978s, episode steps: 644, steps per second:  24, episode reward: 29.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.034646, mae: 3.540054, mean_q: 4.264696, mean_eps: 0.100000\n",
      "📈 Episodio 398: Recompensa total (clipped): 29.000, Pasos: 814, Mean Reward Calculado: 0.035627 (Recompensa/Pasos)\n",
      " 259946/825189: episode: 398, duration: 34.256s, episode steps: 814, steps per second:  24, episode reward: 29.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.150 [0.000, 5.000],  loss: 0.036793, mae: 3.617525, mean_q: 4.355842, mean_eps: 0.100000\n",
      "📊 Paso 260,000/2,000,000 (13.0%) - 24.3 pasos/seg - ETA: 19.9h - Memoria: 8632.10 MB\n",
      "📈 Episodio 399: Recompensa total (clipped): 28.000, Pasos: 630, Mean Reward Calculado: 0.044444 (Recompensa/Pasos)\n",
      " 260576/825189: episode: 399, duration: 26.323s, episode steps: 630, steps per second:  24, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.181 [0.000, 5.000],  loss: 0.038414, mae: 3.622797, mean_q: 4.365586, mean_eps: 0.100000\n",
      "📈 Episodio 400: Recompensa total (clipped): 17.000, Pasos: 462, Mean Reward Calculado: 0.036797 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 400, pasos: 261038)\n",
      "💾 NUEVO MEJOR PROMEDIO: 23.65 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 400 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 17.00\n",
      "   Media últimos 100: 23.65 / 20.0\n",
      "   Mejor promedio histórico: 23.65\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 310\n",
      "   Episodios consecutivos en objetivo: 310\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 310 episodios consecutivos\n",
      " 261038/825189: episode: 400, duration: 58.890s, episode steps: 462, steps per second:   8, episode reward: 17.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.688 [0.000, 5.000],  loss: 0.035305, mae: 3.645516, mean_q: 4.392915, mean_eps: 0.100000\n",
      "📈 Episodio 401: Recompensa total (clipped): 34.000, Pasos: 981, Mean Reward Calculado: 0.034659 (Recompensa/Pasos)\n",
      " 262019/825189: episode: 401, duration: 42.486s, episode steps: 981, steps per second:  23, episode reward: 34.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.274 [0.000, 5.000],  loss: 0.037386, mae: 3.633228, mean_q: 4.379903, mean_eps: 0.100000\n",
      "📈 Episodio 402: Recompensa total (clipped): 12.000, Pasos: 368, Mean Reward Calculado: 0.032609 (Recompensa/Pasos)\n",
      " 262387/825189: episode: 402, duration: 16.030s, episode steps: 368, steps per second:  23, episode reward: 12.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.031143, mae: 3.622740, mean_q: 4.364364, mean_eps: 0.100000\n",
      "📈 Episodio 403: Recompensa total (clipped): 22.000, Pasos: 642, Mean Reward Calculado: 0.034268 (Recompensa/Pasos)\n",
      " 263029/825189: episode: 403, duration: 27.552s, episode steps: 642, steps per second:  23, episode reward: 22.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.349 [0.000, 5.000],  loss: 0.033840, mae: 3.633590, mean_q: 4.379025, mean_eps: 0.100000\n",
      "📈 Episodio 404: Recompensa total (clipped): 18.000, Pasos: 420, Mean Reward Calculado: 0.042857 (Recompensa/Pasos)\n",
      " 263449/825189: episode: 404, duration: 18.070s, episode steps: 420, steps per second:  23, episode reward: 18.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.445 [0.000, 5.000],  loss: 0.034475, mae: 3.615888, mean_q: 4.357704, mean_eps: 0.100000\n",
      "📈 Episodio 405: Recompensa total (clipped): 28.000, Pasos: 868, Mean Reward Calculado: 0.032258 (Recompensa/Pasos)\n",
      " 264317/825189: episode: 405, duration: 37.523s, episode steps: 868, steps per second:  23, episode reward: 28.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.271 [0.000, 5.000],  loss: 0.033872, mae: 3.617422, mean_q: 4.359705, mean_eps: 0.100000\n",
      "📈 Episodio 406: Recompensa total (clipped): 31.000, Pasos: 882, Mean Reward Calculado: 0.035147 (Recompensa/Pasos)\n",
      " 265199/825189: episode: 406, duration: 37.871s, episode steps: 882, steps per second:  23, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.035903, mae: 3.623383, mean_q: 4.364183, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 407: Recompensa total (clipped): 25.000, Pasos: 546, Mean Reward Calculado: 0.045788 (Recompensa/Pasos)\n",
      " 265745/825189: episode: 407, duration: 23.478s, episode steps: 546, steps per second:  23, episode reward: 25.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.032830, mae: 3.671682, mean_q: 4.423701, mean_eps: 0.100000\n",
      "📈 Episodio 408: Recompensa total (clipped): 26.000, Pasos: 730, Mean Reward Calculado: 0.035616 (Recompensa/Pasos)\n",
      " 266475/825189: episode: 408, duration: 31.467s, episode steps: 730, steps per second:  23, episode reward: 26.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.033850, mae: 3.582655, mean_q: 4.315739, mean_eps: 0.100000\n",
      "📈 Episodio 409: Recompensa total (clipped): 31.000, Pasos: 654, Mean Reward Calculado: 0.047401 (Recompensa/Pasos)\n",
      " 267129/825189: episode: 409, duration: 28.483s, episode steps: 654, steps per second:  23, episode reward: 31.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.040493, mae: 3.603283, mean_q: 4.342683, mean_eps: 0.100000\n",
      "📈 Episodio 410: Recompensa total (clipped): 10.000, Pasos: 312, Mean Reward Calculado: 0.032051 (Recompensa/Pasos)\n",
      " 267441/825189: episode: 410, duration: 13.474s, episode steps: 312, steps per second:  23, episode reward: 10.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.039230, mae: 3.634158, mean_q: 4.378616, mean_eps: 0.100000\n",
      "📈 Episodio 411: Recompensa total (clipped): 33.000, Pasos: 842, Mean Reward Calculado: 0.039192 (Recompensa/Pasos)\n",
      " 268283/825189: episode: 411, duration: 36.238s, episode steps: 842, steps per second:  23, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.034401, mae: 3.588330, mean_q: 4.324059, mean_eps: 0.100000\n",
      "📈 Episodio 412: Recompensa total (clipped): 29.000, Pasos: 816, Mean Reward Calculado: 0.035539 (Recompensa/Pasos)\n",
      " 269099/825189: episode: 412, duration: 35.210s, episode steps: 816, steps per second:  23, episode reward: 29.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.039657, mae: 3.594360, mean_q: 4.329455, mean_eps: 0.100000\n",
      "📈 Episodio 413: Recompensa total (clipped): 15.000, Pasos: 416, Mean Reward Calculado: 0.036058 (Recompensa/Pasos)\n",
      " 269515/825189: episode: 413, duration: 17.878s, episode steps: 416, steps per second:  23, episode reward: 15.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.032179, mae: 3.614853, mean_q: 4.355098, mean_eps: 0.100000\n",
      "📈 Episodio 414: Recompensa total (clipped): 28.000, Pasos: 628, Mean Reward Calculado: 0.044586 (Recompensa/Pasos)\n",
      " 270143/825189: episode: 414, duration: 27.257s, episode steps: 628, steps per second:  23, episode reward: 28.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.232 [0.000, 5.000],  loss: 0.035420, mae: 3.587427, mean_q: 4.322461, mean_eps: 0.100000\n",
      "📈 Episodio 415: Recompensa total (clipped): 15.000, Pasos: 430, Mean Reward Calculado: 0.034884 (Recompensa/Pasos)\n",
      " 270573/825189: episode: 415, duration: 18.661s, episode steps: 430, steps per second:  23, episode reward: 15.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.667 [0.000, 5.000],  loss: 0.031806, mae: 3.695687, mean_q: 4.455320, mean_eps: 0.100000\n",
      "📈 Episodio 416: Recompensa total (clipped): 18.000, Pasos: 519, Mean Reward Calculado: 0.034682 (Recompensa/Pasos)\n",
      " 271092/825189: episode: 416, duration: 22.410s, episode steps: 519, steps per second:  23, episode reward: 18.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.032639, mae: 3.644548, mean_q: 4.391733, mean_eps: 0.100000\n",
      "📈 Episodio 417: Recompensa total (clipped): 11.000, Pasos: 368, Mean Reward Calculado: 0.029891 (Recompensa/Pasos)\n",
      " 271460/825189: episode: 417, duration: 16.227s, episode steps: 368, steps per second:  23, episode reward: 11.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.041991, mae: 3.628387, mean_q: 4.369759, mean_eps: 0.100000\n",
      "📈 Episodio 418: Recompensa total (clipped): 27.000, Pasos: 671, Mean Reward Calculado: 0.040238 (Recompensa/Pasos)\n",
      " 272131/825189: episode: 418, duration: 29.185s, episode steps: 671, steps per second:  23, episode reward: 27.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.034310, mae: 3.663253, mean_q: 4.412823, mean_eps: 0.100000\n",
      "📈 Episodio 419: Recompensa total (clipped): 16.000, Pasos: 467, Mean Reward Calculado: 0.034261 (Recompensa/Pasos)\n",
      " 272598/825189: episode: 419, duration: 20.189s, episode steps: 467, steps per second:  23, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.197 [0.000, 5.000],  loss: 0.041080, mae: 3.640502, mean_q: 4.385257, mean_eps: 0.100000\n",
      "📈 Episodio 420: Recompensa total (clipped): 20.000, Pasos: 483, Mean Reward Calculado: 0.041408 (Recompensa/Pasos)\n",
      " 273081/825189: episode: 420, duration: 21.095s, episode steps: 483, steps per second:  23, episode reward: 20.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.033439, mae: 3.616462, mean_q: 4.355851, mean_eps: 0.100000\n",
      "📈 Episodio 421: Recompensa total (clipped): 28.000, Pasos: 659, Mean Reward Calculado: 0.042489 (Recompensa/Pasos)\n",
      " 273740/825189: episode: 421, duration: 28.863s, episode steps: 659, steps per second:  23, episode reward: 28.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.885 [0.000, 5.000],  loss: 0.037140, mae: 3.641023, mean_q: 4.386776, mean_eps: 0.100000\n",
      "📈 Episodio 422: Recompensa total (clipped): 14.000, Pasos: 363, Mean Reward Calculado: 0.038567 (Recompensa/Pasos)\n",
      " 274103/825189: episode: 422, duration: 15.577s, episode steps: 363, steps per second:  23, episode reward: 14.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.028702, mae: 3.640159, mean_q: 4.385407, mean_eps: 0.100000\n",
      "📈 Episodio 423: Recompensa total (clipped): 21.000, Pasos: 539, Mean Reward Calculado: 0.038961 (Recompensa/Pasos)\n",
      " 274642/825189: episode: 423, duration: 23.841s, episode steps: 539, steps per second:  23, episode reward: 21.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.029472, mae: 3.635227, mean_q: 4.379303, mean_eps: 0.100000\n",
      "📈 Episodio 424: Recompensa total (clipped): 30.000, Pasos: 842, Mean Reward Calculado: 0.035629 (Recompensa/Pasos)\n",
      " 275484/825189: episode: 424, duration: 36.668s, episode steps: 842, steps per second:  23, episode reward: 30.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.037449, mae: 3.670549, mean_q: 4.419511, mean_eps: 0.100000\n",
      "📈 Episodio 425: Recompensa total (clipped): 27.000, Pasos: 691, Mean Reward Calculado: 0.039074 (Recompensa/Pasos)\n",
      " 276175/825189: episode: 425, duration: 30.227s, episode steps: 691, steps per second:  23, episode reward: 27.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.038005, mae: 3.653637, mean_q: 4.399481, mean_eps: 0.100000\n",
      "📈 Episodio 426: Recompensa total (clipped): 23.000, Pasos: 610, Mean Reward Calculado: 0.037705 (Recompensa/Pasos)\n",
      " 276785/825189: episode: 426, duration: 26.421s, episode steps: 610, steps per second:  23, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.035628, mae: 3.677031, mean_q: 4.427408, mean_eps: 0.100000\n",
      "📈 Episodio 427: Recompensa total (clipped): 25.000, Pasos: 625, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 277410/825189: episode: 427, duration: 27.186s, episode steps: 625, steps per second:  23, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.034280, mae: 3.677278, mean_q: 4.429667, mean_eps: 0.100000\n",
      "📈 Episodio 428: Recompensa total (clipped): 30.000, Pasos: 699, Mean Reward Calculado: 0.042918 (Recompensa/Pasos)\n",
      " 278109/825189: episode: 428, duration: 30.220s, episode steps: 699, steps per second:  23, episode reward: 30.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.082 [0.000, 5.000],  loss: 0.035402, mae: 3.637000, mean_q: 4.379643, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 429: Recompensa total (clipped): 18.000, Pasos: 510, Mean Reward Calculado: 0.035294 (Recompensa/Pasos)\n",
      " 278619/825189: episode: 429, duration: 22.125s, episode steps: 510, steps per second:  23, episode reward: 18.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.371 [0.000, 5.000],  loss: 0.030233, mae: 3.648291, mean_q: 4.394934, mean_eps: 0.100000\n",
      "📈 Episodio 430: Recompensa total (clipped): 20.000, Pasos: 460, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 279079/825189: episode: 430, duration: 19.847s, episode steps: 460, steps per second:  23, episode reward: 20.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.040008, mae: 3.636697, mean_q: 4.377290, mean_eps: 0.100000\n",
      "📈 Episodio 431: Recompensa total (clipped): 24.000, Pasos: 565, Mean Reward Calculado: 0.042478 (Recompensa/Pasos)\n",
      " 279644/825189: episode: 431, duration: 24.855s, episode steps: 565, steps per second:  23, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.036886, mae: 3.666562, mean_q: 4.414872, mean_eps: 0.100000\n",
      "📊 Paso 280,000/2,000,000 (14.0%) - 24.2 pasos/seg - ETA: 19.8h - Memoria: 8892.61 MB\n",
      "📈 Episodio 432: Recompensa total (clipped): 22.000, Pasos: 626, Mean Reward Calculado: 0.035144 (Recompensa/Pasos)\n",
      " 280270/825189: episode: 432, duration: 27.260s, episode steps: 626, steps per second:  23, episode reward: 22.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.203 [0.000, 5.000],  loss: 0.042436, mae: 3.670236, mean_q: 4.421595, mean_eps: 0.100000\n",
      "📈 Episodio 433: Recompensa total (clipped): 18.000, Pasos: 476, Mean Reward Calculado: 0.037815 (Recompensa/Pasos)\n",
      " 280746/825189: episode: 433, duration: 20.829s, episode steps: 476, steps per second:  23, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.363 [0.000, 5.000],  loss: 0.033560, mae: 3.688080, mean_q: 4.441605, mean_eps: 0.100000\n",
      "📈 Episodio 434: Recompensa total (clipped): 32.000, Pasos: 791, Mean Reward Calculado: 0.040455 (Recompensa/Pasos)\n",
      " 281537/825189: episode: 434, duration: 34.206s, episode steps: 791, steps per second:  23, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.036691, mae: 3.707754, mean_q: 4.465675, mean_eps: 0.100000\n",
      "📈 Episodio 435: Recompensa total (clipped): 21.000, Pasos: 574, Mean Reward Calculado: 0.036585 (Recompensa/Pasos)\n",
      " 282111/825189: episode: 435, duration: 24.933s, episode steps: 574, steps per second:  23, episode reward: 21.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.034688, mae: 3.669006, mean_q: 4.419195, mean_eps: 0.100000\n",
      "📈 Episodio 436: Recompensa total (clipped): 31.000, Pasos: 844, Mean Reward Calculado: 0.036730 (Recompensa/Pasos)\n",
      " 282955/825189: episode: 436, duration: 36.373s, episode steps: 844, steps per second:  23, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.424 [0.000, 5.000],  loss: 0.035827, mae: 3.683569, mean_q: 4.435877, mean_eps: 0.100000\n",
      "📈 Episodio 437: Recompensa total (clipped): 27.000, Pasos: 572, Mean Reward Calculado: 0.047203 (Recompensa/Pasos)\n",
      " 283527/825189: episode: 437, duration: 24.468s, episode steps: 572, steps per second:  23, episode reward: 27.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.043163, mae: 3.627143, mean_q: 4.365044, mean_eps: 0.100000\n",
      "📈 Episodio 438: Recompensa total (clipped): 15.000, Pasos: 444, Mean Reward Calculado: 0.033784 (Recompensa/Pasos)\n",
      " 283971/825189: episode: 438, duration: 19.303s, episode steps: 444, steps per second:  23, episode reward: 15.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.977 [0.000, 5.000],  loss: 0.039797, mae: 3.686540, mean_q: 4.437584, mean_eps: 0.100000\n",
      "📈 Episodio 439: Recompensa total (clipped): 28.000, Pasos: 700, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 284671/825189: episode: 439, duration: 30.427s, episode steps: 700, steps per second:  23, episode reward: 28.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.037180, mae: 3.681392, mean_q: 4.433436, mean_eps: 0.100000\n",
      "📈 Episodio 440: Recompensa total (clipped): 26.000, Pasos: 757, Mean Reward Calculado: 0.034346 (Recompensa/Pasos)\n",
      " 285428/825189: episode: 440, duration: 32.779s, episode steps: 757, steps per second:  23, episode reward: 26.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.041541, mae: 3.671125, mean_q: 4.418378, mean_eps: 0.100000\n",
      "📈 Episodio 441: Recompensa total (clipped): 34.000, Pasos: 852, Mean Reward Calculado: 0.039906 (Recompensa/Pasos)\n",
      " 286280/825189: episode: 441, duration: 36.939s, episode steps: 852, steps per second:  23, episode reward: 34.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.040686, mae: 3.681259, mean_q: 4.434392, mean_eps: 0.100000\n",
      "📈 Episodio 442: Recompensa total (clipped): 17.000, Pasos: 430, Mean Reward Calculado: 0.039535 (Recompensa/Pasos)\n",
      " 286710/825189: episode: 442, duration: 18.848s, episode steps: 430, steps per second:  23, episode reward: 17.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.032988, mae: 3.652157, mean_q: 4.398425, mean_eps: 0.100000\n",
      "📈 Episodio 443: Recompensa total (clipped): 20.000, Pasos: 616, Mean Reward Calculado: 0.032468 (Recompensa/Pasos)\n",
      " 287326/825189: episode: 443, duration: 26.638s, episode steps: 616, steps per second:  23, episode reward: 20.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.034933, mae: 3.678463, mean_q: 4.430044, mean_eps: 0.100000\n",
      "📈 Episodio 444: Recompensa total (clipped): 12.000, Pasos: 295, Mean Reward Calculado: 0.040678 (Recompensa/Pasos)\n",
      " 287621/825189: episode: 444, duration: 12.871s, episode steps: 295, steps per second:  23, episode reward: 12.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.035007, mae: 3.682276, mean_q: 4.432746, mean_eps: 0.100000\n",
      "📈 Episodio 445: Recompensa total (clipped): 33.000, Pasos: 1000, Mean Reward Calculado: 0.033000 (Recompensa/Pasos)\n",
      " 288621/825189: episode: 445, duration: 43.432s, episode steps: 1000, steps per second:  23, episode reward: 33.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.039534, mae: 3.665322, mean_q: 4.411618, mean_eps: 0.100000\n",
      "📈 Episodio 446: Recompensa total (clipped): 32.000, Pasos: 869, Mean Reward Calculado: 0.036824 (Recompensa/Pasos)\n",
      " 289490/825189: episode: 446, duration: 37.956s, episode steps: 869, steps per second:  23, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.034476, mae: 3.643476, mean_q: 4.386681, mean_eps: 0.100000\n",
      "📈 Episodio 447: Recompensa total (clipped): 31.000, Pasos: 977, Mean Reward Calculado: 0.031730 (Recompensa/Pasos)\n",
      " 290467/825189: episode: 447, duration: 42.392s, episode steps: 977, steps per second:  23, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.036544, mae: 3.683790, mean_q: 4.439621, mean_eps: 0.100000\n",
      "📈 Episodio 448: Recompensa total (clipped): 29.000, Pasos: 710, Mean Reward Calculado: 0.040845 (Recompensa/Pasos)\n",
      " 291177/825189: episode: 448, duration: 31.270s, episode steps: 710, steps per second:  23, episode reward: 29.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.030204, mae: 3.697075, mean_q: 4.458251, mean_eps: 0.100000\n",
      "📈 Episodio 449: Recompensa total (clipped): 30.000, Pasos: 822, Mean Reward Calculado: 0.036496 (Recompensa/Pasos)\n",
      " 291999/825189: episode: 449, duration: 35.438s, episode steps: 822, steps per second:  23, episode reward: 30.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.039510, mae: 3.701184, mean_q: 4.458969, mean_eps: 0.100000\n",
      "📈 Episodio 450: Recompensa total (clipped): 31.000, Pasos: 930, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 450, pasos: 292929)\n",
      "💾 NUEVO MEJOR PROMEDIO: 23.17 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 450 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 31.00\n",
      "   Media últimos 100: 23.17 / 20.0\n",
      "   Mejor promedio histórico: 23.17\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 360\n",
      "   Episodios consecutivos en objetivo: 360\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 360 episodios consecutivos\n",
      " 292929/825189: episode: 450, duration: 79.547s, episode steps: 930, steps per second:  12, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.034130, mae: 3.663151, mean_q: 4.412935, mean_eps: 0.100000\n",
      "📈 Episodio 451: Recompensa total (clipped): 10.000, Pasos: 267, Mean Reward Calculado: 0.037453 (Recompensa/Pasos)\n",
      " 293196/825189: episode: 451, duration: 11.900s, episode steps: 267, steps per second:  22, episode reward: 10.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.031144, mae: 3.664412, mean_q: 4.415212, mean_eps: 0.100000\n",
      "📈 Episodio 452: Recompensa total (clipped): 16.000, Pasos: 457, Mean Reward Calculado: 0.035011 (Recompensa/Pasos)\n",
      " 293653/825189: episode: 452, duration: 20.172s, episode steps: 457, steps per second:  23, episode reward: 16.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.041490, mae: 3.645019, mean_q: 4.389976, mean_eps: 0.100000\n",
      "📈 Episodio 453: Recompensa total (clipped): 18.000, Pasos: 475, Mean Reward Calculado: 0.037895 (Recompensa/Pasos)\n",
      " 294128/825189: episode: 453, duration: 21.105s, episode steps: 475, steps per second:  23, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.037254, mae: 3.693790, mean_q: 4.449623, mean_eps: 0.100000\n",
      "📈 Episodio 454: Recompensa total (clipped): 27.000, Pasos: 725, Mean Reward Calculado: 0.037241 (Recompensa/Pasos)\n",
      " 294853/825189: episode: 454, duration: 31.522s, episode steps: 725, steps per second:  23, episode reward: 27.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.043 [0.000, 5.000],  loss: 0.035408, mae: 3.658127, mean_q: 4.406629, mean_eps: 0.100000\n",
      "📈 Episodio 455: Recompensa total (clipped): 17.000, Pasos: 445, Mean Reward Calculado: 0.038202 (Recompensa/Pasos)\n",
      " 295298/825189: episode: 455, duration: 19.296s, episode steps: 445, steps per second:  23, episode reward: 17.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.039546, mae: 3.695622, mean_q: 4.450119, mean_eps: 0.100000\n",
      "📈 Episodio 456: Recompensa total (clipped): 25.000, Pasos: 681, Mean Reward Calculado: 0.036711 (Recompensa/Pasos)\n",
      " 295979/825189: episode: 456, duration: 29.814s, episode steps: 681, steps per second:  23, episode reward: 25.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.037844, mae: 3.670355, mean_q: 4.420242, mean_eps: 0.100000\n",
      "📈 Episodio 457: Recompensa total (clipped): 27.000, Pasos: 628, Mean Reward Calculado: 0.042994 (Recompensa/Pasos)\n",
      " 296607/825189: episode: 457, duration: 27.658s, episode steps: 628, steps per second:  23, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.346 [0.000, 5.000],  loss: 0.034685, mae: 3.657695, mean_q: 4.406415, mean_eps: 0.100000\n",
      "📈 Episodio 458: Recompensa total (clipped): 18.000, Pasos: 480, Mean Reward Calculado: 0.037500 (Recompensa/Pasos)\n",
      " 297087/825189: episode: 458, duration: 21.016s, episode steps: 480, steps per second:  23, episode reward: 18.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.615 [0.000, 5.000],  loss: 0.039277, mae: 3.664434, mean_q: 4.415099, mean_eps: 0.100000\n",
      "📈 Episodio 459: Recompensa total (clipped): 29.000, Pasos: 736, Mean Reward Calculado: 0.039402 (Recompensa/Pasos)\n",
      " 297823/825189: episode: 459, duration: 32.037s, episode steps: 736, steps per second:  23, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.418 [0.000, 5.000],  loss: 0.035265, mae: 3.641820, mean_q: 4.388826, mean_eps: 0.100000\n",
      "📈 Episodio 460: Recompensa total (clipped): 28.000, Pasos: 665, Mean Reward Calculado: 0.042105 (Recompensa/Pasos)\n",
      " 298488/825189: episode: 460, duration: 29.480s, episode steps: 665, steps per second:  23, episode reward: 28.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.165 [0.000, 5.000],  loss: 0.037658, mae: 3.660399, mean_q: 4.410198, mean_eps: 0.100000\n",
      "📈 Episodio 461: Recompensa total (clipped): 24.000, Pasos: 538, Mean Reward Calculado: 0.044610 (Recompensa/Pasos)\n",
      " 299026/825189: episode: 461, duration: 23.638s, episode steps: 538, steps per second:  23, episode reward: 24.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.036709, mae: 3.643640, mean_q: 4.388889, mean_eps: 0.100000\n",
      "📈 Episodio 462: Recompensa total (clipped): 14.000, Pasos: 383, Mean Reward Calculado: 0.036554 (Recompensa/Pasos)\n",
      " 299409/825189: episode: 462, duration: 16.674s, episode steps: 383, steps per second:  23, episode reward: 14.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.034013, mae: 3.658032, mean_q: 4.405113, mean_eps: 0.100000\n",
      "📊 Paso 300,000/2,000,000 (15.0%) - 24.0 pasos/seg - ETA: 19.7h - Memoria: 9052.75 MB\n",
      "📈 Episodio 463: Recompensa total (clipped): 29.000, Pasos: 1027, Mean Reward Calculado: 0.028238 (Recompensa/Pasos)\n",
      " 300436/825189: episode: 463, duration: 45.194s, episode steps: 1027, steps per second:  23, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.031259, mae: 3.658952, mean_q: 4.408212, mean_eps: 0.100000\n",
      "📈 Episodio 464: Recompensa total (clipped): 32.000, Pasos: 912, Mean Reward Calculado: 0.035088 (Recompensa/Pasos)\n",
      " 301348/825189: episode: 464, duration: 39.892s, episode steps: 912, steps per second:  23, episode reward: 32.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.037553, mae: 3.679947, mean_q: 4.430378, mean_eps: 0.100000\n",
      "📈 Episodio 465: Recompensa total (clipped): 34.000, Pasos: 873, Mean Reward Calculado: 0.038946 (Recompensa/Pasos)\n",
      " 302221/825189: episode: 465, duration: 38.549s, episode steps: 873, steps per second:  23, episode reward: 34.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.041509, mae: 3.705564, mean_q: 4.458215, mean_eps: 0.100000\n",
      "📈 Episodio 466: Recompensa total (clipped): 26.000, Pasos: 678, Mean Reward Calculado: 0.038348 (Recompensa/Pasos)\n",
      " 302899/825189: episode: 466, duration: 29.271s, episode steps: 678, steps per second:  23, episode reward: 26.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.295 [0.000, 5.000],  loss: 0.034452, mae: 3.659449, mean_q: 4.405859, mean_eps: 0.100000\n",
      "📈 Episodio 467: Recompensa total (clipped): 15.000, Pasos: 400, Mean Reward Calculado: 0.037500 (Recompensa/Pasos)\n",
      " 303299/825189: episode: 467, duration: 17.486s, episode steps: 400, steps per second:  23, episode reward: 15.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.033590, mae: 3.721414, mean_q: 4.479806, mean_eps: 0.100000\n",
      "📈 Episodio 468: Recompensa total (clipped): 33.000, Pasos: 852, Mean Reward Calculado: 0.038732 (Recompensa/Pasos)\n",
      " 304151/825189: episode: 468, duration: 37.775s, episode steps: 852, steps per second:  23, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.041246, mae: 3.673597, mean_q: 4.423465, mean_eps: 0.100000\n",
      "📈 Episodio 469: Recompensa total (clipped): 20.000, Pasos: 482, Mean Reward Calculado: 0.041494 (Recompensa/Pasos)\n",
      " 304633/825189: episode: 469, duration: 21.346s, episode steps: 482, steps per second:  23, episode reward: 20.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.034775, mae: 3.634271, mean_q: 4.375153, mean_eps: 0.100000\n",
      "📈 Episodio 470: Recompensa total (clipped): 27.000, Pasos: 610, Mean Reward Calculado: 0.044262 (Recompensa/Pasos)\n",
      " 305243/825189: episode: 470, duration: 26.402s, episode steps: 610, steps per second:  23, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.036091, mae: 3.679799, mean_q: 4.430967, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 471: Recompensa total (clipped): 29.000, Pasos: 640, Mean Reward Calculado: 0.045312 (Recompensa/Pasos)\n",
      " 305883/825189: episode: 471, duration: 28.140s, episode steps: 640, steps per second:  23, episode reward: 29.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.036987, mae: 3.639320, mean_q: 4.382701, mean_eps: 0.100000\n",
      "📈 Episodio 472: Recompensa total (clipped): 28.000, Pasos: 634, Mean Reward Calculado: 0.044164 (Recompensa/Pasos)\n",
      " 306517/825189: episode: 472, duration: 27.718s, episode steps: 634, steps per second:  23, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.040740, mae: 3.643851, mean_q: 4.384627, mean_eps: 0.100000\n",
      "📈 Episodio 473: Recompensa total (clipped): 26.000, Pasos: 538, Mean Reward Calculado: 0.048327 (Recompensa/Pasos)\n",
      " 307055/825189: episode: 473, duration: 23.117s, episode steps: 538, steps per second:  23, episode reward: 26.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.036428, mae: 3.663436, mean_q: 4.411606, mean_eps: 0.100000\n",
      "📈 Episodio 474: Recompensa total (clipped): 21.000, Pasos: 464, Mean Reward Calculado: 0.045259 (Recompensa/Pasos)\n",
      " 307519/825189: episode: 474, duration: 20.149s, episode steps: 464, steps per second:  23, episode reward: 21.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.030448, mae: 3.647369, mean_q: 4.392062, mean_eps: 0.100000\n",
      "📈 Episodio 475: Recompensa total (clipped): 34.000, Pasos: 887, Mean Reward Calculado: 0.038331 (Recompensa/Pasos)\n",
      " 308406/825189: episode: 475, duration: 38.063s, episode steps: 887, steps per second:  23, episode reward: 34.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.039988, mae: 3.677355, mean_q: 4.426036, mean_eps: 0.100000\n",
      "📈 Episodio 476: Recompensa total (clipped): 24.000, Pasos: 688, Mean Reward Calculado: 0.034884 (Recompensa/Pasos)\n",
      " 309094/825189: episode: 476, duration: 30.333s, episode steps: 688, steps per second:  23, episode reward: 24.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.042213, mae: 3.657831, mean_q: 4.401982, mean_eps: 0.100000\n",
      "📈 Episodio 477: Recompensa total (clipped): 27.000, Pasos: 604, Mean Reward Calculado: 0.044702 (Recompensa/Pasos)\n",
      " 309698/825189: episode: 477, duration: 26.406s, episode steps: 604, steps per second:  23, episode reward: 27.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.076 [0.000, 5.000],  loss: 0.034123, mae: 3.628327, mean_q: 4.367543, mean_eps: 0.100000\n",
      "📈 Episodio 478: Recompensa total (clipped): 32.000, Pasos: 751, Mean Reward Calculado: 0.042610 (Recompensa/Pasos)\n",
      " 310449/825189: episode: 478, duration: 32.745s, episode steps: 751, steps per second:  23, episode reward: 32.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.036707, mae: 3.681310, mean_q: 4.433574, mean_eps: 0.100000\n",
      "📈 Episodio 479: Recompensa total (clipped): 27.000, Pasos: 747, Mean Reward Calculado: 0.036145 (Recompensa/Pasos)\n",
      " 311196/825189: episode: 479, duration: 32.682s, episode steps: 747, steps per second:  23, episode reward: 27.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.038198, mae: 3.704809, mean_q: 4.461618, mean_eps: 0.100000\n",
      "📈 Episodio 480: Recompensa total (clipped): 12.000, Pasos: 351, Mean Reward Calculado: 0.034188 (Recompensa/Pasos)\n",
      " 311547/825189: episode: 480, duration: 15.375s, episode steps: 351, steps per second:  23, episode reward: 12.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.036620, mae: 3.736672, mean_q: 4.499095, mean_eps: 0.100000\n",
      "📈 Episodio 481: Recompensa total (clipped): 34.000, Pasos: 807, Mean Reward Calculado: 0.042131 (Recompensa/Pasos)\n",
      " 312354/825189: episode: 481, duration: 34.911s, episode steps: 807, steps per second:  23, episode reward: 34.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.035112, mae: 3.632261, mean_q: 4.374615, mean_eps: 0.100000\n",
      "📈 Episodio 482: Recompensa total (clipped): 25.000, Pasos: 529, Mean Reward Calculado: 0.047259 (Recompensa/Pasos)\n",
      " 312883/825189: episode: 482, duration: 22.987s, episode steps: 529, steps per second:  23, episode reward: 25.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.031348, mae: 3.621204, mean_q: 4.359498, mean_eps: 0.100000\n",
      "📈 Episodio 483: Recompensa total (clipped): 27.000, Pasos: 564, Mean Reward Calculado: 0.047872 (Recompensa/Pasos)\n",
      " 313447/825189: episode: 483, duration: 24.714s, episode steps: 564, steps per second:  23, episode reward: 27.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 3.117 [0.000, 5.000],  loss: 0.036657, mae: 3.629879, mean_q: 4.368105, mean_eps: 0.100000\n",
      "📈 Episodio 484: Recompensa total (clipped): 27.000, Pasos: 1050, Mean Reward Calculado: 0.025714 (Recompensa/Pasos)\n",
      " 314497/825189: episode: 484, duration: 45.754s, episode steps: 1050, steps per second:  23, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.034836, mae: 3.705662, mean_q: 4.459326, mean_eps: 0.100000\n",
      "📈 Episodio 485: Recompensa total (clipped): 27.000, Pasos: 704, Mean Reward Calculado: 0.038352 (Recompensa/Pasos)\n",
      " 315201/825189: episode: 485, duration: 31.037s, episode steps: 704, steps per second:  23, episode reward: 27.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.038660, mae: 3.702040, mean_q: 4.454146, mean_eps: 0.100000\n",
      "📈 Episodio 486: Recompensa total (clipped): 31.000, Pasos: 770, Mean Reward Calculado: 0.040260 (Recompensa/Pasos)\n",
      " 315971/825189: episode: 486, duration: 33.578s, episode steps: 770, steps per second:  23, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.036564, mae: 3.700036, mean_q: 4.452230, mean_eps: 0.100000\n",
      "📈 Episodio 487: Recompensa total (clipped): 31.000, Pasos: 980, Mean Reward Calculado: 0.031633 (Recompensa/Pasos)\n",
      " 316951/825189: episode: 487, duration: 42.902s, episode steps: 980, steps per second:  23, episode reward: 31.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.072 [0.000, 5.000],  loss: 0.037161, mae: 3.677379, mean_q: 4.425005, mean_eps: 0.100000\n",
      "📈 Episodio 488: Recompensa total (clipped): 28.000, Pasos: 645, Mean Reward Calculado: 0.043411 (Recompensa/Pasos)\n",
      " 317596/825189: episode: 488, duration: 28.438s, episode steps: 645, steps per second:  23, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.256 [0.000, 5.000],  loss: 0.030814, mae: 3.723893, mean_q: 4.481783, mean_eps: 0.100000\n",
      "📈 Episodio 489: Recompensa total (clipped): 19.000, Pasos: 472, Mean Reward Calculado: 0.040254 (Recompensa/Pasos)\n",
      " 318068/825189: episode: 489, duration: 20.529s, episode steps: 472, steps per second:  23, episode reward: 19.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.033914, mae: 3.663434, mean_q: 4.408243, mean_eps: 0.100000\n",
      "📈 Episodio 490: Recompensa total (clipped): 31.000, Pasos: 848, Mean Reward Calculado: 0.036557 (Recompensa/Pasos)\n",
      " 318916/825189: episode: 490, duration: 37.135s, episode steps: 848, steps per second:  23, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: 0.035273, mae: 3.664642, mean_q: 4.410428, mean_eps: 0.100000\n",
      "📈 Episodio 491: Recompensa total (clipped): 26.000, Pasos: 621, Mean Reward Calculado: 0.041868 (Recompensa/Pasos)\n",
      " 319537/825189: episode: 491, duration: 27.497s, episode steps: 621, steps per second:  23, episode reward: 26.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.036062, mae: 3.613862, mean_q: 4.349573, mean_eps: 0.100000\n",
      "📊 Paso 320,000/2,000,000 (16.0%) - 23.9 pasos/seg - ETA: 19.5h - Memoria: 9052.95 MB\n",
      "📈 Episodio 492: Recompensa total (clipped): 32.000, Pasos: 800, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 320337/825189: episode: 492, duration: 35.333s, episode steps: 800, steps per second:  23, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.033574, mae: 3.656601, mean_q: 4.402361, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 493: Recompensa total (clipped): 18.000, Pasos: 481, Mean Reward Calculado: 0.037422 (Recompensa/Pasos)\n",
      " 320818/825189: episode: 493, duration: 21.084s, episode steps: 481, steps per second:  23, episode reward: 18.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.805 [0.000, 5.000],  loss: 0.030450, mae: 3.692084, mean_q: 4.443983, mean_eps: 0.100000\n",
      "📈 Episodio 494: Recompensa total (clipped): 32.000, Pasos: 799, Mean Reward Calculado: 0.040050 (Recompensa/Pasos)\n",
      " 321617/825189: episode: 494, duration: 35.353s, episode steps: 799, steps per second:  23, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.033581, mae: 3.680632, mean_q: 4.431389, mean_eps: 0.100000\n",
      "📈 Episodio 495: Recompensa total (clipped): 27.000, Pasos: 710, Mean Reward Calculado: 0.038028 (Recompensa/Pasos)\n",
      " 322327/825189: episode: 495, duration: 30.613s, episode steps: 710, steps per second:  23, episode reward: 27.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.645 [0.000, 5.000],  loss: 0.036665, mae: 3.707776, mean_q: 4.466535, mean_eps: 0.100000\n",
      "📈 Episodio 496: Recompensa total (clipped): 28.000, Pasos: 550, Mean Reward Calculado: 0.050909 (Recompensa/Pasos)\n",
      " 322877/825189: episode: 496, duration: 24.375s, episode steps: 550, steps per second:  23, episode reward: 28.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 3.582 [0.000, 5.000],  loss: 0.033208, mae: 3.697707, mean_q: 4.452661, mean_eps: 0.100000\n",
      "📈 Episodio 497: Recompensa total (clipped): 21.000, Pasos: 516, Mean Reward Calculado: 0.040698 (Recompensa/Pasos)\n",
      " 323393/825189: episode: 497, duration: 22.567s, episode steps: 516, steps per second:  23, episode reward: 21.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.510 [0.000, 5.000],  loss: 0.035203, mae: 3.657847, mean_q: 4.402881, mean_eps: 0.100000\n",
      "📈 Episodio 498: Recompensa total (clipped): 26.000, Pasos: 586, Mean Reward Calculado: 0.044369 (Recompensa/Pasos)\n",
      " 323979/825189: episode: 498, duration: 25.561s, episode steps: 586, steps per second:  23, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.754 [0.000, 5.000],  loss: 0.029392, mae: 3.648743, mean_q: 4.392893, mean_eps: 0.100000\n",
      "📈 Episodio 499: Recompensa total (clipped): 27.000, Pasos: 683, Mean Reward Calculado: 0.039531 (Recompensa/Pasos)\n",
      " 324662/825189: episode: 499, duration: 30.049s, episode steps: 683, steps per second:  23, episode reward: 27.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.337 [0.000, 5.000],  loss: 0.033143, mae: 3.714333, mean_q: 4.472970, mean_eps: 0.100000\n",
      "📈 Episodio 500: Recompensa total (clipped): 25.000, Pasos: 642, Mean Reward Calculado: 0.038941 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 500, pasos: 325304)\n",
      "💾 NUEVO MEJOR PROMEDIO: 24.69 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 500 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 25.00\n",
      "   Media últimos 100: 24.69 / 20.0\n",
      "   Mejor promedio histórico: 24.69\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 410\n",
      "   Episodios consecutivos en objetivo: 410\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 410 episodios consecutivos\n",
      " 325304/825189: episode: 500, duration: 72.236s, episode steps: 642, steps per second:   9, episode reward: 25.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.053 [0.000, 5.000],  loss: 0.036312, mae: 3.710500, mean_q: 4.464410, mean_eps: 0.100000\n",
      "📈 Episodio 501: Recompensa total (clipped): 17.000, Pasos: 448, Mean Reward Calculado: 0.037946 (Recompensa/Pasos)\n",
      " 325752/825189: episode: 501, duration: 20.298s, episode steps: 448, steps per second:  22, episode reward: 17.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.248 [0.000, 5.000],  loss: 0.041434, mae: 3.640919, mean_q: 4.379224, mean_eps: 0.100000\n",
      "📈 Episodio 502: Recompensa total (clipped): 31.000, Pasos: 806, Mean Reward Calculado: 0.038462 (Recompensa/Pasos)\n",
      " 326558/825189: episode: 502, duration: 35.605s, episode steps: 806, steps per second:  23, episode reward: 31.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.035472, mae: 3.666062, mean_q: 4.412572, mean_eps: 0.100000\n",
      "📈 Episodio 503: Recompensa total (clipped): 32.000, Pasos: 943, Mean Reward Calculado: 0.033934 (Recompensa/Pasos)\n",
      " 327501/825189: episode: 503, duration: 41.817s, episode steps: 943, steps per second:  23, episode reward: 32.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.017 [0.000, 5.000],  loss: 0.040680, mae: 3.677457, mean_q: 4.425869, mean_eps: 0.100000\n",
      "📈 Episodio 504: Recompensa total (clipped): 31.000, Pasos: 831, Mean Reward Calculado: 0.037304 (Recompensa/Pasos)\n",
      " 328332/825189: episode: 504, duration: 37.255s, episode steps: 831, steps per second:  22, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.039894, mae: 3.689649, mean_q: 4.438651, mean_eps: 0.100000\n",
      "📈 Episodio 505: Recompensa total (clipped): 35.000, Pasos: 841, Mean Reward Calculado: 0.041617 (Recompensa/Pasos)\n",
      " 329173/825189: episode: 505, duration: 37.374s, episode steps: 841, steps per second:  23, episode reward: 35.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.942 [0.000, 5.000],  loss: 0.034582, mae: 3.707174, mean_q: 4.462795, mean_eps: 0.100000\n",
      "📈 Episodio 506: Recompensa total (clipped): 27.000, Pasos: 633, Mean Reward Calculado: 0.042654 (Recompensa/Pasos)\n",
      " 329806/825189: episode: 506, duration: 28.260s, episode steps: 633, steps per second:  22, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.930 [0.000, 5.000],  loss: 0.043628, mae: 3.737121, mean_q: 4.498177, mean_eps: 0.100000\n",
      "📈 Episodio 507: Recompensa total (clipped): 24.000, Pasos: 543, Mean Reward Calculado: 0.044199 (Recompensa/Pasos)\n",
      " 330349/825189: episode: 507, duration: 24.358s, episode steps: 543, steps per second:  22, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.449 [0.000, 5.000],  loss: 0.034409, mae: 3.657973, mean_q: 4.403890, mean_eps: 0.100000\n",
      "📈 Episodio 508: Recompensa total (clipped): 35.000, Pasos: 924, Mean Reward Calculado: 0.037879 (Recompensa/Pasos)\n",
      " 331273/825189: episode: 508, duration: 41.212s, episode steps: 924, steps per second:  22, episode reward: 35.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.310 [0.000, 5.000],  loss: 0.035783, mae: 3.743692, mean_q: 4.507284, mean_eps: 0.100000\n",
      "📈 Episodio 509: Recompensa total (clipped): 26.000, Pasos: 663, Mean Reward Calculado: 0.039216 (Recompensa/Pasos)\n",
      " 331936/825189: episode: 509, duration: 29.306s, episode steps: 663, steps per second:  23, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.505 [0.000, 5.000],  loss: 0.039189, mae: 3.674884, mean_q: 4.423046, mean_eps: 0.100000\n",
      "📈 Episodio 510: Recompensa total (clipped): 23.000, Pasos: 615, Mean Reward Calculado: 0.037398 (Recompensa/Pasos)\n",
      " 332551/825189: episode: 510, duration: 27.304s, episode steps: 615, steps per second:  23, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.167 [0.000, 5.000],  loss: 0.037707, mae: 3.716775, mean_q: 4.474550, mean_eps: 0.100000\n",
      "📈 Episodio 511: Recompensa total (clipped): 28.000, Pasos: 714, Mean Reward Calculado: 0.039216 (Recompensa/Pasos)\n",
      " 333265/825189: episode: 511, duration: 32.180s, episode steps: 714, steps per second:  22, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.102 [0.000, 5.000],  loss: 0.033013, mae: 3.703154, mean_q: 4.455554, mean_eps: 0.100000\n",
      "📈 Episodio 512: Recompensa total (clipped): 29.000, Pasos: 642, Mean Reward Calculado: 0.045171 (Recompensa/Pasos)\n",
      " 333907/825189: episode: 512, duration: 28.204s, episode steps: 642, steps per second:  23, episode reward: 29.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.036062, mae: 3.691234, mean_q: 4.444607, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 513: Recompensa total (clipped): 26.000, Pasos: 745, Mean Reward Calculado: 0.034899 (Recompensa/Pasos)\n",
      " 334652/825189: episode: 513, duration: 32.549s, episode steps: 745, steps per second:  23, episode reward: 26.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.036678, mae: 3.695744, mean_q: 4.447615, mean_eps: 0.100000\n",
      "📈 Episodio 514: Recompensa total (clipped): 19.000, Pasos: 416, Mean Reward Calculado: 0.045673 (Recompensa/Pasos)\n",
      " 335068/825189: episode: 514, duration: 18.473s, episode steps: 416, steps per second:  23, episode reward: 19.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.033905, mae: 3.691946, mean_q: 4.443828, mean_eps: 0.100000\n",
      "📈 Episodio 515: Recompensa total (clipped): 22.000, Pasos: 549, Mean Reward Calculado: 0.040073 (Recompensa/Pasos)\n",
      " 335617/825189: episode: 515, duration: 24.628s, episode steps: 549, steps per second:  22, episode reward: 22.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.037045, mae: 3.693820, mean_q: 4.445052, mean_eps: 0.100000\n",
      "📈 Episodio 516: Recompensa total (clipped): 25.000, Pasos: 540, Mean Reward Calculado: 0.046296 (Recompensa/Pasos)\n",
      " 336157/825189: episode: 516, duration: 23.996s, episode steps: 540, steps per second:  23, episode reward: 25.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.030923, mae: 3.734102, mean_q: 4.494524, mean_eps: 0.100000\n",
      "📈 Episodio 517: Recompensa total (clipped): 23.000, Pasos: 612, Mean Reward Calculado: 0.037582 (Recompensa/Pasos)\n",
      " 336769/825189: episode: 517, duration: 27.176s, episode steps: 612, steps per second:  23, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.038326, mae: 3.703092, mean_q: 4.454902, mean_eps: 0.100000\n",
      "📈 Episodio 518: Recompensa total (clipped): 13.000, Pasos: 346, Mean Reward Calculado: 0.037572 (Recompensa/Pasos)\n",
      " 337115/825189: episode: 518, duration: 15.314s, episode steps: 346, steps per second:  23, episode reward: 13.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.031290, mae: 3.694543, mean_q: 4.447829, mean_eps: 0.100000\n",
      "📈 Episodio 519: Recompensa total (clipped): 13.000, Pasos: 344, Mean Reward Calculado: 0.037791 (Recompensa/Pasos)\n",
      " 337459/825189: episode: 519, duration: 15.192s, episode steps: 344, steps per second:  23, episode reward: 13.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.035 [0.000, 5.000],  loss: 0.037513, mae: 3.721151, mean_q: 4.478777, mean_eps: 0.100000\n",
      "📈 Episodio 520: Recompensa total (clipped): 15.000, Pasos: 380, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 337839/825189: episode: 520, duration: 16.861s, episode steps: 380, steps per second:  23, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.031650, mae: 3.700562, mean_q: 4.455087, mean_eps: 0.100000\n",
      "📈 Episodio 521: Recompensa total (clipped): 31.000, Pasos: 770, Mean Reward Calculado: 0.040260 (Recompensa/Pasos)\n",
      " 338609/825189: episode: 521, duration: 33.979s, episode steps: 770, steps per second:  23, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.034414, mae: 3.665453, mean_q: 4.414797, mean_eps: 0.100000\n",
      "📈 Episodio 522: Recompensa total (clipped): 31.000, Pasos: 667, Mean Reward Calculado: 0.046477 (Recompensa/Pasos)\n",
      " 339276/825189: episode: 522, duration: 29.495s, episode steps: 667, steps per second:  23, episode reward: 31.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.037388, mae: 3.682108, mean_q: 4.431489, mean_eps: 0.100000\n",
      "📈 Episodio 523: Recompensa total (clipped): 25.000, Pasos: 487, Mean Reward Calculado: 0.051335 (Recompensa/Pasos)\n",
      " 339763/825189: episode: 523, duration: 21.351s, episode steps: 487, steps per second:  23, episode reward: 25.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 3.047 [0.000, 5.000],  loss: 0.034710, mae: 3.667413, mean_q: 4.414824, mean_eps: 0.100000\n",
      "📊 Paso 340,000/2,000,000 (17.0%) - 23.8 pasos/seg - ETA: 19.4h - Memoria: 9040.16 MB\n",
      "📈 Episodio 524: Recompensa total (clipped): 11.000, Pasos: 284, Mean Reward Calculado: 0.038732 (Recompensa/Pasos)\n",
      " 340047/825189: episode: 524, duration: 12.306s, episode steps: 284, steps per second:  23, episode reward: 11.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.037101, mae: 3.686189, mean_q: 4.435115, mean_eps: 0.100000\n",
      "📈 Episodio 525: Recompensa total (clipped): 17.000, Pasos: 421, Mean Reward Calculado: 0.040380 (Recompensa/Pasos)\n",
      " 340468/825189: episode: 525, duration: 18.694s, episode steps: 421, steps per second:  23, episode reward: 17.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.969 [0.000, 5.000],  loss: 0.036253, mae: 3.702982, mean_q: 4.456222, mean_eps: 0.100000\n",
      "📈 Episodio 526: Recompensa total (clipped): 16.000, Pasos: 547, Mean Reward Calculado: 0.029250 (Recompensa/Pasos)\n",
      " 341015/825189: episode: 526, duration: 24.179s, episode steps: 547, steps per second:  23, episode reward: 16.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.680 [0.000, 5.000],  loss: 0.040004, mae: 3.671163, mean_q: 4.417058, mean_eps: 0.100000\n",
      "📈 Episodio 527: Recompensa total (clipped): 24.000, Pasos: 705, Mean Reward Calculado: 0.034043 (Recompensa/Pasos)\n",
      " 341720/825189: episode: 527, duration: 31.540s, episode steps: 705, steps per second:  22, episode reward: 24.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.036003, mae: 3.688659, mean_q: 4.439677, mean_eps: 0.100000\n",
      "📈 Episodio 528: Recompensa total (clipped): 26.000, Pasos: 645, Mean Reward Calculado: 0.040310 (Recompensa/Pasos)\n",
      " 342365/825189: episode: 528, duration: 28.695s, episode steps: 645, steps per second:  22, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.031288, mae: 3.694419, mean_q: 4.449827, mean_eps: 0.100000\n",
      "📈 Episodio 529: Recompensa total (clipped): 26.000, Pasos: 604, Mean Reward Calculado: 0.043046 (Recompensa/Pasos)\n",
      " 342969/825189: episode: 529, duration: 26.716s, episode steps: 604, steps per second:  23, episode reward: 26.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.035519, mae: 3.693038, mean_q: 4.445558, mean_eps: 0.100000\n",
      "📈 Episodio 530: Recompensa total (clipped): 11.000, Pasos: 299, Mean Reward Calculado: 0.036789 (Recompensa/Pasos)\n",
      " 343268/825189: episode: 530, duration: 13.516s, episode steps: 299, steps per second:  22, episode reward: 11.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.371 [0.000, 5.000],  loss: 0.045969, mae: 3.679019, mean_q: 4.427950, mean_eps: 0.100000\n",
      "📈 Episodio 531: Recompensa total (clipped): 19.000, Pasos: 474, Mean Reward Calculado: 0.040084 (Recompensa/Pasos)\n",
      " 343742/825189: episode: 531, duration: 20.866s, episode steps: 474, steps per second:  23, episode reward: 19.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.034051, mae: 3.700556, mean_q: 4.455028, mean_eps: 0.100000\n",
      "📈 Episodio 532: Recompensa total (clipped): 19.000, Pasos: 509, Mean Reward Calculado: 0.037328 (Recompensa/Pasos)\n",
      " 344251/825189: episode: 532, duration: 22.449s, episode steps: 509, steps per second:  23, episode reward: 19.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.458 [0.000, 5.000],  loss: 0.031439, mae: 3.689008, mean_q: 4.442516, mean_eps: 0.100000\n",
      "📈 Episodio 533: Recompensa total (clipped): 13.000, Pasos: 337, Mean Reward Calculado: 0.038576 (Recompensa/Pasos)\n",
      " 344588/825189: episode: 533, duration: 14.926s, episode steps: 337, steps per second:  23, episode reward: 13.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.537 [0.000, 5.000],  loss: 0.045239, mae: 3.710848, mean_q: 4.462289, mean_eps: 0.100000\n",
      "📈 Episodio 534: Recompensa total (clipped): 16.000, Pasos: 380, Mean Reward Calculado: 0.042105 (Recompensa/Pasos)\n",
      " 344968/825189: episode: 534, duration: 17.155s, episode steps: 380, steps per second:  22, episode reward: 16.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.038174, mae: 3.697683, mean_q: 4.449506, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 535: Recompensa total (clipped): 12.000, Pasos: 353, Mean Reward Calculado: 0.033994 (Recompensa/Pasos)\n",
      " 345321/825189: episode: 535, duration: 16.198s, episode steps: 353, steps per second:  22, episode reward: 12.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.038179, mae: 3.671575, mean_q: 4.419540, mean_eps: 0.100000\n",
      "📈 Episodio 536: Recompensa total (clipped): 14.000, Pasos: 429, Mean Reward Calculado: 0.032634 (Recompensa/Pasos)\n",
      " 345750/825189: episode: 536, duration: 18.978s, episode steps: 429, steps per second:  23, episode reward: 14.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.039877, mae: 3.681360, mean_q: 4.430410, mean_eps: 0.100000\n",
      "📈 Episodio 537: Recompensa total (clipped): 19.000, Pasos: 448, Mean Reward Calculado: 0.042411 (Recompensa/Pasos)\n",
      " 346198/825189: episode: 537, duration: 20.011s, episode steps: 448, steps per second:  22, episode reward: 19.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.037190, mae: 3.652854, mean_q: 4.396524, mean_eps: 0.100000\n",
      "📈 Episodio 538: Recompensa total (clipped): 9.000, Pasos: 297, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
      " 346495/825189: episode: 538, duration: 13.051s, episode steps: 297, steps per second:  23, episode reward:  9.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.354 [0.000, 5.000],  loss: 0.029867, mae: 3.664879, mean_q: 4.411486, mean_eps: 0.100000\n",
      "📈 Episodio 539: Recompensa total (clipped): 8.000, Pasos: 265, Mean Reward Calculado: 0.030189 (Recompensa/Pasos)\n",
      " 346760/825189: episode: 539, duration: 12.244s, episode steps: 265, steps per second:  22, episode reward:  8.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.035518, mae: 3.676821, mean_q: 4.426945, mean_eps: 0.100000\n",
      "📈 Episodio 540: Recompensa total (clipped): 35.000, Pasos: 965, Mean Reward Calculado: 0.036269 (Recompensa/Pasos)\n",
      " 347725/825189: episode: 540, duration: 43.212s, episode steps: 965, steps per second:  22, episode reward: 35.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.035242, mae: 3.689034, mean_q: 4.440776, mean_eps: 0.100000\n",
      "📈 Episodio 541: Recompensa total (clipped): 20.000, Pasos: 518, Mean Reward Calculado: 0.038610 (Recompensa/Pasos)\n",
      " 348243/825189: episode: 541, duration: 22.863s, episode steps: 518, steps per second:  23, episode reward: 20.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.643 [0.000, 5.000],  loss: 0.036942, mae: 3.671539, mean_q: 4.420471, mean_eps: 0.100000\n",
      "📈 Episodio 542: Recompensa total (clipped): 32.000, Pasos: 874, Mean Reward Calculado: 0.036613 (Recompensa/Pasos)\n",
      " 349117/825189: episode: 542, duration: 38.452s, episode steps: 874, steps per second:  23, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.582 [0.000, 5.000],  loss: 0.034833, mae: 3.724484, mean_q: 4.485005, mean_eps: 0.100000\n",
      "📈 Episodio 543: Recompensa total (clipped): 21.000, Pasos: 584, Mean Reward Calculado: 0.035959 (Recompensa/Pasos)\n",
      " 349701/825189: episode: 543, duration: 25.981s, episode steps: 584, steps per second:  22, episode reward: 21.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.040303, mae: 3.635664, mean_q: 4.376294, mean_eps: 0.100000\n",
      "📈 Episodio 544: Recompensa total (clipped): 26.000, Pasos: 565, Mean Reward Calculado: 0.046018 (Recompensa/Pasos)\n",
      " 350266/825189: episode: 544, duration: 25.146s, episode steps: 565, steps per second:  22, episode reward: 26.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.681 [0.000, 5.000],  loss: 0.033355, mae: 3.688377, mean_q: 4.443270, mean_eps: 0.100000\n",
      "📈 Episodio 545: Recompensa total (clipped): 22.000, Pasos: 513, Mean Reward Calculado: 0.042885 (Recompensa/Pasos)\n",
      " 350779/825189: episode: 545, duration: 22.996s, episode steps: 513, steps per second:  22, episode reward: 22.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.031195, mae: 3.705853, mean_q: 4.465387, mean_eps: 0.100000\n",
      "📈 Episodio 546: Recompensa total (clipped): 26.000, Pasos: 551, Mean Reward Calculado: 0.047187 (Recompensa/Pasos)\n",
      " 351330/825189: episode: 546, duration: 24.530s, episode steps: 551, steps per second:  22, episode reward: 26.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.029888, mae: 3.704412, mean_q: 4.462457, mean_eps: 0.100000\n",
      "📈 Episodio 547: Recompensa total (clipped): 11.000, Pasos: 276, Mean Reward Calculado: 0.039855 (Recompensa/Pasos)\n",
      " 351606/825189: episode: 547, duration: 12.602s, episode steps: 276, steps per second:  22, episode reward: 11.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.041374, mae: 3.676047, mean_q: 4.429105, mean_eps: 0.100000\n",
      "📈 Episodio 548: Recompensa total (clipped): 15.000, Pasos: 429, Mean Reward Calculado: 0.034965 (Recompensa/Pasos)\n",
      " 352035/825189: episode: 548, duration: 19.142s, episode steps: 429, steps per second:  22, episode reward: 15.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.487 [0.000, 5.000],  loss: 0.033647, mae: 3.742284, mean_q: 4.508389, mean_eps: 0.100000\n",
      "📈 Episodio 549: Recompensa total (clipped): 30.000, Pasos: 796, Mean Reward Calculado: 0.037688 (Recompensa/Pasos)\n",
      " 352831/825189: episode: 549, duration: 35.424s, episode steps: 796, steps per second:  22, episode reward: 30.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.035620, mae: 3.700453, mean_q: 4.455754, mean_eps: 0.100000\n",
      "📈 Episodio 550: Recompensa total (clipped): 16.000, Pasos: 421, Mean Reward Calculado: 0.038005 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 550, pasos: 353252)\n",
      "💾 NUEVO MEJOR PROMEDIO: 23.66 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 550 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 16.00\n",
      "   Media últimos 100: 23.66 / 20.0\n",
      "   Mejor promedio histórico: 23.66\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 460\n",
      "   Episodios consecutivos en objetivo: 460\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 460 episodios consecutivos\n",
      " 353252/825189: episode: 550, duration: 60.621s, episode steps: 421, steps per second:   7, episode reward: 16.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.031263, mae: 3.736999, mean_q: 4.499734, mean_eps: 0.100000\n",
      "📈 Episodio 551: Recompensa total (clipped): 30.000, Pasos: 793, Mean Reward Calculado: 0.037831 (Recompensa/Pasos)\n",
      " 354045/825189: episode: 551, duration: 35.392s, episode steps: 793, steps per second:  22, episode reward: 30.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.030142, mae: 3.667583, mean_q: 4.418216, mean_eps: 0.100000\n",
      "📈 Episodio 552: Recompensa total (clipped): 26.000, Pasos: 635, Mean Reward Calculado: 0.040945 (Recompensa/Pasos)\n",
      " 354680/825189: episode: 552, duration: 28.111s, episode steps: 635, steps per second:  23, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.038849, mae: 3.671037, mean_q: 4.421992, mean_eps: 0.100000\n",
      "📈 Episodio 553: Recompensa total (clipped): 23.000, Pasos: 590, Mean Reward Calculado: 0.038983 (Recompensa/Pasos)\n",
      " 355270/825189: episode: 553, duration: 26.825s, episode steps: 590, steps per second:  22, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.038693, mae: 3.717642, mean_q: 4.476995, mean_eps: 0.100000\n",
      "📈 Episodio 554: Recompensa total (clipped): 24.000, Pasos: 604, Mean Reward Calculado: 0.039735 (Recompensa/Pasos)\n",
      " 355874/825189: episode: 554, duration: 27.133s, episode steps: 604, steps per second:  22, episode reward: 24.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.033680, mae: 3.664842, mean_q: 4.412570, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 555: Recompensa total (clipped): 24.000, Pasos: 703, Mean Reward Calculado: 0.034139 (Recompensa/Pasos)\n",
      " 356577/825189: episode: 555, duration: 31.354s, episode steps: 703, steps per second:  22, episode reward: 24.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.032048, mae: 3.696173, mean_q: 4.452853, mean_eps: 0.100000\n",
      "📈 Episodio 556: Recompensa total (clipped): 16.000, Pasos: 368, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 356945/825189: episode: 556, duration: 16.458s, episode steps: 368, steps per second:  22, episode reward: 16.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.037868, mae: 3.729513, mean_q: 4.490836, mean_eps: 0.100000\n",
      "📈 Episodio 557: Recompensa total (clipped): 22.000, Pasos: 550, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 357495/825189: episode: 557, duration: 24.390s, episode steps: 550, steps per second:  23, episode reward: 22.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.038338, mae: 3.704714, mean_q: 4.460078, mean_eps: 0.100000\n",
      "📈 Episodio 558: Recompensa total (clipped): 20.000, Pasos: 519, Mean Reward Calculado: 0.038536 (Recompensa/Pasos)\n",
      " 358014/825189: episode: 558, duration: 23.080s, episode steps: 519, steps per second:  22, episode reward: 20.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.039645, mae: 3.739972, mean_q: 4.500889, mean_eps: 0.100000\n",
      "📈 Episodio 559: Recompensa total (clipped): 22.000, Pasos: 608, Mean Reward Calculado: 0.036184 (Recompensa/Pasos)\n",
      " 358622/825189: episode: 559, duration: 27.087s, episode steps: 608, steps per second:  22, episode reward: 22.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.029729, mae: 3.670930, mean_q: 4.420591, mean_eps: 0.100000\n",
      "📈 Episodio 560: Recompensa total (clipped): 22.000, Pasos: 488, Mean Reward Calculado: 0.045082 (Recompensa/Pasos)\n",
      " 359110/825189: episode: 560, duration: 21.683s, episode steps: 488, steps per second:  23, episode reward: 22.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.037363, mae: 3.713612, mean_q: 4.472626, mean_eps: 0.100000\n",
      "📈 Episodio 561: Recompensa total (clipped): 11.000, Pasos: 274, Mean Reward Calculado: 0.040146 (Recompensa/Pasos)\n",
      " 359384/825189: episode: 561, duration: 12.464s, episode steps: 274, steps per second:  22, episode reward: 11.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.035855, mae: 3.658245, mean_q: 4.404025, mean_eps: 0.100000\n",
      "📈 Episodio 562: Recompensa total (clipped): 15.000, Pasos: 351, Mean Reward Calculado: 0.042735 (Recompensa/Pasos)\n",
      " 359735/825189: episode: 562, duration: 15.585s, episode steps: 351, steps per second:  23, episode reward: 15.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.029838, mae: 3.696478, mean_q: 4.452498, mean_eps: 0.100000\n",
      "📈 Episodio 563: Recompensa total (clipped): 8.000, Pasos: 259, Mean Reward Calculado: 0.030888 (Recompensa/Pasos)\n",
      " 359994/825189: episode: 563, duration: 11.874s, episode steps: 259, steps per second:  22, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.032046, mae: 3.695144, mean_q: 4.449188, mean_eps: 0.100000\n",
      "📊 Paso 360,000/2,000,000 (18.0%) - 23.6 pasos/seg - ETA: 19.3h - Memoria: 9088.42 MB\n",
      "📈 Episodio 564: Recompensa total (clipped): 11.000, Pasos: 282, Mean Reward Calculado: 0.039007 (Recompensa/Pasos)\n",
      " 360276/825189: episode: 564, duration: 12.600s, episode steps: 282, steps per second:  22, episode reward: 11.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.040401, mae: 3.645724, mean_q: 4.390628, mean_eps: 0.100000\n",
      "📈 Episodio 565: Recompensa total (clipped): 24.000, Pasos: 556, Mean Reward Calculado: 0.043165 (Recompensa/Pasos)\n",
      " 360832/825189: episode: 565, duration: 24.781s, episode steps: 556, steps per second:  22, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.033271, mae: 3.723812, mean_q: 4.486424, mean_eps: 0.100000\n",
      "📈 Episodio 566: Recompensa total (clipped): 19.000, Pasos: 441, Mean Reward Calculado: 0.043084 (Recompensa/Pasos)\n",
      " 361273/825189: episode: 566, duration: 19.840s, episode steps: 441, steps per second:  22, episode reward: 19.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.032471, mae: 3.699535, mean_q: 4.456168, mean_eps: 0.100000\n",
      "📈 Episodio 567: Recompensa total (clipped): 21.000, Pasos: 469, Mean Reward Calculado: 0.044776 (Recompensa/Pasos)\n",
      " 361742/825189: episode: 567, duration: 21.019s, episode steps: 469, steps per second:  22, episode reward: 21.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.040455, mae: 3.660849, mean_q: 4.405982, mean_eps: 0.100000\n",
      "📈 Episodio 568: Recompensa total (clipped): 11.000, Pasos: 283, Mean Reward Calculado: 0.038869 (Recompensa/Pasos)\n",
      " 362025/825189: episode: 568, duration: 12.911s, episode steps: 283, steps per second:  22, episode reward: 11.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.036843, mae: 3.689261, mean_q: 4.442246, mean_eps: 0.100000\n",
      "📈 Episodio 569: Recompensa total (clipped): 8.000, Pasos: 251, Mean Reward Calculado: 0.031873 (Recompensa/Pasos)\n",
      " 362276/825189: episode: 569, duration: 11.313s, episode steps: 251, steps per second:  22, episode reward:  8.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.036591, mae: 3.677458, mean_q: 4.426809, mean_eps: 0.100000\n",
      "📈 Episodio 570: Recompensa total (clipped): 13.000, Pasos: 368, Mean Reward Calculado: 0.035326 (Recompensa/Pasos)\n",
      " 362644/825189: episode: 570, duration: 16.631s, episode steps: 368, steps per second:  22, episode reward: 13.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.033892, mae: 3.712831, mean_q: 4.471385, mean_eps: 0.100000\n",
      "📈 Episodio 571: Recompensa total (clipped): 19.000, Pasos: 399, Mean Reward Calculado: 0.047619 (Recompensa/Pasos)\n",
      " 363043/825189: episode: 571, duration: 18.002s, episode steps: 399, steps per second:  22, episode reward: 19.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.039575, mae: 3.689321, mean_q: 4.440212, mean_eps: 0.100000\n",
      "📈 Episodio 572: Recompensa total (clipped): 8.000, Pasos: 253, Mean Reward Calculado: 0.031621 (Recompensa/Pasos)\n",
      " 363296/825189: episode: 572, duration: 11.616s, episode steps: 253, steps per second:  22, episode reward:  8.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.035338, mae: 3.665790, mean_q: 4.414852, mean_eps: 0.100000\n",
      "📈 Episodio 573: Recompensa total (clipped): 14.000, Pasos: 406, Mean Reward Calculado: 0.034483 (Recompensa/Pasos)\n",
      " 363702/825189: episode: 573, duration: 17.962s, episode steps: 406, steps per second:  23, episode reward: 14.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.039288, mae: 3.726563, mean_q: 4.487670, mean_eps: 0.100000\n",
      "📈 Episodio 574: Recompensa total (clipped): 10.000, Pasos: 264, Mean Reward Calculado: 0.037879 (Recompensa/Pasos)\n",
      " 363966/825189: episode: 574, duration: 12.019s, episode steps: 264, steps per second:  22, episode reward: 10.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.035758, mae: 3.660478, mean_q: 4.408578, mean_eps: 0.100000\n",
      "📈 Episodio 575: Recompensa total (clipped): 22.000, Pasos: 487, Mean Reward Calculado: 0.045175 (Recompensa/Pasos)\n",
      " 364453/825189: episode: 575, duration: 21.762s, episode steps: 487, steps per second:  22, episode reward: 22.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.034364, mae: 3.682446, mean_q: 4.436751, mean_eps: 0.100000\n",
      "📈 Episodio 576: Recompensa total (clipped): 17.000, Pasos: 439, Mean Reward Calculado: 0.038724 (Recompensa/Pasos)\n",
      " 364892/825189: episode: 576, duration: 19.507s, episode steps: 439, steps per second:  23, episode reward: 17.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.035880, mae: 3.678340, mean_q: 4.433010, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 577: Recompensa total (clipped): 31.000, Pasos: 950, Mean Reward Calculado: 0.032632 (Recompensa/Pasos)\n",
      " 365842/825189: episode: 577, duration: 41.951s, episode steps: 950, steps per second:  23, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.030991, mae: 3.666752, mean_q: 4.417785, mean_eps: 0.100000\n",
      "📈 Episodio 578: Recompensa total (clipped): 13.000, Pasos: 341, Mean Reward Calculado: 0.038123 (Recompensa/Pasos)\n",
      " 366183/825189: episode: 578, duration: 15.367s, episode steps: 341, steps per second:  22, episode reward: 13.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.036765, mae: 3.668238, mean_q: 4.413032, mean_eps: 0.100000\n",
      "📈 Episodio 579: Recompensa total (clipped): 16.000, Pasos: 479, Mean Reward Calculado: 0.033403 (Recompensa/Pasos)\n",
      " 366662/825189: episode: 579, duration: 21.386s, episode steps: 479, steps per second:  22, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.041644, mae: 3.710822, mean_q: 4.465720, mean_eps: 0.100000\n",
      "📈 Episodio 580: Recompensa total (clipped): 36.000, Pasos: 1010, Mean Reward Calculado: 0.035644 (Recompensa/Pasos)\n",
      " 367672/825189: episode: 580, duration: 45.007s, episode steps: 1010, steps per second:  22, episode reward: 36.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.039041, mae: 3.690656, mean_q: 4.442256, mean_eps: 0.100000\n",
      "📈 Episodio 581: Recompensa total (clipped): 9.000, Pasos: 267, Mean Reward Calculado: 0.033708 (Recompensa/Pasos)\n",
      " 367939/825189: episode: 581, duration: 12.204s, episode steps: 267, steps per second:  22, episode reward:  9.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.146 [0.000, 5.000],  loss: 0.033050, mae: 3.694671, mean_q: 4.448762, mean_eps: 0.100000\n",
      "📈 Episodio 582: Recompensa total (clipped): 14.000, Pasos: 359, Mean Reward Calculado: 0.038997 (Recompensa/Pasos)\n",
      " 368298/825189: episode: 582, duration: 16.192s, episode steps: 359, steps per second:  22, episode reward: 14.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.460 [0.000, 5.000],  loss: 0.042303, mae: 3.711116, mean_q: 4.465337, mean_eps: 0.100000\n",
      "📈 Episodio 583: Recompensa total (clipped): 24.000, Pasos: 566, Mean Reward Calculado: 0.042403 (Recompensa/Pasos)\n",
      " 368864/825189: episode: 583, duration: 25.441s, episode steps: 566, steps per second:  22, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.035450, mae: 3.678423, mean_q: 4.429314, mean_eps: 0.100000\n",
      "📈 Episodio 584: Recompensa total (clipped): 22.000, Pasos: 504, Mean Reward Calculado: 0.043651 (Recompensa/Pasos)\n",
      " 369368/825189: episode: 584, duration: 22.522s, episode steps: 504, steps per second:  22, episode reward: 22.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.034236, mae: 3.678656, mean_q: 4.430087, mean_eps: 0.100000\n",
      "📈 Episodio 585: Recompensa total (clipped): 26.000, Pasos: 661, Mean Reward Calculado: 0.039334 (Recompensa/Pasos)\n",
      " 370029/825189: episode: 585, duration: 29.563s, episode steps: 661, steps per second:  22, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.038679, mae: 3.700872, mean_q: 4.455838, mean_eps: 0.100000\n",
      "📈 Episodio 586: Recompensa total (clipped): 31.000, Pasos: 937, Mean Reward Calculado: 0.033084 (Recompensa/Pasos)\n",
      " 370966/825189: episode: 586, duration: 41.645s, episode steps: 937, steps per second:  22, episode reward: 31.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.036644, mae: 3.741331, mean_q: 4.506289, mean_eps: 0.100000\n",
      "📈 Episodio 587: Recompensa total (clipped): 14.000, Pasos: 351, Mean Reward Calculado: 0.039886 (Recompensa/Pasos)\n",
      " 371317/825189: episode: 587, duration: 15.853s, episode steps: 351, steps per second:  22, episode reward: 14.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.416 [0.000, 5.000],  loss: 0.032935, mae: 3.739414, mean_q: 4.502846, mean_eps: 0.100000\n",
      "📈 Episodio 588: Recompensa total (clipped): 41.000, Pasos: 1210, Mean Reward Calculado: 0.033884 (Recompensa/Pasos)\n",
      " 372527/825189: episode: 588, duration: 54.036s, episode steps: 1210, steps per second:  22, episode reward: 41.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.035478, mae: 3.682173, mean_q: 4.434005, mean_eps: 0.100000\n",
      "📈 Episodio 589: Recompensa total (clipped): 34.000, Pasos: 936, Mean Reward Calculado: 0.036325 (Recompensa/Pasos)\n",
      " 373463/825189: episode: 589, duration: 41.849s, episode steps: 936, steps per second:  22, episode reward: 34.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.038635, mae: 3.680765, mean_q: 4.431018, mean_eps: 0.100000\n",
      "📈 Episodio 590: Recompensa total (clipped): 30.000, Pasos: 777, Mean Reward Calculado: 0.038610 (Recompensa/Pasos)\n",
      " 374240/825189: episode: 590, duration: 34.691s, episode steps: 777, steps per second:  22, episode reward: 30.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.031993, mae: 3.638854, mean_q: 4.382577, mean_eps: 0.100000\n",
      "📈 Episodio 591: Recompensa total (clipped): 24.000, Pasos: 593, Mean Reward Calculado: 0.040472 (Recompensa/Pasos)\n",
      " 374833/825189: episode: 591, duration: 26.840s, episode steps: 593, steps per second:  22, episode reward: 24.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.041413, mae: 3.651240, mean_q: 4.393417, mean_eps: 0.100000\n",
      "📈 Episodio 592: Recompensa total (clipped): 25.000, Pasos: 559, Mean Reward Calculado: 0.044723 (Recompensa/Pasos)\n",
      " 375392/825189: episode: 592, duration: 25.100s, episode steps: 559, steps per second:  22, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.043544, mae: 3.704960, mean_q: 4.457701, mean_eps: 0.100000\n",
      "📈 Episodio 593: Recompensa total (clipped): 25.000, Pasos: 543, Mean Reward Calculado: 0.046041 (Recompensa/Pasos)\n",
      " 375935/825189: episode: 593, duration: 24.234s, episode steps: 543, steps per second:  22, episode reward: 25.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.033241, mae: 3.726726, mean_q: 4.489097, mean_eps: 0.100000\n",
      "📈 Episodio 594: Recompensa total (clipped): 17.000, Pasos: 432, Mean Reward Calculado: 0.039352 (Recompensa/Pasos)\n",
      " 376367/825189: episode: 594, duration: 19.492s, episode steps: 432, steps per second:  22, episode reward: 17.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.034465, mae: 3.693713, mean_q: 4.448018, mean_eps: 0.100000\n",
      "📈 Episodio 595: Recompensa total (clipped): 25.000, Pasos: 503, Mean Reward Calculado: 0.049702 (Recompensa/Pasos)\n",
      " 376870/825189: episode: 595, duration: 22.807s, episode steps: 503, steps per second:  22, episode reward: 25.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.033497, mae: 3.672631, mean_q: 4.422230, mean_eps: 0.100000\n",
      "📈 Episodio 596: Recompensa total (clipped): 28.000, Pasos: 628, Mean Reward Calculado: 0.044586 (Recompensa/Pasos)\n",
      " 377498/825189: episode: 596, duration: 28.223s, episode steps: 628, steps per second:  22, episode reward: 28.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.037980, mae: 3.739463, mean_q: 4.499683, mean_eps: 0.100000\n",
      "📈 Episodio 597: Recompensa total (clipped): 22.000, Pasos: 570, Mean Reward Calculado: 0.038596 (Recompensa/Pasos)\n",
      " 378068/825189: episode: 597, duration: 25.929s, episode steps: 570, steps per second:  22, episode reward: 22.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.031157, mae: 3.687335, mean_q: 4.439357, mean_eps: 0.100000\n",
      "📈 Episodio 598: Recompensa total (clipped): 30.000, Pasos: 737, Mean Reward Calculado: 0.040706 (Recompensa/Pasos)\n",
      " 378805/825189: episode: 598, duration: 33.560s, episode steps: 737, steps per second:  22, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.033923, mae: 3.693096, mean_q: 4.446373, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 599: Recompensa total (clipped): 32.000, Pasos: 821, Mean Reward Calculado: 0.038977 (Recompensa/Pasos)\n",
      " 379626/825189: episode: 599, duration: 36.559s, episode steps: 821, steps per second:  22, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.030906, mae: 3.701390, mean_q: 4.454900, mean_eps: 0.100000\n",
      "📊 Paso 380,000/2,000,000 (19.0%) - 23.5 pasos/seg - ETA: 19.1h - Memoria: 9088.61 MB\n",
      "📈 Episodio 600: Recompensa total (clipped): 18.000, Pasos: 419, Mean Reward Calculado: 0.042959 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 600, pasos: 380045)\n",
      "💾 NUEVO MEJOR PROMEDIO: 21.42 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 600 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 18.00\n",
      "   Media últimos 100: 21.42 / 20.0\n",
      "   Mejor promedio histórico: 21.42\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 510\n",
      "   Episodios consecutivos en objetivo: 510\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 510 episodios consecutivos\n",
      " 380045/825189: episode: 600, duration: 65.414s, episode steps: 419, steps per second:   6, episode reward: 18.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.055 [0.000, 5.000],  loss: 0.032868, mae: 3.629822, mean_q: 4.370329, mean_eps: 0.100000\n",
      "📈 Episodio 601: Recompensa total (clipped): 25.000, Pasos: 585, Mean Reward Calculado: 0.042735 (Recompensa/Pasos)\n",
      " 380630/825189: episode: 601, duration: 29.468s, episode steps: 585, steps per second:  20, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.032975, mae: 3.731440, mean_q: 4.493576, mean_eps: 0.100000\n",
      "📈 Episodio 602: Recompensa total (clipped): 24.000, Pasos: 619, Mean Reward Calculado: 0.038772 (Recompensa/Pasos)\n",
      " 381249/825189: episode: 602, duration: 31.090s, episode steps: 619, steps per second:  20, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.030008, mae: 3.713618, mean_q: 4.474189, mean_eps: 0.100000\n",
      "📈 Episodio 603: Recompensa total (clipped): 32.000, Pasos: 829, Mean Reward Calculado: 0.038601 (Recompensa/Pasos)\n",
      " 382078/825189: episode: 603, duration: 41.737s, episode steps: 829, steps per second:  20, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.031608, mae: 3.691227, mean_q: 4.441985, mean_eps: 0.100000\n",
      "📈 Episodio 604: Recompensa total (clipped): 22.000, Pasos: 503, Mean Reward Calculado: 0.043738 (Recompensa/Pasos)\n",
      " 382581/825189: episode: 604, duration: 25.341s, episode steps: 503, steps per second:  20, episode reward: 22.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.034384, mae: 3.704765, mean_q: 4.460291, mean_eps: 0.100000\n",
      "📈 Episodio 605: Recompensa total (clipped): 23.000, Pasos: 557, Mean Reward Calculado: 0.041293 (Recompensa/Pasos)\n",
      " 383138/825189: episode: 605, duration: 27.914s, episode steps: 557, steps per second:  20, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.037560, mae: 3.738321, mean_q: 4.500449, mean_eps: 0.100000\n",
      "📈 Episodio 606: Recompensa total (clipped): 13.000, Pasos: 316, Mean Reward Calculado: 0.041139 (Recompensa/Pasos)\n",
      " 383454/825189: episode: 606, duration: 16.024s, episode steps: 316, steps per second:  20, episode reward: 13.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.031154, mae: 3.715375, mean_q: 4.474956, mean_eps: 0.100000\n",
      "📈 Episodio 607: Recompensa total (clipped): 26.000, Pasos: 512, Mean Reward Calculado: 0.050781 (Recompensa/Pasos)\n",
      " 383966/825189: episode: 607, duration: 25.888s, episode steps: 512, steps per second:  20, episode reward: 26.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 1.592 [0.000, 5.000],  loss: 0.039246, mae: 3.713094, mean_q: 4.468784, mean_eps: 0.100000\n",
      "📈 Episodio 608: Recompensa total (clipped): 19.000, Pasos: 407, Mean Reward Calculado: 0.046683 (Recompensa/Pasos)\n",
      " 384373/825189: episode: 608, duration: 20.554s, episode steps: 407, steps per second:  20, episode reward: 19.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 1.698 [0.000, 5.000],  loss: 0.032921, mae: 3.707821, mean_q: 4.462409, mean_eps: 0.100000\n",
      "📈 Episodio 609: Recompensa total (clipped): 17.000, Pasos: 593, Mean Reward Calculado: 0.028668 (Recompensa/Pasos)\n",
      " 384966/825189: episode: 609, duration: 29.587s, episode steps: 593, steps per second:  20, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.696 [0.000, 5.000],  loss: 0.032955, mae: 3.721414, mean_q: 4.479530, mean_eps: 0.100000\n",
      "📈 Episodio 610: Recompensa total (clipped): 13.000, Pasos: 335, Mean Reward Calculado: 0.038806 (Recompensa/Pasos)\n",
      " 385301/825189: episode: 610, duration: 16.790s, episode steps: 335, steps per second:  20, episode reward: 13.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.209 [0.000, 5.000],  loss: 0.040134, mae: 3.777189, mean_q: 4.544731, mean_eps: 0.100000\n",
      "📈 Episodio 611: Recompensa total (clipped): 31.000, Pasos: 603, Mean Reward Calculado: 0.051410 (Recompensa/Pasos)\n",
      " 385904/825189: episode: 611, duration: 30.457s, episode steps: 603, steps per second:  20, episode reward: 31.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 1.552 [0.000, 5.000],  loss: 0.036556, mae: 3.708430, mean_q: 4.464971, mean_eps: 0.100000\n",
      "📈 Episodio 612: Recompensa total (clipped): 26.000, Pasos: 686, Mean Reward Calculado: 0.037901 (Recompensa/Pasos)\n",
      " 386590/825189: episode: 612, duration: 34.505s, episode steps: 686, steps per second:  20, episode reward: 26.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.034251, mae: 3.693525, mean_q: 4.446193, mean_eps: 0.100000\n",
      "📈 Episodio 613: Recompensa total (clipped): 25.000, Pasos: 672, Mean Reward Calculado: 0.037202 (Recompensa/Pasos)\n",
      " 387262/825189: episode: 613, duration: 33.901s, episode steps: 672, steps per second:  20, episode reward: 25.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.035543, mae: 3.746263, mean_q: 4.508484, mean_eps: 0.100000\n",
      "📈 Episodio 614: Recompensa total (clipped): 26.000, Pasos: 574, Mean Reward Calculado: 0.045296 (Recompensa/Pasos)\n",
      " 387836/825189: episode: 614, duration: 28.833s, episode steps: 574, steps per second:  20, episode reward: 26.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.033039, mae: 3.702051, mean_q: 4.456605, mean_eps: 0.100000\n",
      "📈 Episodio 615: Recompensa total (clipped): 30.000, Pasos: 739, Mean Reward Calculado: 0.040595 (Recompensa/Pasos)\n",
      " 388575/825189: episode: 615, duration: 37.191s, episode steps: 739, steps per second:  20, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.036699, mae: 3.694020, mean_q: 4.447599, mean_eps: 0.100000\n",
      "📈 Episodio 616: Recompensa total (clipped): 23.000, Pasos: 594, Mean Reward Calculado: 0.038721 (Recompensa/Pasos)\n",
      " 389169/825189: episode: 616, duration: 30.175s, episode steps: 594, steps per second:  20, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.689 [0.000, 5.000],  loss: 0.033859, mae: 3.715735, mean_q: 4.476568, mean_eps: 0.100000\n",
      "📈 Episodio 617: Recompensa total (clipped): 23.000, Pasos: 555, Mean Reward Calculado: 0.041441 (Recompensa/Pasos)\n",
      " 389724/825189: episode: 617, duration: 27.992s, episode steps: 555, steps per second:  20, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.041033, mae: 3.712924, mean_q: 4.472210, mean_eps: 0.100000\n",
      "📈 Episodio 618: Recompensa total (clipped): 24.000, Pasos: 630, Mean Reward Calculado: 0.038095 (Recompensa/Pasos)\n",
      " 390354/825189: episode: 618, duration: 31.747s, episode steps: 630, steps per second:  20, episode reward: 24.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.032251, mae: 3.725010, mean_q: 4.488021, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 619: Recompensa total (clipped): 14.000, Pasos: 388, Mean Reward Calculado: 0.036082 (Recompensa/Pasos)\n",
      " 390742/825189: episode: 619, duration: 19.397s, episode steps: 388, steps per second:  20, episode reward: 14.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.037173, mae: 3.699661, mean_q: 4.457076, mean_eps: 0.100000\n",
      "📈 Episodio 620: Recompensa total (clipped): 31.000, Pasos: 866, Mean Reward Calculado: 0.035797 (Recompensa/Pasos)\n",
      " 391608/825189: episode: 620, duration: 43.717s, episode steps: 866, steps per second:  20, episode reward: 31.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.033089, mae: 3.680891, mean_q: 4.435747, mean_eps: 0.100000\n",
      "📈 Episodio 621: Recompensa total (clipped): 29.000, Pasos: 802, Mean Reward Calculado: 0.036160 (Recompensa/Pasos)\n",
      " 392410/825189: episode: 621, duration: 40.778s, episode steps: 802, steps per second:  20, episode reward: 29.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.036610, mae: 3.684296, mean_q: 4.436678, mean_eps: 0.100000\n",
      "📈 Episodio 622: Recompensa total (clipped): 30.000, Pasos: 726, Mean Reward Calculado: 0.041322 (Recompensa/Pasos)\n",
      " 393136/825189: episode: 622, duration: 36.758s, episode steps: 726, steps per second:  20, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.029933, mae: 3.692176, mean_q: 4.448090, mean_eps: 0.100000\n",
      "📈 Episodio 623: Recompensa total (clipped): 22.000, Pasos: 509, Mean Reward Calculado: 0.043222 (Recompensa/Pasos)\n",
      " 393645/825189: episode: 623, duration: 26.151s, episode steps: 509, steps per second:  19, episode reward: 22.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.035371, mae: 3.743500, mean_q: 4.507219, mean_eps: 0.100000\n",
      "📈 Episodio 624: Recompensa total (clipped): 30.000, Pasos: 757, Mean Reward Calculado: 0.039630 (Recompensa/Pasos)\n",
      " 394402/825189: episode: 624, duration: 38.253s, episode steps: 757, steps per second:  20, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.039963, mae: 3.732519, mean_q: 4.494248, mean_eps: 0.100000\n",
      "📈 Episodio 625: Recompensa total (clipped): 11.000, Pasos: 308, Mean Reward Calculado: 0.035714 (Recompensa/Pasos)\n",
      " 394710/825189: episode: 625, duration: 15.554s, episode steps: 308, steps per second:  20, episode reward: 11.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.036744, mae: 3.662157, mean_q: 4.409416, mean_eps: 0.100000\n",
      "📈 Episodio 626: Recompensa total (clipped): 30.000, Pasos: 603, Mean Reward Calculado: 0.049751 (Recompensa/Pasos)\n",
      " 395313/825189: episode: 626, duration: 30.398s, episode steps: 603, steps per second:  20, episode reward: 30.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.028697, mae: 3.717132, mean_q: 4.478241, mean_eps: 0.100000\n",
      "📈 Episodio 627: Recompensa total (clipped): 33.000, Pasos: 901, Mean Reward Calculado: 0.036626 (Recompensa/Pasos)\n",
      " 396214/825189: episode: 627, duration: 45.948s, episode steps: 901, steps per second:  20, episode reward: 33.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.031998, mae: 3.684011, mean_q: 4.436729, mean_eps: 0.100000\n",
      "📈 Episodio 628: Recompensa total (clipped): 17.000, Pasos: 413, Mean Reward Calculado: 0.041162 (Recompensa/Pasos)\n",
      " 396627/825189: episode: 628, duration: 20.822s, episode steps: 413, steps per second:  20, episode reward: 17.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.036059, mae: 3.695006, mean_q: 4.449944, mean_eps: 0.100000\n",
      "📈 Episodio 629: Recompensa total (clipped): 32.000, Pasos: 862, Mean Reward Calculado: 0.037123 (Recompensa/Pasos)\n",
      " 397489/825189: episode: 629, duration: 43.513s, episode steps: 862, steps per second:  20, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.034901, mae: 3.719901, mean_q: 4.479692, mean_eps: 0.100000\n",
      "📈 Episodio 630: Recompensa total (clipped): 29.000, Pasos: 721, Mean Reward Calculado: 0.040222 (Recompensa/Pasos)\n",
      " 398210/825189: episode: 630, duration: 36.633s, episode steps: 721, steps per second:  20, episode reward: 29.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.032259, mae: 3.684849, mean_q: 4.438326, mean_eps: 0.100000\n",
      "📈 Episodio 631: Recompensa total (clipped): 20.000, Pasos: 464, Mean Reward Calculado: 0.043103 (Recompensa/Pasos)\n",
      " 398674/825189: episode: 631, duration: 23.297s, episode steps: 464, steps per second:  20, episode reward: 20.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.037281, mae: 3.721400, mean_q: 4.481657, mean_eps: 0.100000\n",
      "📈 Episodio 632: Recompensa total (clipped): 25.000, Pasos: 595, Mean Reward Calculado: 0.042017 (Recompensa/Pasos)\n",
      " 399269/825189: episode: 632, duration: 30.205s, episode steps: 595, steps per second:  20, episode reward: 25.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.035091, mae: 3.711930, mean_q: 4.469940, mean_eps: 0.100000\n",
      "📊 Paso 400,000/2,000,000 (20.0%) - 23.3 pasos/seg - ETA: 19.1h - Memoria: 9746.89 MB\n",
      "📈 Episodio 633: Recompensa total (clipped): 35.000, Pasos: 917, Mean Reward Calculado: 0.038168 (Recompensa/Pasos)\n",
      " 400186/825189: episode: 633, duration: 46.359s, episode steps: 917, steps per second:  20, episode reward: 35.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.262 [0.000, 5.000],  loss: 0.039922, mae: 3.707519, mean_q: 4.466769, mean_eps: 0.100000\n",
      "📈 Episodio 634: Recompensa total (clipped): 35.000, Pasos: 956, Mean Reward Calculado: 0.036611 (Recompensa/Pasos)\n",
      " 401142/825189: episode: 634, duration: 48.064s, episode steps: 956, steps per second:  20, episode reward: 35.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.894 [0.000, 5.000],  loss: 0.037327, mae: 3.662748, mean_q: 4.409825, mean_eps: 0.100000\n",
      "📈 Episodio 635: Recompensa total (clipped): 24.000, Pasos: 532, Mean Reward Calculado: 0.045113 (Recompensa/Pasos)\n",
      " 401674/825189: episode: 635, duration: 26.905s, episode steps: 532, steps per second:  20, episode reward: 24.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.006 [0.000, 5.000],  loss: 0.028266, mae: 3.729401, mean_q: 4.495667, mean_eps: 0.100000\n",
      "📈 Episodio 636: Recompensa total (clipped): 9.000, Pasos: 300, Mean Reward Calculado: 0.030000 (Recompensa/Pasos)\n",
      " 401974/825189: episode: 636, duration: 15.084s, episode steps: 300, steps per second:  20, episode reward:  9.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.034024, mae: 3.748666, mean_q: 4.514004, mean_eps: 0.100000\n",
      "📈 Episodio 637: Recompensa total (clipped): 15.000, Pasos: 355, Mean Reward Calculado: 0.042254 (Recompensa/Pasos)\n",
      " 402329/825189: episode: 637, duration: 17.937s, episode steps: 355, steps per second:  20, episode reward: 15.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.034781, mae: 3.656699, mean_q: 4.405406, mean_eps: 0.100000\n",
      "📈 Episodio 638: Recompensa total (clipped): 34.000, Pasos: 799, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 403128/825189: episode: 638, duration: 40.119s, episode steps: 799, steps per second:  20, episode reward: 34.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.035836, mae: 3.707548, mean_q: 4.465958, mean_eps: 0.100000\n",
      "📈 Episodio 639: Recompensa total (clipped): 31.000, Pasos: 885, Mean Reward Calculado: 0.035028 (Recompensa/Pasos)\n",
      " 404013/825189: episode: 639, duration: 44.673s, episode steps: 885, steps per second:  20, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.031805, mae: 3.697522, mean_q: 4.453659, mean_eps: 0.100000\n",
      "📈 Episodio 640: Recompensa total (clipped): 25.000, Pasos: 613, Mean Reward Calculado: 0.040783 (Recompensa/Pasos)\n",
      " 404626/825189: episode: 640, duration: 30.869s, episode steps: 613, steps per second:  20, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.038320, mae: 3.682209, mean_q: 4.434126, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 641: Recompensa total (clipped): 14.000, Pasos: 353, Mean Reward Calculado: 0.039660 (Recompensa/Pasos)\n",
      " 404979/825189: episode: 641, duration: 17.398s, episode steps: 353, steps per second:  20, episode reward: 14.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.030257, mae: 3.678894, mean_q: 4.430439, mean_eps: 0.100000\n",
      "📈 Episodio 642: Recompensa total (clipped): 32.000, Pasos: 794, Mean Reward Calculado: 0.040302 (Recompensa/Pasos)\n",
      " 405773/825189: episode: 642, duration: 44.146s, episode steps: 794, steps per second:  18, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.036178, mae: 3.706141, mean_q: 4.462365, mean_eps: 0.100000\n",
      "📈 Episodio 643: Recompensa total (clipped): 10.000, Pasos: 269, Mean Reward Calculado: 0.037175 (Recompensa/Pasos)\n",
      " 406042/825189: episode: 643, duration: 14.942s, episode steps: 269, steps per second:  18, episode reward: 10.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.040598, mae: 3.667702, mean_q: 4.416317, mean_eps: 0.100000\n",
      "📈 Episodio 644: Recompensa total (clipped): 29.000, Pasos: 544, Mean Reward Calculado: 0.053309 (Recompensa/Pasos)\n",
      " 406586/825189: episode: 644, duration: 30.609s, episode steps: 544, steps per second:  18, episode reward: 29.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.037951, mae: 3.675949, mean_q: 4.421856, mean_eps: 0.100000\n",
      "📈 Episodio 645: Recompensa total (clipped): 22.000, Pasos: 500, Mean Reward Calculado: 0.044000 (Recompensa/Pasos)\n",
      " 407086/825189: episode: 645, duration: 28.086s, episode steps: 500, steps per second:  18, episode reward: 22.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.031814, mae: 3.720944, mean_q: 4.479520, mean_eps: 0.100000\n",
      "📈 Episodio 646: Recompensa total (clipped): 28.000, Pasos: 657, Mean Reward Calculado: 0.042618 (Recompensa/Pasos)\n",
      " 407743/825189: episode: 646, duration: 36.345s, episode steps: 657, steps per second:  18, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.039802, mae: 3.728705, mean_q: 4.490000, mean_eps: 0.100000\n",
      "📈 Episodio 647: Recompensa total (clipped): 23.000, Pasos: 657, Mean Reward Calculado: 0.035008 (Recompensa/Pasos)\n",
      " 408400/825189: episode: 647, duration: 36.802s, episode steps: 657, steps per second:  18, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.034111, mae: 3.671211, mean_q: 4.422404, mean_eps: 0.100000\n",
      "📈 Episodio 648: Recompensa total (clipped): 16.000, Pasos: 433, Mean Reward Calculado: 0.036952 (Recompensa/Pasos)\n",
      " 408833/825189: episode: 648, duration: 24.232s, episode steps: 433, steps per second:  18, episode reward: 16.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.036177, mae: 3.688868, mean_q: 4.443117, mean_eps: 0.100000\n",
      "📈 Episodio 649: Recompensa total (clipped): 25.000, Pasos: 581, Mean Reward Calculado: 0.043029 (Recompensa/Pasos)\n",
      " 409414/825189: episode: 649, duration: 33.074s, episode steps: 581, steps per second:  18, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.037677, mae: 3.705795, mean_q: 4.463051, mean_eps: 0.100000\n",
      "📈 Episodio 650: Recompensa total (clipped): 26.000, Pasos: 610, Mean Reward Calculado: 0.042623 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 650, pasos: 410024)\n",
      "💾 NUEVO MEJOR PROMEDIO: 22.55 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 650 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 26.00\n",
      "   Media últimos 100: 22.55 / 20.0\n",
      "   Mejor promedio histórico: 22.55\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 560\n",
      "   Episodios consecutivos en objetivo: 560\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 560 episodios consecutivos\n",
      " 410024/825189: episode: 650, duration: 83.416s, episode steps: 610, steps per second:   7, episode reward: 26.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.035279, mae: 3.677706, mean_q: 4.430301, mean_eps: 0.100000\n",
      "📈 Episodio 651: Recompensa total (clipped): 38.000, Pasos: 953, Mean Reward Calculado: 0.039874 (Recompensa/Pasos)\n",
      " 410977/825189: episode: 651, duration: 54.109s, episode steps: 953, steps per second:  18, episode reward: 38.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.032235, mae: 3.704910, mean_q: 4.465631, mean_eps: 0.100000\n",
      "📈 Episodio 652: Recompensa total (clipped): 23.000, Pasos: 603, Mean Reward Calculado: 0.038143 (Recompensa/Pasos)\n",
      " 411580/825189: episode: 652, duration: 33.660s, episode steps: 603, steps per second:  18, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.035368, mae: 3.717392, mean_q: 4.479415, mean_eps: 0.100000\n",
      "📈 Episodio 653: Recompensa total (clipped): 33.000, Pasos: 827, Mean Reward Calculado: 0.039903 (Recompensa/Pasos)\n",
      " 412407/825189: episode: 653, duration: 45.939s, episode steps: 827, steps per second:  18, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.033408, mae: 3.676002, mean_q: 4.426640, mean_eps: 0.100000\n",
      "📈 Episodio 654: Recompensa total (clipped): 29.000, Pasos: 664, Mean Reward Calculado: 0.043675 (Recompensa/Pasos)\n",
      " 413071/825189: episode: 654, duration: 36.718s, episode steps: 664, steps per second:  18, episode reward: 29.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.033022, mae: 3.701543, mean_q: 4.461028, mean_eps: 0.100000\n",
      "📈 Episodio 655: Recompensa total (clipped): 34.000, Pasos: 953, Mean Reward Calculado: 0.035677 (Recompensa/Pasos)\n",
      " 414024/825189: episode: 655, duration: 54.600s, episode steps: 953, steps per second:  17, episode reward: 34.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.034479, mae: 3.714765, mean_q: 4.473759, mean_eps: 0.100000\n",
      "📈 Episodio 656: Recompensa total (clipped): 27.000, Pasos: 842, Mean Reward Calculado: 0.032067 (Recompensa/Pasos)\n",
      " 414866/825189: episode: 656, duration: 47.277s, episode steps: 842, steps per second:  18, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.037944, mae: 3.711390, mean_q: 4.468089, mean_eps: 0.100000\n",
      "📈 Episodio 657: Recompensa total (clipped): 30.000, Pasos: 727, Mean Reward Calculado: 0.041265 (Recompensa/Pasos)\n",
      " 415593/825189: episode: 657, duration: 40.414s, episode steps: 727, steps per second:  18, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.032741, mae: 3.711723, mean_q: 4.470095, mean_eps: 0.100000\n",
      "📈 Episodio 658: Recompensa total (clipped): 18.000, Pasos: 419, Mean Reward Calculado: 0.042959 (Recompensa/Pasos)\n",
      " 416012/825189: episode: 658, duration: 23.247s, episode steps: 419, steps per second:  18, episode reward: 18.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.945 [0.000, 5.000],  loss: 0.030523, mae: 3.733376, mean_q: 4.497739, mean_eps: 0.100000\n",
      "📈 Episodio 659: Recompensa total (clipped): 19.000, Pasos: 489, Mean Reward Calculado: 0.038855 (Recompensa/Pasos)\n",
      " 416501/825189: episode: 659, duration: 28.442s, episode steps: 489, steps per second:  17, episode reward: 19.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.030223, mae: 3.677964, mean_q: 4.429751, mean_eps: 0.100000\n",
      "📈 Episodio 660: Recompensa total (clipped): 22.000, Pasos: 514, Mean Reward Calculado: 0.042802 (Recompensa/Pasos)\n",
      " 417015/825189: episode: 660, duration: 28.536s, episode steps: 514, steps per second:  18, episode reward: 22.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.034750, mae: 3.698872, mean_q: 4.454515, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 661: Recompensa total (clipped): 31.000, Pasos: 750, Mean Reward Calculado: 0.041333 (Recompensa/Pasos)\n",
      " 417765/825189: episode: 661, duration: 42.525s, episode steps: 750, steps per second:  18, episode reward: 31.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.038735, mae: 3.678801, mean_q: 4.425248, mean_eps: 0.100000\n",
      "📈 Episodio 662: Recompensa total (clipped): 18.000, Pasos: 486, Mean Reward Calculado: 0.037037 (Recompensa/Pasos)\n",
      " 418251/825189: episode: 662, duration: 27.258s, episode steps: 486, steps per second:  18, episode reward: 18.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.039407, mae: 3.666494, mean_q: 4.415896, mean_eps: 0.100000\n",
      "📈 Episodio 663: Recompensa total (clipped): 26.000, Pasos: 572, Mean Reward Calculado: 0.045455 (Recompensa/Pasos)\n",
      " 418823/825189: episode: 663, duration: 32.742s, episode steps: 572, steps per second:  17, episode reward: 26.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.070 [0.000, 5.000],  loss: 0.033724, mae: 3.697453, mean_q: 4.453731, mean_eps: 0.100000\n",
      "📈 Episodio 664: Recompensa total (clipped): 28.000, Pasos: 750, Mean Reward Calculado: 0.037333 (Recompensa/Pasos)\n",
      " 419573/825189: episode: 664, duration: 42.560s, episode steps: 750, steps per second:  18, episode reward: 28.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.034522, mae: 3.710104, mean_q: 4.467780, mean_eps: 0.100000\n",
      "📊 Paso 420,000/2,000,000 (21.0%) - 22.9 pasos/seg - ETA: 19.2h - Memoria: 9794.84 MB\n",
      "📈 Episodio 665: Recompensa total (clipped): 35.000, Pasos: 973, Mean Reward Calculado: 0.035971 (Recompensa/Pasos)\n",
      " 420546/825189: episode: 665, duration: 54.392s, episode steps: 973, steps per second:  18, episode reward: 35.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.032211, mae: 3.718860, mean_q: 4.480643, mean_eps: 0.100000\n",
      "📈 Episodio 666: Recompensa total (clipped): 30.000, Pasos: 620, Mean Reward Calculado: 0.048387 (Recompensa/Pasos)\n",
      " 421166/825189: episode: 666, duration: 34.971s, episode steps: 620, steps per second:  18, episode reward: 30.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 3.045 [0.000, 5.000],  loss: 0.033106, mae: 3.756912, mean_q: 4.526501, mean_eps: 0.100000\n",
      "📈 Episodio 667: Recompensa total (clipped): 34.000, Pasos: 824, Mean Reward Calculado: 0.041262 (Recompensa/Pasos)\n",
      " 421990/825189: episode: 667, duration: 46.274s, episode steps: 824, steps per second:  18, episode reward: 34.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.033128, mae: 3.710374, mean_q: 4.468286, mean_eps: 0.100000\n",
      "📈 Episodio 668: Recompensa total (clipped): 24.000, Pasos: 544, Mean Reward Calculado: 0.044118 (Recompensa/Pasos)\n",
      " 422534/825189: episode: 668, duration: 30.466s, episode steps: 544, steps per second:  18, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.037652, mae: 3.688274, mean_q: 4.441113, mean_eps: 0.100000\n",
      "📈 Episodio 669: Recompensa total (clipped): 34.000, Pasos: 800, Mean Reward Calculado: 0.042500 (Recompensa/Pasos)\n",
      " 423334/825189: episode: 669, duration: 43.996s, episode steps: 800, steps per second:  18, episode reward: 34.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.033970, mae: 3.698784, mean_q: 4.454795, mean_eps: 0.100000\n",
      "📈 Episodio 670: Recompensa total (clipped): 28.000, Pasos: 741, Mean Reward Calculado: 0.037787 (Recompensa/Pasos)\n",
      " 424075/825189: episode: 670, duration: 41.223s, episode steps: 741, steps per second:  18, episode reward: 28.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.387 [0.000, 5.000],  loss: 0.034568, mae: 3.707077, mean_q: 4.465623, mean_eps: 0.100000\n",
      "📈 Episodio 671: Recompensa total (clipped): 30.000, Pasos: 777, Mean Reward Calculado: 0.038610 (Recompensa/Pasos)\n",
      " 424852/825189: episode: 671, duration: 42.785s, episode steps: 777, steps per second:  18, episode reward: 30.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.033850, mae: 3.733525, mean_q: 4.496619, mean_eps: 0.100000\n",
      "📈 Episodio 672: Recompensa total (clipped): 24.000, Pasos: 518, Mean Reward Calculado: 0.046332 (Recompensa/Pasos)\n",
      " 425370/825189: episode: 672, duration: 28.662s, episode steps: 518, steps per second:  18, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.037212, mae: 3.720964, mean_q: 4.478960, mean_eps: 0.100000\n",
      "📈 Episodio 673: Recompensa total (clipped): 17.000, Pasos: 441, Mean Reward Calculado: 0.038549 (Recompensa/Pasos)\n",
      " 425811/825189: episode: 673, duration: 24.590s, episode steps: 441, steps per second:  18, episode reward: 17.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.032481, mae: 3.745338, mean_q: 4.511577, mean_eps: 0.100000\n",
      "📈 Episodio 674: Recompensa total (clipped): 28.000, Pasos: 631, Mean Reward Calculado: 0.044374 (Recompensa/Pasos)\n",
      " 426442/825189: episode: 674, duration: 35.066s, episode steps: 631, steps per second:  18, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.035261, mae: 3.723061, mean_q: 4.481378, mean_eps: 0.100000\n",
      "📈 Episodio 675: Recompensa total (clipped): 31.000, Pasos: 762, Mean Reward Calculado: 0.040682 (Recompensa/Pasos)\n",
      " 427204/825189: episode: 675, duration: 42.444s, episode steps: 762, steps per second:  18, episode reward: 31.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.038855, mae: 3.733562, mean_q: 4.493868, mean_eps: 0.100000\n",
      "📈 Episodio 676: Recompensa total (clipped): 33.000, Pasos: 784, Mean Reward Calculado: 0.042092 (Recompensa/Pasos)\n",
      " 427988/825189: episode: 676, duration: 43.534s, episode steps: 784, steps per second:  18, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.909 [0.000, 5.000],  loss: 0.034266, mae: 3.731909, mean_q: 4.493364, mean_eps: 0.100000\n",
      "📈 Episodio 677: Recompensa total (clipped): 30.000, Pasos: 648, Mean Reward Calculado: 0.046296 (Recompensa/Pasos)\n",
      " 428636/825189: episode: 677, duration: 35.856s, episode steps: 648, steps per second:  18, episode reward: 30.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.032449, mae: 3.729468, mean_q: 4.493347, mean_eps: 0.100000\n",
      "📈 Episodio 678: Recompensa total (clipped): 25.000, Pasos: 533, Mean Reward Calculado: 0.046904 (Recompensa/Pasos)\n",
      " 429169/825189: episode: 678, duration: 29.397s, episode steps: 533, steps per second:  18, episode reward: 25.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.897 [0.000, 5.000],  loss: 0.036501, mae: 3.680006, mean_q: 4.430998, mean_eps: 0.100000\n",
      "📈 Episodio 679: Recompensa total (clipped): 8.000, Pasos: 267, Mean Reward Calculado: 0.029963 (Recompensa/Pasos)\n",
      " 429436/825189: episode: 679, duration: 14.892s, episode steps: 267, steps per second:  18, episode reward:  8.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.217 [0.000, 5.000],  loss: 0.045713, mae: 3.749354, mean_q: 4.511705, mean_eps: 0.100000\n",
      "📈 Episodio 680: Recompensa total (clipped): 24.000, Pasos: 724, Mean Reward Calculado: 0.033149 (Recompensa/Pasos)\n",
      " 430160/825189: episode: 680, duration: 40.310s, episode steps: 724, steps per second:  18, episode reward: 24.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.036491, mae: 3.729555, mean_q: 4.489431, mean_eps: 0.100000\n",
      "📈 Episodio 681: Recompensa total (clipped): 18.000, Pasos: 461, Mean Reward Calculado: 0.039046 (Recompensa/Pasos)\n",
      " 430621/825189: episode: 681, duration: 25.595s, episode steps: 461, steps per second:  18, episode reward: 18.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.033689, mae: 3.668569, mean_q: 4.415671, mean_eps: 0.100000\n",
      "📈 Episodio 682: Recompensa total (clipped): 32.000, Pasos: 795, Mean Reward Calculado: 0.040252 (Recompensa/Pasos)\n",
      " 431416/825189: episode: 682, duration: 44.029s, episode steps: 795, steps per second:  18, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.033575, mae: 3.678614, mean_q: 4.430496, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 683: Recompensa total (clipped): 27.000, Pasos: 683, Mean Reward Calculado: 0.039531 (Recompensa/Pasos)\n",
      " 432099/825189: episode: 683, duration: 37.930s, episode steps: 683, steps per second:  18, episode reward: 27.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.037603, mae: 3.669598, mean_q: 4.422118, mean_eps: 0.100000\n",
      "📈 Episodio 684: Recompensa total (clipped): 32.000, Pasos: 906, Mean Reward Calculado: 0.035320 (Recompensa/Pasos)\n",
      " 433005/825189: episode: 684, duration: 49.939s, episode steps: 906, steps per second:  18, episode reward: 32.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.035452, mae: 3.688827, mean_q: 4.443299, mean_eps: 0.100000\n",
      "📈 Episodio 685: Recompensa total (clipped): 27.000, Pasos: 662, Mean Reward Calculado: 0.040785 (Recompensa/Pasos)\n",
      " 433667/825189: episode: 685, duration: 36.172s, episode steps: 662, steps per second:  18, episode reward: 27.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.568 [0.000, 5.000],  loss: 0.031279, mae: 3.712100, mean_q: 4.470093, mean_eps: 0.100000\n",
      "📈 Episodio 686: Recompensa total (clipped): 18.000, Pasos: 479, Mean Reward Calculado: 0.037578 (Recompensa/Pasos)\n",
      " 434146/825189: episode: 686, duration: 26.573s, episode steps: 479, steps per second:  18, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.564 [0.000, 5.000],  loss: 0.035331, mae: 3.702364, mean_q: 4.457864, mean_eps: 0.100000\n",
      "📈 Episodio 687: Recompensa total (clipped): 17.000, Pasos: 448, Mean Reward Calculado: 0.037946 (Recompensa/Pasos)\n",
      " 434594/825189: episode: 687, duration: 24.796s, episode steps: 448, steps per second:  18, episode reward: 17.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.034271, mae: 3.673551, mean_q: 4.423035, mean_eps: 0.100000\n",
      "📈 Episodio 688: Recompensa total (clipped): 28.000, Pasos: 620, Mean Reward Calculado: 0.045161 (Recompensa/Pasos)\n",
      " 435214/825189: episode: 688, duration: 34.220s, episode steps: 620, steps per second:  18, episode reward: 28.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.037779, mae: 3.691469, mean_q: 4.445143, mean_eps: 0.100000\n",
      "📈 Episodio 689: Recompensa total (clipped): 22.000, Pasos: 569, Mean Reward Calculado: 0.038664 (Recompensa/Pasos)\n",
      " 435783/825189: episode: 689, duration: 31.541s, episode steps: 569, steps per second:  18, episode reward: 22.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.030933, mae: 3.688217, mean_q: 4.441996, mean_eps: 0.100000\n",
      "📈 Episodio 690: Recompensa total (clipped): 26.000, Pasos: 591, Mean Reward Calculado: 0.043993 (Recompensa/Pasos)\n",
      " 436374/825189: episode: 690, duration: 33.036s, episode steps: 591, steps per second:  18, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.037377, mae: 3.761153, mean_q: 4.528935, mean_eps: 0.100000\n",
      "📈 Episodio 691: Recompensa total (clipped): 29.000, Pasos: 603, Mean Reward Calculado: 0.048093 (Recompensa/Pasos)\n",
      " 436977/825189: episode: 691, duration: 33.524s, episode steps: 603, steps per second:  18, episode reward: 29.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.032360, mae: 3.698901, mean_q: 4.455043, mean_eps: 0.100000\n",
      "📈 Episodio 692: Recompensa total (clipped): 25.000, Pasos: 594, Mean Reward Calculado: 0.042088 (Recompensa/Pasos)\n",
      " 437571/825189: episode: 692, duration: 32.632s, episode steps: 594, steps per second:  18, episode reward: 25.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.031948, mae: 3.718326, mean_q: 4.478529, mean_eps: 0.100000\n",
      "📈 Episodio 693: Recompensa total (clipped): 32.000, Pasos: 763, Mean Reward Calculado: 0.041940 (Recompensa/Pasos)\n",
      " 438334/825189: episode: 693, duration: 42.315s, episode steps: 763, steps per second:  18, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.037300, mae: 3.693179, mean_q: 4.444759, mean_eps: 0.100000\n",
      "📈 Episodio 694: Recompensa total (clipped): 23.000, Pasos: 580, Mean Reward Calculado: 0.039655 (Recompensa/Pasos)\n",
      " 438914/825189: episode: 694, duration: 32.213s, episode steps: 580, steps per second:  18, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.040780, mae: 3.688820, mean_q: 4.437861, mean_eps: 0.100000\n",
      "📈 Episodio 695: Recompensa total (clipped): 24.000, Pasos: 556, Mean Reward Calculado: 0.043165 (Recompensa/Pasos)\n",
      " 439470/825189: episode: 695, duration: 30.882s, episode steps: 556, steps per second:  18, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.039917, mae: 3.669978, mean_q: 4.418387, mean_eps: 0.100000\n",
      "📊 Paso 440,000/2,000,000 (22.0%) - 22.6 pasos/seg - ETA: 19.2h - Memoria: 9795.07 MB\n",
      "📈 Episodio 696: Recompensa total (clipped): 32.000, Pasos: 887, Mean Reward Calculado: 0.036077 (Recompensa/Pasos)\n",
      " 440357/825189: episode: 696, duration: 48.814s, episode steps: 887, steps per second:  18, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.038624, mae: 3.723766, mean_q: 4.485543, mean_eps: 0.100000\n",
      "📈 Episodio 697: Recompensa total (clipped): 28.000, Pasos: 784, Mean Reward Calculado: 0.035714 (Recompensa/Pasos)\n",
      " 441141/825189: episode: 697, duration: 43.181s, episode steps: 784, steps per second:  18, episode reward: 28.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.032929, mae: 3.716770, mean_q: 4.479688, mean_eps: 0.100000\n",
      "📈 Episodio 698: Recompensa total (clipped): 33.000, Pasos: 836, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 441977/825189: episode: 698, duration: 45.884s, episode steps: 836, steps per second:  18, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.036398, mae: 3.745639, mean_q: 4.511069, mean_eps: 0.100000\n",
      "📈 Episodio 699: Recompensa total (clipped): 14.000, Pasos: 344, Mean Reward Calculado: 0.040698 (Recompensa/Pasos)\n",
      " 442321/825189: episode: 699, duration: 18.839s, episode steps: 344, steps per second:  18, episode reward: 14.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.036662, mae: 3.766772, mean_q: 4.534223, mean_eps: 0.100000\n",
      "📈 Episodio 700: Recompensa total (clipped): 20.000, Pasos: 597, Mean Reward Calculado: 0.033501 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 700, pasos: 442918)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.26 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 700 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 20.00\n",
      "   Media últimos 100: 25.26 / 20.0\n",
      "   Mejor promedio histórico: 25.26\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 610\n",
      "   Episodios consecutivos en objetivo: 610\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 610 episodios consecutivos\n",
      " 442918/825189: episode: 700, duration: 85.036s, episode steps: 597, steps per second:   7, episode reward: 20.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.031390, mae: 3.729593, mean_q: 4.493671, mean_eps: 0.100000\n",
      "📈 Episodio 701: Recompensa total (clipped): 25.000, Pasos: 578, Mean Reward Calculado: 0.043253 (Recompensa/Pasos)\n",
      " 443496/825189: episode: 701, duration: 43.323s, episode steps: 578, steps per second:  13, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.037241, mae: 3.719863, mean_q: 4.479973, mean_eps: 0.100000\n",
      "📈 Episodio 702: Recompensa total (clipped): 15.000, Pasos: 364, Mean Reward Calculado: 0.041209 (Recompensa/Pasos)\n",
      " 443860/825189: episode: 702, duration: 27.380s, episode steps: 364, steps per second:  13, episode reward: 15.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.040680, mae: 3.793945, mean_q: 4.566253, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 703: Recompensa total (clipped): 32.000, Pasos: 678, Mean Reward Calculado: 0.047198 (Recompensa/Pasos)\n",
      " 444538/825189: episode: 703, duration: 51.019s, episode steps: 678, steps per second:  13, episode reward: 32.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.031762, mae: 3.720263, mean_q: 4.480297, mean_eps: 0.100000\n",
      "📈 Episodio 704: Recompensa total (clipped): 32.000, Pasos: 799, Mean Reward Calculado: 0.040050 (Recompensa/Pasos)\n",
      " 445337/825189: episode: 704, duration: 59.986s, episode steps: 799, steps per second:  13, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.031150, mae: 3.715428, mean_q: 4.474659, mean_eps: 0.100000\n",
      "📈 Episodio 705: Recompensa total (clipped): 28.000, Pasos: 620, Mean Reward Calculado: 0.045161 (Recompensa/Pasos)\n",
      " 445957/825189: episode: 705, duration: 46.568s, episode steps: 620, steps per second:  13, episode reward: 28.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.035492, mae: 3.737561, mean_q: 4.499281, mean_eps: 0.100000\n",
      "📈 Episodio 706: Recompensa total (clipped): 18.000, Pasos: 377, Mean Reward Calculado: 0.047745 (Recompensa/Pasos)\n",
      " 446334/825189: episode: 706, duration: 28.128s, episode steps: 377, steps per second:  13, episode reward: 18.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.034476, mae: 3.760807, mean_q: 4.531021, mean_eps: 0.100000\n",
      "📈 Episodio 707: Recompensa total (clipped): 25.000, Pasos: 613, Mean Reward Calculado: 0.040783 (Recompensa/Pasos)\n",
      " 446947/825189: episode: 707, duration: 46.070s, episode steps: 613, steps per second:  13, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.037466, mae: 3.726106, mean_q: 4.490437, mean_eps: 0.100000\n",
      "📈 Episodio 708: Recompensa total (clipped): 24.000, Pasos: 613, Mean Reward Calculado: 0.039152 (Recompensa/Pasos)\n",
      " 447560/825189: episode: 708, duration: 46.161s, episode steps: 613, steps per second:  13, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.985 [0.000, 5.000],  loss: 0.036822, mae: 3.715780, mean_q: 4.476803, mean_eps: 0.100000\n",
      "📈 Episodio 709: Recompensa total (clipped): 24.000, Pasos: 500, Mean Reward Calculado: 0.048000 (Recompensa/Pasos)\n",
      " 448060/825189: episode: 709, duration: 37.807s, episode steps: 500, steps per second:  13, episode reward: 24.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.034310, mae: 3.739584, mean_q: 4.506009, mean_eps: 0.100000\n",
      "📈 Episodio 710: Recompensa total (clipped): 24.000, Pasos: 592, Mean Reward Calculado: 0.040541 (Recompensa/Pasos)\n",
      " 448652/825189: episode: 710, duration: 44.534s, episode steps: 592, steps per second:  13, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.033370, mae: 3.702046, mean_q: 4.460039, mean_eps: 0.100000\n",
      "📈 Episodio 711: Recompensa total (clipped): 30.000, Pasos: 671, Mean Reward Calculado: 0.044709 (Recompensa/Pasos)\n",
      " 449323/825189: episode: 711, duration: 50.459s, episode steps: 671, steps per second:  13, episode reward: 30.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.033242, mae: 3.699934, mean_q: 4.454529, mean_eps: 0.100000\n",
      "📈 Episodio 712: Recompensa total (clipped): 35.000, Pasos: 880, Mean Reward Calculado: 0.039773 (Recompensa/Pasos)\n",
      " 450203/825189: episode: 712, duration: 66.209s, episode steps: 880, steps per second:  13, episode reward: 35.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.040344, mae: 3.735931, mean_q: 4.499099, mean_eps: 0.100000\n",
      "📈 Episodio 713: Recompensa total (clipped): 16.000, Pasos: 357, Mean Reward Calculado: 0.044818 (Recompensa/Pasos)\n",
      " 450560/825189: episode: 713, duration: 26.914s, episode steps: 357, steps per second:  13, episode reward: 16.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.034892, mae: 3.799987, mean_q: 4.576649, mean_eps: 0.100000\n",
      "📈 Episodio 714: Recompensa total (clipped): 26.000, Pasos: 592, Mean Reward Calculado: 0.043919 (Recompensa/Pasos)\n",
      " 451152/825189: episode: 714, duration: 44.725s, episode steps: 592, steps per second:  13, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.868 [0.000, 5.000],  loss: 0.033664, mae: 3.738996, mean_q: 4.504496, mean_eps: 0.100000\n",
      "📈 Episodio 715: Recompensa total (clipped): 22.000, Pasos: 583, Mean Reward Calculado: 0.037736 (Recompensa/Pasos)\n",
      " 451735/825189: episode: 715, duration: 43.672s, episode steps: 583, steps per second:  13, episode reward: 22.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.036082, mae: 3.759083, mean_q: 4.525894, mean_eps: 0.100000\n",
      "📈 Episodio 716: Recompensa total (clipped): 34.000, Pasos: 848, Mean Reward Calculado: 0.040094 (Recompensa/Pasos)\n",
      " 452583/825189: episode: 716, duration: 63.674s, episode steps: 848, steps per second:  13, episode reward: 34.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.968 [0.000, 5.000],  loss: 0.035853, mae: 3.714930, mean_q: 4.474546, mean_eps: 0.100000\n",
      "📈 Episodio 717: Recompensa total (clipped): 30.000, Pasos: 720, Mean Reward Calculado: 0.041667 (Recompensa/Pasos)\n",
      " 453303/825189: episode: 717, duration: 53.943s, episode steps: 720, steps per second:  13, episode reward: 30.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.035329, mae: 3.801030, mean_q: 4.576117, mean_eps: 0.100000\n",
      "📈 Episodio 718: Recompensa total (clipped): 11.000, Pasos: 267, Mean Reward Calculado: 0.041199 (Recompensa/Pasos)\n",
      " 453570/825189: episode: 718, duration: 20.069s, episode steps: 267, steps per second:  13, episode reward: 11.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.277 [0.000, 5.000],  loss: 0.035745, mae: 3.775266, mean_q: 4.545096, mean_eps: 0.100000\n",
      "📈 Episodio 719: Recompensa total (clipped): 30.000, Pasos: 789, Mean Reward Calculado: 0.038023 (Recompensa/Pasos)\n",
      " 454359/825189: episode: 719, duration: 58.916s, episode steps: 789, steps per second:  13, episode reward: 30.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.133 [0.000, 5.000],  loss: 0.033422, mae: 3.796832, mean_q: 4.572813, mean_eps: 0.100000\n",
      "📈 Episodio 720: Recompensa total (clipped): 15.000, Pasos: 351, Mean Reward Calculado: 0.042735 (Recompensa/Pasos)\n",
      " 454710/825189: episode: 720, duration: 26.397s, episode steps: 351, steps per second:  13, episode reward: 15.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.188 [0.000, 5.000],  loss: 0.037314, mae: 3.773172, mean_q: 4.544955, mean_eps: 0.100000\n",
      "📈 Episodio 721: Recompensa total (clipped): 29.000, Pasos: 700, Mean Reward Calculado: 0.041429 (Recompensa/Pasos)\n",
      " 455410/825189: episode: 721, duration: 52.685s, episode steps: 700, steps per second:  13, episode reward: 29.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.038052, mae: 3.722533, mean_q: 4.481047, mean_eps: 0.100000\n",
      "📈 Episodio 722: Recompensa total (clipped): 33.000, Pasos: 825, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 456235/825189: episode: 722, duration: 61.941s, episode steps: 825, steps per second:  13, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.035686, mae: 3.762807, mean_q: 4.534156, mean_eps: 0.100000\n",
      "📈 Episodio 723: Recompensa total (clipped): 26.000, Pasos: 508, Mean Reward Calculado: 0.051181 (Recompensa/Pasos)\n",
      " 456743/825189: episode: 723, duration: 38.355s, episode steps: 508, steps per second:  13, episode reward: 26.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.035560, mae: 3.777006, mean_q: 4.547687, mean_eps: 0.100000\n",
      "📈 Episodio 724: Recompensa total (clipped): 35.000, Pasos: 977, Mean Reward Calculado: 0.035824 (Recompensa/Pasos)\n",
      " 457720/825189: episode: 724, duration: 73.519s, episode steps: 977, steps per second:  13, episode reward: 35.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.035730, mae: 3.777549, mean_q: 4.550583, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 725: Recompensa total (clipped): 24.000, Pasos: 616, Mean Reward Calculado: 0.038961 (Recompensa/Pasos)\n",
      " 458336/825189: episode: 725, duration: 46.193s, episode steps: 616, steps per second:  13, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.039173, mae: 3.792556, mean_q: 4.565313, mean_eps: 0.100000\n",
      "📈 Episodio 726: Recompensa total (clipped): 27.000, Pasos: 596, Mean Reward Calculado: 0.045302 (Recompensa/Pasos)\n",
      " 458932/825189: episode: 726, duration: 45.157s, episode steps: 596, steps per second:  13, episode reward: 27.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.042653, mae: 3.783562, mean_q: 4.554034, mean_eps: 0.100000\n",
      "📈 Episodio 727: Recompensa total (clipped): 14.000, Pasos: 401, Mean Reward Calculado: 0.034913 (Recompensa/Pasos)\n",
      " 459333/825189: episode: 727, duration: 30.464s, episode steps: 401, steps per second:  13, episode reward: 14.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.041004, mae: 3.726601, mean_q: 4.482260, mean_eps: 0.100000\n",
      "📈 Episodio 728: Recompensa total (clipped): 18.000, Pasos: 510, Mean Reward Calculado: 0.035294 (Recompensa/Pasos)\n",
      " 459843/825189: episode: 728, duration: 38.087s, episode steps: 510, steps per second:  13, episode reward: 18.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.027327, mae: 3.762071, mean_q: 4.531393, mean_eps: 0.100000\n",
      "📊 Paso 460,000/2,000,000 (23.0%) - 22.0 pasos/seg - ETA: 19.5h - Memoria: 11008.66 MB\n",
      "📈 Episodio 729: Recompensa total (clipped): 22.000, Pasos: 600, Mean Reward Calculado: 0.036667 (Recompensa/Pasos)\n",
      " 460443/825189: episode: 729, duration: 45.086s, episode steps: 600, steps per second:  13, episode reward: 22.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.039584, mae: 3.782237, mean_q: 4.554313, mean_eps: 0.100000\n",
      "📈 Episodio 730: Recompensa total (clipped): 16.000, Pasos: 352, Mean Reward Calculado: 0.045455 (Recompensa/Pasos)\n",
      " 460795/825189: episode: 730, duration: 26.476s, episode steps: 352, steps per second:  13, episode reward: 16.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.027965, mae: 3.778670, mean_q: 4.551992, mean_eps: 0.100000\n",
      "📈 Episodio 731: Recompensa total (clipped): 23.000, Pasos: 600, Mean Reward Calculado: 0.038333 (Recompensa/Pasos)\n",
      " 461395/825189: episode: 731, duration: 44.868s, episode steps: 600, steps per second:  13, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.036147, mae: 3.846568, mean_q: 4.630115, mean_eps: 0.100000\n",
      "📈 Episodio 732: Recompensa total (clipped): 25.000, Pasos: 553, Mean Reward Calculado: 0.045208 (Recompensa/Pasos)\n",
      " 461948/825189: episode: 732, duration: 41.710s, episode steps: 553, steps per second:  13, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.246 [0.000, 5.000],  loss: 0.035450, mae: 3.801537, mean_q: 4.577868, mean_eps: 0.100000\n",
      "📈 Episodio 733: Recompensa total (clipped): 31.000, Pasos: 726, Mean Reward Calculado: 0.042700 (Recompensa/Pasos)\n",
      " 462674/825189: episode: 733, duration: 54.510s, episode steps: 726, steps per second:  13, episode reward: 31.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.040229, mae: 3.795245, mean_q: 4.568442, mean_eps: 0.100000\n",
      "📈 Episodio 734: Recompensa total (clipped): 33.000, Pasos: 879, Mean Reward Calculado: 0.037543 (Recompensa/Pasos)\n",
      " 463553/825189: episode: 734, duration: 65.773s, episode steps: 879, steps per second:  13, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.037650, mae: 3.783015, mean_q: 4.555621, mean_eps: 0.100000\n",
      "📈 Episodio 735: Recompensa total (clipped): 30.000, Pasos: 629, Mean Reward Calculado: 0.047695 (Recompensa/Pasos)\n",
      " 464182/825189: episode: 735, duration: 47.358s, episode steps: 629, steps per second:  13, episode reward: 30.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.034860, mae: 3.765545, mean_q: 4.539186, mean_eps: 0.100000\n",
      "📈 Episodio 736: Recompensa total (clipped): 17.000, Pasos: 344, Mean Reward Calculado: 0.049419 (Recompensa/Pasos)\n",
      " 464526/825189: episode: 736, duration: 25.920s, episode steps: 344, steps per second:  13, episode reward: 17.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 3.424 [0.000, 5.000],  loss: 0.044719, mae: 3.811444, mean_q: 4.587155, mean_eps: 0.100000\n",
      "📈 Episodio 737: Recompensa total (clipped): 23.000, Pasos: 592, Mean Reward Calculado: 0.038851 (Recompensa/Pasos)\n",
      " 465118/825189: episode: 737, duration: 44.203s, episode steps: 592, steps per second:  13, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.035356, mae: 3.770859, mean_q: 4.541315, mean_eps: 0.100000\n",
      "📈 Episodio 738: Recompensa total (clipped): 34.000, Pasos: 1012, Mean Reward Calculado: 0.033597 (Recompensa/Pasos)\n",
      " 466130/825189: episode: 738, duration: 75.789s, episode steps: 1012, steps per second:  13, episode reward: 34.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.429 [0.000, 5.000],  loss: 0.037499, mae: 3.794830, mean_q: 4.570039, mean_eps: 0.100000\n",
      "📈 Episodio 739: Recompensa total (clipped): 23.000, Pasos: 575, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 466705/825189: episode: 739, duration: 43.222s, episode steps: 575, steps per second:  13, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.037 [0.000, 5.000],  loss: 0.038343, mae: 3.781649, mean_q: 4.556483, mean_eps: 0.100000\n",
      "📈 Episodio 740: Recompensa total (clipped): 22.000, Pasos: 597, Mean Reward Calculado: 0.036851 (Recompensa/Pasos)\n",
      " 467302/825189: episode: 740, duration: 44.696s, episode steps: 597, steps per second:  13, episode reward: 22.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.038316, mae: 3.768698, mean_q: 4.540646, mean_eps: 0.100000\n",
      "📈 Episodio 741: Recompensa total (clipped): 35.000, Pasos: 907, Mean Reward Calculado: 0.038589 (Recompensa/Pasos)\n",
      " 468209/825189: episode: 741, duration: 68.392s, episode steps: 907, steps per second:  13, episode reward: 35.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.041241, mae: 3.751409, mean_q: 4.518862, mean_eps: 0.100000\n",
      "📈 Episodio 742: Recompensa total (clipped): 32.000, Pasos: 822, Mean Reward Calculado: 0.038929 (Recompensa/Pasos)\n",
      " 469031/825189: episode: 742, duration: 61.707s, episode steps: 822, steps per second:  13, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.035243, mae: 3.750556, mean_q: 4.520083, mean_eps: 0.100000\n",
      "📈 Episodio 743: Recompensa total (clipped): 38.000, Pasos: 1125, Mean Reward Calculado: 0.033778 (Recompensa/Pasos)\n",
      " 470156/825189: episode: 743, duration: 84.173s, episode steps: 1125, steps per second:  13, episode reward: 38.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.037961, mae: 3.756865, mean_q: 4.525837, mean_eps: 0.100000\n",
      "📈 Episodio 744: Recompensa total (clipped): 26.000, Pasos: 630, Mean Reward Calculado: 0.041270 (Recompensa/Pasos)\n",
      " 470786/825189: episode: 744, duration: 47.449s, episode steps: 630, steps per second:  13, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.029208, mae: 3.795074, mean_q: 4.576757, mean_eps: 0.100000\n",
      "📈 Episodio 745: Recompensa total (clipped): 25.000, Pasos: 607, Mean Reward Calculado: 0.041186 (Recompensa/Pasos)\n",
      " 471393/825189: episode: 745, duration: 45.790s, episode steps: 607, steps per second:  13, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.037686, mae: 3.808173, mean_q: 4.590280, mean_eps: 0.100000\n",
      "📈 Episodio 746: Recompensa total (clipped): 32.000, Pasos: 777, Mean Reward Calculado: 0.041184 (Recompensa/Pasos)\n",
      " 472170/825189: episode: 746, duration: 58.171s, episode steps: 777, steps per second:  13, episode reward: 32.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.042614, mae: 3.740768, mean_q: 4.503698, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 747: Recompensa total (clipped): 30.000, Pasos: 805, Mean Reward Calculado: 0.037267 (Recompensa/Pasos)\n",
      " 472975/825189: episode: 747, duration: 60.542s, episode steps: 805, steps per second:  13, episode reward: 30.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.033312, mae: 3.778074, mean_q: 4.552399, mean_eps: 0.100000\n",
      "📈 Episodio 748: Recompensa total (clipped): 33.000, Pasos: 837, Mean Reward Calculado: 0.039427 (Recompensa/Pasos)\n",
      " 473812/825189: episode: 748, duration: 62.943s, episode steps: 837, steps per second:  13, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.038037, mae: 3.799183, mean_q: 4.576538, mean_eps: 0.100000\n",
      "📈 Episodio 749: Recompensa total (clipped): 27.000, Pasos: 591, Mean Reward Calculado: 0.045685 (Recompensa/Pasos)\n",
      " 474403/825189: episode: 749, duration: 44.541s, episode steps: 591, steps per second:  13, episode reward: 27.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.037291, mae: 3.786583, mean_q: 4.558816, mean_eps: 0.100000\n",
      "📈 Episodio 750: Recompensa total (clipped): 25.000, Pasos: 574, Mean Reward Calculado: 0.043554 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 750, pasos: 474977)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.22 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 750 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 25.00\n",
      "   Media últimos 100: 26.22 / 20.0\n",
      "   Mejor promedio histórico: 26.22\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 660\n",
      "   Episodios consecutivos en objetivo: 660\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 660 episodios consecutivos\n",
      " 474977/825189: episode: 750, duration: 96.172s, episode steps: 574, steps per second:   6, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.040295, mae: 3.764090, mean_q: 4.533513, mean_eps: 0.100000\n",
      "📈 Episodio 751: Recompensa total (clipped): 10.000, Pasos: 290, Mean Reward Calculado: 0.034483 (Recompensa/Pasos)\n",
      " 475267/825189: episode: 751, duration: 22.444s, episode steps: 290, steps per second:  13, episode reward: 10.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.238 [0.000, 5.000],  loss: 0.038148, mae: 3.842372, mean_q: 4.630846, mean_eps: 0.100000\n",
      "📈 Episodio 752: Recompensa total (clipped): 31.000, Pasos: 734, Mean Reward Calculado: 0.042234 (Recompensa/Pasos)\n",
      " 476001/825189: episode: 752, duration: 57.739s, episode steps: 734, steps per second:  13, episode reward: 31.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.035306, mae: 3.743706, mean_q: 4.509493, mean_eps: 0.100000\n",
      "📈 Episodio 753: Recompensa total (clipped): 23.000, Pasos: 555, Mean Reward Calculado: 0.041441 (Recompensa/Pasos)\n",
      " 476556/825189: episode: 753, duration: 43.766s, episode steps: 555, steps per second:  13, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.036799, mae: 3.796618, mean_q: 4.574480, mean_eps: 0.100000\n",
      "📈 Episodio 754: Recompensa total (clipped): 27.000, Pasos: 608, Mean Reward Calculado: 0.044408 (Recompensa/Pasos)\n",
      " 477164/825189: episode: 754, duration: 48.274s, episode steps: 608, steps per second:  13, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.034655, mae: 3.740177, mean_q: 4.505197, mean_eps: 0.100000\n",
      "📈 Episodio 755: Recompensa total (clipped): 33.000, Pasos: 1101, Mean Reward Calculado: 0.029973 (Recompensa/Pasos)\n",
      " 478265/825189: episode: 755, duration: 86.936s, episode steps: 1101, steps per second:  13, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.038503, mae: 3.787967, mean_q: 4.563257, mean_eps: 0.100000\n",
      "📈 Episodio 756: Recompensa total (clipped): 22.000, Pasos: 531, Mean Reward Calculado: 0.041431 (Recompensa/Pasos)\n",
      " 478796/825189: episode: 756, duration: 41.727s, episode steps: 531, steps per second:  13, episode reward: 22.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.043404, mae: 3.781767, mean_q: 4.553178, mean_eps: 0.100000\n",
      "📈 Episodio 757: Recompensa total (clipped): 25.000, Pasos: 614, Mean Reward Calculado: 0.040717 (Recompensa/Pasos)\n",
      " 479410/825189: episode: 757, duration: 48.603s, episode steps: 614, steps per second:  13, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.036082, mae: 3.785240, mean_q: 4.558414, mean_eps: 0.100000\n",
      "📈 Episodio 758: Recompensa total (clipped): 28.000, Pasos: 556, Mean Reward Calculado: 0.050360 (Recompensa/Pasos)\n",
      " 479966/825189: episode: 758, duration: 43.770s, episode steps: 556, steps per second:  13, episode reward: 28.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.035219, mae: 3.759233, mean_q: 4.528304, mean_eps: 0.100000\n",
      "📊 Paso 480,000/2,000,000 (24.0%) - 21.3 pasos/seg - ETA: 19.8h - Memoria: 11106.25 MB\n",
      "📈 Episodio 759: Recompensa total (clipped): 34.000, Pasos: 968, Mean Reward Calculado: 0.035124 (Recompensa/Pasos)\n",
      " 480934/825189: episode: 759, duration: 76.263s, episode steps: 968, steps per second:  13, episode reward: 34.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.032760, mae: 3.816491, mean_q: 4.600846, mean_eps: 0.100000\n",
      "📈 Episodio 760: Recompensa total (clipped): 22.000, Pasos: 634, Mean Reward Calculado: 0.034700 (Recompensa/Pasos)\n",
      " 481568/825189: episode: 760, duration: 49.798s, episode steps: 634, steps per second:  13, episode reward: 22.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.030414, mae: 3.788305, mean_q: 4.564324, mean_eps: 0.100000\n",
      "📈 Episodio 761: Recompensa total (clipped): 25.000, Pasos: 518, Mean Reward Calculado: 0.048263 (Recompensa/Pasos)\n",
      " 482086/825189: episode: 761, duration: 40.821s, episode steps: 518, steps per second:  13, episode reward: 25.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.032707, mae: 3.771896, mean_q: 4.544708, mean_eps: 0.100000\n",
      "📈 Episodio 762: Recompensa total (clipped): 31.000, Pasos: 694, Mean Reward Calculado: 0.044669 (Recompensa/Pasos)\n",
      " 482780/825189: episode: 762, duration: 54.762s, episode steps: 694, steps per second:  13, episode reward: 31.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.034464, mae: 3.765802, mean_q: 4.535359, mean_eps: 0.100000\n",
      "📈 Episodio 763: Recompensa total (clipped): 32.000, Pasos: 856, Mean Reward Calculado: 0.037383 (Recompensa/Pasos)\n",
      " 483636/825189: episode: 763, duration: 67.498s, episode steps: 856, steps per second:  13, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.037546, mae: 3.777736, mean_q: 4.551790, mean_eps: 0.100000\n",
      "📈 Episodio 764: Recompensa total (clipped): 31.000, Pasos: 777, Mean Reward Calculado: 0.039897 (Recompensa/Pasos)\n",
      " 484413/825189: episode: 764, duration: 60.980s, episode steps: 777, steps per second:  13, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.039052, mae: 3.850629, mean_q: 4.636767, mean_eps: 0.100000\n",
      "📈 Episodio 765: Recompensa total (clipped): 28.000, Pasos: 742, Mean Reward Calculado: 0.037736 (Recompensa/Pasos)\n",
      " 485155/825189: episode: 765, duration: 58.189s, episode steps: 742, steps per second:  13, episode reward: 28.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.040024, mae: 3.792700, mean_q: 4.567434, mean_eps: 0.100000\n",
      "📈 Episodio 766: Recompensa total (clipped): 25.000, Pasos: 519, Mean Reward Calculado: 0.048170 (Recompensa/Pasos)\n",
      " 485674/825189: episode: 766, duration: 40.860s, episode steps: 519, steps per second:  13, episode reward: 25.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.038265, mae: 3.800184, mean_q: 4.579273, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 767: Recompensa total (clipped): 23.000, Pasos: 604, Mean Reward Calculado: 0.038079 (Recompensa/Pasos)\n",
      " 486278/825189: episode: 767, duration: 47.574s, episode steps: 604, steps per second:  13, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.035372, mae: 3.807008, mean_q: 4.586423, mean_eps: 0.100000\n",
      "📈 Episodio 768: Recompensa total (clipped): 21.000, Pasos: 471, Mean Reward Calculado: 0.044586 (Recompensa/Pasos)\n",
      " 486749/825189: episode: 768, duration: 36.944s, episode steps: 471, steps per second:  13, episode reward: 21.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.372 [0.000, 5.000],  loss: 0.043984, mae: 3.762818, mean_q: 4.532739, mean_eps: 0.100000\n",
      "📈 Episodio 769: Recompensa total (clipped): 22.000, Pasos: 517, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 487266/825189: episode: 769, duration: 40.505s, episode steps: 517, steps per second:  13, episode reward: 22.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.039408, mae: 3.808175, mean_q: 4.586547, mean_eps: 0.100000\n",
      "📈 Episodio 770: Recompensa total (clipped): 25.000, Pasos: 683, Mean Reward Calculado: 0.036603 (Recompensa/Pasos)\n",
      " 487949/825189: episode: 770, duration: 53.579s, episode steps: 683, steps per second:  13, episode reward: 25.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.041919, mae: 3.777981, mean_q: 4.550838, mean_eps: 0.100000\n",
      "📈 Episodio 771: Recompensa total (clipped): 27.000, Pasos: 642, Mean Reward Calculado: 0.042056 (Recompensa/Pasos)\n",
      " 488591/825189: episode: 771, duration: 50.166s, episode steps: 642, steps per second:  13, episode reward: 27.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.036506, mae: 3.824136, mean_q: 4.604519, mean_eps: 0.100000\n",
      "📈 Episodio 772: Recompensa total (clipped): 30.000, Pasos: 715, Mean Reward Calculado: 0.041958 (Recompensa/Pasos)\n",
      " 489306/825189: episode: 772, duration: 56.513s, episode steps: 715, steps per second:  13, episode reward: 30.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.040088, mae: 3.800099, mean_q: 4.577366, mean_eps: 0.100000\n",
      "📈 Episodio 773: Recompensa total (clipped): 25.000, Pasos: 526, Mean Reward Calculado: 0.047529 (Recompensa/Pasos)\n",
      " 489832/825189: episode: 773, duration: 41.602s, episode steps: 526, steps per second:  13, episode reward: 25.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.047591, mae: 3.804620, mean_q: 4.580245, mean_eps: 0.100000\n",
      "📈 Episodio 774: Recompensa total (clipped): 34.000, Pasos: 870, Mean Reward Calculado: 0.039080 (Recompensa/Pasos)\n",
      " 490702/825189: episode: 774, duration: 68.434s, episode steps: 870, steps per second:  13, episode reward: 34.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.039725, mae: 3.793115, mean_q: 4.568664, mean_eps: 0.100000\n",
      "📈 Episodio 775: Recompensa total (clipped): 33.000, Pasos: 704, Mean Reward Calculado: 0.046875 (Recompensa/Pasos)\n",
      " 491406/825189: episode: 775, duration: 55.230s, episode steps: 704, steps per second:  13, episode reward: 33.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.036157, mae: 3.783664, mean_q: 4.557923, mean_eps: 0.100000\n",
      "📈 Episodio 776: Recompensa total (clipped): 25.000, Pasos: 638, Mean Reward Calculado: 0.039185 (Recompensa/Pasos)\n",
      " 492044/825189: episode: 776, duration: 50.275s, episode steps: 638, steps per second:  13, episode reward: 25.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.042282, mae: 3.820991, mean_q: 4.600513, mean_eps: 0.100000\n",
      "📈 Episodio 777: Recompensa total (clipped): 27.000, Pasos: 684, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 492728/825189: episode: 777, duration: 54.017s, episode steps: 684, steps per second:  13, episode reward: 27.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.037687, mae: 3.789723, mean_q: 4.562803, mean_eps: 0.100000\n",
      "📈 Episodio 778: Recompensa total (clipped): 34.000, Pasos: 857, Mean Reward Calculado: 0.039673 (Recompensa/Pasos)\n",
      " 493585/825189: episode: 778, duration: 67.564s, episode steps: 857, steps per second:  13, episode reward: 34.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.680 [0.000, 5.000],  loss: 0.038873, mae: 3.791717, mean_q: 4.567313, mean_eps: 0.100000\n",
      "📈 Episodio 779: Recompensa total (clipped): 13.000, Pasos: 371, Mean Reward Calculado: 0.035040 (Recompensa/Pasos)\n",
      " 493956/825189: episode: 779, duration: 29.353s, episode steps: 371, steps per second:  13, episode reward: 13.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.523 [0.000, 5.000],  loss: 0.041650, mae: 3.775809, mean_q: 4.548195, mean_eps: 0.100000\n",
      "📈 Episodio 780: Recompensa total (clipped): 15.000, Pasos: 381, Mean Reward Calculado: 0.039370 (Recompensa/Pasos)\n",
      " 494337/825189: episode: 780, duration: 30.179s, episode steps: 381, steps per second:  13, episode reward: 15.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.845 [0.000, 5.000],  loss: 0.036725, mae: 3.784741, mean_q: 4.559450, mean_eps: 0.100000\n",
      "📈 Episodio 781: Recompensa total (clipped): 35.000, Pasos: 852, Mean Reward Calculado: 0.041080 (Recompensa/Pasos)\n",
      " 495189/825189: episode: 781, duration: 66.942s, episode steps: 852, steps per second:  13, episode reward: 35.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.038618, mae: 3.790790, mean_q: 4.563362, mean_eps: 0.100000\n",
      "📈 Episodio 782: Recompensa total (clipped): 16.000, Pasos: 394, Mean Reward Calculado: 0.040609 (Recompensa/Pasos)\n",
      " 495583/825189: episode: 782, duration: 31.059s, episode steps: 394, steps per second:  13, episode reward: 16.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.043185, mae: 3.831120, mean_q: 4.612508, mean_eps: 0.100000\n",
      "📈 Episodio 783: Recompensa total (clipped): 15.000, Pasos: 305, Mean Reward Calculado: 0.049180 (Recompensa/Pasos)\n",
      " 495888/825189: episode: 783, duration: 24.196s, episode steps: 305, steps per second:  13, episode reward: 15.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.039277, mae: 3.798975, mean_q: 4.576925, mean_eps: 0.100000\n",
      "📈 Episodio 784: Recompensa total (clipped): 33.000, Pasos: 793, Mean Reward Calculado: 0.041614 (Recompensa/Pasos)\n",
      " 496681/825189: episode: 784, duration: 62.725s, episode steps: 793, steps per second:  13, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.036718, mae: 3.785300, mean_q: 4.557709, mean_eps: 0.100000\n",
      "📈 Episodio 785: Recompensa total (clipped): 25.000, Pasos: 529, Mean Reward Calculado: 0.047259 (Recompensa/Pasos)\n",
      " 497210/825189: episode: 785, duration: 41.510s, episode steps: 529, steps per second:  13, episode reward: 25.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.035621, mae: 3.805993, mean_q: 4.584853, mean_eps: 0.100000\n",
      "📈 Episodio 786: Recompensa total (clipped): 28.000, Pasos: 696, Mean Reward Calculado: 0.040230 (Recompensa/Pasos)\n",
      " 497906/825189: episode: 786, duration: 54.634s, episode steps: 696, steps per second:  13, episode reward: 28.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.037212, mae: 3.827154, mean_q: 4.611628, mean_eps: 0.100000\n",
      "📈 Episodio 787: Recompensa total (clipped): 33.000, Pasos: 944, Mean Reward Calculado: 0.034958 (Recompensa/Pasos)\n",
      " 498850/825189: episode: 787, duration: 74.186s, episode steps: 944, steps per second:  13, episode reward: 33.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.041451, mae: 3.783784, mean_q: 4.557835, mean_eps: 0.100000\n",
      "📈 Episodio 788: Recompensa total (clipped): 23.000, Pasos: 574, Mean Reward Calculado: 0.040070 (Recompensa/Pasos)\n",
      " 499424/825189: episode: 788, duration: 45.454s, episode steps: 574, steps per second:  13, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.037982, mae: 3.799651, mean_q: 4.576382, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 789: Recompensa total (clipped): 16.000, Pasos: 383, Mean Reward Calculado: 0.041775 (Recompensa/Pasos)\n",
      " 499807/825189: episode: 789, duration: 30.433s, episode steps: 383, steps per second:  13, episode reward: 16.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.240 [0.000, 5.000],  loss: 0.037177, mae: 3.830829, mean_q: 4.614350, mean_eps: 0.100000\n",
      "📊 Paso 500,000/2,000,000 (25.0%) - 20.8 pasos/seg - ETA: 20.1h - Memoria: 11106.88 MB\n",
      "📈 Episodio 790: Recompensa total (clipped): 31.000, Pasos: 828, Mean Reward Calculado: 0.037440 (Recompensa/Pasos)\n",
      " 500635/825189: episode: 790, duration: 64.903s, episode steps: 828, steps per second:  13, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.034073, mae: 3.846990, mean_q: 4.636002, mean_eps: 0.100000\n",
      "📈 Episodio 791: Recompensa total (clipped): 26.000, Pasos: 641, Mean Reward Calculado: 0.040562 (Recompensa/Pasos)\n",
      " 501276/825189: episode: 791, duration: 50.522s, episode steps: 641, steps per second:  13, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.028841, mae: 3.855584, mean_q: 4.644347, mean_eps: 0.100000\n",
      "📈 Episodio 792: Recompensa total (clipped): 31.000, Pasos: 819, Mean Reward Calculado: 0.037851 (Recompensa/Pasos)\n",
      " 502095/825189: episode: 792, duration: 64.175s, episode steps: 819, steps per second:  13, episode reward: 31.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.036098, mae: 3.839059, mean_q: 4.623110, mean_eps: 0.100000\n",
      "📈 Episodio 793: Recompensa total (clipped): 32.000, Pasos: 738, Mean Reward Calculado: 0.043360 (Recompensa/Pasos)\n",
      " 502833/825189: episode: 793, duration: 58.048s, episode steps: 738, steps per second:  13, episode reward: 32.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.036279, mae: 3.831767, mean_q: 4.615106, mean_eps: 0.100000\n",
      "📈 Episodio 794: Recompensa total (clipped): 32.000, Pasos: 778, Mean Reward Calculado: 0.041131 (Recompensa/Pasos)\n",
      " 503611/825189: episode: 794, duration: 60.910s, episode steps: 778, steps per second:  13, episode reward: 32.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.035117, mae: 3.814828, mean_q: 4.596966, mean_eps: 0.100000\n",
      "📈 Episodio 795: Recompensa total (clipped): 25.000, Pasos: 618, Mean Reward Calculado: 0.040453 (Recompensa/Pasos)\n",
      " 504229/825189: episode: 795, duration: 48.651s, episode steps: 618, steps per second:  13, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.039035, mae: 3.827413, mean_q: 4.610520, mean_eps: 0.100000\n",
      "📈 Episodio 796: Recompensa total (clipped): 33.000, Pasos: 720, Mean Reward Calculado: 0.045833 (Recompensa/Pasos)\n",
      " 504949/825189: episode: 796, duration: 56.403s, episode steps: 720, steps per second:  13, episode reward: 33.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.045527, mae: 3.871343, mean_q: 4.661622, mean_eps: 0.100000\n",
      "📈 Episodio 797: Recompensa total (clipped): 19.000, Pasos: 392, Mean Reward Calculado: 0.048469 (Recompensa/Pasos)\n",
      " 505341/825189: episode: 797, duration: 30.706s, episode steps: 392, steps per second:  13, episode reward: 19.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.042322, mae: 3.823118, mean_q: 4.604848, mean_eps: 0.100000\n",
      "📈 Episodio 798: Recompensa total (clipped): 16.000, Pasos: 510, Mean Reward Calculado: 0.031373 (Recompensa/Pasos)\n",
      " 505851/825189: episode: 798, duration: 39.803s, episode steps: 510, steps per second:  13, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.040958, mae: 3.843581, mean_q: 4.628139, mean_eps: 0.100000\n",
      "📈 Episodio 799: Recompensa total (clipped): 19.000, Pasos: 428, Mean Reward Calculado: 0.044393 (Recompensa/Pasos)\n",
      " 506279/825189: episode: 799, duration: 33.546s, episode steps: 428, steps per second:  13, episode reward: 19.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.465 [0.000, 5.000],  loss: 0.047292, mae: 3.827589, mean_q: 4.608918, mean_eps: 0.100000\n",
      "📈 Episodio 800: Recompensa total (clipped): 27.000, Pasos: 807, Mean Reward Calculado: 0.033457 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 800, pasos: 507086)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.00 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 800 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 27.00\n",
      "   Media últimos 100: 26.00 / 20.0\n",
      "   Mejor promedio histórico: 26.00\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 710\n",
      "   Episodios consecutivos en objetivo: 710\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 710 episodios consecutivos\n",
      " 507086/825189: episode: 800, duration: 119.437s, episode steps: 807, steps per second:   7, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.041162, mae: 3.819182, mean_q: 4.600320, mean_eps: 0.100000\n",
      "📈 Episodio 801: Recompensa total (clipped): 32.000, Pasos: 861, Mean Reward Calculado: 0.037166 (Recompensa/Pasos)\n",
      " 507947/825189: episode: 801, duration: 67.799s, episode steps: 861, steps per second:  13, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.036929, mae: 3.779304, mean_q: 4.551537, mean_eps: 0.100000\n",
      "📈 Episodio 802: Recompensa total (clipped): 21.000, Pasos: 543, Mean Reward Calculado: 0.038674 (Recompensa/Pasos)\n",
      " 508490/825189: episode: 802, duration: 42.493s, episode steps: 543, steps per second:  13, episode reward: 21.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.039361, mae: 3.797315, mean_q: 4.574003, mean_eps: 0.100000\n",
      "📈 Episodio 803: Recompensa total (clipped): 31.000, Pasos: 685, Mean Reward Calculado: 0.045255 (Recompensa/Pasos)\n",
      " 509175/825189: episode: 803, duration: 53.741s, episode steps: 685, steps per second:  13, episode reward: 31.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.041170, mae: 3.824457, mean_q: 4.607981, mean_eps: 0.100000\n",
      "📈 Episodio 804: Recompensa total (clipped): 33.000, Pasos: 827, Mean Reward Calculado: 0.039903 (Recompensa/Pasos)\n",
      " 510002/825189: episode: 804, duration: 64.918s, episode steps: 827, steps per second:  13, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.989 [0.000, 5.000],  loss: 0.032092, mae: 3.798016, mean_q: 4.576959, mean_eps: 0.100000\n",
      "📈 Episodio 805: Recompensa total (clipped): 30.000, Pasos: 808, Mean Reward Calculado: 0.037129 (Recompensa/Pasos)\n",
      " 510810/825189: episode: 805, duration: 63.876s, episode steps: 808, steps per second:  13, episode reward: 30.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.048 [0.000, 5.000],  loss: 0.039237, mae: 3.839412, mean_q: 4.625501, mean_eps: 0.100000\n",
      "📈 Episodio 806: Recompensa total (clipped): 26.000, Pasos: 572, Mean Reward Calculado: 0.045455 (Recompensa/Pasos)\n",
      " 511382/825189: episode: 806, duration: 45.079s, episode steps: 572, steps per second:  13, episode reward: 26.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.032503, mae: 3.871246, mean_q: 4.667105, mean_eps: 0.100000\n",
      "📈 Episodio 807: Recompensa total (clipped): 36.000, Pasos: 962, Mean Reward Calculado: 0.037422 (Recompensa/Pasos)\n",
      " 512344/825189: episode: 807, duration: 75.800s, episode steps: 962, steps per second:  13, episode reward: 36.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.932 [0.000, 5.000],  loss: 0.039511, mae: 3.883877, mean_q: 4.678698, mean_eps: 0.100000\n",
      "📈 Episodio 808: Recompensa total (clipped): 26.000, Pasos: 633, Mean Reward Calculado: 0.041074 (Recompensa/Pasos)\n",
      " 512977/825189: episode: 808, duration: 50.094s, episode steps: 633, steps per second:  13, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.155 [0.000, 5.000],  loss: 0.036852, mae: 3.849016, mean_q: 4.635650, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 809: Recompensa total (clipped): 18.000, Pasos: 389, Mean Reward Calculado: 0.046272 (Recompensa/Pasos)\n",
      " 513366/825189: episode: 809, duration: 30.618s, episode steps: 389, steps per second:  13, episode reward: 18.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.039464, mae: 3.826952, mean_q: 4.608905, mean_eps: 0.100000\n",
      "📈 Episodio 810: Recompensa total (clipped): 24.000, Pasos: 564, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 513930/825189: episode: 810, duration: 47.545s, episode steps: 564, steps per second:  12, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.034611, mae: 3.863199, mean_q: 4.651716, mean_eps: 0.100000\n",
      "📈 Episodio 811: Recompensa total (clipped): 31.000, Pasos: 1040, Mean Reward Calculado: 0.029808 (Recompensa/Pasos)\n",
      " 514970/825189: episode: 811, duration: 85.870s, episode steps: 1040, steps per second:  12, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.033827, mae: 3.845562, mean_q: 4.631324, mean_eps: 0.100000\n",
      "📈 Episodio 812: Recompensa total (clipped): 30.000, Pasos: 809, Mean Reward Calculado: 0.037083 (Recompensa/Pasos)\n",
      " 515779/825189: episode: 812, duration: 63.780s, episode steps: 809, steps per second:  13, episode reward: 30.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.037178, mae: 3.850092, mean_q: 4.635329, mean_eps: 0.100000\n",
      "📈 Episodio 813: Recompensa total (clipped): 34.000, Pasos: 973, Mean Reward Calculado: 0.034943 (Recompensa/Pasos)\n",
      " 516752/825189: episode: 813, duration: 76.828s, episode steps: 973, steps per second:  13, episode reward: 34.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.036607, mae: 3.838152, mean_q: 4.619918, mean_eps: 0.100000\n",
      "📈 Episodio 814: Recompensa total (clipped): 33.000, Pasos: 1075, Mean Reward Calculado: 0.030698 (Recompensa/Pasos)\n",
      " 517827/825189: episode: 814, duration: 84.352s, episode steps: 1075, steps per second:  13, episode reward: 33.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.147 [0.000, 5.000],  loss: 0.041917, mae: 3.876062, mean_q: 4.665713, mean_eps: 0.100000\n",
      "📈 Episodio 815: Recompensa total (clipped): 25.000, Pasos: 500, Mean Reward Calculado: 0.050000 (Recompensa/Pasos)\n",
      " 518327/825189: episode: 815, duration: 39.416s, episode steps: 500, steps per second:  13, episode reward: 25.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 3.370 [0.000, 5.000],  loss: 0.038468, mae: 3.827779, mean_q: 4.606495, mean_eps: 0.100000\n",
      "📈 Episodio 816: Recompensa total (clipped): 16.000, Pasos: 389, Mean Reward Calculado: 0.041131 (Recompensa/Pasos)\n",
      " 518716/825189: episode: 816, duration: 31.037s, episode steps: 389, steps per second:  13, episode reward: 16.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.152 [0.000, 5.000],  loss: 0.040079, mae: 3.831266, mean_q: 4.614304, mean_eps: 0.100000\n",
      "📈 Episodio 817: Recompensa total (clipped): 25.000, Pasos: 524, Mean Reward Calculado: 0.047710 (Recompensa/Pasos)\n",
      " 519240/825189: episode: 817, duration: 41.655s, episode steps: 524, steps per second:  13, episode reward: 25.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.966 [0.000, 5.000],  loss: 0.033894, mae: 3.880626, mean_q: 4.675249, mean_eps: 0.100000\n",
      "📊 Paso 520,000/2,000,000 (26.0%) - 20.2 pasos/seg - ETA: 20.3h - Memoria: 11145.09 MB\n",
      "📈 Episodio 818: Recompensa total (clipped): 31.000, Pasos: 906, Mean Reward Calculado: 0.034216 (Recompensa/Pasos)\n",
      " 520146/825189: episode: 818, duration: 71.535s, episode steps: 906, steps per second:  13, episode reward: 31.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.036967, mae: 3.886663, mean_q: 4.680570, mean_eps: 0.100000\n",
      "📈 Episodio 819: Recompensa total (clipped): 14.000, Pasos: 422, Mean Reward Calculado: 0.033175 (Recompensa/Pasos)\n",
      " 520568/825189: episode: 819, duration: 33.444s, episode steps: 422, steps per second:  13, episode reward: 14.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.031475, mae: 3.852214, mean_q: 4.637816, mean_eps: 0.100000\n",
      "📈 Episodio 820: Recompensa total (clipped): 33.000, Pasos: 803, Mean Reward Calculado: 0.041096 (Recompensa/Pasos)\n",
      " 521371/825189: episode: 820, duration: 62.941s, episode steps: 803, steps per second:  13, episode reward: 33.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.034524, mae: 3.828700, mean_q: 4.609271, mean_eps: 0.100000\n",
      "📈 Episodio 821: Recompensa total (clipped): 27.000, Pasos: 617, Mean Reward Calculado: 0.043760 (Recompensa/Pasos)\n",
      " 521988/825189: episode: 821, duration: 48.698s, episode steps: 617, steps per second:  13, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.034636, mae: 3.848348, mean_q: 4.634123, mean_eps: 0.100000\n",
      "📈 Episodio 822: Recompensa total (clipped): 29.000, Pasos: 633, Mean Reward Calculado: 0.045814 (Recompensa/Pasos)\n",
      " 522621/825189: episode: 822, duration: 50.093s, episode steps: 633, steps per second:  13, episode reward: 29.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.670 [0.000, 5.000],  loss: 0.037243, mae: 3.844993, mean_q: 4.630152, mean_eps: 0.100000\n",
      "📈 Episodio 823: Recompensa total (clipped): 35.000, Pasos: 903, Mean Reward Calculado: 0.038760 (Recompensa/Pasos)\n",
      " 523524/825189: episode: 823, duration: 70.740s, episode steps: 903, steps per second:  13, episode reward: 35.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.036471, mae: 3.878438, mean_q: 4.671287, mean_eps: 0.100000\n",
      "📈 Episodio 824: Recompensa total (clipped): 21.000, Pasos: 487, Mean Reward Calculado: 0.043121 (Recompensa/Pasos)\n",
      " 524011/825189: episode: 824, duration: 38.487s, episode steps: 487, steps per second:  13, episode reward: 21.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.041489, mae: 3.833692, mean_q: 4.616262, mean_eps: 0.100000\n",
      "📈 Episodio 825: Recompensa total (clipped): 31.000, Pasos: 716, Mean Reward Calculado: 0.043296 (Recompensa/Pasos)\n",
      " 524727/825189: episode: 825, duration: 56.084s, episode steps: 716, steps per second:  13, episode reward: 31.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.041883, mae: 3.842365, mean_q: 4.625011, mean_eps: 0.100000\n",
      "📈 Episodio 826: Recompensa total (clipped): 26.000, Pasos: 686, Mean Reward Calculado: 0.037901 (Recompensa/Pasos)\n",
      " 525413/825189: episode: 826, duration: 54.250s, episode steps: 686, steps per second:  13, episode reward: 26.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.039373, mae: 3.817718, mean_q: 4.595374, mean_eps: 0.100000\n",
      "📈 Episodio 827: Recompensa total (clipped): 24.000, Pasos: 579, Mean Reward Calculado: 0.041451 (Recompensa/Pasos)\n",
      " 525992/825189: episode: 827, duration: 45.649s, episode steps: 579, steps per second:  13, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.045208, mae: 3.838744, mean_q: 4.621470, mean_eps: 0.100000\n",
      "📈 Episodio 828: Recompensa total (clipped): 29.000, Pasos: 884, Mean Reward Calculado: 0.032805 (Recompensa/Pasos)\n",
      " 526876/825189: episode: 828, duration: 69.610s, episode steps: 884, steps per second:  13, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.037496, mae: 3.869946, mean_q: 4.662483, mean_eps: 0.100000\n",
      "📈 Episodio 829: Recompensa total (clipped): 26.000, Pasos: 627, Mean Reward Calculado: 0.041467 (Recompensa/Pasos)\n",
      " 527503/825189: episode: 829, duration: 49.317s, episode steps: 627, steps per second:  13, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.039169, mae: 3.823783, mean_q: 4.604364, mean_eps: 0.100000\n",
      "📈 Episodio 830: Recompensa total (clipped): 19.000, Pasos: 460, Mean Reward Calculado: 0.041304 (Recompensa/Pasos)\n",
      " 527963/825189: episode: 830, duration: 36.164s, episode steps: 460, steps per second:  13, episode reward: 19.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.032854, mae: 3.826770, mean_q: 4.613117, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 831: Recompensa total (clipped): 26.000, Pasos: 631, Mean Reward Calculado: 0.041204 (Recompensa/Pasos)\n",
      " 528594/825189: episode: 831, duration: 49.675s, episode steps: 631, steps per second:  13, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.036467, mae: 3.851824, mean_q: 4.641081, mean_eps: 0.100000\n",
      "📈 Episodio 832: Recompensa total (clipped): 33.000, Pasos: 969, Mean Reward Calculado: 0.034056 (Recompensa/Pasos)\n",
      " 529563/825189: episode: 832, duration: 75.800s, episode steps: 969, steps per second:  13, episode reward: 33.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.036111, mae: 3.864419, mean_q: 4.654007, mean_eps: 0.100000\n",
      "📈 Episodio 833: Recompensa total (clipped): 25.000, Pasos: 560, Mean Reward Calculado: 0.044643 (Recompensa/Pasos)\n",
      " 530123/825189: episode: 833, duration: 44.058s, episode steps: 560, steps per second:  13, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.182 [0.000, 5.000],  loss: 0.032894, mae: 3.860648, mean_q: 4.648027, mean_eps: 0.100000\n",
      "📈 Episodio 834: Recompensa total (clipped): 34.000, Pasos: 852, Mean Reward Calculado: 0.039906 (Recompensa/Pasos)\n",
      " 530975/825189: episode: 834, duration: 67.149s, episode steps: 852, steps per second:  13, episode reward: 34.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.040210, mae: 3.852813, mean_q: 4.638190, mean_eps: 0.100000\n",
      "📈 Episodio 835: Recompensa total (clipped): 28.000, Pasos: 646, Mean Reward Calculado: 0.043344 (Recompensa/Pasos)\n",
      " 531621/825189: episode: 835, duration: 51.027s, episode steps: 646, steps per second:  13, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.033977, mae: 3.832312, mean_q: 4.613969, mean_eps: 0.100000\n",
      "📈 Episodio 836: Recompensa total (clipped): 30.000, Pasos: 716, Mean Reward Calculado: 0.041899 (Recompensa/Pasos)\n",
      " 532337/825189: episode: 836, duration: 56.476s, episode steps: 716, steps per second:  13, episode reward: 30.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.035845, mae: 3.862949, mean_q: 4.651481, mean_eps: 0.100000\n",
      "📈 Episodio 837: Recompensa total (clipped): 23.000, Pasos: 502, Mean Reward Calculado: 0.045817 (Recompensa/Pasos)\n",
      " 532839/825189: episode: 837, duration: 39.461s, episode steps: 502, steps per second:  13, episode reward: 23.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 3.175 [0.000, 5.000],  loss: 0.032595, mae: 3.847860, mean_q: 4.633561, mean_eps: 0.100000\n",
      "📈 Episodio 838: Recompensa total (clipped): 24.000, Pasos: 480, Mean Reward Calculado: 0.050000 (Recompensa/Pasos)\n",
      " 533319/825189: episode: 838, duration: 37.802s, episode steps: 480, steps per second:  13, episode reward: 24.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.040598, mae: 3.785124, mean_q: 4.555890, mean_eps: 0.100000\n",
      "📈 Episodio 839: Recompensa total (clipped): 14.000, Pasos: 455, Mean Reward Calculado: 0.030769 (Recompensa/Pasos)\n",
      " 533774/825189: episode: 839, duration: 35.903s, episode steps: 455, steps per second:  13, episode reward: 14.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.027970, mae: 3.810178, mean_q: 4.588047, mean_eps: 0.100000\n",
      "📈 Episodio 840: Recompensa total (clipped): 31.000, Pasos: 884, Mean Reward Calculado: 0.035068 (Recompensa/Pasos)\n",
      " 534658/825189: episode: 840, duration: 69.675s, episode steps: 884, steps per second:  13, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.037071, mae: 3.867413, mean_q: 4.654473, mean_eps: 0.100000\n",
      "📈 Episodio 841: Recompensa total (clipped): 21.000, Pasos: 505, Mean Reward Calculado: 0.041584 (Recompensa/Pasos)\n",
      " 535163/825189: episode: 841, duration: 39.652s, episode steps: 505, steps per second:  13, episode reward: 21.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.037129, mae: 3.817210, mean_q: 4.590103, mean_eps: 0.100000\n",
      "📈 Episodio 842: Recompensa total (clipped): 27.000, Pasos: 612, Mean Reward Calculado: 0.044118 (Recompensa/Pasos)\n",
      " 535775/825189: episode: 842, duration: 48.167s, episode steps: 612, steps per second:  13, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.033958, mae: 3.829226, mean_q: 4.611012, mean_eps: 0.100000\n",
      "📈 Episodio 843: Recompensa total (clipped): 27.000, Pasos: 696, Mean Reward Calculado: 0.038793 (Recompensa/Pasos)\n",
      " 536471/825189: episode: 843, duration: 54.778s, episode steps: 696, steps per second:  13, episode reward: 27.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.036141, mae: 3.878857, mean_q: 4.669851, mean_eps: 0.100000\n",
      "📈 Episodio 844: Recompensa total (clipped): 27.000, Pasos: 627, Mean Reward Calculado: 0.043062 (Recompensa/Pasos)\n",
      " 537098/825189: episode: 844, duration: 49.397s, episode steps: 627, steps per second:  13, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.034732, mae: 3.851706, mean_q: 4.636805, mean_eps: 0.100000\n",
      "📈 Episodio 845: Recompensa total (clipped): 29.000, Pasos: 568, Mean Reward Calculado: 0.051056 (Recompensa/Pasos)\n",
      " 537666/825189: episode: 845, duration: 44.737s, episode steps: 568, steps per second:  13, episode reward: 29.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.035441, mae: 3.803080, mean_q: 4.575294, mean_eps: 0.100000\n",
      "📈 Episodio 846: Recompensa total (clipped): 27.000, Pasos: 611, Mean Reward Calculado: 0.044190 (Recompensa/Pasos)\n",
      " 538277/825189: episode: 846, duration: 48.555s, episode steps: 611, steps per second:  13, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.036846, mae: 3.858072, mean_q: 4.642631, mean_eps: 0.100000\n",
      "📈 Episodio 847: Recompensa total (clipped): 16.000, Pasos: 424, Mean Reward Calculado: 0.037736 (Recompensa/Pasos)\n",
      " 538701/825189: episode: 847, duration: 33.572s, episode steps: 424, steps per second:  13, episode reward: 16.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.041037, mae: 3.850296, mean_q: 4.631631, mean_eps: 0.100000\n",
      "📈 Episodio 848: Recompensa total (clipped): 29.000, Pasos: 797, Mean Reward Calculado: 0.036386 (Recompensa/Pasos)\n",
      " 539498/825189: episode: 848, duration: 62.741s, episode steps: 797, steps per second:  13, episode reward: 29.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.609 [0.000, 5.000],  loss: 0.035532, mae: 3.836356, mean_q: 4.616807, mean_eps: 0.100000\n",
      "📊 Paso 540,000/2,000,000 (27.0%) - 19.8 pasos/seg - ETA: 20.5h - Memoria: 11145.22 MB\n",
      "📈 Episodio 849: Recompensa total (clipped): 28.000, Pasos: 721, Mean Reward Calculado: 0.038835 (Recompensa/Pasos)\n",
      " 540219/825189: episode: 849, duration: 56.651s, episode steps: 721, steps per second:  13, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.043909, mae: 3.846537, mean_q: 4.628818, mean_eps: 0.100000\n",
      "📈 Episodio 850: Recompensa total (clipped): 30.000, Pasos: 926, Mean Reward Calculado: 0.032397 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 850, pasos: 541145)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.41 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 850 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 30.00\n",
      "   Media últimos 100: 26.41 / 20.0\n",
      "   Mejor promedio histórico: 26.41\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 760\n",
      "   Episodios consecutivos en objetivo: 760\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 760 episodios consecutivos\n",
      " 541145/825189: episode: 850, duration: 129.366s, episode steps: 926, steps per second:   7, episode reward: 30.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.036328, mae: 3.831744, mean_q: 4.612905, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 851: Recompensa total (clipped): 18.000, Pasos: 554, Mean Reward Calculado: 0.032491 (Recompensa/Pasos)\n",
      " 541699/825189: episode: 851, duration: 45.338s, episode steps: 554, steps per second:  12, episode reward: 18.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.036589, mae: 3.859637, mean_q: 4.646992, mean_eps: 0.100000\n",
      "📈 Episodio 852: Recompensa total (clipped): 32.000, Pasos: 899, Mean Reward Calculado: 0.035595 (Recompensa/Pasos)\n",
      " 542598/825189: episode: 852, duration: 73.768s, episode steps: 899, steps per second:  12, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.033244, mae: 3.879070, mean_q: 4.670666, mean_eps: 0.100000\n",
      "📈 Episodio 853: Recompensa total (clipped): 27.000, Pasos: 568, Mean Reward Calculado: 0.047535 (Recompensa/Pasos)\n",
      " 543166/825189: episode: 853, duration: 46.871s, episode steps: 568, steps per second:  12, episode reward: 27.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.033748, mae: 3.858746, mean_q: 4.648536, mean_eps: 0.100000\n",
      "📈 Episodio 854: Recompensa total (clipped): 24.000, Pasos: 620, Mean Reward Calculado: 0.038710 (Recompensa/Pasos)\n",
      " 543786/825189: episode: 854, duration: 64.438s, episode steps: 620, steps per second:  10, episode reward: 24.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.042585, mae: 3.815518, mean_q: 4.592079, mean_eps: 0.100000\n",
      "📈 Episodio 855: Recompensa total (clipped): 18.000, Pasos: 451, Mean Reward Calculado: 0.039911 (Recompensa/Pasos)\n",
      " 544237/825189: episode: 855, duration: 40.547s, episode steps: 451, steps per second:  11, episode reward: 18.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.039699, mae: 3.871086, mean_q: 4.660259, mean_eps: 0.100000\n",
      "📈 Episodio 856: Recompensa total (clipped): 31.000, Pasos: 718, Mean Reward Calculado: 0.043175 (Recompensa/Pasos)\n",
      " 544955/825189: episode: 856, duration: 59.100s, episode steps: 718, steps per second:  12, episode reward: 31.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.039640, mae: 3.839255, mean_q: 4.621975, mean_eps: 0.100000\n",
      "📈 Episodio 857: Recompensa total (clipped): 23.000, Pasos: 613, Mean Reward Calculado: 0.037520 (Recompensa/Pasos)\n",
      " 545568/825189: episode: 857, duration: 50.893s, episode steps: 613, steps per second:  12, episode reward: 23.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.046774, mae: 3.842590, mean_q: 4.624016, mean_eps: 0.100000\n",
      "📈 Episodio 858: Recompensa total (clipped): 30.000, Pasos: 631, Mean Reward Calculado: 0.047544 (Recompensa/Pasos)\n",
      " 546199/825189: episode: 858, duration: 52.056s, episode steps: 631, steps per second:  12, episode reward: 30.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.040175, mae: 3.863100, mean_q: 4.650114, mean_eps: 0.100000\n",
      "📈 Episodio 859: Recompensa total (clipped): 31.000, Pasos: 795, Mean Reward Calculado: 0.038994 (Recompensa/Pasos)\n",
      " 546994/825189: episode: 859, duration: 65.579s, episode steps: 795, steps per second:  12, episode reward: 31.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.030163, mae: 3.858304, mean_q: 4.647739, mean_eps: 0.100000\n",
      "📈 Episodio 860: Recompensa total (clipped): 28.000, Pasos: 596, Mean Reward Calculado: 0.046980 (Recompensa/Pasos)\n",
      " 547590/825189: episode: 860, duration: 48.916s, episode steps: 596, steps per second:  12, episode reward: 28.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.032165, mae: 3.856335, mean_q: 4.643202, mean_eps: 0.100000\n",
      "📈 Episodio 861: Recompensa total (clipped): 31.000, Pasos: 620, Mean Reward Calculado: 0.050000 (Recompensa/Pasos)\n",
      " 548210/825189: episode: 861, duration: 50.773s, episode steps: 620, steps per second:  12, episode reward: 31.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.038239, mae: 3.821070, mean_q: 4.597473, mean_eps: 0.100000\n",
      "📈 Episodio 862: Recompensa total (clipped): 29.000, Pasos: 618, Mean Reward Calculado: 0.046926 (Recompensa/Pasos)\n",
      " 548828/825189: episode: 862, duration: 51.193s, episode steps: 618, steps per second:  12, episode reward: 29.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.035975, mae: 3.829709, mean_q: 4.608814, mean_eps: 0.100000\n",
      "📈 Episodio 863: Recompensa total (clipped): 32.000, Pasos: 690, Mean Reward Calculado: 0.046377 (Recompensa/Pasos)\n",
      " 549518/825189: episode: 863, duration: 56.864s, episode steps: 690, steps per second:  12, episode reward: 32.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.040296, mae: 3.824882, mean_q: 4.600449, mean_eps: 0.100000\n",
      "📈 Episodio 864: Recompensa total (clipped): 28.000, Pasos: 686, Mean Reward Calculado: 0.040816 (Recompensa/Pasos)\n",
      " 550204/825189: episode: 864, duration: 56.337s, episode steps: 686, steps per second:  12, episode reward: 28.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.036833, mae: 3.875773, mean_q: 4.665508, mean_eps: 0.100000\n",
      "📈 Episodio 865: Recompensa total (clipped): 27.000, Pasos: 622, Mean Reward Calculado: 0.043408 (Recompensa/Pasos)\n",
      " 550826/825189: episode: 865, duration: 51.416s, episode steps: 622, steps per second:  12, episode reward: 27.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.040902, mae: 3.828191, mean_q: 4.605761, mean_eps: 0.100000\n",
      "📈 Episodio 866: Recompensa total (clipped): 34.000, Pasos: 916, Mean Reward Calculado: 0.037118 (Recompensa/Pasos)\n",
      " 551742/825189: episode: 866, duration: 75.663s, episode steps: 916, steps per second:  12, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.480 [0.000, 5.000],  loss: 0.034293, mae: 3.829401, mean_q: 4.610517, mean_eps: 0.100000\n",
      "📈 Episodio 867: Recompensa total (clipped): 26.000, Pasos: 666, Mean Reward Calculado: 0.039039 (Recompensa/Pasos)\n",
      " 552408/825189: episode: 867, duration: 55.239s, episode steps: 666, steps per second:  12, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.038355, mae: 3.805384, mean_q: 4.579689, mean_eps: 0.100000\n",
      "📈 Episodio 868: Recompensa total (clipped): 30.000, Pasos: 705, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 553113/825189: episode: 868, duration: 58.614s, episode steps: 705, steps per second:  12, episode reward: 30.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.035093, mae: 3.821751, mean_q: 4.599556, mean_eps: 0.100000\n",
      "📈 Episodio 869: Recompensa total (clipped): 31.000, Pasos: 809, Mean Reward Calculado: 0.038319 (Recompensa/Pasos)\n",
      " 553922/825189: episode: 869, duration: 66.682s, episode steps: 809, steps per second:  12, episode reward: 31.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.031770, mae: 3.848249, mean_q: 4.630127, mean_eps: 0.100000\n",
      "📈 Episodio 870: Recompensa total (clipped): 32.000, Pasos: 758, Mean Reward Calculado: 0.042216 (Recompensa/Pasos)\n",
      " 554680/825189: episode: 870, duration: 62.642s, episode steps: 758, steps per second:  12, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.037212, mae: 3.863546, mean_q: 4.647950, mean_eps: 0.100000\n",
      "📈 Episodio 871: Recompensa total (clipped): 13.000, Pasos: 349, Mean Reward Calculado: 0.037249 (Recompensa/Pasos)\n",
      " 555029/825189: episode: 871, duration: 29.073s, episode steps: 349, steps per second:  12, episode reward: 13.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.036944, mae: 3.814463, mean_q: 4.588888, mean_eps: 0.100000\n",
      "📈 Episodio 872: Recompensa total (clipped): 25.000, Pasos: 600, Mean Reward Calculado: 0.041667 (Recompensa/Pasos)\n",
      " 555629/825189: episode: 872, duration: 49.572s, episode steps: 600, steps per second:  12, episode reward: 25.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.036691, mae: 3.865170, mean_q: 4.649960, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 873: Recompensa total (clipped): 28.000, Pasos: 739, Mean Reward Calculado: 0.037889 (Recompensa/Pasos)\n",
      " 556368/825189: episode: 873, duration: 61.180s, episode steps: 739, steps per second:  12, episode reward: 28.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.036482, mae: 3.852403, mean_q: 4.633920, mean_eps: 0.100000\n",
      "📈 Episodio 874: Recompensa total (clipped): 27.000, Pasos: 553, Mean Reward Calculado: 0.048825 (Recompensa/Pasos)\n",
      " 556921/825189: episode: 874, duration: 46.047s, episode steps: 553, steps per second:  12, episode reward: 27.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 3.101 [0.000, 5.000],  loss: 0.037610, mae: 3.804739, mean_q: 4.576580, mean_eps: 0.100000\n",
      "📈 Episodio 875: Recompensa total (clipped): 32.000, Pasos: 766, Mean Reward Calculado: 0.041775 (Recompensa/Pasos)\n",
      " 557687/825189: episode: 875, duration: 63.130s, episode steps: 766, steps per second:  12, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.036743, mae: 3.832807, mean_q: 4.611235, mean_eps: 0.100000\n",
      "📈 Episodio 876: Recompensa total (clipped): 30.000, Pasos: 784, Mean Reward Calculado: 0.038265 (Recompensa/Pasos)\n",
      " 558471/825189: episode: 876, duration: 64.887s, episode steps: 784, steps per second:  12, episode reward: 30.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.040678, mae: 3.806601, mean_q: 4.582691, mean_eps: 0.100000\n",
      "📈 Episodio 877: Recompensa total (clipped): 24.000, Pasos: 774, Mean Reward Calculado: 0.031008 (Recompensa/Pasos)\n",
      " 559245/825189: episode: 877, duration: 64.078s, episode steps: 774, steps per second:  12, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.030898, mae: 3.787753, mean_q: 4.559898, mean_eps: 0.100000\n",
      "📈 Episodio 878: Recompensa total (clipped): 25.000, Pasos: 717, Mean Reward Calculado: 0.034868 (Recompensa/Pasos)\n",
      " 559962/825189: episode: 878, duration: 59.080s, episode steps: 717, steps per second:  12, episode reward: 25.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.037242, mae: 3.802562, mean_q: 4.576733, mean_eps: 0.100000\n",
      "📊 Paso 560,000/2,000,000 (28.0%) - 19.3 pasos/seg - ETA: 20.7h - Memoria: 11098.73 MB\n",
      "📈 Episodio 879: Recompensa total (clipped): 24.000, Pasos: 556, Mean Reward Calculado: 0.043165 (Recompensa/Pasos)\n",
      " 560518/825189: episode: 879, duration: 45.868s, episode steps: 556, steps per second:  12, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.034882, mae: 3.851982, mean_q: 4.635572, mean_eps: 0.100000\n",
      "📈 Episodio 880: Recompensa total (clipped): 20.000, Pasos: 566, Mean Reward Calculado: 0.035336 (Recompensa/Pasos)\n",
      " 561084/825189: episode: 880, duration: 46.777s, episode steps: 566, steps per second:  12, episode reward: 20.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.031634, mae: 3.819358, mean_q: 4.598427, mean_eps: 0.100000\n",
      "📈 Episodio 881: Recompensa total (clipped): 28.000, Pasos: 693, Mean Reward Calculado: 0.040404 (Recompensa/Pasos)\n",
      " 561777/825189: episode: 881, duration: 57.175s, episode steps: 693, steps per second:  12, episode reward: 28.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.267 [0.000, 5.000],  loss: 0.036799, mae: 3.829745, mean_q: 4.612666, mean_eps: 0.100000\n",
      "📈 Episodio 882: Recompensa total (clipped): 27.000, Pasos: 572, Mean Reward Calculado: 0.047203 (Recompensa/Pasos)\n",
      " 562349/825189: episode: 882, duration: 47.134s, episode steps: 572, steps per second:  12, episode reward: 27.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.036040, mae: 3.811887, mean_q: 4.591735, mean_eps: 0.100000\n",
      "📈 Episodio 883: Recompensa total (clipped): 13.000, Pasos: 360, Mean Reward Calculado: 0.036111 (Recompensa/Pasos)\n",
      " 562709/825189: episode: 883, duration: 29.816s, episode steps: 360, steps per second:  12, episode reward: 13.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.033054, mae: 3.852107, mean_q: 4.638912, mean_eps: 0.100000\n",
      "📈 Episodio 884: Recompensa total (clipped): 23.000, Pasos: 798, Mean Reward Calculado: 0.028822 (Recompensa/Pasos)\n",
      " 563507/825189: episode: 884, duration: 65.398s, episode steps: 798, steps per second:  12, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.718 [0.000, 5.000],  loss: 0.039856, mae: 3.866801, mean_q: 4.652382, mean_eps: 0.100000\n",
      "📈 Episodio 885: Recompensa total (clipped): 16.000, Pasos: 417, Mean Reward Calculado: 0.038369 (Recompensa/Pasos)\n",
      " 563924/825189: episode: 885, duration: 34.582s, episode steps: 417, steps per second:  12, episode reward: 16.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.039239, mae: 3.828738, mean_q: 4.608132, mean_eps: 0.100000\n",
      "📈 Episodio 886: Recompensa total (clipped): 32.000, Pasos: 969, Mean Reward Calculado: 0.033024 (Recompensa/Pasos)\n",
      " 564893/825189: episode: 886, duration: 79.509s, episode steps: 969, steps per second:  12, episode reward: 32.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.034826, mae: 3.858596, mean_q: 4.645911, mean_eps: 0.100000\n",
      "📈 Episodio 887: Recompensa total (clipped): 33.000, Pasos: 783, Mean Reward Calculado: 0.042146 (Recompensa/Pasos)\n",
      " 565676/825189: episode: 887, duration: 64.269s, episode steps: 783, steps per second:  12, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.036531, mae: 3.860678, mean_q: 4.647426, mean_eps: 0.100000\n",
      "📈 Episodio 888: Recompensa total (clipped): 23.000, Pasos: 556, Mean Reward Calculado: 0.041367 (Recompensa/Pasos)\n",
      " 566232/825189: episode: 888, duration: 45.972s, episode steps: 556, steps per second:  12, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.108 [0.000, 5.000],  loss: 0.038495, mae: 3.860561, mean_q: 4.646741, mean_eps: 0.100000\n",
      "📈 Episodio 889: Recompensa total (clipped): 23.000, Pasos: 595, Mean Reward Calculado: 0.038655 (Recompensa/Pasos)\n",
      " 566827/825189: episode: 889, duration: 48.773s, episode steps: 595, steps per second:  12, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.487 [0.000, 5.000],  loss: 0.041604, mae: 3.840925, mean_q: 4.621274, mean_eps: 0.100000\n",
      "📈 Episodio 890: Recompensa total (clipped): 25.000, Pasos: 579, Mean Reward Calculado: 0.043178 (Recompensa/Pasos)\n",
      " 567406/825189: episode: 890, duration: 47.775s, episode steps: 579, steps per second:  12, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.446 [0.000, 5.000],  loss: 0.038369, mae: 3.894656, mean_q: 4.684781, mean_eps: 0.100000\n",
      "📈 Episodio 891: Recompensa total (clipped): 32.000, Pasos: 889, Mean Reward Calculado: 0.035996 (Recompensa/Pasos)\n",
      " 568295/825189: episode: 891, duration: 73.188s, episode steps: 889, steps per second:  12, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.037680, mae: 3.846186, mean_q: 4.625575, mean_eps: 0.100000\n",
      "📈 Episodio 892: Recompensa total (clipped): 33.000, Pasos: 895, Mean Reward Calculado: 0.036872 (Recompensa/Pasos)\n",
      " 569190/825189: episode: 892, duration: 74.021s, episode steps: 895, steps per second:  12, episode reward: 33.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.034678, mae: 3.839054, mean_q: 4.618354, mean_eps: 0.100000\n",
      "📈 Episodio 893: Recompensa total (clipped): 13.000, Pasos: 411, Mean Reward Calculado: 0.031630 (Recompensa/Pasos)\n",
      " 569601/825189: episode: 893, duration: 33.927s, episode steps: 411, steps per second:  12, episode reward: 13.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.032842, mae: 3.834698, mean_q: 4.616589, mean_eps: 0.100000\n",
      "📈 Episodio 894: Recompensa total (clipped): 26.000, Pasos: 569, Mean Reward Calculado: 0.045694 (Recompensa/Pasos)\n",
      " 570170/825189: episode: 894, duration: 46.909s, episode steps: 569, steps per second:  12, episode reward: 26.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.036909, mae: 3.857354, mean_q: 4.641457, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 895: Recompensa total (clipped): 28.000, Pasos: 655, Mean Reward Calculado: 0.042748 (Recompensa/Pasos)\n",
      " 570825/825189: episode: 895, duration: 54.162s, episode steps: 655, steps per second:  12, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.475 [0.000, 5.000],  loss: 0.032833, mae: 3.850353, mean_q: 4.637950, mean_eps: 0.100000\n",
      "📈 Episodio 896: Recompensa total (clipped): 30.000, Pasos: 922, Mean Reward Calculado: 0.032538 (Recompensa/Pasos)\n",
      " 571747/825189: episode: 896, duration: 76.007s, episode steps: 922, steps per second:  12, episode reward: 30.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.037874, mae: 3.846884, mean_q: 4.630281, mean_eps: 0.100000\n",
      "📈 Episodio 897: Recompensa total (clipped): 33.000, Pasos: 871, Mean Reward Calculado: 0.037887 (Recompensa/Pasos)\n",
      " 572618/825189: episode: 897, duration: 72.517s, episode steps: 871, steps per second:  12, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.031252, mae: 3.851302, mean_q: 4.637153, mean_eps: 0.100000\n",
      "📈 Episodio 898: Recompensa total (clipped): 18.000, Pasos: 436, Mean Reward Calculado: 0.041284 (Recompensa/Pasos)\n",
      " 573054/825189: episode: 898, duration: 36.947s, episode steps: 436, steps per second:  12, episode reward: 18.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.036033, mae: 3.856999, mean_q: 4.639355, mean_eps: 0.100000\n",
      "📈 Episodio 899: Recompensa total (clipped): 25.000, Pasos: 569, Mean Reward Calculado: 0.043937 (Recompensa/Pasos)\n",
      " 573623/825189: episode: 899, duration: 47.002s, episode steps: 569, steps per second:  12, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.035820, mae: 3.866824, mean_q: 4.653733, mean_eps: 0.100000\n",
      "📈 Episodio 900: Recompensa total (clipped): 20.000, Pasos: 523, Mean Reward Calculado: 0.038241 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 900, pasos: 574146)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.56 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 900 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 20.00\n",
      "   Media últimos 100: 26.56 / 20.0\n",
      "   Mejor promedio histórico: 26.56\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 810\n",
      "   Episodios consecutivos en objetivo: 810\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 810 episodios consecutivos\n",
      " 574146/825189: episode: 900, duration: 110.547s, episode steps: 523, steps per second:   5, episode reward: 20.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.034451, mae: 3.877555, mean_q: 4.666428, mean_eps: 0.100000\n",
      "📈 Episodio 901: Recompensa total (clipped): 21.000, Pasos: 577, Mean Reward Calculado: 0.036395 (Recompensa/Pasos)\n",
      " 574723/825189: episode: 901, duration: 48.510s, episode steps: 577, steps per second:  12, episode reward: 21.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.035514, mae: 3.857958, mean_q: 4.643568, mean_eps: 0.100000\n",
      "📈 Episodio 902: Recompensa total (clipped): 35.000, Pasos: 907, Mean Reward Calculado: 0.038589 (Recompensa/Pasos)\n",
      " 575630/825189: episode: 902, duration: 77.119s, episode steps: 907, steps per second:  12, episode reward: 35.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.037963, mae: 3.852173, mean_q: 4.635403, mean_eps: 0.100000\n",
      "📈 Episodio 903: Recompensa total (clipped): 27.000, Pasos: 649, Mean Reward Calculado: 0.041602 (Recompensa/Pasos)\n",
      " 576279/825189: episode: 903, duration: 54.915s, episode steps: 649, steps per second:  12, episode reward: 27.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.042187, mae: 3.879194, mean_q: 4.669651, mean_eps: 0.100000\n",
      "📈 Episodio 904: Recompensa total (clipped): 17.000, Pasos: 478, Mean Reward Calculado: 0.035565 (Recompensa/Pasos)\n",
      " 576757/825189: episode: 904, duration: 40.677s, episode steps: 478, steps per second:  12, episode reward: 17.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.042750, mae: 3.844051, mean_q: 4.624661, mean_eps: 0.100000\n",
      "📈 Episodio 905: Recompensa total (clipped): 25.000, Pasos: 627, Mean Reward Calculado: 0.039872 (Recompensa/Pasos)\n",
      " 577384/825189: episode: 905, duration: 53.243s, episode steps: 627, steps per second:  12, episode reward: 25.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.035000, mae: 3.841280, mean_q: 4.624179, mean_eps: 0.100000\n",
      "📈 Episodio 906: Recompensa total (clipped): 32.000, Pasos: 806, Mean Reward Calculado: 0.039702 (Recompensa/Pasos)\n",
      " 578190/825189: episode: 906, duration: 68.173s, episode steps: 806, steps per second:  12, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.038628, mae: 3.876036, mean_q: 4.664368, mean_eps: 0.100000\n",
      "📈 Episodio 907: Recompensa total (clipped): 25.000, Pasos: 710, Mean Reward Calculado: 0.035211 (Recompensa/Pasos)\n",
      " 578900/825189: episode: 907, duration: 60.215s, episode steps: 710, steps per second:  12, episode reward: 25.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.030 [0.000, 5.000],  loss: 0.041791, mae: 3.854202, mean_q: 4.636911, mean_eps: 0.100000\n",
      "📈 Episodio 908: Recompensa total (clipped): 30.000, Pasos: 772, Mean Reward Calculado: 0.038860 (Recompensa/Pasos)\n",
      " 579672/825189: episode: 908, duration: 65.749s, episode steps: 772, steps per second:  12, episode reward: 30.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.032308, mae: 3.827134, mean_q: 4.610466, mean_eps: 0.100000\n",
      "📊 Paso 580,000/2,000,000 (29.0%) - 18.9 pasos/seg - ETA: 20.9h - Memoria: 11078.14 MB\n",
      "📈 Episodio 909: Recompensa total (clipped): 28.000, Pasos: 774, Mean Reward Calculado: 0.036176 (Recompensa/Pasos)\n",
      " 580446/825189: episode: 909, duration: 65.569s, episode steps: 774, steps per second:  12, episode reward: 28.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.038002, mae: 3.815199, mean_q: 4.595751, mean_eps: 0.100000\n",
      "📈 Episodio 910: Recompensa total (clipped): 40.000, Pasos: 1107, Mean Reward Calculado: 0.036134 (Recompensa/Pasos)\n",
      " 581553/825189: episode: 910, duration: 93.192s, episode steps: 1107, steps per second:  12, episode reward: 40.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.037253, mae: 3.821453, mean_q: 4.602160, mean_eps: 0.100000\n",
      "📈 Episodio 911: Recompensa total (clipped): 30.000, Pasos: 617, Mean Reward Calculado: 0.048622 (Recompensa/Pasos)\n",
      " 582170/825189: episode: 911, duration: 51.983s, episode steps: 617, steps per second:  12, episode reward: 30.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.036641, mae: 3.829569, mean_q: 4.611328, mean_eps: 0.100000\n",
      "📈 Episodio 912: Recompensa total (clipped): 25.000, Pasos: 750, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      " 582920/825189: episode: 912, duration: 63.555s, episode steps: 750, steps per second:  12, episode reward: 25.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.033642, mae: 3.856972, mean_q: 4.643192, mean_eps: 0.100000\n",
      "📈 Episodio 913: Recompensa total (clipped): 23.000, Pasos: 477, Mean Reward Calculado: 0.048218 (Recompensa/Pasos)\n",
      " 583397/825189: episode: 913, duration: 41.392s, episode steps: 477, steps per second:  12, episode reward: 23.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.031771, mae: 3.856122, mean_q: 4.642832, mean_eps: 0.100000\n",
      "📈 Episodio 914: Recompensa total (clipped): 28.000, Pasos: 716, Mean Reward Calculado: 0.039106 (Recompensa/Pasos)\n",
      " 584113/825189: episode: 914, duration: 62.509s, episode steps: 716, steps per second:  11, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.040143, mae: 3.843557, mean_q: 4.624773, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 915: Recompensa total (clipped): 30.000, Pasos: 757, Mean Reward Calculado: 0.039630 (Recompensa/Pasos)\n",
      " 584870/825189: episode: 915, duration: 65.139s, episode steps: 757, steps per second:  12, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.033258, mae: 3.863610, mean_q: 4.652660, mean_eps: 0.100000\n",
      "📈 Episodio 916: Recompensa total (clipped): 33.000, Pasos: 713, Mean Reward Calculado: 0.046283 (Recompensa/Pasos)\n",
      " 585583/825189: episode: 916, duration: 60.862s, episode steps: 713, steps per second:  12, episode reward: 33.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.038074, mae: 3.865570, mean_q: 4.653716, mean_eps: 0.100000\n",
      "📈 Episodio 917: Recompensa total (clipped): 21.000, Pasos: 497, Mean Reward Calculado: 0.042254 (Recompensa/Pasos)\n",
      " 586080/825189: episode: 917, duration: 43.330s, episode steps: 497, steps per second:  11, episode reward: 21.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.171 [0.000, 5.000],  loss: 0.035747, mae: 3.859325, mean_q: 4.646392, mean_eps: 0.100000\n",
      "📈 Episodio 918: Recompensa total (clipped): 23.000, Pasos: 545, Mean Reward Calculado: 0.042202 (Recompensa/Pasos)\n",
      " 586625/825189: episode: 918, duration: 47.208s, episode steps: 545, steps per second:  12, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.152 [0.000, 5.000],  loss: 0.040152, mae: 3.880812, mean_q: 4.671993, mean_eps: 0.100000\n",
      "📈 Episodio 919: Recompensa total (clipped): 26.000, Pasos: 607, Mean Reward Calculado: 0.042834 (Recompensa/Pasos)\n",
      " 587232/825189: episode: 919, duration: 52.820s, episode steps: 607, steps per second:  11, episode reward: 26.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.038918, mae: 3.829642, mean_q: 4.610273, mean_eps: 0.100000\n",
      "📈 Episodio 920: Recompensa total (clipped): 24.000, Pasos: 512, Mean Reward Calculado: 0.046875 (Recompensa/Pasos)\n",
      " 587744/825189: episode: 920, duration: 45.346s, episode steps: 512, steps per second:  11, episode reward: 24.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 3.143 [0.000, 5.000],  loss: 0.037033, mae: 3.823944, mean_q: 4.601667, mean_eps: 0.100000\n",
      "📈 Episodio 921: Recompensa total (clipped): 25.000, Pasos: 554, Mean Reward Calculado: 0.045126 (Recompensa/Pasos)\n",
      " 588298/825189: episode: 921, duration: 47.923s, episode steps: 554, steps per second:  12, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.036843, mae: 3.847685, mean_q: 4.630209, mean_eps: 0.100000\n",
      "📈 Episodio 922: Recompensa total (clipped): 32.000, Pasos: 820, Mean Reward Calculado: 0.039024 (Recompensa/Pasos)\n",
      " 589118/825189: episode: 922, duration: 70.613s, episode steps: 820, steps per second:  12, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.037381, mae: 3.836167, mean_q: 4.616104, mean_eps: 0.100000\n",
      "📈 Episodio 923: Recompensa total (clipped): 23.000, Pasos: 568, Mean Reward Calculado: 0.040493 (Recompensa/Pasos)\n",
      " 589686/825189: episode: 923, duration: 48.752s, episode steps: 568, steps per second:  12, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.030244, mae: 3.814407, mean_q: 4.592948, mean_eps: 0.100000\n",
      "📈 Episodio 924: Recompensa total (clipped): 19.000, Pasos: 494, Mean Reward Calculado: 0.038462 (Recompensa/Pasos)\n",
      " 590180/825189: episode: 924, duration: 42.414s, episode steps: 494, steps per second:  12, episode reward: 19.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.030235, mae: 3.852154, mean_q: 4.638099, mean_eps: 0.100000\n",
      "📈 Episodio 925: Recompensa total (clipped): 22.000, Pasos: 602, Mean Reward Calculado: 0.036545 (Recompensa/Pasos)\n",
      " 590782/825189: episode: 925, duration: 51.883s, episode steps: 602, steps per second:  12, episode reward: 22.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.034054, mae: 3.837269, mean_q: 4.618116, mean_eps: 0.100000\n",
      "📈 Episodio 926: Recompensa total (clipped): 23.000, Pasos: 510, Mean Reward Calculado: 0.045098 (Recompensa/Pasos)\n",
      " 591292/825189: episode: 926, duration: 43.653s, episode steps: 510, steps per second:  12, episode reward: 23.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.032515, mae: 3.850420, mean_q: 4.634542, mean_eps: 0.100000\n",
      "📈 Episodio 927: Recompensa total (clipped): 17.000, Pasos: 421, Mean Reward Calculado: 0.040380 (Recompensa/Pasos)\n",
      " 591713/825189: episode: 927, duration: 36.443s, episode steps: 421, steps per second:  12, episode reward: 17.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.037270, mae: 3.838627, mean_q: 4.617768, mean_eps: 0.100000\n",
      "📈 Episodio 928: Recompensa total (clipped): 12.000, Pasos: 304, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 592017/825189: episode: 928, duration: 26.133s, episode steps: 304, steps per second:  12, episode reward: 12.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.040805, mae: 3.872024, mean_q: 4.656668, mean_eps: 0.100000\n",
      "📈 Episodio 929: Recompensa total (clipped): 30.000, Pasos: 644, Mean Reward Calculado: 0.046584 (Recompensa/Pasos)\n",
      " 592661/825189: episode: 929, duration: 55.276s, episode steps: 644, steps per second:  12, episode reward: 30.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.921 [0.000, 5.000],  loss: 0.030631, mae: 3.840233, mean_q: 4.621868, mean_eps: 0.100000\n",
      "📈 Episodio 930: Recompensa total (clipped): 17.000, Pasos: 360, Mean Reward Calculado: 0.047222 (Recompensa/Pasos)\n",
      " 593021/825189: episode: 930, duration: 31.128s, episode steps: 360, steps per second:  12, episode reward: 17.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.037779, mae: 3.828535, mean_q: 4.604753, mean_eps: 0.100000\n",
      "📈 Episodio 931: Recompensa total (clipped): 20.000, Pasos: 557, Mean Reward Calculado: 0.035907 (Recompensa/Pasos)\n",
      " 593578/825189: episode: 931, duration: 47.432s, episode steps: 557, steps per second:  12, episode reward: 20.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.033866, mae: 3.808878, mean_q: 4.581557, mean_eps: 0.100000\n",
      "📈 Episodio 932: Recompensa total (clipped): 30.000, Pasos: 717, Mean Reward Calculado: 0.041841 (Recompensa/Pasos)\n",
      " 594295/825189: episode: 932, duration: 61.321s, episode steps: 717, steps per second:  12, episode reward: 30.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.897 [0.000, 5.000],  loss: 0.035767, mae: 3.826424, mean_q: 4.602057, mean_eps: 0.100000\n",
      "📈 Episodio 933: Recompensa total (clipped): 34.000, Pasos: 950, Mean Reward Calculado: 0.035789 (Recompensa/Pasos)\n",
      " 595245/825189: episode: 933, duration: 81.941s, episode steps: 950, steps per second:  12, episode reward: 34.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.034295, mae: 3.821898, mean_q: 4.598997, mean_eps: 0.100000\n",
      "📈 Episodio 934: Recompensa total (clipped): 34.000, Pasos: 924, Mean Reward Calculado: 0.036797 (Recompensa/Pasos)\n",
      " 596169/825189: episode: 934, duration: 79.726s, episode steps: 924, steps per second:  12, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.041435, mae: 3.824559, mean_q: 4.602161, mean_eps: 0.100000\n",
      "📈 Episodio 935: Recompensa total (clipped): 17.000, Pasos: 428, Mean Reward Calculado: 0.039720 (Recompensa/Pasos)\n",
      " 596597/825189: episode: 935, duration: 37.187s, episode steps: 428, steps per second:  12, episode reward: 17.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.028911, mae: 3.852308, mean_q: 4.642230, mean_eps: 0.100000\n",
      "📈 Episodio 936: Recompensa total (clipped): 23.000, Pasos: 597, Mean Reward Calculado: 0.038526 (Recompensa/Pasos)\n",
      " 597194/825189: episode: 936, duration: 51.338s, episode steps: 597, steps per second:  12, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.037177, mae: 3.847998, mean_q: 4.631476, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 937: Recompensa total (clipped): 32.000, Pasos: 849, Mean Reward Calculado: 0.037691 (Recompensa/Pasos)\n",
      " 598043/825189: episode: 937, duration: 72.298s, episode steps: 849, steps per second:  12, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.038786, mae: 3.806677, mean_q: 4.582332, mean_eps: 0.100000\n",
      "📈 Episodio 938: Recompensa total (clipped): 25.000, Pasos: 583, Mean Reward Calculado: 0.042882 (Recompensa/Pasos)\n",
      " 598626/825189: episode: 938, duration: 50.180s, episode steps: 583, steps per second:  12, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.036365, mae: 3.774349, mean_q: 4.543626, mean_eps: 0.100000\n",
      "📈 Episodio 939: Recompensa total (clipped): 20.000, Pasos: 634, Mean Reward Calculado: 0.031546 (Recompensa/Pasos)\n",
      " 599260/825189: episode: 939, duration: 54.911s, episode steps: 634, steps per second:  12, episode reward: 20.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.037140, mae: 3.810686, mean_q: 4.586799, mean_eps: 0.100000\n",
      "📈 Episodio 940: Recompensa total (clipped): 22.000, Pasos: 518, Mean Reward Calculado: 0.042471 (Recompensa/Pasos)\n",
      " 599778/825189: episode: 940, duration: 44.509s, episode steps: 518, steps per second:  12, episode reward: 22.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.038785, mae: 3.811146, mean_q: 4.586114, mean_eps: 0.100000\n",
      "📊 Paso 600,000/2,000,000 (30.0%) - 18.5 pasos/seg - ETA: 21.1h - Memoria: 9842.29 MB\n",
      "📈 Episodio 941: Recompensa total (clipped): 13.000, Pasos: 376, Mean Reward Calculado: 0.034574 (Recompensa/Pasos)\n",
      " 600154/825189: episode: 941, duration: 32.408s, episode steps: 376, steps per second:  12, episode reward: 13.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.031183, mae: 3.857359, mean_q: 4.646760, mean_eps: 0.100000\n",
      "📈 Episodio 942: Recompensa total (clipped): 30.000, Pasos: 892, Mean Reward Calculado: 0.033632 (Recompensa/Pasos)\n",
      " 601046/825189: episode: 942, duration: 76.365s, episode steps: 892, steps per second:  12, episode reward: 30.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.031154, mae: 3.882231, mean_q: 4.675475, mean_eps: 0.100000\n",
      "📈 Episodio 943: Recompensa total (clipped): 24.000, Pasos: 546, Mean Reward Calculado: 0.043956 (Recompensa/Pasos)\n",
      " 601592/825189: episode: 943, duration: 46.746s, episode steps: 546, steps per second:  12, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.040700, mae: 3.893624, mean_q: 4.682510, mean_eps: 0.100000\n",
      "📈 Episodio 944: Recompensa total (clipped): 18.000, Pasos: 357, Mean Reward Calculado: 0.050420 (Recompensa/Pasos)\n",
      " 601949/825189: episode: 944, duration: 30.576s, episode steps: 357, steps per second:  12, episode reward: 18.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.030312, mae: 3.843683, mean_q: 4.629601, mean_eps: 0.100000\n",
      "📈 Episodio 945: Recompensa total (clipped): 31.000, Pasos: 768, Mean Reward Calculado: 0.040365 (Recompensa/Pasos)\n",
      " 602717/825189: episode: 945, duration: 65.614s, episode steps: 768, steps per second:  12, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.031137, mae: 3.860487, mean_q: 4.647885, mean_eps: 0.100000\n",
      "📈 Episodio 946: Recompensa total (clipped): 32.000, Pasos: 960, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      " 603677/825189: episode: 946, duration: 81.309s, episode steps: 960, steps per second:  12, episode reward: 32.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.031032, mae: 3.862605, mean_q: 4.652706, mean_eps: 0.100000\n",
      "📈 Episodio 947: Recompensa total (clipped): 31.000, Pasos: 640, Mean Reward Calculado: 0.048438 (Recompensa/Pasos)\n",
      " 604317/825189: episode: 947, duration: 54.283s, episode steps: 640, steps per second:  12, episode reward: 31.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.035197, mae: 3.826517, mean_q: 4.606122, mean_eps: 0.100000\n",
      "📈 Episodio 948: Recompensa total (clipped): 29.000, Pasos: 695, Mean Reward Calculado: 0.041727 (Recompensa/Pasos)\n",
      " 605012/825189: episode: 948, duration: 58.104s, episode steps: 695, steps per second:  12, episode reward: 29.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.033669, mae: 3.875003, mean_q: 4.664163, mean_eps: 0.100000\n",
      "📈 Episodio 949: Recompensa total (clipped): 33.000, Pasos: 709, Mean Reward Calculado: 0.046544 (Recompensa/Pasos)\n",
      " 605721/825189: episode: 949, duration: 59.838s, episode steps: 709, steps per second:  12, episode reward: 33.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.034483, mae: 3.828466, mean_q: 4.607241, mean_eps: 0.100000\n",
      "📈 Episodio 950: Recompensa total (clipped): 15.000, Pasos: 397, Mean Reward Calculado: 0.037783 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 950, pasos: 606118)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.87 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 950 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 15.00\n",
      "   Media últimos 100: 25.87 / 20.0\n",
      "   Mejor promedio histórico: 25.87\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 860\n",
      "   Episodios consecutivos en objetivo: 860\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 860 episodios consecutivos\n",
      " 606118/825189: episode: 950, duration: 105.116s, episode steps: 397, steps per second:   4, episode reward: 15.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.032366, mae: 3.839740, mean_q: 4.622829, mean_eps: 0.100000\n",
      "📈 Episodio 951: Recompensa total (clipped): 17.000, Pasos: 445, Mean Reward Calculado: 0.038202 (Recompensa/Pasos)\n",
      " 606563/825189: episode: 951, duration: 38.011s, episode steps: 445, steps per second:  12, episode reward: 17.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.036883, mae: 3.842559, mean_q: 4.624711, mean_eps: 0.100000\n",
      "📈 Episodio 952: Recompensa total (clipped): 10.000, Pasos: 292, Mean Reward Calculado: 0.034247 (Recompensa/Pasos)\n",
      " 606855/825189: episode: 952, duration: 24.789s, episode steps: 292, steps per second:  12, episode reward: 10.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.040224, mae: 3.825039, mean_q: 4.597948, mean_eps: 0.100000\n",
      "📈 Episodio 953: Recompensa total (clipped): 29.000, Pasos: 699, Mean Reward Calculado: 0.041488 (Recompensa/Pasos)\n",
      " 607554/825189: episode: 953, duration: 59.312s, episode steps: 699, steps per second:  12, episode reward: 29.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.034560, mae: 3.875316, mean_q: 4.662315, mean_eps: 0.100000\n",
      "📈 Episodio 954: Recompensa total (clipped): 32.000, Pasos: 736, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 608290/825189: episode: 954, duration: 62.374s, episode steps: 736, steps per second:  12, episode reward: 32.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.036180, mae: 3.846424, mean_q: 4.626003, mean_eps: 0.100000\n",
      "📈 Episodio 955: Recompensa total (clipped): 29.000, Pasos: 667, Mean Reward Calculado: 0.043478 (Recompensa/Pasos)\n",
      " 608957/825189: episode: 955, duration: 56.508s, episode steps: 667, steps per second:  12, episode reward: 29.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.036575, mae: 3.844058, mean_q: 4.625574, mean_eps: 0.100000\n",
      "📈 Episodio 956: Recompensa total (clipped): 24.000, Pasos: 565, Mean Reward Calculado: 0.042478 (Recompensa/Pasos)\n",
      " 609522/825189: episode: 956, duration: 47.761s, episode steps: 565, steps per second:  12, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.036901, mae: 3.855244, mean_q: 4.638968, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 957: Recompensa total (clipped): 31.000, Pasos: 777, Mean Reward Calculado: 0.039897 (Recompensa/Pasos)\n",
      " 610299/825189: episode: 957, duration: 65.720s, episode steps: 777, steps per second:  12, episode reward: 31.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.034673, mae: 3.854164, mean_q: 4.640337, mean_eps: 0.100000\n",
      "📈 Episodio 958: Recompensa total (clipped): 26.000, Pasos: 645, Mean Reward Calculado: 0.040310 (Recompensa/Pasos)\n",
      " 610944/825189: episode: 958, duration: 55.002s, episode steps: 645, steps per second:  12, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.035691, mae: 3.875053, mean_q: 4.664337, mean_eps: 0.100000\n",
      "📈 Episodio 959: Recompensa total (clipped): 33.000, Pasos: 858, Mean Reward Calculado: 0.038462 (Recompensa/Pasos)\n",
      " 611802/825189: episode: 959, duration: 72.892s, episode steps: 858, steps per second:  12, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.038321, mae: 3.857026, mean_q: 4.640852, mean_eps: 0.100000\n",
      "📈 Episodio 960: Recompensa total (clipped): 15.000, Pasos: 415, Mean Reward Calculado: 0.036145 (Recompensa/Pasos)\n",
      " 612217/825189: episode: 960, duration: 35.267s, episode steps: 415, steps per second:  12, episode reward: 15.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.035533, mae: 3.837634, mean_q: 4.617095, mean_eps: 0.100000\n",
      "📈 Episodio 961: Recompensa total (clipped): 34.000, Pasos: 919, Mean Reward Calculado: 0.036997 (Recompensa/Pasos)\n",
      " 613136/825189: episode: 961, duration: 78.251s, episode steps: 919, steps per second:  12, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.033615, mae: 3.831965, mean_q: 4.610894, mean_eps: 0.100000\n",
      "📈 Episodio 962: Recompensa total (clipped): 29.000, Pasos: 584, Mean Reward Calculado: 0.049658 (Recompensa/Pasos)\n",
      " 613720/825189: episode: 962, duration: 50.868s, episode steps: 584, steps per second:  11, episode reward: 29.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.038459, mae: 3.852548, mean_q: 4.635779, mean_eps: 0.100000\n",
      "📈 Episodio 963: Recompensa total (clipped): 26.000, Pasos: 646, Mean Reward Calculado: 0.040248 (Recompensa/Pasos)\n",
      " 614366/825189: episode: 963, duration: 55.769s, episode steps: 646, steps per second:  12, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.035015, mae: 3.841198, mean_q: 4.620489, mean_eps: 0.100000\n",
      "📈 Episodio 964: Recompensa total (clipped): 31.000, Pasos: 716, Mean Reward Calculado: 0.043296 (Recompensa/Pasos)\n",
      " 615082/825189: episode: 964, duration: 61.506s, episode steps: 716, steps per second:  12, episode reward: 31.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.034131, mae: 3.821331, mean_q: 4.595843, mean_eps: 0.100000\n",
      "📈 Episodio 965: Recompensa total (clipped): 35.000, Pasos: 1077, Mean Reward Calculado: 0.032498 (Recompensa/Pasos)\n",
      " 616159/825189: episode: 965, duration: 92.277s, episode steps: 1077, steps per second:  12, episode reward: 35.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.033170, mae: 3.857232, mean_q: 4.642383, mean_eps: 0.100000\n",
      "📈 Episodio 966: Recompensa total (clipped): 33.000, Pasos: 879, Mean Reward Calculado: 0.037543 (Recompensa/Pasos)\n",
      " 617038/825189: episode: 966, duration: 75.790s, episode steps: 879, steps per second:  12, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.032601, mae: 3.862213, mean_q: 4.647835, mean_eps: 0.100000\n",
      "📈 Episodio 967: Recompensa total (clipped): 30.000, Pasos: 694, Mean Reward Calculado: 0.043228 (Recompensa/Pasos)\n",
      " 617732/825189: episode: 967, duration: 60.194s, episode steps: 694, steps per second:  12, episode reward: 30.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.034542, mae: 3.851327, mean_q: 4.636232, mean_eps: 0.100000\n",
      "📈 Episodio 968: Recompensa total (clipped): 16.000, Pasos: 474, Mean Reward Calculado: 0.033755 (Recompensa/Pasos)\n",
      " 618206/825189: episode: 968, duration: 41.135s, episode steps: 474, steps per second:  12, episode reward: 16.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.040743, mae: 3.871978, mean_q: 4.658017, mean_eps: 0.100000\n",
      "📈 Episodio 969: Recompensa total (clipped): 25.000, Pasos: 587, Mean Reward Calculado: 0.042589 (Recompensa/Pasos)\n",
      " 618793/825189: episode: 969, duration: 51.227s, episode steps: 587, steps per second:  11, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.037886, mae: 3.875517, mean_q: 4.663148, mean_eps: 0.100000\n",
      "📈 Episodio 970: Recompensa total (clipped): 32.000, Pasos: 779, Mean Reward Calculado: 0.041078 (Recompensa/Pasos)\n",
      " 619572/825189: episode: 970, duration: 66.605s, episode steps: 779, steps per second:  12, episode reward: 32.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.043052, mae: 3.863424, mean_q: 4.645344, mean_eps: 0.100000\n",
      "📊 Paso 620,000/2,000,000 (31.0%) - 18.1 pasos/seg - ETA: 21.2h - Memoria: 10752.08 MB\n",
      "📈 Episodio 971: Recompensa total (clipped): 34.000, Pasos: 752, Mean Reward Calculado: 0.045213 (Recompensa/Pasos)\n",
      " 620324/825189: episode: 971, duration: 65.831s, episode steps: 752, steps per second:  11, episode reward: 34.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.033752, mae: 3.828035, mean_q: 4.605259, mean_eps: 0.100000\n",
      "📈 Episodio 972: Recompensa total (clipped): 31.000, Pasos: 687, Mean Reward Calculado: 0.045124 (Recompensa/Pasos)\n",
      " 621011/825189: episode: 972, duration: 59.581s, episode steps: 687, steps per second:  12, episode reward: 31.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.036024, mae: 3.839385, mean_q: 4.618755, mean_eps: 0.100000\n",
      "📈 Episodio 973: Recompensa total (clipped): 24.000, Pasos: 553, Mean Reward Calculado: 0.043400 (Recompensa/Pasos)\n",
      " 621564/825189: episode: 973, duration: 47.347s, episode steps: 553, steps per second:  12, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.026613, mae: 3.868421, mean_q: 4.655473, mean_eps: 0.100000\n",
      "📈 Episodio 974: Recompensa total (clipped): 28.000, Pasos: 646, Mean Reward Calculado: 0.043344 (Recompensa/Pasos)\n",
      " 622210/825189: episode: 974, duration: 55.035s, episode steps: 646, steps per second:  12, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.038423, mae: 3.859766, mean_q: 4.645013, mean_eps: 0.100000\n",
      "📈 Episodio 975: Recompensa total (clipped): 33.000, Pasos: 666, Mean Reward Calculado: 0.049550 (Recompensa/Pasos)\n",
      " 622876/825189: episode: 975, duration: 56.875s, episode steps: 666, steps per second:  12, episode reward: 33.000, mean reward:  0.050 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.033858, mae: 3.859490, mean_q: 4.645907, mean_eps: 0.100000\n",
      "📈 Episodio 976: Recompensa total (clipped): 29.000, Pasos: 811, Mean Reward Calculado: 0.035758 (Recompensa/Pasos)\n",
      " 623687/825189: episode: 976, duration: 70.327s, episode steps: 811, steps per second:  12, episode reward: 29.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.042 [0.000, 5.000],  loss: 0.033062, mae: 3.871746, mean_q: 4.659382, mean_eps: 0.100000\n",
      "📈 Episodio 977: Recompensa total (clipped): 30.000, Pasos: 696, Mean Reward Calculado: 0.043103 (Recompensa/Pasos)\n",
      " 624383/825189: episode: 977, duration: 60.431s, episode steps: 696, steps per second:  12, episode reward: 30.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.894 [0.000, 5.000],  loss: 0.029310, mae: 3.821114, mean_q: 4.600632, mean_eps: 0.100000\n",
      "📈 Episodio 978: Recompensa total (clipped): 33.000, Pasos: 839, Mean Reward Calculado: 0.039333 (Recompensa/Pasos)\n",
      " 625222/825189: episode: 978, duration: 72.242s, episode steps: 839, steps per second:  12, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.033895, mae: 3.869453, mean_q: 4.658495, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 979: Recompensa total (clipped): 26.000, Pasos: 632, Mean Reward Calculado: 0.041139 (Recompensa/Pasos)\n",
      " 625854/825189: episode: 979, duration: 55.074s, episode steps: 632, steps per second:  11, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.411 [0.000, 5.000],  loss: 0.030689, mae: 3.865406, mean_q: 4.656877, mean_eps: 0.100000\n",
      "📈 Episodio 980: Recompensa total (clipped): 23.000, Pasos: 769, Mean Reward Calculado: 0.029909 (Recompensa/Pasos)\n",
      " 626623/825189: episode: 980, duration: 65.661s, episode steps: 769, steps per second:  12, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.036098, mae: 3.841542, mean_q: 4.626119, mean_eps: 0.100000\n",
      "📈 Episodio 981: Recompensa total (clipped): 29.000, Pasos: 660, Mean Reward Calculado: 0.043939 (Recompensa/Pasos)\n",
      " 627283/825189: episode: 981, duration: 57.564s, episode steps: 660, steps per second:  11, episode reward: 29.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.037947, mae: 3.835669, mean_q: 4.616012, mean_eps: 0.100000\n",
      "📈 Episodio 982: Recompensa total (clipped): 26.000, Pasos: 563, Mean Reward Calculado: 0.046181 (Recompensa/Pasos)\n",
      " 627846/825189: episode: 982, duration: 48.576s, episode steps: 563, steps per second:  12, episode reward: 26.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.033401, mae: 3.812911, mean_q: 4.590600, mean_eps: 0.100000\n",
      "📈 Episodio 983: Recompensa total (clipped): 29.000, Pasos: 750, Mean Reward Calculado: 0.038667 (Recompensa/Pasos)\n",
      " 628596/825189: episode: 983, duration: 64.352s, episode steps: 750, steps per second:  12, episode reward: 29.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.037926, mae: 3.890791, mean_q: 4.682543, mean_eps: 0.100000\n",
      "📈 Episodio 984: Recompensa total (clipped): 34.000, Pasos: 850, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 629446/825189: episode: 984, duration: 73.173s, episode steps: 850, steps per second:  12, episode reward: 34.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.033329, mae: 3.832014, mean_q: 4.614210, mean_eps: 0.100000\n",
      "📈 Episodio 985: Recompensa total (clipped): 25.000, Pasos: 564, Mean Reward Calculado: 0.044326 (Recompensa/Pasos)\n",
      " 630010/825189: episode: 985, duration: 48.491s, episode steps: 564, steps per second:  12, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.032405, mae: 3.823709, mean_q: 4.602629, mean_eps: 0.100000\n",
      "📈 Episodio 986: Recompensa total (clipped): 33.000, Pasos: 799, Mean Reward Calculado: 0.041302 (Recompensa/Pasos)\n",
      " 630809/825189: episode: 986, duration: 69.110s, episode steps: 799, steps per second:  12, episode reward: 33.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.034018, mae: 3.810545, mean_q: 4.589839, mean_eps: 0.100000\n",
      "📈 Episodio 987: Recompensa total (clipped): 30.000, Pasos: 676, Mean Reward Calculado: 0.044379 (Recompensa/Pasos)\n",
      " 631485/825189: episode: 987, duration: 58.562s, episode steps: 676, steps per second:  12, episode reward: 30.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.033985, mae: 3.783439, mean_q: 4.557269, mean_eps: 0.100000\n",
      "📈 Episodio 988: Recompensa total (clipped): 32.000, Pasos: 857, Mean Reward Calculado: 0.037340 (Recompensa/Pasos)\n",
      " 632342/825189: episode: 988, duration: 73.063s, episode steps: 857, steps per second:  12, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.036697, mae: 3.819279, mean_q: 4.599375, mean_eps: 0.100000\n",
      "📈 Episodio 989: Recompensa total (clipped): 20.000, Pasos: 600, Mean Reward Calculado: 0.033333 (Recompensa/Pasos)\n",
      " 632942/825189: episode: 989, duration: 51.276s, episode steps: 600, steps per second:  12, episode reward: 20.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.215 [0.000, 5.000],  loss: 0.031194, mae: 3.800077, mean_q: 4.577814, mean_eps: 0.100000\n",
      "📈 Episodio 990: Recompensa total (clipped): 34.000, Pasos: 890, Mean Reward Calculado: 0.038202 (Recompensa/Pasos)\n",
      " 633832/825189: episode: 990, duration: 76.872s, episode steps: 890, steps per second:  12, episode reward: 34.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.237 [0.000, 5.000],  loss: 0.030432, mae: 3.814325, mean_q: 4.594383, mean_eps: 0.100000\n",
      "📈 Episodio 991: Recompensa total (clipped): 32.000, Pasos: 863, Mean Reward Calculado: 0.037080 (Recompensa/Pasos)\n",
      " 634695/825189: episode: 991, duration: 74.261s, episode steps: 863, steps per second:  12, episode reward: 32.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.036433, mae: 3.828249, mean_q: 4.604729, mean_eps: 0.100000\n",
      "📈 Episodio 992: Recompensa total (clipped): 26.000, Pasos: 589, Mean Reward Calculado: 0.044143 (Recompensa/Pasos)\n",
      " 635284/825189: episode: 992, duration: 50.719s, episode steps: 589, steps per second:  12, episode reward: 26.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.028612, mae: 3.811152, mean_q: 4.588315, mean_eps: 0.100000\n",
      "📈 Episodio 993: Recompensa total (clipped): 28.000, Pasos: 719, Mean Reward Calculado: 0.038943 (Recompensa/Pasos)\n",
      " 636003/825189: episode: 993, duration: 61.417s, episode steps: 719, steps per second:  12, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.033934, mae: 3.774724, mean_q: 4.542837, mean_eps: 0.100000\n",
      "📈 Episodio 994: Recompensa total (clipped): 33.000, Pasos: 737, Mean Reward Calculado: 0.044776 (Recompensa/Pasos)\n",
      " 636740/825189: episode: 994, duration: 67.207s, episode steps: 737, steps per second:  11, episode reward: 33.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.033394, mae: 3.803562, mean_q: 4.581141, mean_eps: 0.100000\n",
      "📈 Episodio 995: Recompensa total (clipped): 12.000, Pasos: 490, Mean Reward Calculado: 0.024490 (Recompensa/Pasos)\n",
      " 637230/825189: episode: 995, duration: 42.098s, episode steps: 490, steps per second:  12, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.276 [0.000, 5.000],  loss: 0.036872, mae: 3.776180, mean_q: 4.545492, mean_eps: 0.100000\n",
      "📈 Episodio 996: Recompensa total (clipped): 32.000, Pasos: 761, Mean Reward Calculado: 0.042050 (Recompensa/Pasos)\n",
      " 637991/825189: episode: 996, duration: 64.620s, episode steps: 761, steps per second:  12, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.264 [0.000, 5.000],  loss: 0.034103, mae: 3.837357, mean_q: 4.620958, mean_eps: 0.100000\n",
      "📈 Episodio 997: Recompensa total (clipped): 5.000, Pasos: 319, Mean Reward Calculado: 0.015674 (Recompensa/Pasos)\n",
      " 638310/825189: episode: 997, duration: 26.858s, episode steps: 319, steps per second:  12, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.172 [0.000, 5.000],  loss: 0.036268, mae: 3.763433, mean_q: 4.531167, mean_eps: 0.100000\n",
      "📈 Episodio 998: Recompensa total (clipped): 38.000, Pasos: 1067, Mean Reward Calculado: 0.035614 (Recompensa/Pasos)\n",
      " 639377/825189: episode: 998, duration: 91.674s, episode steps: 1067, steps per second:  12, episode reward: 38.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.111 [0.000, 5.000],  loss: 0.037473, mae: 3.801922, mean_q: 4.575155, mean_eps: 0.100000\n",
      "📈 Episodio 999: Recompensa total (clipped): 24.000, Pasos: 520, Mean Reward Calculado: 0.046154 (Recompensa/Pasos)\n",
      " 639897/825189: episode: 999, duration: 45.098s, episode steps: 520, steps per second:  12, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.036695, mae: 3.787343, mean_q: 4.555457, mean_eps: 0.100000\n",
      "📊 Paso 640,000/2,000,000 (32.0%) - 17.8 pasos/seg - ETA: 21.2h - Memoria: 7121.43 MB\n",
      "📈 Episodio 1000: Recompensa total (clipped): 33.000, Pasos: 885, Mean Reward Calculado: 0.037288 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 1000, pasos: 640782)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.59 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 1000 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 33.00\n",
      "   Media últimos 100: 26.59 / 20.0\n",
      "   Mejor promedio histórico: 26.59\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 910\n",
      "   Episodios consecutivos en objetivo: 910\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 910 episodios consecutivos\n",
      "💾 Guardado modelo principal lastest: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_model.h5\n",
      "💾 Guardado modelo target lastest: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_target.h5\n",
      "💾 Memoria lastest guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_lastest_memory.pkl\n",
      "💾 Checkpoint lastest guardado (ep: 1000, pasos: 640782)\n",
      "✅ Checkpoint guardado para episodio 1000\n",
      " 640782/825189: episode: 1000, duration: 226.003s, episode steps: 885, steps per second:   4, episode reward: 33.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.036326, mae: 3.799441, mean_q: 4.574542, mean_eps: 0.100000\n",
      "📈 Episodio 1001: Recompensa total (clipped): 33.000, Pasos: 779, Mean Reward Calculado: 0.042362 (Recompensa/Pasos)\n",
      " 641561/825189: episode: 1001, duration: 66.748s, episode steps: 779, steps per second:  12, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.034625, mae: 3.811701, mean_q: 4.588835, mean_eps: 0.100000\n",
      "📈 Episodio 1002: Recompensa total (clipped): 29.000, Pasos: 680, Mean Reward Calculado: 0.042647 (Recompensa/Pasos)\n",
      " 642241/825189: episode: 1002, duration: 58.042s, episode steps: 680, steps per second:  12, episode reward: 29.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.037791, mae: 3.837422, mean_q: 4.622807, mean_eps: 0.100000\n",
      "📈 Episodio 1003: Recompensa total (clipped): 8.000, Pasos: 285, Mean Reward Calculado: 0.028070 (Recompensa/Pasos)\n",
      " 642526/825189: episode: 1003, duration: 24.473s, episode steps: 285, steps per second:  12, episode reward:  8.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.034520, mae: 3.770521, mean_q: 4.536179, mean_eps: 0.100000\n",
      "📈 Episodio 1004: Recompensa total (clipped): 18.000, Pasos: 440, Mean Reward Calculado: 0.040909 (Recompensa/Pasos)\n",
      " 642966/825189: episode: 1004, duration: 37.666s, episode steps: 440, steps per second:  12, episode reward: 18.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.105 [0.000, 5.000],  loss: 0.032338, mae: 3.801679, mean_q: 4.577454, mean_eps: 0.100000\n",
      "📈 Episodio 1005: Recompensa total (clipped): 29.000, Pasos: 654, Mean Reward Calculado: 0.044343 (Recompensa/Pasos)\n",
      " 643620/825189: episode: 1005, duration: 56.711s, episode steps: 654, steps per second:  12, episode reward: 29.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.170 [0.000, 5.000],  loss: 0.036276, mae: 3.775790, mean_q: 4.544705, mean_eps: 0.100000\n",
      "📈 Episodio 1006: Recompensa total (clipped): 34.000, Pasos: 777, Mean Reward Calculado: 0.043758 (Recompensa/Pasos)\n",
      " 644397/825189: episode: 1006, duration: 66.571s, episode steps: 777, steps per second:  12, episode reward: 34.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.245 [0.000, 5.000],  loss: 0.037008, mae: 3.824011, mean_q: 4.604086, mean_eps: 0.100000\n",
      "📈 Episodio 1007: Recompensa total (clipped): 29.000, Pasos: 632, Mean Reward Calculado: 0.045886 (Recompensa/Pasos)\n",
      " 645029/825189: episode: 1007, duration: 53.740s, episode steps: 632, steps per second:  12, episode reward: 29.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.036674, mae: 3.823563, mean_q: 4.604868, mean_eps: 0.100000\n",
      "📈 Episodio 1008: Recompensa total (clipped): 34.000, Pasos: 907, Mean Reward Calculado: 0.037486 (Recompensa/Pasos)\n",
      " 645936/825189: episode: 1008, duration: 77.164s, episode steps: 907, steps per second:  12, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.038845, mae: 3.809449, mean_q: 4.583816, mean_eps: 0.100000\n",
      "📈 Episodio 1009: Recompensa total (clipped): 31.000, Pasos: 725, Mean Reward Calculado: 0.042759 (Recompensa/Pasos)\n",
      " 646661/825189: episode: 1009, duration: 62.075s, episode steps: 725, steps per second:  12, episode reward: 31.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.033313, mae: 3.814149, mean_q: 4.594639, mean_eps: 0.100000\n",
      "📈 Episodio 1010: Recompensa total (clipped): 17.000, Pasos: 402, Mean Reward Calculado: 0.042289 (Recompensa/Pasos)\n",
      " 647063/825189: episode: 1010, duration: 34.069s, episode steps: 402, steps per second:  12, episode reward: 17.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.035252, mae: 3.807397, mean_q: 4.583216, mean_eps: 0.100000\n",
      "📈 Episodio 1011: Recompensa total (clipped): 32.000, Pasos: 847, Mean Reward Calculado: 0.037780 (Recompensa/Pasos)\n",
      " 647910/825189: episode: 1011, duration: 72.292s, episode steps: 847, steps per second:  12, episode reward: 32.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.036901, mae: 3.820962, mean_q: 4.598465, mean_eps: 0.100000\n",
      "📈 Episodio 1012: Recompensa total (clipped): 9.000, Pasos: 320, Mean Reward Calculado: 0.028125 (Recompensa/Pasos)\n",
      " 648230/825189: episode: 1012, duration: 27.352s, episode steps: 320, steps per second:  12, episode reward:  9.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.966 [0.000, 5.000],  loss: 0.034299, mae: 3.799449, mean_q: 4.575902, mean_eps: 0.100000\n",
      "📈 Episodio 1013: Recompensa total (clipped): 26.000, Pasos: 669, Mean Reward Calculado: 0.038864 (Recompensa/Pasos)\n",
      " 648899/825189: episode: 1013, duration: 57.039s, episode steps: 669, steps per second:  12, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.038743, mae: 3.788648, mean_q: 4.558555, mean_eps: 0.100000\n",
      "📈 Episodio 1014: Recompensa total (clipped): 23.000, Pasos: 507, Mean Reward Calculado: 0.045365 (Recompensa/Pasos)\n",
      " 649406/825189: episode: 1014, duration: 43.312s, episode steps: 507, steps per second:  12, episode reward: 23.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.041494, mae: 3.765442, mean_q: 4.531272, mean_eps: 0.100000\n",
      "📈 Episodio 1015: Recompensa total (clipped): 15.000, Pasos: 418, Mean Reward Calculado: 0.035885 (Recompensa/Pasos)\n",
      " 649824/825189: episode: 1015, duration: 35.759s, episode steps: 418, steps per second:  12, episode reward: 15.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.935 [0.000, 5.000],  loss: 0.036353, mae: 3.837893, mean_q: 4.621801, mean_eps: 0.100000\n",
      "📈 Episodio 1016: Recompensa total (clipped): 29.000, Pasos: 877, Mean Reward Calculado: 0.033067 (Recompensa/Pasos)\n",
      " 650701/825189: episode: 1016, duration: 75.280s, episode steps: 877, steps per second:  12, episode reward: 29.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.223 [0.000, 5.000],  loss: 0.037668, mae: 3.781580, mean_q: 4.553440, mean_eps: 0.100000\n",
      "📈 Episodio 1017: Recompensa total (clipped): 23.000, Pasos: 592, Mean Reward Calculado: 0.038851 (Recompensa/Pasos)\n",
      " 651293/825189: episode: 1017, duration: 50.683s, episode steps: 592, steps per second:  12, episode reward: 23.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 3.441 [0.000, 5.000],  loss: 0.044231, mae: 3.842548, mean_q: 4.623379, mean_eps: 0.100000\n",
      "📈 Episodio 1018: Recompensa total (clipped): 13.000, Pasos: 485, Mean Reward Calculado: 0.026804 (Recompensa/Pasos)\n",
      " 651778/825189: episode: 1018, duration: 41.464s, episode steps: 485, steps per second:  12, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.682 [0.000, 5.000],  loss: 0.035118, mae: 3.824816, mean_q: 4.606330, mean_eps: 0.100000\n",
      "📈 Episodio 1019: Recompensa total (clipped): 29.000, Pasos: 758, Mean Reward Calculado: 0.038259 (Recompensa/Pasos)\n",
      " 652536/825189: episode: 1019, duration: 64.896s, episode steps: 758, steps per second:  12, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.679 [0.000, 5.000],  loss: 0.033624, mae: 3.828882, mean_q: 4.609763, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1020: Recompensa total (clipped): 43.000, Pasos: 1033, Mean Reward Calculado: 0.041626 (Recompensa/Pasos)\n",
      " 653569/825189: episode: 1020, duration: 88.566s, episode steps: 1033, steps per second:  12, episode reward: 43.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.032637, mae: 3.787181, mean_q: 4.560918, mean_eps: 0.100000\n",
      "📈 Episodio 1021: Recompensa total (clipped): 12.000, Pasos: 478, Mean Reward Calculado: 0.025105 (Recompensa/Pasos)\n",
      " 654047/825189: episode: 1021, duration: 40.800s, episode steps: 478, steps per second:  12, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.035303, mae: 3.856596, mean_q: 4.644428, mean_eps: 0.100000\n",
      "📈 Episodio 1022: Recompensa total (clipped): 25.000, Pasos: 543, Mean Reward Calculado: 0.046041 (Recompensa/Pasos)\n",
      " 654590/825189: episode: 1022, duration: 46.283s, episode steps: 543, steps per second:  12, episode reward: 25.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 3.157 [0.000, 5.000],  loss: 0.041553, mae: 3.817522, mean_q: 4.592163, mean_eps: 0.100000\n",
      "📈 Episodio 1023: Recompensa total (clipped): 20.000, Pasos: 422, Mean Reward Calculado: 0.047393 (Recompensa/Pasos)\n",
      " 655012/825189: episode: 1023, duration: 36.122s, episode steps: 422, steps per second:  12, episode reward: 20.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.044831, mae: 3.788858, mean_q: 4.554943, mean_eps: 0.100000\n",
      "📈 Episodio 1024: Recompensa total (clipped): 29.000, Pasos: 723, Mean Reward Calculado: 0.040111 (Recompensa/Pasos)\n",
      " 655735/825189: episode: 1024, duration: 61.774s, episode steps: 723, steps per second:  12, episode reward: 29.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.037916, mae: 3.795437, mean_q: 4.567011, mean_eps: 0.100000\n",
      "📈 Episodio 1025: Recompensa total (clipped): 35.000, Pasos: 900, Mean Reward Calculado: 0.038889 (Recompensa/Pasos)\n",
      " 656635/825189: episode: 1025, duration: 76.832s, episode steps: 900, steps per second:  12, episode reward: 35.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.039672, mae: 3.777777, mean_q: 4.548620, mean_eps: 0.100000\n",
      "📈 Episodio 1026: Recompensa total (clipped): 18.000, Pasos: 475, Mean Reward Calculado: 0.037895 (Recompensa/Pasos)\n",
      " 657110/825189: episode: 1026, duration: 40.621s, episode steps: 475, steps per second:  12, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.037198, mae: 3.828851, mean_q: 4.608683, mean_eps: 0.100000\n",
      "📈 Episodio 1027: Recompensa total (clipped): 36.000, Pasos: 970, Mean Reward Calculado: 0.037113 (Recompensa/Pasos)\n",
      " 658080/825189: episode: 1027, duration: 82.896s, episode steps: 970, steps per second:  12, episode reward: 36.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.036787, mae: 3.800961, mean_q: 4.574416, mean_eps: 0.100000\n",
      "📈 Episodio 1028: Recompensa total (clipped): 30.000, Pasos: 683, Mean Reward Calculado: 0.043924 (Recompensa/Pasos)\n",
      " 658763/825189: episode: 1028, duration: 57.884s, episode steps: 683, steps per second:  12, episode reward: 30.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.031348, mae: 3.794548, mean_q: 4.570856, mean_eps: 0.100000\n",
      "📈 Episodio 1029: Recompensa total (clipped): 16.000, Pasos: 485, Mean Reward Calculado: 0.032990 (Recompensa/Pasos)\n",
      " 659248/825189: episode: 1029, duration: 41.357s, episode steps: 485, steps per second:  12, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.042018, mae: 3.740783, mean_q: 4.499998, mean_eps: 0.100000\n",
      "📈 Episodio 1030: Recompensa total (clipped): 30.000, Pasos: 738, Mean Reward Calculado: 0.040650 (Recompensa/Pasos)\n",
      " 659986/825189: episode: 1030, duration: 62.708s, episode steps: 738, steps per second:  12, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.035841, mae: 3.815012, mean_q: 4.594036, mean_eps: 0.100000\n",
      "📊 Paso 660,000/2,000,000 (33.0%) - 17.4 pasos/seg - ETA: 21.3h - Memoria: 10815.32 MB\n",
      "📈 Episodio 1031: Recompensa total (clipped): 31.000, Pasos: 886, Mean Reward Calculado: 0.034989 (Recompensa/Pasos)\n",
      " 660872/825189: episode: 1031, duration: 74.991s, episode steps: 886, steps per second:  12, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.036874, mae: 3.802483, mean_q: 4.580655, mean_eps: 0.100000\n",
      "📈 Episodio 1032: Recompensa total (clipped): 29.000, Pasos: 757, Mean Reward Calculado: 0.038309 (Recompensa/Pasos)\n",
      " 661629/825189: episode: 1032, duration: 64.101s, episode steps: 757, steps per second:  12, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.159 [0.000, 5.000],  loss: 0.037864, mae: 3.759928, mean_q: 4.529497, mean_eps: 0.100000\n",
      "📈 Episodio 1033: Recompensa total (clipped): 29.000, Pasos: 836, Mean Reward Calculado: 0.034689 (Recompensa/Pasos)\n",
      " 662465/825189: episode: 1033, duration: 70.385s, episode steps: 836, steps per second:  12, episode reward: 29.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.191 [0.000, 5.000],  loss: 0.036842, mae: 3.774932, mean_q: 4.545094, mean_eps: 0.100000\n",
      "📈 Episodio 1034: Recompensa total (clipped): 25.000, Pasos: 581, Mean Reward Calculado: 0.043029 (Recompensa/Pasos)\n",
      " 663046/825189: episode: 1034, duration: 49.537s, episode steps: 581, steps per second:  12, episode reward: 25.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.031329, mae: 3.770338, mean_q: 4.542646, mean_eps: 0.100000\n",
      "📈 Episodio 1035: Recompensa total (clipped): 35.000, Pasos: 714, Mean Reward Calculado: 0.049020 (Recompensa/Pasos)\n",
      " 663760/825189: episode: 1035, duration: 60.331s, episode steps: 714, steps per second:  12, episode reward: 35.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.029324, mae: 3.800013, mean_q: 4.576174, mean_eps: 0.100000\n",
      "📈 Episodio 1036: Recompensa total (clipped): 18.000, Pasos: 450, Mean Reward Calculado: 0.040000 (Recompensa/Pasos)\n",
      " 664210/825189: episode: 1036, duration: 38.169s, episode steps: 450, steps per second:  12, episode reward: 18.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.034898, mae: 3.837078, mean_q: 4.618430, mean_eps: 0.100000\n",
      "📈 Episodio 1037: Recompensa total (clipped): 32.000, Pasos: 910, Mean Reward Calculado: 0.035165 (Recompensa/Pasos)\n",
      " 665120/825189: episode: 1037, duration: 76.920s, episode steps: 910, steps per second:  12, episode reward: 32.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.092 [0.000, 5.000],  loss: 0.033130, mae: 3.783482, mean_q: 4.554743, mean_eps: 0.100000\n",
      "📈 Episodio 1038: Recompensa total (clipped): 25.000, Pasos: 611, Mean Reward Calculado: 0.040917 (Recompensa/Pasos)\n",
      " 665731/825189: episode: 1038, duration: 51.626s, episode steps: 611, steps per second:  12, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.033615, mae: 3.809716, mean_q: 4.585734, mean_eps: 0.100000\n",
      "📈 Episodio 1039: Recompensa total (clipped): 17.000, Pasos: 373, Mean Reward Calculado: 0.045576 (Recompensa/Pasos)\n",
      " 666104/825189: episode: 1039, duration: 31.750s, episode steps: 373, steps per second:  12, episode reward: 17.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.036856, mae: 3.788238, mean_q: 4.554848, mean_eps: 0.100000\n",
      "📈 Episodio 1040: Recompensa total (clipped): 35.000, Pasos: 1090, Mean Reward Calculado: 0.032110 (Recompensa/Pasos)\n",
      " 667194/825189: episode: 1040, duration: 91.990s, episode steps: 1090, steps per second:  12, episode reward: 35.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.037303, mae: 3.787949, mean_q: 4.558271, mean_eps: 0.100000\n",
      "📈 Episodio 1041: Recompensa total (clipped): 35.000, Pasos: 812, Mean Reward Calculado: 0.043103 (Recompensa/Pasos)\n",
      " 668006/825189: episode: 1041, duration: 68.532s, episode steps: 812, steps per second:  12, episode reward: 35.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.038005, mae: 3.777299, mean_q: 4.542908, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1042: Recompensa total (clipped): 16.000, Pasos: 378, Mean Reward Calculado: 0.042328 (Recompensa/Pasos)\n",
      " 668384/825189: episode: 1042, duration: 32.063s, episode steps: 378, steps per second:  12, episode reward: 16.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.031197, mae: 3.783850, mean_q: 4.553835, mean_eps: 0.100000\n",
      "📈 Episodio 1043: Recompensa total (clipped): 34.000, Pasos: 890, Mean Reward Calculado: 0.038202 (Recompensa/Pasos)\n",
      " 669274/825189: episode: 1043, duration: 75.167s, episode steps: 890, steps per second:  12, episode reward: 34.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.035610, mae: 3.783773, mean_q: 4.553036, mean_eps: 0.100000\n",
      "📈 Episodio 1044: Recompensa total (clipped): 16.000, Pasos: 395, Mean Reward Calculado: 0.040506 (Recompensa/Pasos)\n",
      " 669669/825189: episode: 1044, duration: 33.498s, episode steps: 395, steps per second:  12, episode reward: 16.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.033898, mae: 3.796615, mean_q: 4.569174, mean_eps: 0.100000\n",
      "📈 Episodio 1045: Recompensa total (clipped): 29.000, Pasos: 712, Mean Reward Calculado: 0.040730 (Recompensa/Pasos)\n",
      " 670381/825189: episode: 1045, duration: 60.093s, episode steps: 712, steps per second:  12, episode reward: 29.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.709 [0.000, 5.000],  loss: 0.031481, mae: 3.765513, mean_q: 4.532950, mean_eps: 0.100000\n",
      "📈 Episodio 1046: Recompensa total (clipped): 14.000, Pasos: 393, Mean Reward Calculado: 0.035623 (Recompensa/Pasos)\n",
      " 670774/825189: episode: 1046, duration: 33.069s, episode steps: 393, steps per second:  12, episode reward: 14.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.033606, mae: 3.788704, mean_q: 4.560865, mean_eps: 0.100000\n",
      "📈 Episodio 1047: Recompensa total (clipped): 31.000, Pasos: 843, Mean Reward Calculado: 0.036773 (Recompensa/Pasos)\n",
      " 671617/825189: episode: 1047, duration: 72.213s, episode steps: 843, steps per second:  12, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.029249, mae: 3.771984, mean_q: 4.538474, mean_eps: 0.100000\n",
      "📈 Episodio 1048: Recompensa total (clipped): 35.000, Pasos: 958, Mean Reward Calculado: 0.036534 (Recompensa/Pasos)\n",
      " 672575/825189: episode: 1048, duration: 82.705s, episode steps: 958, steps per second:  12, episode reward: 35.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.103 [0.000, 5.000],  loss: 0.033804, mae: 3.785579, mean_q: 4.556589, mean_eps: 0.100000\n",
      "📈 Episodio 1049: Recompensa total (clipped): 32.000, Pasos: 741, Mean Reward Calculado: 0.043185 (Recompensa/Pasos)\n",
      " 673316/825189: episode: 1049, duration: 64.094s, episode steps: 741, steps per second:  12, episode reward: 32.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.031605, mae: 3.777867, mean_q: 4.546583, mean_eps: 0.100000\n",
      "📈 Episodio 1050: Recompensa total (clipped): 29.000, Pasos: 662, Mean Reward Calculado: 0.043807 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 1050, pasos: 673978)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.85 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 1050 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 29.00\n",
      "   Media últimos 100: 26.85 / 20.0\n",
      "   Mejor promedio histórico: 26.85\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 960\n",
      "   Episodios consecutivos en objetivo: 960\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 960 episodios consecutivos\n",
      " 673978/825189: episode: 1050, duration: 128.714s, episode steps: 662, steps per second:   5, episode reward: 29.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.119 [0.000, 5.000],  loss: 0.034315, mae: 3.789244, mean_q: 4.559821, mean_eps: 0.100000\n",
      "📈 Episodio 1051: Recompensa total (clipped): 22.000, Pasos: 553, Mean Reward Calculado: 0.039783 (Recompensa/Pasos)\n",
      " 674531/825189: episode: 1051, duration: 48.269s, episode steps: 553, steps per second:  11, episode reward: 22.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.356 [0.000, 5.000],  loss: 0.032661, mae: 3.771442, mean_q: 4.537564, mean_eps: 0.100000\n",
      "📈 Episodio 1052: Recompensa total (clipped): 29.000, Pasos: 763, Mean Reward Calculado: 0.038008 (Recompensa/Pasos)\n",
      " 675294/825189: episode: 1052, duration: 66.926s, episode steps: 763, steps per second:  11, episode reward: 29.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 3.257 [0.000, 5.000],  loss: 0.036011, mae: 3.782842, mean_q: 4.554615, mean_eps: 0.100000\n",
      "📈 Episodio 1053: Recompensa total (clipped): 32.000, Pasos: 991, Mean Reward Calculado: 0.032291 (Recompensa/Pasos)\n",
      " 676285/825189: episode: 1053, duration: 86.975s, episode steps: 991, steps per second:  11, episode reward: 32.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.034282, mae: 3.791911, mean_q: 4.566591, mean_eps: 0.100000\n",
      "📈 Episodio 1054: Recompensa total (clipped): 23.000, Pasos: 530, Mean Reward Calculado: 0.043396 (Recompensa/Pasos)\n",
      " 676815/825189: episode: 1054, duration: 47.867s, episode steps: 530, steps per second:  11, episode reward: 23.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.300 [0.000, 5.000],  loss: 0.039509, mae: 3.790747, mean_q: 4.563156, mean_eps: 0.100000\n",
      "📈 Episodio 1055: Recompensa total (clipped): 31.000, Pasos: 890, Mean Reward Calculado: 0.034831 (Recompensa/Pasos)\n",
      " 677705/825189: episode: 1055, duration: 79.139s, episode steps: 890, steps per second:  11, episode reward: 31.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 3.475 [0.000, 5.000],  loss: 0.034386, mae: 3.775653, mean_q: 4.546518, mean_eps: 0.100000\n",
      "📈 Episodio 1056: Recompensa total (clipped): 29.000, Pasos: 620, Mean Reward Calculado: 0.046774 (Recompensa/Pasos)\n",
      " 678325/825189: episode: 1056, duration: 55.380s, episode steps: 620, steps per second:  11, episode reward: 29.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.028306, mae: 3.766188, mean_q: 4.533245, mean_eps: 0.100000\n",
      "📈 Episodio 1057: Recompensa total (clipped): 16.000, Pasos: 388, Mean Reward Calculado: 0.041237 (Recompensa/Pasos)\n",
      " 678713/825189: episode: 1057, duration: 35.069s, episode steps: 388, steps per second:  11, episode reward: 16.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.039264, mae: 3.782434, mean_q: 4.551336, mean_eps: 0.100000\n",
      "📈 Episodio 1058: Recompensa total (clipped): 13.000, Pasos: 308, Mean Reward Calculado: 0.042208 (Recompensa/Pasos)\n",
      " 679021/825189: episode: 1058, duration: 27.817s, episode steps: 308, steps per second:  11, episode reward: 13.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.029637, mae: 3.819877, mean_q: 4.597441, mean_eps: 0.100000\n",
      "📈 Episodio 1059: Recompensa total (clipped): 23.000, Pasos: 649, Mean Reward Calculado: 0.035439 (Recompensa/Pasos)\n",
      " 679670/825189: episode: 1059, duration: 57.242s, episode steps: 649, steps per second:  11, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.035242, mae: 3.779813, mean_q: 4.547423, mean_eps: 0.100000\n",
      "📊 Paso 680,000/2,000,000 (34.0%) - 17.2 pasos/seg - ETA: 21.4h - Memoria: 11130.20 MB\n",
      "📈 Episodio 1060: Recompensa total (clipped): 21.000, Pasos: 477, Mean Reward Calculado: 0.044025 (Recompensa/Pasos)\n",
      " 680147/825189: episode: 1060, duration: 41.617s, episode steps: 477, steps per second:  11, episode reward: 21.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.035407, mae: 3.781868, mean_q: 4.550246, mean_eps: 0.100000\n",
      "📈 Episodio 1061: Recompensa total (clipped): 34.000, Pasos: 914, Mean Reward Calculado: 0.037199 (Recompensa/Pasos)\n",
      " 681061/825189: episode: 1061, duration: 80.318s, episode steps: 914, steps per second:  11, episode reward: 34.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.032624, mae: 3.808520, mean_q: 4.585824, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1062: Recompensa total (clipped): 31.000, Pasos: 679, Mean Reward Calculado: 0.045655 (Recompensa/Pasos)\n",
      " 681740/825189: episode: 1062, duration: 59.726s, episode steps: 679, steps per second:  11, episode reward: 31.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.032856, mae: 3.813453, mean_q: 4.588082, mean_eps: 0.100000\n",
      "📈 Episodio 1063: Recompensa total (clipped): 32.000, Pasos: 764, Mean Reward Calculado: 0.041885 (Recompensa/Pasos)\n",
      " 682504/825189: episode: 1063, duration: 67.775s, episode steps: 764, steps per second:  11, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.031556, mae: 3.784617, mean_q: 4.555070, mean_eps: 0.100000\n",
      "📈 Episodio 1064: Recompensa total (clipped): 33.000, Pasos: 861, Mean Reward Calculado: 0.038328 (Recompensa/Pasos)\n",
      " 683365/825189: episode: 1064, duration: 75.892s, episode steps: 861, steps per second:  11, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.034818, mae: 3.792596, mean_q: 4.564911, mean_eps: 0.100000\n",
      "📈 Episodio 1065: Recompensa total (clipped): 35.000, Pasos: 761, Mean Reward Calculado: 0.045992 (Recompensa/Pasos)\n",
      " 684126/825189: episode: 1065, duration: 67.057s, episode steps: 761, steps per second:  11, episode reward: 35.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.034698, mae: 3.779836, mean_q: 4.550085, mean_eps: 0.100000\n",
      "📈 Episodio 1066: Recompensa total (clipped): 29.000, Pasos: 690, Mean Reward Calculado: 0.042029 (Recompensa/Pasos)\n",
      " 684816/825189: episode: 1066, duration: 61.681s, episode steps: 690, steps per second:  11, episode reward: 29.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.031818, mae: 3.796686, mean_q: 4.570829, mean_eps: 0.100000\n",
      "📈 Episodio 1067: Recompensa total (clipped): 8.000, Pasos: 356, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
      " 685172/825189: episode: 1067, duration: 32.180s, episode steps: 356, steps per second:  11, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.037714, mae: 3.798241, mean_q: 4.568683, mean_eps: 0.100000\n",
      "📈 Episodio 1068: Recompensa total (clipped): 28.000, Pasos: 758, Mean Reward Calculado: 0.036939 (Recompensa/Pasos)\n",
      " 685930/825189: episode: 1068, duration: 68.217s, episode steps: 758, steps per second:  11, episode reward: 28.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.035597, mae: 3.768856, mean_q: 4.535257, mean_eps: 0.100000\n",
      "📈 Episodio 1069: Recompensa total (clipped): 30.000, Pasos: 723, Mean Reward Calculado: 0.041494 (Recompensa/Pasos)\n",
      " 686653/825189: episode: 1069, duration: 64.431s, episode steps: 723, steps per second:  11, episode reward: 30.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.033126, mae: 3.803268, mean_q: 4.575679, mean_eps: 0.100000\n",
      "📈 Episodio 1070: Recompensa total (clipped): 28.000, Pasos: 645, Mean Reward Calculado: 0.043411 (Recompensa/Pasos)\n",
      " 687298/825189: episode: 1070, duration: 57.745s, episode steps: 645, steps per second:  11, episode reward: 28.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.035124, mae: 3.836894, mean_q: 4.617167, mean_eps: 0.100000\n",
      "📈 Episodio 1071: Recompensa total (clipped): 26.000, Pasos: 669, Mean Reward Calculado: 0.038864 (Recompensa/Pasos)\n",
      " 687967/825189: episode: 1071, duration: 59.893s, episode steps: 669, steps per second:  11, episode reward: 26.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.034576, mae: 3.832714, mean_q: 4.613068, mean_eps: 0.100000\n",
      "📈 Episodio 1072: Recompensa total (clipped): 30.000, Pasos: 840, Mean Reward Calculado: 0.035714 (Recompensa/Pasos)\n",
      " 688807/825189: episode: 1072, duration: 75.888s, episode steps: 840, steps per second:  11, episode reward: 30.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.037814, mae: 3.766214, mean_q: 4.531625, mean_eps: 0.100000\n",
      "📈 Episodio 1073: Recompensa total (clipped): 19.000, Pasos: 425, Mean Reward Calculado: 0.044706 (Recompensa/Pasos)\n",
      " 689232/825189: episode: 1073, duration: 38.331s, episode steps: 425, steps per second:  11, episode reward: 19.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.040418, mae: 3.826521, mean_q: 4.602468, mean_eps: 0.100000\n",
      "📈 Episodio 1074: Recompensa total (clipped): 33.000, Pasos: 960, Mean Reward Calculado: 0.034375 (Recompensa/Pasos)\n",
      " 690192/825189: episode: 1074, duration: 86.949s, episode steps: 960, steps per second:  11, episode reward: 33.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.036081, mae: 3.802454, mean_q: 4.577930, mean_eps: 0.100000\n",
      "📈 Episodio 1075: Recompensa total (clipped): 33.000, Pasos: 822, Mean Reward Calculado: 0.040146 (Recompensa/Pasos)\n",
      " 691014/825189: episode: 1075, duration: 74.583s, episode steps: 822, steps per second:  11, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.037359, mae: 3.773297, mean_q: 4.543751, mean_eps: 0.100000\n",
      "📈 Episodio 1076: Recompensa total (clipped): 33.000, Pasos: 1031, Mean Reward Calculado: 0.032008 (Recompensa/Pasos)\n",
      " 692045/825189: episode: 1076, duration: 91.837s, episode steps: 1031, steps per second:  11, episode reward: 33.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.035478, mae: 3.805865, mean_q: 4.583681, mean_eps: 0.100000\n",
      "📈 Episodio 1077: Recompensa total (clipped): 28.000, Pasos: 685, Mean Reward Calculado: 0.040876 (Recompensa/Pasos)\n",
      " 692730/825189: episode: 1077, duration: 61.538s, episode steps: 685, steps per second:  11, episode reward: 28.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.035221, mae: 3.755861, mean_q: 4.520691, mean_eps: 0.100000\n",
      "📈 Episodio 1078: Recompensa total (clipped): 24.000, Pasos: 520, Mean Reward Calculado: 0.046154 (Recompensa/Pasos)\n",
      " 693250/825189: episode: 1078, duration: 46.065s, episode steps: 520, steps per second:  11, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.937 [0.000, 5.000],  loss: 0.028924, mae: 3.820543, mean_q: 4.599414, mean_eps: 0.100000\n",
      "📈 Episodio 1079: Recompensa total (clipped): 16.000, Pasos: 386, Mean Reward Calculado: 0.041451 (Recompensa/Pasos)\n",
      " 693636/825189: episode: 1079, duration: 34.346s, episode steps: 386, steps per second:  11, episode reward: 16.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: 0.030623, mae: 3.741248, mean_q: 4.503263, mean_eps: 0.100000\n",
      "📈 Episodio 1080: Recompensa total (clipped): 24.000, Pasos: 579, Mean Reward Calculado: 0.041451 (Recompensa/Pasos)\n",
      " 694215/825189: episode: 1080, duration: 51.616s, episode steps: 579, steps per second:  11, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.041107, mae: 3.820969, mean_q: 4.596687, mean_eps: 0.100000\n",
      "📈 Episodio 1081: Recompensa total (clipped): 24.000, Pasos: 518, Mean Reward Calculado: 0.046332 (Recompensa/Pasos)\n",
      " 694733/825189: episode: 1081, duration: 46.142s, episode steps: 518, steps per second:  11, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.595 [0.000, 5.000],  loss: 0.033467, mae: 3.818710, mean_q: 4.597147, mean_eps: 0.100000\n",
      "📈 Episodio 1082: Recompensa total (clipped): 22.000, Pasos: 502, Mean Reward Calculado: 0.043825 (Recompensa/Pasos)\n",
      " 695235/825189: episode: 1082, duration: 45.141s, episode steps: 502, steps per second:  11, episode reward: 22.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.031588, mae: 3.823462, mean_q: 4.603939, mean_eps: 0.100000\n",
      "📈 Episodio 1083: Recompensa total (clipped): 7.000, Pasos: 268, Mean Reward Calculado: 0.026119 (Recompensa/Pasos)\n",
      " 695503/825189: episode: 1083, duration: 22.960s, episode steps: 268, steps per second:  12, episode reward:  7.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.034766, mae: 3.712951, mean_q: 4.468400, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1084: Recompensa total (clipped): 34.000, Pasos: 805, Mean Reward Calculado: 0.042236 (Recompensa/Pasos)\n",
      " 696308/825189: episode: 1084, duration: 71.705s, episode steps: 805, steps per second:  11, episode reward: 34.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.030228, mae: 3.802282, mean_q: 4.579993, mean_eps: 0.100000\n",
      "📈 Episodio 1085: Recompensa total (clipped): 24.000, Pasos: 534, Mean Reward Calculado: 0.044944 (Recompensa/Pasos)\n",
      " 696842/825189: episode: 1085, duration: 47.907s, episode steps: 534, steps per second:  11, episode reward: 24.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.031223, mae: 3.791277, mean_q: 4.566054, mean_eps: 0.100000\n",
      "📈 Episodio 1086: Recompensa total (clipped): 23.000, Pasos: 546, Mean Reward Calculado: 0.042125 (Recompensa/Pasos)\n",
      " 697388/825189: episode: 1086, duration: 48.525s, episode steps: 546, steps per second:  11, episode reward: 23.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.030118, mae: 3.752429, mean_q: 4.518773, mean_eps: 0.100000\n",
      "📈 Episodio 1087: Recompensa total (clipped): 29.000, Pasos: 652, Mean Reward Calculado: 0.044479 (Recompensa/Pasos)\n",
      " 698040/825189: episode: 1087, duration: 57.898s, episode steps: 652, steps per second:  11, episode reward: 29.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.033139, mae: 3.796230, mean_q: 4.568343, mean_eps: 0.100000\n",
      "📈 Episodio 1088: Recompensa total (clipped): 15.000, Pasos: 321, Mean Reward Calculado: 0.046729 (Recompensa/Pasos)\n",
      " 698361/825189: episode: 1088, duration: 29.130s, episode steps: 321, steps per second:  11, episode reward: 15.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 3.330 [0.000, 5.000],  loss: 0.032853, mae: 3.792992, mean_q: 4.564392, mean_eps: 0.100000\n",
      "📈 Episodio 1089: Recompensa total (clipped): 30.000, Pasos: 923, Mean Reward Calculado: 0.032503 (Recompensa/Pasos)\n",
      " 699284/825189: episode: 1089, duration: 83.336s, episode steps: 923, steps per second:  11, episode reward: 30.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.032284, mae: 3.756101, mean_q: 4.520857, mean_eps: 0.100000\n",
      "📊 Paso 700,000/2,000,000 (35.0%) - 16.9 pasos/seg - ETA: 21.4h - Memoria: 6875.62 MB\n",
      "📈 Episodio 1090: Recompensa total (clipped): 39.000, Pasos: 1135, Mean Reward Calculado: 0.034361 (Recompensa/Pasos)\n",
      " 700419/825189: episode: 1090, duration: 101.031s, episode steps: 1135, steps per second:  11, episode reward: 39.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.034819, mae: 3.781117, mean_q: 4.550154, mean_eps: 0.100000\n",
      "📈 Episodio 1091: Recompensa total (clipped): 24.000, Pasos: 560, Mean Reward Calculado: 0.042857 (Recompensa/Pasos)\n",
      " 700979/825189: episode: 1091, duration: 50.315s, episode steps: 560, steps per second:  11, episode reward: 24.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.032793, mae: 3.736860, mean_q: 4.499363, mean_eps: 0.100000\n",
      "📈 Episodio 1092: Recompensa total (clipped): 30.000, Pasos: 702, Mean Reward Calculado: 0.042735 (Recompensa/Pasos)\n",
      " 701681/825189: episode: 1092, duration: 62.687s, episode steps: 702, steps per second:  11, episode reward: 30.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.035431, mae: 3.771658, mean_q: 4.541511, mean_eps: 0.100000\n",
      "📈 Episodio 1093: Recompensa total (clipped): 24.000, Pasos: 547, Mean Reward Calculado: 0.043876 (Recompensa/Pasos)\n",
      " 702228/825189: episode: 1093, duration: 48.784s, episode steps: 547, steps per second:  11, episode reward: 24.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.032064, mae: 3.785824, mean_q: 4.558801, mean_eps: 0.100000\n",
      "📈 Episodio 1094: Recompensa total (clipped): 17.000, Pasos: 490, Mean Reward Calculado: 0.034694 (Recompensa/Pasos)\n",
      " 702718/825189: episode: 1094, duration: 44.653s, episode steps: 490, steps per second:  11, episode reward: 17.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.039201, mae: 3.761890, mean_q: 4.530058, mean_eps: 0.100000\n",
      "📈 Episodio 1095: Recompensa total (clipped): 26.000, Pasos: 706, Mean Reward Calculado: 0.036827 (Recompensa/Pasos)\n",
      " 703424/825189: episode: 1095, duration: 63.114s, episode steps: 706, steps per second:  11, episode reward: 26.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.034208, mae: 3.759766, mean_q: 4.528911, mean_eps: 0.100000\n",
      "📈 Episodio 1096: Recompensa total (clipped): 34.000, Pasos: 1000, Mean Reward Calculado: 0.034000 (Recompensa/Pasos)\n",
      " 704424/825189: episode: 1096, duration: 89.755s, episode steps: 1000, steps per second:  11, episode reward: 34.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.033904, mae: 3.789993, mean_q: 4.564423, mean_eps: 0.100000\n",
      "📈 Episodio 1097: Recompensa total (clipped): 28.000, Pasos: 597, Mean Reward Calculado: 0.046901 (Recompensa/Pasos)\n",
      " 705021/825189: episode: 1097, duration: 53.558s, episode steps: 597, steps per second:  11, episode reward: 28.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.035830, mae: 3.765346, mean_q: 4.532429, mean_eps: 0.100000\n",
      "📈 Episodio 1098: Recompensa total (clipped): 19.000, Pasos: 515, Mean Reward Calculado: 0.036893 (Recompensa/Pasos)\n",
      " 705536/825189: episode: 1098, duration: 46.423s, episode steps: 515, steps per second:  11, episode reward: 19.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.032213, mae: 3.760903, mean_q: 4.530501, mean_eps: 0.100000\n",
      "📈 Episodio 1099: Recompensa total (clipped): 24.000, Pasos: 520, Mean Reward Calculado: 0.046154 (Recompensa/Pasos)\n",
      " 706056/825189: episode: 1099, duration: 46.471s, episode steps: 520, steps per second:  11, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.031592, mae: 3.794810, mean_q: 4.568791, mean_eps: 0.100000\n",
      "📈 Episodio 1100: Recompensa total (clipped): 25.000, Pasos: 566, Mean Reward Calculado: 0.044170 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 1100, pasos: 706622)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.93 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 1100 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 25.00\n",
      "   Media últimos 100: 25.93 / 20.0\n",
      "   Mejor promedio histórico: 25.93\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 1010\n",
      "   Episodios consecutivos en objetivo: 1010\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 1010 episodios consecutivos\n",
      " 706622/825189: episode: 1100, duration: 129.581s, episode steps: 566, steps per second:   4, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.033399, mae: 3.779958, mean_q: 4.551885, mean_eps: 0.100000\n",
      "📈 Episodio 1101: Recompensa total (clipped): 31.000, Pasos: 1043, Mean Reward Calculado: 0.029722 (Recompensa/Pasos)\n",
      " 707665/825189: episode: 1101, duration: 91.941s, episode steps: 1043, steps per second:  11, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.035098, mae: 3.770854, mean_q: 4.540628, mean_eps: 0.100000\n",
      "📈 Episodio 1102: Recompensa total (clipped): 37.000, Pasos: 1097, Mean Reward Calculado: 0.033728 (Recompensa/Pasos)\n",
      " 708762/825189: episode: 1102, duration: 97.241s, episode steps: 1097, steps per second:  11, episode reward: 37.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.038205, mae: 3.776685, mean_q: 4.545058, mean_eps: 0.100000\n",
      "📈 Episodio 1103: Recompensa total (clipped): 26.000, Pasos: 703, Mean Reward Calculado: 0.036984 (Recompensa/Pasos)\n",
      " 709465/825189: episode: 1103, duration: 62.084s, episode steps: 703, steps per second:  11, episode reward: 26.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.035001, mae: 3.788316, mean_q: 4.561592, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1104: Recompensa total (clipped): 27.000, Pasos: 612, Mean Reward Calculado: 0.044118 (Recompensa/Pasos)\n",
      " 710077/825189: episode: 1104, duration: 52.618s, episode steps: 612, steps per second:  12, episode reward: 27.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.040370, mae: 3.781537, mean_q: 4.549438, mean_eps: 0.100000\n",
      "📈 Episodio 1105: Recompensa total (clipped): 25.000, Pasos: 614, Mean Reward Calculado: 0.040717 (Recompensa/Pasos)\n",
      " 710691/825189: episode: 1105, duration: 52.570s, episode steps: 614, steps per second:  12, episode reward: 25.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.039492, mae: 3.801211, mean_q: 4.575959, mean_eps: 0.100000\n",
      "📈 Episodio 1106: Recompensa total (clipped): 23.000, Pasos: 567, Mean Reward Calculado: 0.040564 (Recompensa/Pasos)\n",
      " 711258/825189: episode: 1106, duration: 48.778s, episode steps: 567, steps per second:  12, episode reward: 23.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.032670, mae: 3.818384, mean_q: 4.597454, mean_eps: 0.100000\n",
      "📈 Episodio 1107: Recompensa total (clipped): 31.000, Pasos: 822, Mean Reward Calculado: 0.037713 (Recompensa/Pasos)\n",
      " 712080/825189: episode: 1107, duration: 70.603s, episode steps: 822, steps per second:  12, episode reward: 31.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.034502, mae: 3.771630, mean_q: 4.539246, mean_eps: 0.100000\n",
      "📈 Episodio 1108: Recompensa total (clipped): 26.000, Pasos: 629, Mean Reward Calculado: 0.041335 (Recompensa/Pasos)\n",
      " 712709/825189: episode: 1108, duration: 54.245s, episode steps: 629, steps per second:  12, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.036593, mae: 3.804655, mean_q: 4.578051, mean_eps: 0.100000\n",
      "📈 Episodio 1109: Recompensa total (clipped): 31.000, Pasos: 836, Mean Reward Calculado: 0.037081 (Recompensa/Pasos)\n",
      " 713545/825189: episode: 1109, duration: 71.930s, episode steps: 836, steps per second:  12, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 1.791 [0.000, 5.000],  loss: 0.029151, mae: 3.802096, mean_q: 4.577546, mean_eps: 0.100000\n",
      "📈 Episodio 1110: Recompensa total (clipped): 20.000, Pasos: 514, Mean Reward Calculado: 0.038911 (Recompensa/Pasos)\n",
      " 714059/825189: episode: 1110, duration: 44.021s, episode steps: 514, steps per second:  12, episode reward: 20.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.030867, mae: 3.762782, mean_q: 4.528636, mean_eps: 0.100000\n",
      "📈 Episodio 1111: Recompensa total (clipped): 32.000, Pasos: 771, Mean Reward Calculado: 0.041505 (Recompensa/Pasos)\n",
      " 714830/825189: episode: 1111, duration: 65.941s, episode steps: 771, steps per second:  12, episode reward: 32.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.034759, mae: 3.815051, mean_q: 4.590145, mean_eps: 0.100000\n",
      "📈 Episodio 1112: Recompensa total (clipped): 23.000, Pasos: 519, Mean Reward Calculado: 0.044316 (Recompensa/Pasos)\n",
      " 715349/825189: episode: 1112, duration: 44.783s, episode steps: 519, steps per second:  12, episode reward: 23.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.035326, mae: 3.816578, mean_q: 4.591852, mean_eps: 0.100000\n",
      "📈 Episodio 1113: Recompensa total (clipped): 34.000, Pasos: 735, Mean Reward Calculado: 0.046259 (Recompensa/Pasos)\n",
      " 716084/825189: episode: 1113, duration: 63.269s, episode steps: 735, steps per second:  12, episode reward: 34.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.980 [0.000, 5.000],  loss: 0.034632, mae: 3.776472, mean_q: 4.546389, mean_eps: 0.100000\n",
      "📈 Episodio 1114: Recompensa total (clipped): 29.000, Pasos: 677, Mean Reward Calculado: 0.042836 (Recompensa/Pasos)\n",
      " 716761/825189: episode: 1114, duration: 58.381s, episode steps: 677, steps per second:  12, episode reward: 29.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.040918, mae: 3.767867, mean_q: 4.533274, mean_eps: 0.100000\n",
      "📈 Episodio 1115: Recompensa total (clipped): 22.000, Pasos: 484, Mean Reward Calculado: 0.045455 (Recompensa/Pasos)\n",
      " 717245/825189: episode: 1115, duration: 41.727s, episode steps: 484, steps per second:  12, episode reward: 22.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 1.955 [0.000, 5.000],  loss: 0.033540, mae: 3.792958, mean_q: 4.562487, mean_eps: 0.100000\n",
      "📈 Episodio 1116: Recompensa total (clipped): 22.000, Pasos: 579, Mean Reward Calculado: 0.037997 (Recompensa/Pasos)\n",
      " 717824/825189: episode: 1116, duration: 49.873s, episode steps: 579, steps per second:  12, episode reward: 22.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.038315, mae: 3.802509, mean_q: 4.573374, mean_eps: 0.100000\n",
      "📈 Episodio 1117: Recompensa total (clipped): 26.000, Pasos: 616, Mean Reward Calculado: 0.042208 (Recompensa/Pasos)\n",
      " 718440/825189: episode: 1117, duration: 53.217s, episode steps: 616, steps per second:  12, episode reward: 26.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.034991, mae: 3.781460, mean_q: 4.549188, mean_eps: 0.100000\n",
      "📈 Episodio 1118: Recompensa total (clipped): 22.000, Pasos: 536, Mean Reward Calculado: 0.041045 (Recompensa/Pasos)\n",
      " 718976/825189: episode: 1118, duration: 46.183s, episode steps: 536, steps per second:  12, episode reward: 22.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.036993, mae: 3.823085, mean_q: 4.598442, mean_eps: 0.100000\n",
      "📈 Episodio 1119: Recompensa total (clipped): 25.000, Pasos: 554, Mean Reward Calculado: 0.045126 (Recompensa/Pasos)\n",
      " 719530/825189: episode: 1119, duration: 47.929s, episode steps: 554, steps per second:  12, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.406 [0.000, 5.000],  loss: 0.032162, mae: 3.801700, mean_q: 4.574462, mean_eps: 0.100000\n",
      "📈 Episodio 1120: Recompensa total (clipped): 18.000, Pasos: 400, Mean Reward Calculado: 0.045000 (Recompensa/Pasos)\n",
      " 719930/825189: episode: 1120, duration: 34.463s, episode steps: 400, steps per second:  12, episode reward: 18.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.031588, mae: 3.808413, mean_q: 4.584934, mean_eps: 0.100000\n",
      "📊 Paso 720,000/2,000,000 (36.0%) - 16.6 pasos/seg - ETA: 21.4h - Memoria: 10786.84 MB\n",
      "📈 Episodio 1121: Recompensa total (clipped): 34.000, Pasos: 754, Mean Reward Calculado: 0.045093 (Recompensa/Pasos)\n",
      " 720684/825189: episode: 1121, duration: 64.569s, episode steps: 754, steps per second:  12, episode reward: 34.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 3.321 [0.000, 5.000],  loss: 0.029262, mae: 3.820028, mean_q: 4.599461, mean_eps: 0.100000\n",
      "📈 Episodio 1122: Recompensa total (clipped): 29.000, Pasos: 695, Mean Reward Calculado: 0.041727 (Recompensa/Pasos)\n",
      " 721379/825189: episode: 1122, duration: 59.526s, episode steps: 695, steps per second:  12, episode reward: 29.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.034362, mae: 3.834847, mean_q: 4.614338, mean_eps: 0.100000\n",
      "📈 Episodio 1123: Recompensa total (clipped): 33.000, Pasos: 868, Mean Reward Calculado: 0.038018 (Recompensa/Pasos)\n",
      " 722247/825189: episode: 1123, duration: 74.134s, episode steps: 868, steps per second:  12, episode reward: 33.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.033997, mae: 3.844676, mean_q: 4.627882, mean_eps: 0.100000\n",
      "📈 Episodio 1124: Recompensa total (clipped): 26.000, Pasos: 657, Mean Reward Calculado: 0.039574 (Recompensa/Pasos)\n",
      " 722904/825189: episode: 1124, duration: 56.592s, episode steps: 657, steps per second:  12, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.030771, mae: 3.815308, mean_q: 4.593414, mean_eps: 0.100000\n",
      "📈 Episodio 1125: Recompensa total (clipped): 22.000, Pasos: 529, Mean Reward Calculado: 0.041588 (Recompensa/Pasos)\n",
      " 723433/825189: episode: 1125, duration: 45.591s, episode steps: 529, steps per second:  12, episode reward: 22.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.044424, mae: 3.818580, mean_q: 4.593747, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1126: Recompensa total (clipped): 25.000, Pasos: 564, Mean Reward Calculado: 0.044326 (Recompensa/Pasos)\n",
      " 723997/825189: episode: 1126, duration: 48.311s, episode steps: 564, steps per second:  12, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.032099, mae: 3.837431, mean_q: 4.622108, mean_eps: 0.100000\n",
      "📈 Episodio 1127: Recompensa total (clipped): 25.000, Pasos: 550, Mean Reward Calculado: 0.045455 (Recompensa/Pasos)\n",
      " 724547/825189: episode: 1127, duration: 47.000s, episode steps: 550, steps per second:  12, episode reward: 25.000, mean reward:  0.045 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.032708, mae: 3.823635, mean_q: 4.602368, mean_eps: 0.100000\n",
      "📈 Episodio 1128: Recompensa total (clipped): 29.000, Pasos: 788, Mean Reward Calculado: 0.036802 (Recompensa/Pasos)\n",
      " 725335/825189: episode: 1128, duration: 67.596s, episode steps: 788, steps per second:  12, episode reward: 29.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.038264, mae: 3.835767, mean_q: 4.617482, mean_eps: 0.100000\n",
      "📈 Episodio 1129: Recompensa total (clipped): 18.000, Pasos: 433, Mean Reward Calculado: 0.041570 (Recompensa/Pasos)\n",
      " 725768/825189: episode: 1129, duration: 37.393s, episode steps: 433, steps per second:  12, episode reward: 18.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.037167, mae: 3.841296, mean_q: 4.622015, mean_eps: 0.100000\n",
      "📈 Episodio 1130: Recompensa total (clipped): 33.000, Pasos: 706, Mean Reward Calculado: 0.046742 (Recompensa/Pasos)\n",
      " 726474/825189: episode: 1130, duration: 60.696s, episode steps: 706, steps per second:  12, episode reward: 33.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.034541, mae: 3.842007, mean_q: 4.621174, mean_eps: 0.100000\n",
      "📈 Episodio 1131: Recompensa total (clipped): 24.000, Pasos: 581, Mean Reward Calculado: 0.041308 (Recompensa/Pasos)\n",
      " 727055/825189: episode: 1131, duration: 49.892s, episode steps: 581, steps per second:  12, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.033643, mae: 3.796429, mean_q: 4.569397, mean_eps: 0.100000\n",
      "📈 Episodio 1132: Recompensa total (clipped): 27.000, Pasos: 723, Mean Reward Calculado: 0.037344 (Recompensa/Pasos)\n",
      " 727778/825189: episode: 1132, duration: 62.058s, episode steps: 723, steps per second:  12, episode reward: 27.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 3.069 [0.000, 5.000],  loss: 0.036518, mae: 3.842724, mean_q: 4.622681, mean_eps: 0.100000\n",
      "📈 Episodio 1133: Recompensa total (clipped): 29.000, Pasos: 590, Mean Reward Calculado: 0.049153 (Recompensa/Pasos)\n",
      " 728368/825189: episode: 1133, duration: 50.797s, episode steps: 590, steps per second:  12, episode reward: 29.000, mean reward:  0.049 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.039986, mae: 3.787885, mean_q: 4.555957, mean_eps: 0.100000\n",
      "📈 Episodio 1134: Recompensa total (clipped): 28.000, Pasos: 634, Mean Reward Calculado: 0.044164 (Recompensa/Pasos)\n",
      " 729002/825189: episode: 1134, duration: 54.450s, episode steps: 634, steps per second:  12, episode reward: 28.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.038480, mae: 3.860340, mean_q: 4.644766, mean_eps: 0.100000\n",
      "📈 Episodio 1135: Recompensa total (clipped): 16.000, Pasos: 406, Mean Reward Calculado: 0.039409 (Recompensa/Pasos)\n",
      " 729408/825189: episode: 1135, duration: 34.878s, episode steps: 406, steps per second:  12, episode reward: 16.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.029137, mae: 3.813545, mean_q: 4.589109, mean_eps: 0.100000\n",
      "📈 Episodio 1136: Recompensa total (clipped): 33.000, Pasos: 857, Mean Reward Calculado: 0.038506 (Recompensa/Pasos)\n",
      " 730265/825189: episode: 1136, duration: 73.745s, episode steps: 857, steps per second:  12, episode reward: 33.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.031762, mae: 3.812237, mean_q: 4.589062, mean_eps: 0.100000\n",
      "📈 Episodio 1137: Recompensa total (clipped): 26.000, Pasos: 644, Mean Reward Calculado: 0.040373 (Recompensa/Pasos)\n",
      " 730909/825189: episode: 1137, duration: 55.351s, episode steps: 644, steps per second:  12, episode reward: 26.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.033965, mae: 3.807347, mean_q: 4.582832, mean_eps: 0.100000\n",
      "📈 Episodio 1138: Recompensa total (clipped): 32.000, Pasos: 748, Mean Reward Calculado: 0.042781 (Recompensa/Pasos)\n",
      " 731657/825189: episode: 1138, duration: 64.348s, episode steps: 748, steps per second:  12, episode reward: 32.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.037708, mae: 3.798258, mean_q: 4.571847, mean_eps: 0.100000\n",
      "📈 Episodio 1139: Recompensa total (clipped): 12.000, Pasos: 418, Mean Reward Calculado: 0.028708 (Recompensa/Pasos)\n",
      " 732075/825189: episode: 1139, duration: 35.823s, episode steps: 418, steps per second:  12, episode reward: 12.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.030195, mae: 3.814767, mean_q: 4.594248, mean_eps: 0.100000\n",
      "📈 Episodio 1140: Recompensa total (clipped): 11.000, Pasos: 256, Mean Reward Calculado: 0.042969 (Recompensa/Pasos)\n",
      " 732331/825189: episode: 1140, duration: 22.081s, episode steps: 256, steps per second:  12, episode reward: 11.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.042324, mae: 3.817174, mean_q: 4.590904, mean_eps: 0.100000\n",
      "📈 Episodio 1141: Recompensa total (clipped): 33.000, Pasos: 834, Mean Reward Calculado: 0.039568 (Recompensa/Pasos)\n",
      " 733165/825189: episode: 1141, duration: 71.744s, episode steps: 834, steps per second:  12, episode reward: 33.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.038244, mae: 3.799387, mean_q: 4.573246, mean_eps: 0.100000\n",
      "📈 Episodio 1142: Recompensa total (clipped): 22.000, Pasos: 539, Mean Reward Calculado: 0.040816 (Recompensa/Pasos)\n",
      " 733704/825189: episode: 1142, duration: 46.354s, episode steps: 539, steps per second:  12, episode reward: 22.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 3.026 [0.000, 5.000],  loss: 0.032339, mae: 3.787799, mean_q: 4.559823, mean_eps: 0.100000\n",
      "📈 Episodio 1143: Recompensa total (clipped): 33.000, Pasos: 777, Mean Reward Calculado: 0.042471 (Recompensa/Pasos)\n",
      " 734481/825189: episode: 1143, duration: 66.861s, episode steps: 777, steps per second:  12, episode reward: 33.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.037505, mae: 3.820010, mean_q: 4.596144, mean_eps: 0.100000\n",
      "📈 Episodio 1144: Recompensa total (clipped): 28.000, Pasos: 720, Mean Reward Calculado: 0.038889 (Recompensa/Pasos)\n",
      " 735201/825189: episode: 1144, duration: 61.719s, episode steps: 720, steps per second:  12, episode reward: 28.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.037384, mae: 3.809441, mean_q: 4.582903, mean_eps: 0.100000\n",
      "📈 Episodio 1145: Recompensa total (clipped): 14.000, Pasos: 371, Mean Reward Calculado: 0.037736 (Recompensa/Pasos)\n",
      " 735572/825189: episode: 1145, duration: 31.992s, episode steps: 371, steps per second:  12, episode reward: 14.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.031634, mae: 3.832111, mean_q: 4.613172, mean_eps: 0.100000\n",
      "📈 Episodio 1146: Recompensa total (clipped): 22.000, Pasos: 509, Mean Reward Calculado: 0.043222 (Recompensa/Pasos)\n",
      " 736081/825189: episode: 1146, duration: 44.061s, episode steps: 509, steps per second:  12, episode reward: 22.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.035995, mae: 3.790538, mean_q: 4.559214, mean_eps: 0.100000\n",
      "📈 Episodio 1147: Recompensa total (clipped): 21.000, Pasos: 472, Mean Reward Calculado: 0.044492 (Recompensa/Pasos)\n",
      " 736553/825189: episode: 1147, duration: 40.739s, episode steps: 472, steps per second:  12, episode reward: 21.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.037229, mae: 3.807583, mean_q: 4.581276, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1148: Recompensa total (clipped): 32.000, Pasos: 676, Mean Reward Calculado: 0.047337 (Recompensa/Pasos)\n",
      " 737229/825189: episode: 1148, duration: 58.082s, episode steps: 676, steps per second:  12, episode reward: 32.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.033651, mae: 3.791503, mean_q: 4.562880, mean_eps: 0.100000\n",
      "📈 Episodio 1149: Recompensa total (clipped): 32.000, Pasos: 888, Mean Reward Calculado: 0.036036 (Recompensa/Pasos)\n",
      " 738117/825189: episode: 1149, duration: 76.075s, episode steps: 888, steps per second:  12, episode reward: 32.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.033520, mae: 3.793584, mean_q: 4.565289, mean_eps: 0.100000\n",
      "📈 Episodio 1150: Recompensa total (clipped): 34.000, Pasos: 885, Mean Reward Calculado: 0.038418 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 1150, pasos: 739002)\n",
      "💾 NUEVO MEJOR PROMEDIO: 26.04 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 1150 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 34.00\n",
      "   Media últimos 100: 26.04 / 20.0\n",
      "   Mejor promedio histórico: 26.04\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 1060\n",
      "   Episodios consecutivos en objetivo: 1060\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 1060 episodios consecutivos\n",
      " 739002/825189: episode: 1150, duration: 149.933s, episode steps: 885, steps per second:   6, episode reward: 34.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.040395, mae: 3.775618, mean_q: 4.541342, mean_eps: 0.100000\n",
      "📈 Episodio 1151: Recompensa total (clipped): 20.000, Pasos: 473, Mean Reward Calculado: 0.042283 (Recompensa/Pasos)\n",
      " 739475/825189: episode: 1151, duration: 40.867s, episode steps: 473, steps per second:  12, episode reward: 20.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.028149, mae: 3.788973, mean_q: 4.563502, mean_eps: 0.100000\n",
      "📈 Episodio 1152: Recompensa total (clipped): 18.000, Pasos: 468, Mean Reward Calculado: 0.038462 (Recompensa/Pasos)\n",
      " 739943/825189: episode: 1152, duration: 40.632s, episode steps: 468, steps per second:  12, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.037547, mae: 3.805556, mean_q: 4.577308, mean_eps: 0.100000\n",
      "📊 Paso 740,000/2,000,000 (37.0%) - 16.4 pasos/seg - ETA: 21.3h - Memoria: 11025.61 MB\n",
      "📈 Episodio 1153: Recompensa total (clipped): 27.000, Pasos: 727, Mean Reward Calculado: 0.037139 (Recompensa/Pasos)\n",
      " 740670/825189: episode: 1153, duration: 63.023s, episode steps: 727, steps per second:  12, episode reward: 27.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.030938, mae: 3.828009, mean_q: 4.608230, mean_eps: 0.100000\n",
      "📈 Episodio 1154: Recompensa total (clipped): 21.000, Pasos: 541, Mean Reward Calculado: 0.038817 (Recompensa/Pasos)\n",
      " 741211/825189: episode: 1154, duration: 46.678s, episode steps: 541, steps per second:  12, episode reward: 21.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.037421, mae: 3.849208, mean_q: 4.631774, mean_eps: 0.100000\n",
      "📈 Episodio 1155: Recompensa total (clipped): 24.000, Pasos: 571, Mean Reward Calculado: 0.042032 (Recompensa/Pasos)\n",
      " 741782/825189: episode: 1155, duration: 49.579s, episode steps: 571, steps per second:  12, episode reward: 24.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.034452, mae: 3.832947, mean_q: 4.612548, mean_eps: 0.100000\n",
      "📈 Episodio 1156: Recompensa total (clipped): 31.000, Pasos: 656, Mean Reward Calculado: 0.047256 (Recompensa/Pasos)\n",
      " 742438/825189: episode: 1156, duration: 58.314s, episode steps: 656, steps per second:  11, episode reward: 31.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.032663, mae: 3.845458, mean_q: 4.629076, mean_eps: 0.100000\n",
      "📈 Episodio 1157: Recompensa total (clipped): 15.000, Pasos: 365, Mean Reward Calculado: 0.041096 (Recompensa/Pasos)\n",
      " 742803/825189: episode: 1157, duration: 32.370s, episode steps: 365, steps per second:  11, episode reward: 15.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.029833, mae: 3.796392, mean_q: 4.571430, mean_eps: 0.100000\n",
      "📈 Episodio 1158: Recompensa total (clipped): 10.000, Pasos: 266, Mean Reward Calculado: 0.037594 (Recompensa/Pasos)\n",
      " 743069/825189: episode: 1158, duration: 23.807s, episode steps: 266, steps per second:  11, episode reward: 10.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.033343, mae: 3.825987, mean_q: 4.603874, mean_eps: 0.100000\n",
      "📈 Episodio 1159: Recompensa total (clipped): 30.000, Pasos: 755, Mean Reward Calculado: 0.039735 (Recompensa/Pasos)\n",
      " 743824/825189: episode: 1159, duration: 66.953s, episode steps: 755, steps per second:  11, episode reward: 30.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.035782, mae: 3.779739, mean_q: 4.548201, mean_eps: 0.100000\n",
      "📈 Episodio 1160: Recompensa total (clipped): 32.000, Pasos: 786, Mean Reward Calculado: 0.040712 (Recompensa/Pasos)\n",
      " 744610/825189: episode: 1160, duration: 70.102s, episode steps: 786, steps per second:  11, episode reward: 32.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.039811, mae: 3.814463, mean_q: 4.588690, mean_eps: 0.100000\n",
      "📈 Episodio 1161: Recompensa total (clipped): 7.000, Pasos: 266, Mean Reward Calculado: 0.026316 (Recompensa/Pasos)\n",
      " 744876/825189: episode: 1161, duration: 23.862s, episode steps: 266, steps per second:  11, episode reward:  7.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.523 [0.000, 5.000],  loss: 0.035042, mae: 3.817212, mean_q: 4.596550, mean_eps: 0.100000\n",
      "📈 Episodio 1162: Recompensa total (clipped): 24.000, Pasos: 521, Mean Reward Calculado: 0.046065 (Recompensa/Pasos)\n",
      " 745397/825189: episode: 1162, duration: 45.605s, episode steps: 521, steps per second:  11, episode reward: 24.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.039109, mae: 3.840096, mean_q: 4.619003, mean_eps: 0.100000\n",
      "📈 Episodio 1163: Recompensa total (clipped): 21.000, Pasos: 554, Mean Reward Calculado: 0.037906 (Recompensa/Pasos)\n",
      " 745951/825189: episode: 1163, duration: 49.052s, episode steps: 554, steps per second:  11, episode reward: 21.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.031207, mae: 3.792890, mean_q: 4.565963, mean_eps: 0.100000\n",
      "📈 Episodio 1164: Recompensa total (clipped): 25.000, Pasos: 589, Mean Reward Calculado: 0.042445 (Recompensa/Pasos)\n",
      " 746540/825189: episode: 1164, duration: 52.417s, episode steps: 589, steps per second:  11, episode reward: 25.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.038259, mae: 3.832803, mean_q: 4.612711, mean_eps: 0.100000\n",
      "📈 Episodio 1165: Recompensa total (clipped): 21.000, Pasos: 553, Mean Reward Calculado: 0.037975 (Recompensa/Pasos)\n",
      " 747093/825189: episode: 1165, duration: 49.349s, episode steps: 553, steps per second:  11, episode reward: 21.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.031979, mae: 3.838666, mean_q: 4.617574, mean_eps: 0.100000\n",
      "📈 Episodio 1166: Recompensa total (clipped): 25.000, Pasos: 565, Mean Reward Calculado: 0.044248 (Recompensa/Pasos)\n",
      " 747658/825189: episode: 1166, duration: 49.940s, episode steps: 565, steps per second:  11, episode reward: 25.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.034481, mae: 3.829375, mean_q: 4.607710, mean_eps: 0.100000\n",
      "📈 Episodio 1167: Recompensa total (clipped): 23.000, Pasos: 573, Mean Reward Calculado: 0.040140 (Recompensa/Pasos)\n",
      " 748231/825189: episode: 1167, duration: 51.453s, episode steps: 573, steps per second:  11, episode reward: 23.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.041943, mae: 3.808790, mean_q: 4.580459, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1168: Recompensa total (clipped): 25.000, Pasos: 634, Mean Reward Calculado: 0.039432 (Recompensa/Pasos)\n",
      " 748865/825189: episode: 1168, duration: 57.326s, episode steps: 634, steps per second:  11, episode reward: 25.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.041185, mae: 3.835223, mean_q: 4.613137, mean_eps: 0.100000\n",
      "📈 Episodio 1169: Recompensa total (clipped): 33.000, Pasos: 806, Mean Reward Calculado: 0.040943 (Recompensa/Pasos)\n",
      " 749671/825189: episode: 1169, duration: 73.201s, episode steps: 806, steps per second:  11, episode reward: 33.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.029405, mae: 3.797869, mean_q: 4.569681, mean_eps: 0.100000\n",
      "📈 Episodio 1170: Recompensa total (clipped): 13.000, Pasos: 479, Mean Reward Calculado: 0.027140 (Recompensa/Pasos)\n",
      " 750150/825189: episode: 1170, duration: 43.682s, episode steps: 479, steps per second:  11, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.031940, mae: 3.821239, mean_q: 4.599513, mean_eps: 0.100000\n",
      "📈 Episodio 1171: Recompensa total (clipped): 32.000, Pasos: 686, Mean Reward Calculado: 0.046647 (Recompensa/Pasos)\n",
      " 750836/825189: episode: 1171, duration: 64.686s, episode steps: 686, steps per second:  11, episode reward: 32.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.033386, mae: 3.852724, mean_q: 4.636776, mean_eps: 0.100000\n",
      "📈 Episodio 1172: Recompensa total (clipped): 21.000, Pasos: 637, Mean Reward Calculado: 0.032967 (Recompensa/Pasos)\n",
      " 751473/825189: episode: 1172, duration: 61.982s, episode steps: 637, steps per second:  10, episode reward: 21.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.036187, mae: 3.836835, mean_q: 4.616688, mean_eps: 0.100000\n",
      "📈 Episodio 1173: Recompensa total (clipped): 34.000, Pasos: 836, Mean Reward Calculado: 0.040670 (Recompensa/Pasos)\n",
      " 752309/825189: episode: 1173, duration: 77.678s, episode steps: 836, steps per second:  11, episode reward: 34.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.033897, mae: 3.816251, mean_q: 4.593880, mean_eps: 0.100000\n",
      "📈 Episodio 1174: Recompensa total (clipped): 26.000, Pasos: 611, Mean Reward Calculado: 0.042553 (Recompensa/Pasos)\n",
      " 752920/825189: episode: 1174, duration: 55.759s, episode steps: 611, steps per second:  11, episode reward: 26.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.037791, mae: 3.858042, mean_q: 4.639059, mean_eps: 0.100000\n",
      "📈 Episodio 1175: Recompensa total (clipped): 33.000, Pasos: 903, Mean Reward Calculado: 0.036545 (Recompensa/Pasos)\n",
      " 753823/825189: episode: 1175, duration: 84.553s, episode steps: 903, steps per second:  11, episode reward: 33.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.039937, mae: 3.842159, mean_q: 4.621461, mean_eps: 0.100000\n",
      "📈 Episodio 1176: Recompensa total (clipped): 31.000, Pasos: 837, Mean Reward Calculado: 0.037037 (Recompensa/Pasos)\n",
      " 754660/825189: episode: 1176, duration: 78.365s, episode steps: 837, steps per second:  11, episode reward: 31.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.037105, mae: 3.851556, mean_q: 4.634022, mean_eps: 0.100000\n",
      "📈 Episodio 1177: Recompensa total (clipped): 31.000, Pasos: 871, Mean Reward Calculado: 0.035591 (Recompensa/Pasos)\n",
      " 755531/825189: episode: 1177, duration: 77.676s, episode steps: 871, steps per second:  11, episode reward: 31.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.032679, mae: 3.848567, mean_q: 4.632138, mean_eps: 0.100000\n",
      "📈 Episodio 1178: Recompensa total (clipped): 19.000, Pasos: 456, Mean Reward Calculado: 0.041667 (Recompensa/Pasos)\n",
      " 755987/825189: episode: 1178, duration: 40.435s, episode steps: 456, steps per second:  11, episode reward: 19.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.037720, mae: 3.836364, mean_q: 4.615805, mean_eps: 0.100000\n",
      "📈 Episodio 1179: Recompensa total (clipped): 36.000, Pasos: 912, Mean Reward Calculado: 0.039474 (Recompensa/Pasos)\n",
      " 756899/825189: episode: 1179, duration: 80.316s, episode steps: 912, steps per second:  11, episode reward: 36.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.033264, mae: 3.824216, mean_q: 4.599952, mean_eps: 0.100000\n",
      "📈 Episodio 1180: Recompensa total (clipped): 32.000, Pasos: 792, Mean Reward Calculado: 0.040404 (Recompensa/Pasos)\n",
      " 757691/825189: episode: 1180, duration: 70.253s, episode steps: 792, steps per second:  11, episode reward: 32.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.037756, mae: 3.842492, mean_q: 4.623346, mean_eps: 0.100000\n",
      "📈 Episodio 1181: Recompensa total (clipped): 24.000, Pasos: 509, Mean Reward Calculado: 0.047151 (Recompensa/Pasos)\n",
      " 758200/825189: episode: 1181, duration: 45.788s, episode steps: 509, steps per second:  11, episode reward: 24.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.032833, mae: 3.835716, mean_q: 4.618269, mean_eps: 0.100000\n",
      "📈 Episodio 1182: Recompensa total (clipped): 8.000, Pasos: 248, Mean Reward Calculado: 0.032258 (Recompensa/Pasos)\n",
      " 758448/825189: episode: 1182, duration: 22.590s, episode steps: 248, steps per second:  11, episode reward:  8.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.037142, mae: 3.854480, mean_q: 4.639936, mean_eps: 0.100000\n",
      "📈 Episodio 1183: Recompensa total (clipped): 21.000, Pasos: 488, Mean Reward Calculado: 0.043033 (Recompensa/Pasos)\n",
      " 758936/825189: episode: 1183, duration: 43.483s, episode steps: 488, steps per second:  11, episode reward: 21.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.036297, mae: 3.854379, mean_q: 4.637920, mean_eps: 0.100000\n",
      "📈 Episodio 1184: Recompensa total (clipped): 32.000, Pasos: 818, Mean Reward Calculado: 0.039120 (Recompensa/Pasos)\n",
      " 759754/825189: episode: 1184, duration: 73.082s, episode steps: 818, steps per second:  11, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.034819, mae: 3.872090, mean_q: 4.659913, mean_eps: 0.100000\n",
      "📊 Paso 760,000/2,000,000 (38.0%) - 16.2 pasos/seg - ETA: 21.2h - Memoria: 11011.61 MB\n",
      "📈 Episodio 1185: Recompensa total (clipped): 14.000, Pasos: 468, Mean Reward Calculado: 0.029915 (Recompensa/Pasos)\n",
      " 760222/825189: episode: 1185, duration: 41.460s, episode steps: 468, steps per second:  11, episode reward: 14.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.039787, mae: 3.838526, mean_q: 4.618806, mean_eps: 0.100000\n",
      "📈 Episodio 1186: Recompensa total (clipped): 26.000, Pasos: 631, Mean Reward Calculado: 0.041204 (Recompensa/Pasos)\n",
      " 760853/825189: episode: 1186, duration: 57.124s, episode steps: 631, steps per second:  11, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.037592, mae: 3.877169, mean_q: 4.666202, mean_eps: 0.100000\n",
      "📈 Episodio 1187: Recompensa total (clipped): 18.000, Pasos: 474, Mean Reward Calculado: 0.037975 (Recompensa/Pasos)\n",
      " 761327/825189: episode: 1187, duration: 41.978s, episode steps: 474, steps per second:  11, episode reward: 18.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.033827, mae: 3.842807, mean_q: 4.627093, mean_eps: 0.100000\n",
      "📈 Episodio 1188: Recompensa total (clipped): 12.000, Pasos: 273, Mean Reward Calculado: 0.043956 (Recompensa/Pasos)\n",
      " 761600/825189: episode: 1188, duration: 24.499s, episode steps: 273, steps per second:  11, episode reward: 12.000, mean reward:  0.044 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.034159, mae: 3.865155, mean_q: 4.649133, mean_eps: 0.100000\n",
      "📈 Episodio 1189: Recompensa total (clipped): 22.000, Pasos: 568, Mean Reward Calculado: 0.038732 (Recompensa/Pasos)\n",
      " 762168/825189: episode: 1189, duration: 51.055s, episode steps: 568, steps per second:  11, episode reward: 22.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.030743, mae: 3.851027, mean_q: 4.635847, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Episodio 1190: Recompensa total (clipped): 29.000, Pasos: 728, Mean Reward Calculado: 0.039835 (Recompensa/Pasos)\n",
      " 762896/825189: episode: 1190, duration: 64.563s, episode steps: 728, steps per second:  11, episode reward: 29.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.035879, mae: 3.855667, mean_q: 4.641345, mean_eps: 0.100000\n",
      "📈 Episodio 1191: Recompensa total (clipped): 18.000, Pasos: 448, Mean Reward Calculado: 0.040179 (Recompensa/Pasos)\n",
      " 763344/825189: episode: 1191, duration: 40.165s, episode steps: 448, steps per second:  11, episode reward: 18.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 3.181 [0.000, 5.000],  loss: 0.036072, mae: 3.850313, mean_q: 4.635272, mean_eps: 0.100000\n",
      "📈 Episodio 1192: Recompensa total (clipped): 26.000, Pasos: 631, Mean Reward Calculado: 0.041204 (Recompensa/Pasos)\n",
      " 763975/825189: episode: 1192, duration: 56.273s, episode steps: 631, steps per second:  11, episode reward: 26.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.033313, mae: 3.868489, mean_q: 4.659237, mean_eps: 0.100000\n",
      "📈 Episodio 1193: Recompensa total (clipped): 32.000, Pasos: 816, Mean Reward Calculado: 0.039216 (Recompensa/Pasos)\n",
      " 764791/825189: episode: 1193, duration: 72.762s, episode steps: 816, steps per second:  11, episode reward: 32.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.032574, mae: 3.846145, mean_q: 4.628817, mean_eps: 0.100000\n",
      "📈 Episodio 1194: Recompensa total (clipped): 36.000, Pasos: 942, Mean Reward Calculado: 0.038217 (Recompensa/Pasos)\n",
      " 765733/825189: episode: 1194, duration: 84.085s, episode steps: 942, steps per second:  11, episode reward: 36.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.035169, mae: 3.861841, mean_q: 4.647542, mean_eps: 0.100000\n",
      "📈 Episodio 1195: Recompensa total (clipped): 33.000, Pasos: 720, Mean Reward Calculado: 0.045833 (Recompensa/Pasos)\n",
      " 766453/825189: episode: 1195, duration: 64.059s, episode steps: 720, steps per second:  11, episode reward: 33.000, mean reward:  0.046 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.034546, mae: 3.874408, mean_q: 4.661556, mean_eps: 0.100000\n",
      "📈 Episodio 1196: Recompensa total (clipped): 31.000, Pasos: 647, Mean Reward Calculado: 0.047913 (Recompensa/Pasos)\n",
      " 767100/825189: episode: 1196, duration: 58.106s, episode steps: 647, steps per second:  11, episode reward: 31.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.034332, mae: 3.879109, mean_q: 4.666892, mean_eps: 0.100000\n",
      "📈 Episodio 1197: Recompensa total (clipped): 25.000, Pasos: 526, Mean Reward Calculado: 0.047529 (Recompensa/Pasos)\n",
      " 767626/825189: episode: 1197, duration: 47.583s, episode steps: 526, steps per second:  11, episode reward: 25.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.031841, mae: 3.835019, mean_q: 4.614279, mean_eps: 0.100000\n",
      "📈 Episodio 1198: Recompensa total (clipped): 8.000, Pasos: 313, Mean Reward Calculado: 0.025559 (Recompensa/Pasos)\n",
      " 767939/825189: episode: 1198, duration: 28.676s, episode steps: 313, steps per second:  11, episode reward:  8.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.037545, mae: 3.859243, mean_q: 4.643034, mean_eps: 0.100000\n",
      "📈 Episodio 1199: Recompensa total (clipped): 8.000, Pasos: 237, Mean Reward Calculado: 0.033755 (Recompensa/Pasos)\n",
      " 768176/825189: episode: 1199, duration: 22.216s, episode steps: 237, steps per second:  11, episode reward:  8.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.035257, mae: 3.908419, mean_q: 4.704752, mean_eps: 0.100000\n",
      "📈 Episodio 1200: Recompensa total (clipped): 41.000, Pasos: 1219, Mean Reward Calculado: 0.033634 (Recompensa/Pasos)\n",
      "💾 Guardado modelo principal best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_model.h5\n",
      "💾 Guardado modelo target best: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_target.h5\n",
      "💾 Memoria best guardada: checkpoints/DUELING_DQN_REPLAY/DUELING_DQN_REPLAY_best_memory.pkl\n",
      "💾 Checkpoint best guardado (ep: 1200, pasos: 769395)\n",
      "💾 NUEVO MEJOR PROMEDIO: 25.17 - Guardado en checkpoints/DUELING_DQN_REPLAY\n",
      "\n",
      "📊 EPISODIO 1200 - PROGRESO HACIA OBJETIVO\n",
      "   Reward actual: 41.00\n",
      "   Media últimos 100: 25.17 / 20.0\n",
      "   Mejor promedio histórico: 25.17\n",
      "   Estado: 🎯 OBJETIVO ALCANZADO!\n",
      "   Episodios en objetivo: 1110\n",
      "   Episodios consecutivos en objetivo: 1110\n",
      "🏆 ¡MODELO ESTABLE EN OBJETIVO! 1110 episodios consecutivos\n",
      " 769395/825189: episode: 1200, duration: 191.582s, episode steps: 1219, steps per second:   6, episode reward: 41.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.037486, mae: 3.880244, mean_q: 4.668490, mean_eps: 0.100000\n",
      "📈 Episodio 1201: Recompensa total (clipped): 24.000, Pasos: 592, Mean Reward Calculado: 0.040541 (Recompensa/Pasos)\n",
      " 769987/825189: episode: 1201, duration: 54.630s, episode steps: 592, steps per second:  11, episode reward: 24.000, mean reward:  0.041 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.036367, mae: 3.847899, mean_q: 4.630518, mean_eps: 0.100000\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **PARTE 4** - *Análisis del entrenamiento*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelos_a_procesar = ['DQN','DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68dzFiUS2U6L"
   },
   "outputs": [],
   "source": [
    "def parse_datos_episodio_desde_archivo(ruta_archivo, ruta_csv, to_csv=False):\n",
    "    \"\"\"\n",
    "    Analiza el contenido de un archivo dado para extraer campos de datos específicos para cada episodio,\n",
    "    estrictamente comprobando el formato de la segunda línea para evitar datos \"basura\".\n",
    "    Además, calcula la \"Recompensa acumulada media\" de forma progresiva.\n",
    "\n",
    "    Argumentos:\n",
    "        ruta_archivo (str): La ruta al archivo de texto de entrada que contiene la información del episodio.\n",
    "\n",
    "    Retorna:\n",
    "        pandas.DataFrame: Un DataFrame con las columnas especificadas para cada episodio,\n",
    "                          incluyendo la \"Recompensa acumulada media\",\n",
    "                          o None si el archivo no se puede leer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intenta abrir y leer el archivo\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "            texto = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontró el archivo '{ruta_archivo}'. Por favor, verifica la ruta.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al leer el archivo: {e}\")\n",
    "        return None\n",
    "\n",
    "    datos = [] # Lista para almacenar los diccionarios de cada episodio\n",
    "    lineas = texto.strip().split('\\n') # Divide el texto en líneas y elimina espacios en blanco al inicio/final\n",
    "    i = 0 # Índice para recorrer las líneas\n",
    "\n",
    "    while i < len(lineas):\n",
    "        linea1 = lineas[i]\n",
    "        \n",
    "        # Expresión regular para la primera línea del episodio (Ej: 📈 Episodio X: Recompensa total...)\n",
    "        # Permanece estricta ya que es el marcador de inicio de un episodio.\n",
    "        coincidencia1 = re.match(\n",
    "            r\"📈 Episodio (\\d+): Recompensa total \\(clipped\\): ([\\d.]+), Pasos: (\\d+), Mean Reward Calculado: ([\\d.]+)\",\n",
    "            linea1\n",
    "        )\n",
    "\n",
    "        if coincidencia1:\n",
    "            episodio = int(coincidencia1.group(1))\n",
    "            # Si la primera línea coincide, extrae los datos principales del episodio\n",
    "            datos_episodio = {\n",
    "                'Episodio': episodio,\n",
    "                'Recompensa total': None,\n",
    "                'Pasos': int(coincidencia1.group(3)),\n",
    "                'Mean Reward Calculado': float(coincidencia1.group(4))\n",
    "            }\n",
    "\n",
    "            # Inicializa las columnas de la segunda línea a None por si no se encuentran\n",
    "            datos_episodio['duracion (s)'] = None\n",
    "            datos_episodio['steps per second'] = None\n",
    "            datos_episodio['Recompensa Episodio'] = None\n",
    "            datos_episodio['mean action'] = None\n",
    "            datos_episodio['loss'] = None\n",
    "\n",
    "\n",
    "            segunda_linea_encontrada = False\n",
    "            j = i + 1 # Inicia la búsqueda de la segunda línea a partir de la siguiente\n",
    "\n",
    "            # Bucle interno para buscar la línea de métricas detalladas (la \"segunda línea\")\n",
    "            while j < len(lineas):\n",
    "                potencial_linea2 = lineas[j]\n",
    "                \n",
    "                # Criterio clave REVISADO: Buscamos \"episode: <numero_episodio_actual>,\" en la línea.\n",
    "                # Esto es más robusto ante variaciones de espacios o \"basura\" antes de las métricas.\n",
    "                numero_episodio_actual = coincidencia1.group(1) # Obtenemos el número de episodio de la línea 1\n",
    "                if re.search(rf\"episode:\\s*{re.escape(numero_episodio_actual)},\", potencial_linea2):\n",
    "                    \n",
    "                    # Si esta línea contiene el número de episodio al que corresponde,\n",
    "                    # es muy probable que sea nuestra línea de métricas detalladas.\n",
    "                    # Extrae las métricas individuales de forma flexible.\n",
    "                    duracion_coincidencia = re.search(r\"duration:\\s*([\\d.]+)s\", potencial_linea2)\n",
    "                    pasos_por_segundo_coincidencia = re.search(r\"steps\\s*(?:per\\s*)?second:\\s*(\\d+)\", potencial_linea2)\n",
    "                    recompensa_episodio_coincidencia = re.search(r\"episode\\s*reward:\\s*([\\d.]+)\", potencial_linea2)\n",
    "                    accion_media_coincidencia = re.search(r\"mean\\s*action:\\s*([\\d.]+)\", potencial_linea2)\n",
    "                    perdida_coincidencia = re.search(r\"loss:\\s*([-\\d.]+)\", potencial_linea2) # Maneja '--'\n",
    "\n",
    "                    datos_episodio['duracion (s)'] = float(duracion_coincidencia.group(1)) if duracion_coincidencia else 0\n",
    "                    datos_episodio['steps per second'] = int(pasos_por_segundo_coincidencia.group(1)) if pasos_por_segundo_coincidencia else None\n",
    "                    datos_episodio['Recompensa Episodio'] = float(recompensa_episodio_coincidencia.group(1)) if recompensa_episodio_coincidencia else None\n",
    "                    datos_episodio['mean action'] = float(accion_media_coincidencia.group(1)) if accion_media_coincidencia else None\n",
    "                    \n",
    "                    # Manejo especial para 'loss', que puede ser '--'\n",
    "                    if perdida_coincidencia and perdida_coincidencia.group(1) != '--':\n",
    "                        datos_episodio['loss'] = float(perdida_coincidencia.group(1))\n",
    "                    else:\n",
    "                        datos_episodio['loss'] = None\n",
    "                    \n",
    "                    segunda_linea_encontrada = True\n",
    "                    i = j # Actualiza el índice principal 'i' a la posición de esta segunda línea\n",
    "                    break # Sale del bucle interno, ya encontramos nuestra línea2\n",
    "\n",
    "                # Si encontramos el inicio de un NUEVO episodio, significa que la línea de métricas\n",
    "                # para el episodio actual no existe o no tiene el formato esperado.\n",
    "                elif re.match(r\"📈 Episodio \\d+:\", potencial_linea2):\n",
    "                    break # Salimos del bucle interno sin encontrar la segunda línea\n",
    "                \n",
    "                j += 1 # Avanza a la siguiente línea para buscar la segunda línea\n",
    "\n",
    "            datos.append(datos_episodio) # Añade los datos del episodio (completos o con Nones)\n",
    "            i += 1 # Incrementa el índice principal 'i'. El bucle principal continuará desde aquí.\n",
    "                   # Si se encontró la línea2, 'i' ya se actualizó a 'j' y este i+=1 lo mueve a j+1.\n",
    "                   # Si no se encontró la línea2, 'i' sigue en su valor original y este i+=1 lo mueve a i+1.\n",
    "        else:\n",
    "            i += 1 # Si la línea actual no es el inicio de un episodio, simplemente avanza a la siguiente línea\n",
    "                   # (esto ignora las líneas de \"basura\" que no son ni inicio de episodio ni la línea de métricas esperada).\n",
    "\n",
    "    df = pd.DataFrame(datos)    \n",
    "    # Calcular el promedio de los últimos 100 episodios (media móvil)\n",
    "    # min_periods=1 asegura que la media se calcule incluso si hay menos de 100 episodios al principio\n",
    "    df['Recompensa total'] = round(df['Recompensa Episodio'].rolling(window=100, min_periods=1).mean(), 2)  \n",
    "    \n",
    "    # --- CÁLCULO DE TIEMPO ACUMULADO ---\n",
    "    # Convertir 'duration (s)' a tipo numérico, forzando errores a NaN\n",
    "    df['duration (s)'] = pd.to_numeric(df['duracion (s)'], errors='coerce')\n",
    "    # Rellenar cualquier NaN en duration (s) con 0 para el cálculo acumulado si es apropiado\n",
    "    df['duration (s)'] = df['duration (s)'].fillna(0)\n",
    "    # Calcular el tiempo acumulado sumando las duraciones individuales\n",
    "    df['Tiempo acumulado'] = df['duration (s)'].cumsum()    \n",
    "    # --- FIN CÁLCULO DE TIEMPO ACUMULADO ---   \n",
    "    \n",
    "    # Opcional: guardar el DataFrame en un archivo CSV\n",
    "    if(to_csv):\n",
    "        df.to_csv(ruta_csv, index=False)\n",
    "    \n",
    "    return df       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar recompensas\n",
    "# Crear subplots (uno al lado del otro)\n",
    "def graficar_reward(datos, rewards):\n",
    "    # Definir colores para cada modelo\n",
    "    colores = ['blue', 'orange', 'green', 'purple', 'red', 'cyan']\n",
    "    # Parámetros\n",
    "    max_por_fila = 2  # Máximo de subplots por fila\n",
    "    num_modelos = len(rewards_ep)\n",
    "    num_filas = math.ceil(num_modelos / max_por_fila)\n",
    "    num_columnas = min(num_modelos, max_por_fila)\n",
    "    \n",
    "    # Crear subplots dinámicos\n",
    "    fig, axs = plt.subplots(num_filas, num_columnas, figsize=(8 * num_columnas, 5 * num_filas))\n",
    "    axs = np.array(axs).reshape(-1)  # Asegura que axs sea una lista 1D para iterar    \n",
    "\n",
    "    # Dibujar cada modelo por separado\n",
    "    for i, (nombre, datos_gf) in enumerate(datos.items()):\n",
    "        axs[i].plot(datos_gf, label='Recompensa/Episodio', color=colores[i % len(colores)], alpha=0.3)\n",
    "        recompensas_total = rewards[nombre]\n",
    "        axs[i].plot(recompensas_total, label='Recompensa Total', color=colores[i % len(colores)], linewidth=2)\n",
    "        axs[i].axhline(y=target_reward, color='black', linestyle='--', label='Objetivo')\n",
    "        axs[i].set_title(f'Recompensa por Episodio y Recompensa Acumulada Media {nombre}')\n",
    "        axs[i].set_xlabel('Número de Episodios')\n",
    "        axs[i].set_ylabel('Recompensa')\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Eliminar subplots vacíos si sobran\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Título general\n",
    "    fig.suptitle(\"Comparación de Recompensas por Episodio y Totales entre Modelos\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # deja espacio arriba para el suptitle\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_superpuesto(datos, titulo, etiqueta, target_reward=0):\n",
    "    # Graficar curvas\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for nombre, datos_gf in datos.items():\n",
    "        plt.plot(datos_gf, label=nombre)\n",
    "\n",
    "    if(target_reward>0):\n",
    "        plt.axhline(y=target_reward, color='black', linestyle='--', label='Objetivo')\n",
    "    plt.title(f'Comparación de {titulo} por Episodio (Evaluación)')\n",
    "    plt.xlabel('Número de Episodios')\n",
    "    plt.ylabel(f'{etiqueta}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar curvas\n",
    "# Crear subplots (uno al lado del otro)\n",
    "def graficar_repartido(datos, titulo, etiqueta, target_reward=0):\n",
    "    # Definir colores para cada modelo\n",
    "    colores = ['blue', 'orange', 'green', 'purple', 'red', 'cyan']\n",
    "    # Parámetros\n",
    "    max_por_fila = 2  # Máximo de subplots por fila\n",
    "    num_modelos = len(rewards_ep)\n",
    "    num_filas = math.ceil(num_modelos / max_por_fila)\n",
    "    num_columnas = min(num_modelos, max_por_fila)\n",
    "    \n",
    "    # Crear subplots dinámicos\n",
    "    fig, axs = plt.subplots(num_filas, num_columnas, figsize=(8 * num_columnas, 5 * num_filas))\n",
    "    axs = np.array(axs).reshape(-1)  # Asegura que axs sea una lista 1D para iterar    \n",
    "\n",
    "    # Dibujar cada modelo por separado\n",
    "    for i, (nombre, datos_gf) in enumerate(datos.items()):\n",
    "        axs[i].plot(datos_gf, label='Recompensa/Episodio', color=colores[i % len(colores)], alpha=0.3)  \n",
    "        if(target_reward>0):\n",
    "            axs[i].axhline(y=target_reward, color='black', linestyle='--', label='Objetivo')\n",
    "        axs[i].set_title(f'{titulo} por Episodio - {nombre}')\n",
    "        axs[i].set_xlabel('Número de Episodios')\n",
    "        axs[i].set_ylabel(f'{etiqueta}')\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Eliminar subplots vacíos si sobran\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Título general\n",
    "    fig.suptitle(f\"Comparación de {titulo} por Episodio y Totales entre Modelos\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # deja espacio arriba para el suptitle\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import math  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "target_reward = 20\n",
    "parse_output=False\n",
    "to_csv=True\n",
    "\n",
    "resumen_resultados = []\n",
    "rewards_ep = {}\n",
    "rewards = {}\n",
    "loss = {}\n",
    "pasos = {}\n",
    "duracion = {}\n",
    "\n",
    "for name in Modelos_a_procesar:\n",
    "    df_desde_csv = None\n",
    "    ruta_del_archivo = f'{file_Output}{name}.txt'\n",
    "    ruta_archivo_csv = f'{file_Output}{name}.csv'\n",
    "    # Asegúrate de que el fichero esté en la misma carpeta, o proporciona la ruta completa\n",
    "    # Parseo del fichero:\n",
    "    # Controlamos si es necesario parsear el output. Si ya lo hemos hecho no es necesario\n",
    "    if(parse_output):    \n",
    "        df_desde_archivo = parse_datos_episodio_desde_archivo(ruta_del_archivo, ruta_archivo_csv, to_csv=to_csv)   \n",
    "    # Opcional: guardar el DataFrame en un archivo CSV\n",
    "    if(to_csv):    \n",
    "        try:\n",
    "            # Lee el CSV. Pandas es muy bueno infiriendo el formato por defecto.\n",
    "            df_desde_csv = pd.read_csv(ruta_archivo_csv, sep=\",\") \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: No se encontró el csv '{ruta_archivo_csv}'. Por favor, verifica la ruta.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error al leer el cvs: {e}\")\n",
    "\n",
    "    if df_desde_csv is not None:    \n",
    "        rewards_df = df_desde_csv['Recompensa total']\n",
    "        mean = np.mean(rewards_df)\n",
    "        std = np.std(rewards_df)\n",
    "        max_r = np.max(rewards_df)\n",
    "        min_r = np.min(rewards_df)\n",
    "        objetivo = mean >= target_reward\n",
    "\n",
    "        resumen_resultados.append({\n",
    "            \"Modelo\": name,\n",
    "            \"Media\": mean,\n",
    "            \"STD\": std,\n",
    "            \"Máximo\": max_r,\n",
    "            \"Mínimo\": min_r,\n",
    "            \"Objetivo alcanzado\": objetivo\n",
    "        })\n",
    "\n",
    "        rewards_ep[name] = df_desde_csv['Recompensa Episodio']\n",
    "        rewards[name] = df_desde_csv['Recompensa total']\n",
    "        loss[name] = df_desde_csv['loss']\n",
    "        pasos[name] = df_desde_csv['Pasos']\n",
    "        duracion[name] = df_desde_csv['Tiempo acumulado'] / 3600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados = pd.DataFrame(resumen_resultados)\n",
    "print(\"\\n📊 COMPARATIVA DE MODELOS:\")\n",
    "print(df_resultados.to_string(index=False))\n",
    "\n",
    "graficar_superpuesto(rewards, 'Recompensas', 'Recompensa (con clipping)', target_reward=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_reward(rewards_ep, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_superpuesto(duracion, 'Tiempo acumulado', 'Horas ejecución', target_reward=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_repartido(pasos, 'Pasos realizados', 'N.Pasos', target_reward=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_repartido(loss, 'Pérdida', 'Pérdida', target_reward=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
