{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah \n",
    "* Alumno 2: Cuesta Cifuentes, Jair \n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbVRjvHCJ8UF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IN_LOCAL = True\n",
    "\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "if not IN_LOCAL:\n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "except ImportError:\n",
    "    print(\"WARNING: gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (14 cores)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# Ajuste recomendado (seguro y eficiente)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)   # dentro de cada operación\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)   # entre operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "INPUT_SHAPE = (84, 84)\n",
    "state_size = [84, 84, 3]          # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "action_size = env.action_space.n  # 6 acciones posibles\n",
    "learning_rate =  0.00025          # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST        # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST        # Máximo de pasos posibles por episodio\n",
    "total_episodios = 100          # Episodios totales para el entrenamiento\n",
    "max_steps = 3000               # Máximo de pasos posibles por episodio\n",
    "batch_size = 32                # Tamaño del lote (batch)\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.01            # Probabilidad mínima de exploración\n",
    "decay_rate = 0.00001           # Tasa de decaimiento exponencial para la probabilidad de exploración\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "gamma = 0.95                   # Tasa de descuento\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "memory_size = 5000             # Número de experiencias que la memoria puede guardar\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "WINDOW_LENGTH = 3              # Número de fotogramas apilados\n",
    "\n",
    "### CAMBIA ESTO A FALSE SI SOLO QUIERES VER AL AGENTE ENTRENADO\n",
    "training = False\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"    \n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, INPUT_SHAPE, interpolation=cv2.INTER_AREA)\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Limita las recompensas a un rango [-1, 1].\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            reward (float): Recompensa cruda del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada en el rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "    \n",
    "\n",
    "    def process_step(self, reward, terminal, metrics):\n",
    "        return self.process_reward(reward), terminal, metrics    \n",
    "    \n",
    "    def process_action(self, action):\n",
    "        return action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(22): \n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase FrameStack para apilar frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Clase para gestionar una pila de fotogramas consecutivos del entorno, utilizada para capturar\n",
    "    el contexto temporal en juegos de Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Mantiene una cola (deque) de fotogramas preprocesados con un tamaño máximo definido por\n",
    "    max_length, apilándolos para formar un estado con información de movimiento.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "        frames (deque): Cola de fotogramas preprocesados con longitud máxima max_length.\n",
    "        max_length (int): Número máximo de fotogramas a apilar (e.g., WINDOW_LENGTH).\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        append(frame): Añade un nuevo fotograma a la pila, eliminando el más antiguo si es necesario.\n",
    "        get_stacked_state(): Devuelve el estado apilado como un array NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frames=4):\n",
    "        \"\"\"\n",
    "        Inicializa la pila de fotogramas.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            max_length (int): Número máximo de fotogramas a mantener en la pila.\n",
    "        \"\"\"\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frames.clear()\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Añade un fotograma preprocesado a la pila.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            frame (np.ndarray): Fotograma preprocesado (e.g., imagen en escala de grises de 84x84).\n",
    "        \"\"\"\n",
    "        # Si es el primer frame, llenamos el deque\n",
    "        if len(self.frames) == 0:\n",
    "            for _ in range(self.num_frames):\n",
    "                self.frames.append(frame)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Devuelve el estado apilado como un array NumPy con los fotogramas actuales.\n",
    "\n",
    "        Si la pila no está llena, repite el último fotograma hasta completar max_length.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Array de forma (84, 84, max_length) con los fotogramas apilados.\n",
    "        \"\"\"\n",
    "        # Convertir a array con shape (84, 84, 4)\n",
    "        return np.stack(self.frames, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Clase para implementar un búfer de memoria de repetición.\n",
    "    Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite muestreo aleatorio.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        # print(f\"[DEBUG - ReplayMemory __init__] Memoria interna inicializada como: {type(self.memory)}\")       \n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Guarda una transición.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un lote de transiciones aleatoriamente.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el tamaño actual de la memoria.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Verifica si hay suficientes transiciones para muestrear.\"\"\"\n",
    "        return len(self.memory) >= batch_size    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proceso no usaremos DQNAgent de keras-rl2, en su lugar desarrolla clases personalizadas (DQNetwork, DDQNetwork, DDQNetworkWithReplay):\n",
    "\n",
    "* Propósito Educativo: Para aprender y controlar cada aspecto de DQN/DDQN, analizando la lógica desde cero.\n",
    "* Personalización: Para implementar variantes como DDQN y memoria de repetición personalizada, que no son directamente soportadas por DQNAgent.\n",
    "* Optimización en CPU: Para reducir el uso de memoria y optimizar el rendimiento en un entorno sin GPU.\n",
    "* Flexibilidad Experimental: Para facilitar la comparación entre DQN, DDQN, y DDQN con replay, y permitir ajustes.\n",
    "* Evitar Limitaciones de keras-rl2: Para superar restricciones, como la falta de soporte nativo para DDQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [],
   "source": [
    "class DQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación ELU.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow para distinguir múltiples redes.\n",
    "    \"\"\"   \n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        super(DQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Crear la red\n",
    "        # Renombrado de self.model a self.main_network para consistencia\n",
    "        self.main_network = self._crear_red('dqn_main') \n",
    "\n",
    "        # Construir el modelo\n",
    "        input_shape = (None,) + tuple(state_size)\n",
    "        self.main_network.build(input_shape)\n",
    "        \n",
    "        # Definir el optimizador\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        # Definir la función de pérdida (error cuadrático medio)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def _crear_red(self, network_name): \n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DQN.\n",
    "               \n",
    "        Retorna:\n",
    "        --------\n",
    "            tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            # Primera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            # Segunda capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            # Tercera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            # Aplanar la salida de la última convolucional            \n",
    "            tf.keras.layers.Flatten(name='dqn_flatten'),\n",
    "            # Capa completamente conectada con activación ELU            \n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            # Capa de salida que devuelve valores Q para cada acción            \n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name='dqn_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Ejecuta la red neuronal para un batch de estados y define la lógica para hacer la propagación hacia adelante \n",
    "        (forward pass).  Se llama automáticamente en una clase que hereda de tf.keras.Model o tf.keras.layers.Layer.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor  - Tensor con los estados de entrada, shape = (batch_size, *state_size)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q para cada acción, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs)    \n",
    "\n",
    "    @tf.function    \n",
    "    def train_step(self, states, actions, target_q):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento: calcula la pérdida y aplica gradientes.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor -    Batch de estados de entrada, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor -   Acciones tomadas, codificadas one-hot, shape = (batch_size, action_size)\n",
    "        target_q : tf.Tensor -  Valores objetivo Q, shape = (batch_size,)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor -      Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.call(states)  # Salida con valores Q para todas las acciones\n",
    "            # Obtenemos Q para las acciones tomadas multiplicando por la máscara one-hot y sumando\n",
    "            q_action = tf.reduce_sum(q_values * actions, axis=1)\n",
    "            # Si target_q es (batch_size, num_actions), entonces:\n",
    "            target_q_action = tf.reduce_sum(tf.expand_dims(target_q, axis=1) * actions, axis=1)            \n",
    "            # Calculamos la pérdida MSE entre Q predicho y target_Q\n",
    "            loss = self.loss_fn(target_q_action, q_action)\n",
    "\n",
    "        # Calculamos los gradientes y actualizamos los pesos\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN), para reducir la sobreestimación en DQN.\n",
    "    \n",
    "    DDQN usa dos redes: una principal (main) y una objetivo (target).\n",
    "    La red principal selecciona las acciones, mientras que la red objetivo\n",
    "    evalúa los valores Q, reduciendo así el sesgo de sobreestimación.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    tau : float, opcional    - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau=0.001, name='DDQNetwork'):\n",
    "        super(DDQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Red principal (main network)\n",
    "        self.main_network = self._crear_red('ddqn_main')\n",
    "        # Red objetivo (target network)\n",
    "        self.target_network = self._crear_red('ddqn_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta\n",
    "        input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        self.main_network.build(input_shape)\n",
    "        self.target_network.build(input_shape)        \n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        self.update_target_network(tau=1.0)\n",
    "        \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DDQN.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Nombre identificador de la red ('main' o 'target').\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "\n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo usando actualización suave (soft update).\n",
    "        \n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización. Si es None, usa self.tau.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)    \n",
    "        \n",
    "    @tf.function        \n",
    "    def train_step(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        En DDQN:\n",
    "        1. La red principal selecciona la mejor acción para next_states\n",
    "        2. La red objetivo evalúa el valor Q de esa acción\n",
    "        3. Se calcula el target Q usando la ecuación de Bellman\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetworkWithReplay(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN) con memoria de repetición para mejorar la estabilidad del aprendizaje.\n",
    "\n",
    "    Esta clase extiende la funcionalidad de DDQN (como en `DDQNetwork`) al incorporar una memoria de repetición\n",
    "    (`ReplayMemory`) que almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite\n",
    "    muestrear lotes aleatorios para el entrenamiento. Esto rompe la correlación temporal entre experiencias\n",
    "    consecutivas, mejorando la eficiencia y estabilidad del aprendizaje en el entorno Space Invaders.\n",
    "    - Añade un búfer de memoria (`self.memory`) para almacenar transiciones.\n",
    "    - Incluye métodos `store_transition` y `train_from_memory` para gestionar la memoria de repetición.\n",
    "    - El entrenamiento puede usar lotes muestreados de la memoria en lugar de transiciones individuales.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (e.g., [84, 84, 4] para 4 frames de 84x84).\n",
    "    action_size : int        - Número de acciones posibles en el entorno (e.g., 6 para Space Invaders).\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    memory_size : int        - Capacidad del búfer de memoria de repetición (default: 20000).\n",
    "    tau : float              - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str               - Nombre del modelo para identificación (default: 'DDQNetworkWithReplay').\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size = 5000, tau=0.001, name='DDQNetworkWithReplay'):    \n",
    "        # Inicializa la clase base tf.keras.Model con el nombre proporcionado.\n",
    "        # El nombre ayuda a identificar el modelo en logs o al guardar pesos. \n",
    "        super(DDQNetworkWithReplay, self).__init__(name=name)\n",
    "     \n",
    "        # Almacena los hiperparámetros básicos, idénticos a `DDQNetwork`.\n",
    "        # - state_size: Forma de los estados (e.g., [84, 84, 4] para una pila de 4 frames).\n",
    "        # - action_size: Número de acciones posibles (e.g., 6 para Space Invaders).\n",
    "        # - learning_rate: Tasa de aprendizaje para el optimizador (e.g., 0.00025).\n",
    "        # - memory_size : Tamaño máximo del búfer de memoria (default: 5000).        \n",
    "        # - tau: Factor para la actualización suave de la red objetivo (e.g., 0.001).        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "        # Inicializa el búfer de memoria de repetición con la capacidad especificada.\n",
    "        # Permite almacenar hasta `memory_size` transiciones (estado, acción, recompensa, siguiente estado, done).\n",
    "        # DEBUG: Imprime antes de inicializar self.memory\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Inicializando self.memory...\")\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Tipo de self.memory: {type(self.memory)}\")\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Métodos de self.memory: {[method for method in dir(self.memory) if not method.startswith('_')]}\")\n",
    "\n",
    "        \n",
    "        # Crear redes principal y objetivo, idénticas:\n",
    "        # - main_network: Selecciona acciones y se entrena activamente.\n",
    "        # - target_network: Evalúa valores Q objetivo para estabilidad.\n",
    "        self.main_network = self._crear_red('ddqn_rply_main')\n",
    "        self.target_network = self._crear_red('ddqn_rply_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta (e.g., (None, 84, 84, 4)).\n",
    "        # El `None` permite lotes de tamaño variable. Esto asegura que los pesos\n",
    "        # se inicialicen correctamente antes del entrenamiento.\n",
    "        input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        self.main_network.build(input_shape)\n",
    "        self.target_network.build(input_shape)        \n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        # Configura el optimizador Adam y la pérdida MSE, usados en `train_step`.\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        # Copia los pesos de `main_network` a `target_network` al inicio (tau=1.0 significa\n",
    "        # copia completa). Esto asegura que ambas redes comiencen idénticas.\n",
    "        self.update_target_network(tau=1.0)\n",
    "        \n",
    "     \n",
    "    @property\n",
    "    def memory(self):\n",
    "        \"\"\"Propiedad para acceder a la memoria de repetición de forma segura.\"\"\"\n",
    "        if not hasattr(self, '_replay_memory'):\n",
    "            raise AttributeError(\"La memoria de repetición no ha sido inicializada correctamente\")\n",
    "        return self._replay_memory\n",
    "    \n",
    "    @memory.setter\n",
    "    def memory(self, value):\n",
    "        \"\"\"Setter para la propiedad memory - previene sobrescritura accidental.\"\"\"\n",
    "        if not isinstance(value, ReplayMemory):\n",
    "            raise TypeError(f\"memory debe ser una instancia de ReplayMemory, no {type(value)}\")\n",
    "        self._replay_memory = value        \n",
    "        \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red neuronal convolucional para DDQN.\n",
    "        La arquitectura es estándar para juegos de Atari:\n",
    "        - 3 capas convolucionales (32, 64, 64 filtros) con kernels 8x8, 4x4, 3x3.\n",
    "        - 1 capa densa de 512 unidades.\n",
    "        - Capa de salida con `action_size` unidades (valores Q).\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Identificador para nombrar las capas ('main' o 'target').\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        # - Conv2D: Extrae características espaciales de los frames.\n",
    "        # - strides: Reducen la dimensionalidad (downsampling).\n",
    "        # - padding='same': Mantiene el tamaño espacial.\n",
    "        # - activation='elu': Mitiga problemas de gradientes (mejor que ReLU).\n",
    "        # - glorot_uniform: Inicialización estándar para redes profundas.\n",
    "        # - Flatten: Convierte la salida convolucional en un vector.\n",
    "        # - Dense(512): Combina características para aprendizaje complejo.\n",
    "        # - Dense(action_size): Salida lineal para valores Q por acción.        \n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='elu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True - se usa \n",
    "                             principalmente para entrenamiento, donde se calculan \n",
    "                             gradientes, pero se puede sobrescribir a False para inferencia\n",
    "                            (e.g., selección de acciones en `simple_train`).\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "\n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size).\n",
    "        training : bool    - Modo entrenamiento (True) o inferencia (False).\n",
    "                             (default: False - Esto asegura que la red objetivo opere \n",
    "                             en modo inferencia, proporcionando valores Q estables \n",
    "                             sin calcular gradientes.        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size).\n",
    "\n",
    "        Con `training=False` fijo (ver tu pregunta\n",
    "        anterior). Esto asegura que la red objetivo opere en modo inferencia, proporcionando\n",
    "        valores Q estables sin calcular gradientes.\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza los pesos de la red objetivo con una actualización suave:\n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización (default: self.tau).\n",
    "              Usa `tau=1.0` al inicio para copia completa, y `tau=0.001` durante el \n",
    "              entrenamiento para actualizaciones graduales, estabilizando el aprendizaje.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una transición en la memoria de repetición. Este método es exclusivo de \n",
    "        `DDQNetworkWithReplay`. Permite guardar transiciones en `self.memory` para su uso \n",
    "        posterior en `train_from_memory`.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state : np.ndarray - Estado actual.\n",
    "        action : int       - Acción tomada.\n",
    "        reward : float     - Recompensa obtenida.\n",
    "        next_state : np.ndarray - Siguiente estado.\n",
    "        done : bool        - Indica si el episodio terminó.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory_obj = self.memory  # Usa la propiedad\n",
    "            if not hasattr(memory_obj, 'push'):\n",
    "                print(f\"[ERROR] memory no tiene método 'push'. Tipo actual: {type(memory_obj)}\")\n",
    "                print(f\"[ERROR] Métodos disponibles: {[method for method in dir(memory_obj) if not method.startswith('_')]}\")\n",
    "                print(f\"[ERROR] Atributos de self: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "                raise AttributeError(f\"memory (tipo: {type(memory_obj)}) no tiene método 'push'\")\n",
    "            \n",
    "            memory_obj.push(state, action, reward, next_state, done)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error en store_transition: {e}\")\n",
    "            print(f\"[ERROR] Tipo de self: {type(self)}\")\n",
    "            print(f\"[ERROR] Atributos de self relacionados con memory: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "            raise\n",
    "    \n",
    "    @tf.function        \n",
    "    def train_step(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        Calcula la pérdida usando la ecuación de Bellman para DDQN:\n",
    "        1. La red principal selecciona la acción óptima para next_states.\n",
    "        2. La red objetivo evalúa el valor Q de esa acción.\n",
    "        3. Target Q = reward + γ * Q_target(next_state, argmax(Q_main(next_state))).\n",
    "        Usa `@tf.function` para optimizar la ejecución compilando el método en un grafo de \n",
    "        TensorFlow, mejorando el rendimiento.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss   \n",
    "\n",
    "    def train_from_memory(self, batch_size, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Entrena la red usando un lote aleatorio muestreado de la memoria de repetición. Este \n",
    "        método es exclusivo de `DDQNetworkWithReplay`:\n",
    "        - Muestrea `batch_size` transiciones de `self.memory`.\n",
    "        - Convierte las transiciones a tensores para usarlas en `train_step`.\n",
    "        - Permite entrenar con experiencias pasadas, mejorando la estabilidad.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        batch_size : int  - Tamaño del lote a muestrear (e.g., 32).\n",
    "        gamma : float     - Factor de descuento (default: 0.99).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor or None - Pérdida calculada, o None si no hay suficientes transiciones.\n",
    "        \"\"\"\n",
    "        # Verificar que hay suficientes transiciones\n",
    "        if not self.memory.can_sample(batch_size):\n",
    "            print(f\"[WARNING] No hay suficientes transiciones en memoria. Actual: {len(self.memory)}, Requerido: {batch_size}\")\n",
    "            return None\n",
    "           \n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
    "        actions = tf.stack([tf.one_hot(a, self.action_size) for a in actions])\n",
    "        #actions = tf.convert_to_tensor(np.array([tf.one_hot(a, self.action_size) for a in actions]), dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "        loss = self.train_step(states, actions, rewards, next_states, dones, gamma)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNetworkWithReplay(DDQNetworkWithReplay):\n",
    "    \"\"\"\n",
    "    Dueling Double Deep Q-Network (Dueling DDQN) con memoria de repetición para aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase extiende `DDQNetworkWithReplay` para implementar una arquitectura dueling, que separa la estimación\n",
    "    del valor del estado (`V(s)`) y la ventaja de las acciones (`A(s,a)`) en la red neuronal, mejorando la precisión\n",
    "    de los valores Q. Está diseñada para el entorno *SpaceInvaders-v0* de OpenAI Gym, integrándose con el pipeline\n",
    "    de entrenamiento (`simple_train`, `crear_modelo`, `AtariProcessor`). La memoria de repetición y el mecanismo\n",
    "    de Double DQN aseguran estabilidad y eficiencia en el aprendizaje.\n",
    "    \n",
    "    Características principales:\n",
    "    - Arquitectura dueling: Divide la red en flujos de valor y ventaja, combinados como `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "    - Double DQN: Usa la red principal para seleccionar acciones y la red objetivo para evaluarlas, reduciendo la sobreestimación.\n",
    "    - Memoria de repetición: Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y muestrea lotes aleatorios.\n",
    "    - Actualización suave de la red objetivo con factor `tau`.\n",
    "    - Optimizada para frames preprocesados de Atari (84x84 en escala de grises, apilados).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size=5000, tau=0.001, name=\"DuelingDQNWithReplay\"):\n",
    "        \"\"\"\n",
    "        Inicializa la red Dueling DDQN con memoria de repetición.\n",
    "\n",
    "        Configura los hiperparámetros, inicializa la memoria de repetición, y crea las redes principal y objetivo\n",
    "        usando la arquitectura dueling definida en `build_model`. Hereda la funcionalidad de `DDQNetworkWithReplay`\n",
    "        para la gestión de la memoria y el entrenamiento.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state_size : tuple/list - Forma de los estados (e.g., [3, 84, 84] para una pila de 3 frames).\n",
    "        action_size : int       - Número de acciones posibles (e.g., 6 para *SpaceInvaders-v0*).\n",
    "        learning_rate : float   - Tasa de aprendizaje para el optimizador Adam (e.g., 0.00025).\n",
    "        memory_size : int       - Tamaño máximo del búfer de memoria (default: 5000).\n",
    "        tau : float             - Factor para la actualización suave de la red objetivo (default: 0.001).\n",
    "        name : str              - Identificador del modelo (default: 'DuelingDQNWithReplay').\n",
    "        \"\"\"\n",
    "        # Inicializa la clase padre `DDQNetworkWithReplay` con los parámetros proporcionados.\n",
    "        # Esto configura la memoria de repetición (`ReplayMemory`), las redes principal y objetivo,\n",
    "        # el optimizador, y la función de pérdida, heredando métodos como `train_step` y `update_target_network`.\n",
    "        super().__init__(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau, name=name)\n",
    "\n",
    "    def _crear_red(self, name):\n",
    "        \"\"\"\n",
    "        Construye la red neuronal Dueling DDQN para estimar valores Q.\n",
    "\n",
    "        Define una arquitectura dueling con capas convolucionales para procesar frames de Atari, seguida de\n",
    "        dos flujos separados: uno para el valor del estado (`V(s)`) y otro para la ventaja de las acciones (`A(s,a)`).\n",
    "        Los flujos se combinan para producir los valores Q: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "        La red es compilada con el optimizador Adam y la pérdida de error cuadrático medio (MSE).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Model - Red neuronal compilada que mapea estados a valores Q.\n",
    "        \"\"\"\n",
    "        # Define la forma de entrada basada en `state_size` (e.g., [84, 84, 3] para 3 frames apilados).\n",
    "        # La entrada espera frames de 84x84 píxeles en escala de grises con `WINDOW_LENGTH` canales.\n",
    "        input_shape = tuple(self.state_size)  # Correcto: (84, 84, 3)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Capas convolucionales para extraer características espaciales de los frames.\n",
    "        # - Conv2D: Filtros de tamaño creciente (32, 64, 64) con kernels 8x8, 4x4, 3x3.\n",
    "        # - Strides: Reducen la dimensionalidad (downsampling) para eficiencia computacional.\n",
    "        # - Activación ReLU: Introduce no linealidad para modelar patrones complejos.\n",
    "        # - Padding='valid': No agrega relleno, reduciendo el tamaño de salida.\n",
    "        x = Conv2D(16, (8, 8), strides=4, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv1')(inputs)\n",
    "        x = Conv2D(32, (4, 4), strides=2, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv2')(x)\n",
    "        x = Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv3')(x)\n",
    "        x = Flatten(name=f'Dueling_{name}_flatten')(x)  # Convierte la salida convolucional en un vector 1D.\n",
    "        \n",
    "        # Flujo de valor (`V(s)`): Estima cuán bueno es estar en un estado.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(1): Salida escalar que representa el valor del estado.\n",
    "        value_fc = Dense(256, activation='ReLU')(x)\n",
    "        value = Dense(1, activation='linear', name=f'Dueling_{name}_value')(value_fc)\n",
    "        \n",
    "        # Flujo de ventaja (`A(s,a)`): Estima la ventaja relativa de cada acción.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(action_size): Salida vectorial con una ventaja por acción.\n",
    "        advantage_fc = Dense(256, activation='ReLU')(x)\n",
    "        advantage = Dense(self.action_size, activation='linear', name=f'Dueling_{name}_advantage')(advantage_fc)\n",
    "        \n",
    "        # Combinación dueling: Calcula Q-values restando la media de las ventajas.\n",
    "        # - Lambda: Calcula la media de las ventajas sobre el eje de acciones.\n",
    "        # - Lambda: Resta la media para centrar las ventajas.\n",
    "        # - Add: Combina el valor y las ventajas centradas: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a))).\n",
    "        advantage_mean = Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(advantage)\n",
    "        q_values = Add()([value, Lambda(lambda a: a - advantage_mean)(advantage)])\n",
    "        \n",
    "        # Crea el modelo Keras que mapea estados a valores Q.\n",
    "        model = Model(inputs=inputs, outputs=q_values)\n",
    "        \n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "### 2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear clases a prefijos\n",
    "class_to_prefix = {\n",
    "        DQNetwork: \"DQN\",\n",
    "        DDQNetwork: \"DDQN\",\n",
    "        DDQNetworkWithReplay: \"DDQN_Replay\",\n",
    "        DuelingDQNetworkWithReplay: \"DuelingDQN_Replay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state, processor, frame_stack):\n",
    "    \"\"\"\n",
    "    Preprocesa una observación del entorno para generar un estado apilado de fotogramas.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------  \n",
    "        observation (np.ndarray):   Observación cruda del entorno (un fotograma RGB).\n",
    "        processor (AtariProcessor): Objeto procesador que convierte la observación a escala de grises y la redimensiona.\n",
    "        frame_stack (FrameStack):   Objeto que gestiona la pila de fotogramas para mantener el contexto temporal.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        np.ndarray: Estado preprocesado, consistente en una pila de fotogramas (shape: [84, 28, WINDOW_LENGTH]).\n",
    "    \"\"\"\n",
    "    processed_frame = processor.process_observation(state)\n",
    "    frame_stack.add_frame(processed_frame)\n",
    "    state = frame_stack.get_state()\n",
    "    return processor.process_state_batch(np.expand_dims(state, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path=None, memory_path=None, memory_size=2000):\n",
    "    \"\"\"\n",
    "    Carga los pesos del modelo y, si aplica, la memoria de repetición, con soporte para buscar el último checkpoint.\n",
    "\n",
    "    Esta función carga los pesos de un modelo de red neuronal desde un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), carga la memoria desde un archivo pickle (.pkl).\n",
    "    Si `use_latest_checkpoint` es True, busca automáticamente el checkpoint y memoria más recientes en `checkpoint_dir`\n",
    "    basándose en el número de episodio en el nombre del archivo.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para verificar si el modelo soporta memoria de repetición y para determinar el prefijo de los\n",
    "            nombres de archivo de checkpoint.\n",
    "        class_to_prefix (dict): Diccionario que mapea clases de modelos a prefijos (e.g., {DQNetwork: 'DQN'}).\n",
    "        checkpoint_path: str, opcional. Directorio donde se buscan o guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Usado cuando `use_latest_checkpoint` es True o para derivar rutas si\n",
    "            `checkpoint_path`/`memory_path` no están especificados. Por defecto es \"checkpoints\".\n",
    "        memory_path: str, opcional. Ruta completa al archivo .pkl que contiene la memoria de repetición (e.g.,\n",
    "            'checkpoints/DuelingDQN_Replay_memory_ep10.pkl'). Solo aplica a modelos con replay. Si se proporciona,\n",
    "            tiene prioridad sobre la búsqueda automática. Por defecto es None.\n",
    "        memory_size: int, opcional. Tamaño máximo de la memoria de repetición (número de transiciones almacenadas).\n",
    "            Se usa para inicializar una memoria vacía si no se carga ninguna. Por defect\n",
    "\n",
    "    Returns:\n",
    "        int: El número de episodio correspondiente al checkpoint cargado (extraído del nombre del archivo, e.g., 10\n",
    "            para 'DQN_checkpoint_ep10.h5'). Retorna 0 si no se carga ningún checkpoint o si falla la carga.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la búsqueda o carga de archivos, retornando 0 en caso de fallo.\n",
    "    \"\"\"\n",
    "    if dqnet_class not in class_to_prefix:\n",
    "        print(f\"[ERROR] - Clase {dqnet_class.__name__} no soportada\")\n",
    "        return 0\n",
    "    \n",
    "    prefijo = class_to_prefix[dqnet_class]    \n",
    "    checkpoint_pattern = re.compile(rf'^{prefijo}_checkpoint_ep(\\d+)\\.h5$')\n",
    "    memory_pattern = re.compile(rf'^{prefijo}_memory_ep(\\d+)\\.pkl$')\n",
    "    latest_checkpoint = None\n",
    "    latest_memory = None    \n",
    "    latest_episode = 0\n",
    "\n",
    "    try:\n",
    "        # Asegurar que el directorio existe\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"[INFO] - Directorio {checkpoint_path} no existe, comenzando desde cero\")\n",
    "            return 0\n",
    "\n",
    "        # Buscar en el directorio\n",
    "        for file in os.listdir(checkpoint_path):\n",
    "            # Buscar checkpoints\n",
    "            checkpoint_match = checkpoint_pattern.match(file)\n",
    "            if checkpoint_match:\n",
    "                episode = int(checkpoint_match.group(1))\n",
    "                if episode > latest_episode:\n",
    "                    latest_episode = episode\n",
    "                    latest_checkpoint = os.path.join(checkpoint_path, file)\n",
    "\n",
    "        # Buscar memoria correspondiente al mismo episodio\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay] and latest_episode > 0:\n",
    "            memory_file = f\"{prefijo}_memory_ep{latest_episode}.pkl\"\n",
    "            memory_path_candidate = os.path.join(checkpoint_path, memory_file)\n",
    "            if os.path.exists(memory_path_candidate):\n",
    "                latest_memory = memory_path_candidate\n",
    "\n",
    "        if latest_checkpoint:\n",
    "            print(f\"[INFO] - Último checkpoint encontrado: {latest_checkpoint} (episodio {latest_episode})\")\n",
    "            if latest_memory:\n",
    "                print(f\"[INFO] - Última memoria encontrada: {latest_memory} (episodio {latest_episode})\")\n",
    "        else:\n",
    "            print(f\"[INFO] - No se encontraron checkpoints para {prefijo} en {checkpoint_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] - Error buscando checkpoints: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Cargar pesos desde checkpoint si se proporciona\n",
    "    if latest_checkpoint and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            \"\"\" \n",
    "            \n",
    "            import h5py\n",
    "\n",
    "            new_path = 'checkpoints\\DuelingDQN_Replay_checkpoint_ep10_renamed.h5'\n",
    "\n",
    "            with h5py.File(latest_checkpoint, 'r') as old_f:\n",
    "                with h5py.File(new_path, 'w') as new_f:\n",
    "                    for key in old_f.keys():\n",
    "                        if key == 'model': # Si el nombre es 'model'\n",
    "                          old_f.copy(key, new_f, name='model_1') # Cópialo con el nombre 'model_1'\n",
    "                        else:\n",
    "                            old_f.copy(key, new_f) # Copia los demás objetos tal cual\n",
    "                            \"\"\"\n",
    "            # Ahora intenta cargar desde new_path            \n",
    "            dqnet.main_network.load_weights(latest_checkpoint)\n",
    "            print(f\"[INFO] - Pesos cargados desde {latest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudieron cargar, desde {latest_checkpoint}, los pesos: {e}\")\n",
    "            latest_episode = 0  # Resetear episodio si falla            \n",
    "    else:\n",
    "        print(\"[INFO] - No se proporcionó checkpoint, comenzando desde cero\")\n",
    "\n",
    "    # Cargar memoria de repetición si se proporciona y es un modelo con replay\n",
    "    if latest_memory and os.path.exists(latest_memory) and dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        try:\n",
    "            with open(latest_memory, 'rb') as f:\n",
    "                dqnet.memory = pickle.load(f)\n",
    "            print(f\"[INFO] - Memoria de repetición cargada desde {latest_memory}\")\n",
    "            print(f\"[INFO] - Tamaño de la memoria cargada: {len(dqnet.memory)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudo cargar la memoria: {e}\")\n",
    "            dqnet.memory = ReplayMemory(capacity=memory_size)  # Reiniciar si falla\n",
    "    elif dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        print(\"[INFO] - No se proporcionó memoria, inicializando memoria vacía\")\n",
    "        dqnet.memory = ReplayMemory(capacity=memory_size)  # Asegurar que la memoria esté inicializada\n",
    "        \n",
    "    return latest_episode        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path=\"checkpoints\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Guarda los pesos del modelo y, si aplica, la memoria de repetición en archivos.\n",
    "\n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay. Debe tener un método `save_weights` para guardar pesos y, si usa memoria, un\n",
    "            atributo `memory`.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para determinar el prefijo del nombre de archivo y verificar si el modelo soporta memoria de repetición.\n",
    "        prefijo (str): Cadena que identifica el tipo de modelo y se usa como prefijo en los nombres de los archivos\n",
    "                       de checkpoint y memoria (e.g., 'DQN', 'DDQN', 'DDQN_Replay', 'DuelingDQN_Replay'). \n",
    "                       Se deriva del mapeo class_to_prefix basado en dqnet_class.            \n",
    "        episode: int. Número del episodio actual del entrenamiento. Se usa para nombrar los archivos de checkpoint y\n",
    "            memoria (e.g., `checkpoint_ep10.h5` para el episodio 10).\n",
    "        checkpoint_dir: str, opcional. Directorio donde se guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Se crea el directorio si no existe. Por defecto es \"checkpoints\".\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la escritura de archivos, pero no interrumpe la ejecución.\n",
    "    \"\"\"\n",
    "    # Crear directorio de checkpoints si no existe\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    memory_path_save = None\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo\n",
    "        checkpoint_path_save = os.path.join(checkpoint_path, f'{prefijo}_checkpoint_ep{episode + 1}{suffix}.h5')\n",
    "        dqnet.main_network.save_weights(checkpoint_path_save)\n",
    "        print(f\"💾 Guardado: {checkpoint_path_save}\")\n",
    "\n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "            memory_path_save = os.path.join(checkpoint_path, f'{prefijo}_memory_ep{episode + 1}{suffix}.pkl')\n",
    "            try:\n",
    "                with open(memory_path_save, 'wb') as f:\n",
    "                    pickle.dump(dqnet.memory, f)\n",
    "                print(f\"💾 Memoria guardada: {memory_path_save}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error guardando memoria: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint: {e}\")\n",
    "\n",
    "    return checkpoint_path_save, memory_path_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(env,\n",
    "                 dqnet_class, \n",
    "                 processor, \n",
    "                 class_to_prefix, \n",
    "                 epsilon_start,\n",
    "                 total_episodios,\n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, memory_size=2000, \n",
    "                 tau=0.001,\n",
    "                 start_episode=0,                  # Episodio desde el cual retomar\n",
    "                 checkpoint_path=checkpoint_path,  # Ruta al archivo de checkpoint\n",
    "                 memory_path=None                  # Ruta al archivo de memoria                 \n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN en el entorno especificado con soporte para reanudar desde un checkpoint.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        env : gym.Env -          Entorno de OpenAI Gym sobre el cual entrenar (ej: 'SpaceInvaders-v0').\n",
    "        dqnet_class : class -    Clase que implementa la red neuronal (ej: DQNetwork). Debe heredar de `tf.keras.Model`.\n",
    "        processor : objeto  -    Objeto encargado de procesar las observaciones crudas del entorno \n",
    "                                  (por ejemplo, redimensionar y convertir a escala de grises).\n",
    "        class_to_prefix : dict - Diccionario que asocia el nombre de la clase de red a un prefijo identificador para guardar pesos y datos.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de tomar una acción aleatoria, exploración).\n",
    "        total_episodios : int -  Número total de episodios a entrenar.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.        \n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).      \n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "        tf.keras.Model: Modelo entrenado.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si dqnet_class no está en class_to_prefix.\n",
    "        Exception: Captura errores durante el entrenamiento o la carga de archivos.\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Asegurar eager execution y comportamiento NumPy para TensorFlow en este loop de entrenamiento.\n",
    "    import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    np_config.enable_numpy_behavior()\n",
    "    tf.keras.backend.clear_session() # Mantener esto para asegurar un estado limpio para cada modelo\n",
    "    print(f\"[INFO] - Eager execution habilitado para {dqnet_class.__name__} (verificado: {tf.executing_eagerly()})\")    \n",
    "    \n",
    "    print(f\"[DEBUG] - Configuración:\")\n",
    "    print(f\"  - State size: {state_size}\")\n",
    "    print(f\"  - Action size: {action_size}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Gamma: {gamma}\")\n",
    "    print(f\"  - TensorFlow eager: {tf.executing_eagerly()}\")\n",
    "    \n",
    "    # Crear la red - FORZAR state_size a 4 canales para frame stacking\n",
    "    print(f\"[DEBUG] - Tamaño original del estado (state_size): {state_size}\")  # DEBUG\n",
    "\n",
    "    # Paso condicional de memory_size solo para DDQNetworkWithReplay\n",
    "    if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)       \n",
    "    elif dqnet_class in [DDQNetwork]:  # si esta clase sí necesita tau\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, tau=tau)\n",
    "    else:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate)    \n",
    "            \n",
    "    # Construir explícitamente el modelo con la forma de entrada esperada\n",
    "    # La forma de entrada al método 'call' del modelo es (batch_size, 84, 84, 4)\n",
    "    input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "    dqnet.build(input_shape)\n",
    "\n",
    "    print(\"[DEBUG] - Información de la red:\\n\")  # DEBUG\n",
    "    print(dqnet.summary())    \n",
    "    # -----------------------\n",
    "    print(f\"[DEBUG] - Red creada correctamente\\n\\n\")  # DEBUG\n",
    " \n",
    "    # Cargar pesos y memoria\n",
    "    start_episode_actual = load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path, memory_path, memory_size)     \n",
    "\n",
    "    scores = []\n",
    "    # Calcular epsilon inicial basado en el start_episode_actual de inicio\n",
    "    epsilon = max(epsilon_stop, epsilon_start * (epsilon_decay ** start_episode_actual))\n",
    "    print(f\"[INFO] - Epsilon inicial ajustado a {epsilon:.3f} para episodio {start_episode_actual}\")\n",
    "\n",
    "    # El bucle de `tqdm` debe ir desde `start_episode_actual` ===\n",
    "    # para que el contador interno de tqdm y el `episode` en el bucle se correspondan con el episodio real.\n",
    "    for episode in trange(start_episode_actual, total_episodios, desc=\"Training\"): # Iterar usando la barra de tqdm\n",
    "        observation = env.reset()\n",
    "        frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "        state = preprocess_state(observation, processor, frame_stack)       \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:     \n",
    "            # Selección epsilon-greedy\n",
    "            if np.random.random() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Método más robusto para obtener acción\n",
    "                state_input = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "                q_values = dqnet(state_input, training=False)\n",
    "                # action = tf.argmax(q_values[0]).numpy()        \n",
    "                action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "                action = tf.argmax(q_values[0]).numpy()                \n",
    "                                \n",
    "            # Ejecutar acción\n",
    "            next_observation, reward, done, _ = env.step(action)            \n",
    "            # Procesar siguiente estado       \n",
    "            next_state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            # Procesar reward (clip entre -1 y 1)\n",
    "            reward = processor.process_reward(reward)     \n",
    "\n",
    "            # Calcular Q-value Objetivo\n",
    "            next_state_tf = tf.convert_to_tensor(np.expand_dims(next_state, axis=0), dtype=tf.float32)   \n",
    "            # Preparar tensores para entrenamiento\n",
    "            states_tf = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            actions_onehot = tf.one_hot([action], action_size)\n",
    "\n",
    "            prefijo = class_to_prefix[dqnet_class]\n",
    "            if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:     \n",
    "                # Almacenar en memoria\n",
    "                dqnet.store_transition(state, action, reward, next_state, done)\n",
    "                # Entrenar desde memoria si hay suficientes experiencias\n",
    "                if len(dqnet.memory) >= batch_size and steps % 4 == 0:\n",
    "                    loss = dqnet.train_from_memory(batch_size, gamma)                                     \n",
    "            elif dqnet_class == DDQNetwork:\n",
    "                # DDQN: Usar train_step interno con lógica DDQN\n",
    "                loss = dqnet.train_step(states_tf, actions_onehot, reward, next_state_tf, done, gamma)\n",
    "                # Actualizar red objetivo cada 10 pasos (para optimizar tiempos)\n",
    "                if steps % 10 == 0:\n",
    "                    dqnet.update_target_network()           \n",
    "            else:\n",
    "                # DQN: Calcular Q-valor objetivo manualmente\n",
    "                next_q_vals = dqnet(next_state_tf, training=False)\n",
    "                max_next_q = tf.reduce_max(next_q_vals[0])\n",
    "                # max_next_q = tf.keras.backend.get_value(tf.reduce_max(next_q_vals[0], axis=1))           \n",
    "                target_q_value = reward + gamma * max_next_q * (1.0 - float(done))      \n",
    "                target_q_tensor = tf.convert_to_tensor([target_q_value], dtype=tf.float32)                \n",
    "                # Entrenamiento usando método interno\n",
    "                loss = dqnet.train_step(states_tf, actions_onehot, target_q_tensor)\n",
    "\n",
    "            # Actualizar estado\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                print(f\"  [INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                \n",
    "                break\n",
    "                \n",
    "            if steps % 100 == 0:\n",
    "                print(f\"  [INFO] - Ep {episode:3d} Step {steps:3d}: Training active\")\n",
    "                \n",
    "        # Actualizar epsilon\n",
    "        if epsilon > epsilon_stop:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        avg_score = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)\n",
    "        \n",
    "        print(f\"\\n📊 Episodio {episode + 1}/{total_episodios}\")\n",
    "        print(f\"   Score: {total_reward:.1f} | Steps: {steps}\")\n",
    "        print(f\"   Epsilon: {epsilon:.3f} | Avg Score: {avg_score:.2f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Guardar cada 10 episodios\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path)\n",
    "\n",
    "        # Al final del bucle de episodios\n",
    "        gc.collect()\n",
    "\n",
    "    env.close()    \n",
    "    print(\"\\n🎯 Entrenamiento completado!\")\n",
    "    return dqnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple(trained_model, processor, episodes=3, render=False, max_steps=1000, record_video=False, video_dir=\"videos\"):\n",
    "    \"\"\"\n",
    "    Probar la red entrenada (DQN o DDQN) en el entorno SpaceInvaders-v0.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        trained_model: tf.keras.Model - Modelo entrenado (DQNetwork o DDQNetwork).\n",
    "        processor: AtariProcessor -     Procesador para preprocesar observaciones.\n",
    "        episodes: int -                 Número de episodios de prueba.\n",
    "        max_steps: int -                Máximo de pasos por episodio.\n",
    "        render: bool -                  Si se debe renderizar el entorno.\n",
    "    Retorna:\n",
    "    --------\n",
    "        float: Promedio de recompensa en los episodios de prueba.\n",
    "    \"\"\"\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "    scores = []    \n",
    "    \n",
    "    # Imprimir información de depuración\n",
    "    model_type = trained_model.__class__.__name__\n",
    "    print(f\"[DEBUG] - Probando modelo: {model_type}\")\n",
    "    print(f\"[DEBUG] - Episodios de prueba: {episodes}, Max pasos: {max_steps}, Render: {render}\\n\")    \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        state = preprocess_state(observation, processor, frame_stack)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Sin exploración - solo usar la red\n",
    "            state_tensor = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            q_values = trained_model(state_tensor, training=False)\n",
    "            action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "            action = tf.argmax(q_values[0]).numpy()                 \n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"  [TEST-INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                      \n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        print(f\"[TEST] - Episodio de prueba {episode + 1}: Score = {total_reward}, Steps = {steps}\")\n",
    "    \n",
    "    avg_test_score = np.mean(scores)\n",
    "    print(f\"\\n[TEST] - Promedio de recompensa en {episodes} episodios de prueba: {avg_test_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return avg_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Función de utilidad para crear y comparar los modelos que se vayan creando\n",
    "def crear_modelo(model_type, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 total_episodios, \n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, \n",
    "                 epsilon_start, \n",
    "                 memory_size, \n",
    "                 tau, \n",
    "                 learning_rate=0.001,\n",
    "                 start_episode=0,\n",
    "                 checkpoint_path=checkpoint_path,\n",
    "                 memory_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Factory function para crear modelos DQN o DDQN.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_type : str -       Tipo de modelo a entrenar. Puede ser 'DQN' o 'DDQN'.\n",
    "        state_size : tuple -     Dimensiones del estado de entrada (por ejemplo, (84, 84, 4)).\n",
    "        action_size : int-       Número de acciones posibles en el entorno.\n",
    "        total_episodios : int-   Número total de episodios de entrenamiento.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de exploración).\n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        learning_rate : float -  Tasa de aprendizaje para el optimizador (default: 0.001).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        DQNetwork o DDQNetwork - Modelo creado según el tipo especificado\n",
    "    \"\"\"\n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()\n",
    "    \n",
    "    print(\"-\" * 60)  \n",
    "    if model_type.upper() == 'DQN':\n",
    "        print(\"Entrenando DQN simple para Space Invaders...\")\n",
    "        # Entrenar -------------------------------------\n",
    "        trained_dqn = simple_train(\n",
    "            env, DQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,           \n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DDQN':\n",
    "        print(\"Entrenando DDQN simple para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,            \n",
    "            tau=tau, start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )             \n",
    "    elif model_type.upper() == 'DDQN_REPLAY':\n",
    "        print(\"Entrenando DDQN con Replay Memory para Space Invaders...\")\n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DUELING_DQN_REPLAY':\n",
    "        print(\"Entrenando Dueling DQN con Replay Memory para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DuelingDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )   \n",
    "    else:\n",
    "        raise ValueError(\"model_type debe ser 'DQN', 'DDQN', 'DDQN_REPLAY' o 'DUELING_DQN_REPLAY' \")\n",
    "\n",
    "    print(\"-\" * 60)       \n",
    "    return trained_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIÓN AUXILIAR PARA PROCESAR CADA TIPO DE MODELO ---\n",
    "def _procesar_modelo_rl(\n",
    "    model_name_str: str,\n",
    "    model_class: type,\n",
    "    training_flag: bool,\n",
    "    # Parámetros generales para crear_modelo y simple_train\n",
    "    state_size: list, \n",
    "    action_size: int, \n",
    "    learning_rate: float,\n",
    "    tau: float, \n",
    "    memory_size: int, \n",
    "    checkpoint_path: str,\n",
    "    env, \n",
    "    processor, \n",
    "    epsilon_start, \n",
    "    total_episodios, \n",
    "    max_steps, \n",
    "    batch_size, \n",
    "    gamma\n",
    "):\n",
    "    \"\"\"\n",
    "    Función auxiliar que procesa la creación o carga de un modelo de RL (DQN/DDQN) \n",
    "    y ejecuta entrenamiento si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_name_str : str-        Nombre del modelo (usado para logs o checkpoints, por ejemplo: 'DQN', 'DDQN').\n",
    "        model_class : type -         Clase del modelo que hereda de `tf.keras.Model`, como `DQNetwork` o `DDQNetwork`.\n",
    "        training_flag : bool -       Si es True, se entrena el modelo desde cero o desde un checkpoint.\n",
    "                                        Si es False, se carga el modelo desde el checkpoint.\n",
    "        state_size : list -          Tamaño del estado de entrada, por ejemplo [84, 84, 4].\n",
    "        action_size : int -          Número de acciones posibles en el entorno.\n",
    "        learning_rate : float -      Tasa de aprendizaje para el optimizador.\n",
    "        tau : float -                Factor de actualización suave de pesos entre redes (target/main) usado en DDQN.\n",
    "        memory_size : int -          Tamaño de la memoria de repetición.\n",
    "        checkpoint_path : str -      Ruta para guardar o cargar los pesos del modelo entrenado (`.h5`).\n",
    "        env : gym.Env -              Entorno de Gym sobre el cual se entrena/evalúa el agente.\n",
    "        processor : objeto -         Objeto procesador que transforma la observación del entorno a la forma esperada por la red.\n",
    "        epsilon_start : float -      Valor inicial de epsilon (para exploración).\n",
    "        total_episodios : int -      Número total de episodios a entrenar.\n",
    "        max_steps : int -            Número máximo de pasos por episodio.\n",
    "        batch_size : int -           Tamaño de lote para el entrenamiento.\n",
    "        gamma : float -              Factor de descuento para los futuros Q-valores.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        model : tf.keras.Model -       Modelo ya entrenado o cargado desde checkpoint, listo para evaluación o inferencia.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Iniciando Entrenamiento y Carga para {model_name_str} ===\")  \n",
    "    \n",
    "    model_instance = None\n",
    "    if training_flag:\n",
    "        # Llama a la función crear_modelo existente para el entrenamiento.\n",
    "        # crear_modelo ya maneja los parámetros específicos de cada tipo de red.\n",
    "        model_instance = crear_modelo(\n",
    "            model_name_str, state_size, action_size, \n",
    "            total_episodios, max_steps, batch_size, gamma, epsilon_start, # Pasando los nuevos parámetros\n",
    "            memory_size=memory_size, tau=tau, learning_rate=learning_rate,\n",
    "            start_episode=0,checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    else:\n",
    "        # Si no estamos entrenando, creamos una instancia vacía para cargar pesos.\n",
    "        # Necesitamos instanciar la clase de modelo directamente.\n",
    "        if model_name_str in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)\n",
    "        elif model_name_str == 'DDQN':\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, tau=tau)\n",
    "        else: # 'DQN'\n",
    "            model_instance = model_class(state_size, action_size, learning_rate)\n",
    "        \n",
    "        # Construir el modelo si no está ya construido (necesario para cargar pesos).\n",
    "        # La forma de entrada debe incluir la dimensión del batch (None).\n",
    "        model_instance.build((None,) + tuple(state_size))\n",
    "\n",
    "    # Intentar cargar los pesos del \"mejor modelo\" guardado.\n",
    "    prefijo_modelo = class_to_prefix[model_class]\n",
    "    best_model_path = os.path.join(checkpoint_path, f'{prefijo_modelo}_best_model.h5')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            model_instance.load_weights(best_model_path)\n",
    "            print(f\"Cargado el mejor modelo {model_name_str} para pruebas desde: {best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: No se pudieron cargar los pesos del mejor modelo {model_name_str}: {e}\")\n",
    "            # Si la carga falla y no estábamos entrenando, el modelo no está listo para probar.\n",
    "            if not training_flag:\n",
    "                model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "    else:\n",
    "        if training_flag and model_instance is not None:\n",
    "            print(f\"No se encontró un mejor modelo {model_name_str} guardado. Se usará la instancia recién entrenada.\")\n",
    "        else:\n",
    "            print(f\"No se encontró el mejor modelo {model_name_str} guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de {model_name_str}.\")\n",
    "            model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "\n",
    "    if model_instance is not None:\n",
    "        print(f\"{model_name_str} listo para pruebas.\")\n",
    "    return model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Eager execution habilitado\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DQN ===\n",
      "No se encontró el mejor modelo DQN guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de DQN.\n",
      "[DQN] Modelo no disponible para pruebas.\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DDQN ===\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "No se encontró el mejor modelo DDQN guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de DDQN.\n",
      "[DDQN] Modelo no disponible para pruebas.\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DDQN_REPLAY ===\n",
      "------------------------------------------------------------\n",
      "Entrenando DDQN con Replay Memory para Space Invaders...\n",
      "[INFO] - Eager execution habilitado para DDQNetworkWithReplay (verificado: False)\n",
      "[DEBUG] - Configuración:\n",
      "  - State size: [84, 84, 3]\n",
      "  - Action size: 6\n",
      "  - Learning rate: 0.00025\n",
      "  - Gamma: 0.95\n",
      "  - TensorFlow eager: False\n",
      "[DEBUG] - Tamaño original del estado (state_size): [84, 84, 3]\n",
      "[DEBUG - DDQNetworkWithReplay __init__] Inicializando self.memory...\n",
      "[DEBUG - DDQNetworkWithReplay __init__] Tipo de self.memory: <class '__main__.ReplayMemory'>\n",
      "[DEBUG - DDQNetworkWithReplay __init__] Métodos de self.memory: ['can_sample', 'capacity', 'memory', 'push', 'sample']\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "[DEBUG] - Información de la red:\n",
      "\n",
      "Model: \"DDQNetworkWithReplay\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddqn_rply_main_network (Seq  multiple                 1013590   \n",
      " uential)                                                        \n",
      "                                                                 \n",
      " ddqn_rply_target_network (S  multiple                 1013590   \n",
      " equential)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,027,180\n",
      "Trainable params: 2,027,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[DEBUG] - Red creada correctamente\n",
      "\n",
      "\n",
      "[INFO] - No se encontraron checkpoints para DDQN_Replay en checkpoints\n",
      "[INFO] - No se proporcionó checkpoint, comenzando desde cero\n",
      "[INFO] - No se proporcionó memoria, inicializando memoria vacía\n",
      "[INFO] - Epsilon inicial ajustado a 1.000 para episodio 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                                                                                | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [INFO] - Ep   0 Step 100: Training active\n",
      "  [INFO] - Ep   0 Step 200: Training active\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":   \n",
    "    import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "    # Limpiar cualquier grafo anterior\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Enable NumPy behavior and force eager execution at the start\n",
    "    np_config.enable_numpy_behavior()\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    print(\"[INFO] - Eager execution habilitado\")\n",
    "    \n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True \n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "    episode_render = False   \n",
    "    \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', DQNetwork, False), # NO Entrenar DQN\n",
    "        ('DDQN', DDQNetwork, False), # No entrenar DDQN, solo cargar si existe\n",
    "        ('DDQN_REPLAY', DDQNetworkWithReplay, True), # No entrenar DDQN_REPLAY\n",
    "        ('DUELING_DQN_REPLAY', DuelingDQNetworkWithReplay, False) # Entrenar DuelingDQN_REPLAY\n",
    "    ]    \n",
    "\n",
    "    for model_name, model_class, training_specific_flag in modelos_a_procesar:\n",
    "        # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "        entrenarSN = training_global and training_specific_flag    \n",
    "        \n",
    "        current_model_instance = _procesar_modelo_rl(\n",
    "            model_name_str=model_name,\n",
    "            model_class=model_class,\n",
    "            training_flag=entrenarSN,\n",
    "            state_size=state_size, \n",
    "            action_size=action_size, \n",
    "            learning_rate=learning_rate,\n",
    "            tau=tau, \n",
    "            memory_size=memory_size, \n",
    "            checkpoint_path=checkpoint_path,\n",
    "            env=env, processor=processor, epsilon_start=epsilon_start, total_episodios=total_episodios,\n",
    "            max_steps=max_steps, batch_size=batch_size, gamma=gamma\n",
    "        )\n",
    "        if current_model_instance is not None:\n",
    "            trained_models[model_name] = current_model_instance\n",
    "        else:\n",
    "            print(f\"[{model_name}] Modelo no disponible para pruebas.\")\n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------ PRUEBA FINAL Y GRABACIÓN DE VIDEO --------------------------------------\n",
    "    print(\"\\n====== Resumen de Resultados y Prueba Final ======\")   \n",
    "\n",
    "    best_overall_score = -np.inf\n",
    "    best_overall_model_type = None\n",
    "    record_video = False\n",
    "    test_results = {}\n",
    "\n",
    "    for model_name, model_instance in trained_models.items():\n",
    "        print(f\"\\n=== Probando {model_name} entrenada ===\")\n",
    "        score = test_simple(\n",
    "            trained_model=model_instance, \n",
    "            processor=processor, \n",
    "            episodes=5,              # Puedes aumentar esto para una prueba más robusta\n",
    "            render=episode_render,   # No renderizar en la ventana, solo grabar\n",
    "            record_video=record_video, \n",
    "            video_dir=f'videos/{model_name}_final_game' # Directorio de video específico para cada modelo\n",
    "        )\n",
    "        test_results[model_name] = score\n",
    "        \n",
    "        # Actualizar el mejor modelo global\n",
    "        if score > best_overall_score:\n",
    "            best_overall_score = score\n",
    "            best_overall_model_type = model_name\n",
    "\n",
    "    print(\"\\n====== Resultados Promedio de Pruebas ======\")\n",
    "    for model_name, score in test_results.items():\n",
    "        print(f\"{model_name} Promedio: {score:.2f}\")\n",
    "\n",
    "    if best_overall_model_type:\n",
    "        print(f\"\\nEl MEJOR modelo general es: {best_overall_model_type} con un score promedio de {best_overall_score:.2f}\")\n",
    "        print(f\"Puedes encontrar el video de su ejecución en el directorio 'videos/{best_overall_model_type}_final_game'\")\n",
    "    else:\n",
    "        print(\"No se entrenó ningún modelo o no se pudo determinar el mejor para la prueba final.\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mghMiar08)",
   "language": "python",
   "name": "mghmiar08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
