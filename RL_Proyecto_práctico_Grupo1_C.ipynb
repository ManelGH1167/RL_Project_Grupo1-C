{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah \n",
    "* Alumno 2: Cuesta Cifuentes, Jair \n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoints', 'Configuración local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'Documents', 'Downloads', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'Menú Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'oo.ipynb', 'Pictures', 'Plantillas', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_práctico_Grupo1_C (1).ipynb', 'RL_Proyecto_práctico_Grupo1_C.ipynb', 'RL_Proyecto_práctico_Grupo1_C.py', 'RL_Proyecto_práctico_Grupo1_C_v2.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled221.ipynb', 'Videos', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UbVRjvHCJ8UF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IN_LOCAL = True\n",
    "\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "if not IN_LOCAL:\n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "except ImportError:\n",
    "    print(\"WARNING: gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (14 cores)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# Ajuste recomendado (seguro y eficiente)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)   # dentro de cada operación\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)   # entre operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: obs_type \"image\" should be replaced with the image type, one of: rgb, grayscale\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "INPUT_SHAPE = (84, 84)\n",
    "state_size = [84, 84, 3]          # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "action_size = env.action_space.n  # 6 acciones posibles\n",
    "learning_rate =  0.00025          # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST        # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST        # Máximo de pasos posibles por episodio\n",
    "total_episodios = 100          # Episodios totales para el entrenamiento\n",
    "max_steps = 3000               # Máximo de pasos posibles por episodio\n",
    "batch_size = 32                # Tamaño del lote (batch)\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.01            # Probabilidad mínima de exploración\n",
    "decay_rate = 0.00001           # Tasa de decaimiento exponencial para la probabilidad de exploración\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "gamma = 0.95                   # Tasa de descuento\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "memory_size = 5000             # Número de experiencias que la memoria puede guardar\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "WINDOW_LENGTH = 3              # Número de fotogramas apilados\n",
    "\n",
    "### CAMBIA ESTO A FALSE SI SOLO QUIERES VER AL AGENTE ENTRENADO\n",
    "training = False\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"    \n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = tf.image.rgb_to_grayscale(observation)\n",
    "        \n",
    "        # Crop the screen (remove the part below the player)\n",
    "        # [Up: Down, Left: right]\n",
    "        cropped_frame = img[18:-12,4:-12]\n",
    "        \n",
    "        img = tf.image.resize(cropped_frame, INPUT_SHAPE)\n",
    "        \n",
    "        # Remove the channel dimension (from [84, 84, 1] to [84, 84])\n",
    "        img = tf.squeeze(img, axis=-1)\n",
    "        # Convert to NumPy array\n",
    "        processed_observation = img.numpy() if tf.executing_eagerly() else tf.keras.backend.eval(img)\n",
    "        # Ensure the shape matches INPUT_SHAPE        \n",
    "\n",
    "    \n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "       \n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Limita las recompensas a un rango [-1, 1].\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            reward (float): Recompensa cruda del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada en el rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "    \n",
    "\n",
    "    def process_step(self, reward, terminal, metrics):\n",
    "        return self.process_reward(reward), terminal, metrics    \n",
    "    \n",
    "    def process_action(self, action):\n",
    "        return action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(22): \n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase FrameStack para apilar frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Clase para gestionar una pila de fotogramas consecutivos del entorno, utilizada para capturar\n",
    "    el contexto temporal en juegos de Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Mantiene una cola (deque) de fotogramas preprocesados con un tamaño máximo definido por\n",
    "    max_length, apilándolos para formar un estado con información de movimiento.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "        frames (deque): Cola de fotogramas preprocesados con longitud máxima max_length.\n",
    "        max_length (int): Número máximo de fotogramas a apilar (e.g., WINDOW_LENGTH).\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        append(frame): Añade un nuevo fotograma a la pila, eliminando el más antiguo si es necesario.\n",
    "        get_stacked_state(): Devuelve el estado apilado como un array NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frames=4):\n",
    "        \"\"\"\n",
    "        Inicializa la pila de fotogramas.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            max_length (int): Número máximo de fotogramas a mantener en la pila.\n",
    "        \"\"\"\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frames.clear()\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Añade un fotograma preprocesado a la pila.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            frame (np.ndarray): Fotograma preprocesado (e.g., imagen en escala de grises de 84x84).\n",
    "        \"\"\"\n",
    "        # Si es el primer frame, llenamos el deque\n",
    "        if len(self.frames) == 0:\n",
    "            for _ in range(self.num_frames):\n",
    "                self.frames.append(frame)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Devuelve el estado apilado como un array NumPy con los fotogramas actuales.\n",
    "\n",
    "        Si la pila no está llena, repite el último fotograma hasta completar max_length.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Array de forma (84, 84, max_length) con los fotogramas apilados.\n",
    "        \"\"\"\n",
    "        # Convertir a array con shape (84, 84, 4)\n",
    "        return np.stack(self.frames, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Clase para implementar un búfer de memoria de repetición.\n",
    "    Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite muestreo aleatorio.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        # print(f\"[DEBUG - ReplayMemory __init__] Memoria interna inicializada como: {type(self.memory)}\")       \n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Guarda una transición.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Muestra un lote de transiciones aleatoriamente.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el tamaño actual de la memoria.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Verifica si hay suficientes transiciones para muestrear.\"\"\"\n",
    "        return len(self.memory) >= batch_size    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proceso no usaremos DQNAgent de keras-rl2, en su lugar desarrolla clases personalizadas (DQNetwork, DDQNetwork, DDQNetworkWithReplay):\n",
    "\n",
    "* Propósito Educativo: Para aprender y controlar cada aspecto de DQN/DDQN, analizando la lógica desde cero.\n",
    "* Personalización: Para implementar variantes como DDQN y memoria de repetición personalizada, que no son directamente soportadas por DQNAgent.\n",
    "* Optimización en CPU: Para reducir el uso de memoria y optimizar el rendimiento en un entorno sin GPU.\n",
    "* Flexibilidad Experimental: Para facilitar la comparación entre DQN, DDQN, y DDQN con replay, y permitir ajustes.\n",
    "* Evitar Limitaciones de keras-rl2: Para superar restricciones, como la falta de soporte nativo para DDQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [],
   "source": [
    "class DQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación ELU.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow para distinguir múltiples redes.\n",
    "    \"\"\"   \n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        super(DQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Crear la red\n",
    "        # Renombrado de self.model a self.main_network para consistencia\n",
    "        self.main_network = self._crear_red('dqn_main') \n",
    "\n",
    "        # Construir el modelo\n",
    "        input_shape = (None,) + tuple(state_size)\n",
    "        self.main_network.build(input_shape)\n",
    "        \n",
    "        # Definir el optimizador\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        # Definir la función de pérdida (error cuadrático medio)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def _crear_red(self, network_name): \n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DQN.\n",
    "               \n",
    "        Retorna:\n",
    "        --------\n",
    "            tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            # Primera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            # Segunda capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            # Tercera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            # Aplanar la salida de la última convolucional            \n",
    "            tf.keras.layers.Flatten(name='dqn_flatten'),\n",
    "            # Capa completamente conectada con activación ELU            \n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            # Capa de salida que devuelve valores Q para cada acción            \n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name='dqn_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Ejecuta la red neuronal para un batch de estados y define la lógica para hacer la propagación hacia adelante \n",
    "        (forward pass).  Se llama automáticamente en una clase que hereda de tf.keras.Model o tf.keras.layers.Layer.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor  - Tensor con los estados de entrada, shape = (batch_size, *state_size)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q para cada acción, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs)    \n",
    "\n",
    "    @tf.function    \n",
    "    def train_step(self, states, actions, target_q):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento: calcula la pérdida y aplica gradientes.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor -    Batch de estados de entrada, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor -   Acciones tomadas, codificadas one-hot, shape = (batch_size, action_size)\n",
    "        target_q : tf.Tensor -  Valores objetivo Q, shape = (batch_size,)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor -      Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.call(states)  # Salida con valores Q para todas las acciones\n",
    "            # Obtenemos Q para las acciones tomadas multiplicando por la máscara one-hot y sumando\n",
    "            q_action = tf.reduce_sum(q_values * actions, axis=1)\n",
    "            # Si target_q es (batch_size, num_actions), entonces:\n",
    "            target_q_action = tf.reduce_sum(tf.expand_dims(target_q, axis=1) * actions, axis=1)            \n",
    "            # Calculamos la pérdida MSE entre Q predicho y target_Q\n",
    "            loss = self.loss_fn(target_q_action, q_action)\n",
    "\n",
    "        # Calculamos los gradientes y actualizamos los pesos\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN), para reducir la sobreestimación en DQN.\n",
    "    \n",
    "    DDQN usa dos redes: una principal (main) y una objetivo (target).\n",
    "    La red principal selecciona las acciones, mientras que la red objetivo\n",
    "    evalúa los valores Q, reduciendo así el sesgo de sobreestimación.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    tau : float, opcional    - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau=0.001, name='DDQNetwork'):\n",
    "        super(DDQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Red principal (main network)\n",
    "        self.main_network = self._crear_red('ddqn_main')\n",
    "        # Red objetivo (target network)\n",
    "        self.target_network = self._crear_red('ddqn_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta\n",
    "        input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        self.main_network.build(input_shape)\n",
    "        self.target_network.build(input_shape)        \n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)       \n",
    "        # Definir la función de pérdida (error cuadrático medio)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        self.update_target_network(tau=1.0)\n",
    "        \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DDQN.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Nombre identificador de la red ('main' o 'target').\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='he_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='he_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='he_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='he_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='he_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "\n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo usando actualización suave (soft update).\n",
    "        \n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización. Si es None, usa self.tau.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)    \n",
    "        \n",
    "  #  @tf.function        \n",
    "    def train_step2(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        En DDQN:\n",
    "        1. La red principal selecciona la mejor acción para next_states\n",
    "        2. La red objetivo evalúa el valor Q de esa acción\n",
    "        3. Se calcula el target Q usando la ecuación de Bellman\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss    \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN:\n",
    "        1. Red principal selecciona la acción.\n",
    "        2. Red objetivo evalúa el valor Q de esa acción.\n",
    "        3. Calcula el target Q y la pérdida MSE.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Q(s, a) actual de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "\n",
    "            # Acción óptima según la red principal\n",
    "            next_q_main = self.main_network(next_states, training=False)\n",
    "            next_actions = tf.argmax(next_q_main, axis=1)\n",
    "            next_actions_onehot = tf.one_hot(next_actions, self.action_size)\n",
    "\n",
    "            # Q valores del target\n",
    "            next_q_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_target * next_actions_onehot, axis=1)\n",
    "\n",
    "            # Q objetivo\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - dones)\n",
    "\n",
    "            # MSE entre Q actual y target\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetworkWithReplay(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN) con memoria de repetición para mejorar la estabilidad del aprendizaje.\n",
    "\n",
    "    Esta clase extiende la funcionalidad de DDQN (como en `DDQNetwork`) al incorporar una memoria de repetición\n",
    "    (`ReplayMemory`) que almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite\n",
    "    muestrear lotes aleatorios para el entrenamiento. Esto rompe la correlación temporal entre experiencias\n",
    "    consecutivas, mejorando la eficiencia y estabilidad del aprendizaje en el entorno Space Invaders.\n",
    "    - Añade un búfer de memoria (`self.memory`) para almacenar transiciones.\n",
    "    - Incluye métodos `store_transition` y `train_from_memory` para gestionar la memoria de repetición.\n",
    "    - El entrenamiento puede usar lotes muestreados de la memoria en lugar de transiciones individuales.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (e.g., [84, 84, 4] para 4 frames de 84x84).\n",
    "    action_size : int        - Número de acciones posibles en el entorno (e.g., 6 para Space Invaders).\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    memory_size : int        - Capacidad del búfer de memoria de repetición (default: 20000).\n",
    "    tau : float              - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str               - Nombre del modelo para identificación (default: 'DDQNetworkWithReplay').\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size = 5000, tau=0.001, name='DDQNetworkWithReplay'):    \n",
    "        # Inicializa la clase base tf.keras.Model con el nombre proporcionado.\n",
    "        # El nombre ayuda a identificar el modelo en logs o al guardar pesos. \n",
    "        super(DDQNetworkWithReplay, self).__init__(name=name)\n",
    "     \n",
    "        # Almacena los hiperparámetros básicos, idénticos a `DDQNetwork`.\n",
    "        # - state_size: Forma de los estados (e.g., [84, 84, 4] para una pila de 4 frames).\n",
    "        # - action_size: Número de acciones posibles (e.g., 6 para Space Invaders).\n",
    "        # - learning_rate: Tasa de aprendizaje para el optimizador (e.g., 0.00025).\n",
    "        # - memory_size : Tamaño máximo del búfer de memoria (default: 5000).        \n",
    "        # - tau: Factor para la actualización suave de la red objetivo (e.g., 0.001).        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "        # Inicializa el búfer de memoria de repetición con la capacidad especificada.\n",
    "        # Permite almacenar hasta `memory_size` transiciones (estado, acción, recompensa, siguiente estado, done).\n",
    "        # DEBUG: Imprime antes de inicializar self.memory\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Inicializando self.memory...\")\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Tipo de self.memory: {type(self.memory)}\")\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Métodos de self.memory: {[method for method in dir(self.memory) if not method.startswith('_')]}\")\n",
    "\n",
    "        \n",
    "        # Crear redes principal y objetivo, idénticas:\n",
    "        # - main_network: Selecciona acciones y se entrena activamente.\n",
    "        # - target_network: Evalúa valores Q objetivo para estabilidad.\n",
    "        self.main_network = self._crear_red('ddqn_rply_main')\n",
    "        self.target_network = self._crear_red('ddqn_rply_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta (e.g., (None, 84, 84, 4)).\n",
    "        # El `None` permite lotes de tamaño variable. Esto asegura que los pesos\n",
    "        # se inicialicen correctamente antes del entrenamiento.\n",
    "        input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        self.main_network.build(input_shape)\n",
    "        self.target_network.build(input_shape)        \n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        # Configura el optimizador Adam y la pérdida MSE, usados en `train_step`.\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        # Copia los pesos de `main_network` a `target_network` al inicio (tau=1.0 significa\n",
    "        # copia completa). Esto asegura que ambas redes comiencen idénticas.\n",
    "        self.update_target_network(tau=1.0)\n",
    "        \n",
    "     \n",
    "    @property\n",
    "    def memory(self):\n",
    "        \"\"\"Propiedad para acceder a la memoria de repetición de forma segura.\"\"\"\n",
    "        if not hasattr(self, '_replay_memory'):\n",
    "            raise AttributeError(\"La memoria de repetición no ha sido inicializada correctamente\")\n",
    "        return self._replay_memory\n",
    "    \n",
    "    @memory.setter\n",
    "    def memory(self, value):\n",
    "        \"\"\"Setter para la propiedad memory - previene sobrescritura accidental.\"\"\"\n",
    "        if not isinstance(value, ReplayMemory):\n",
    "            raise TypeError(f\"memory debe ser una instancia de ReplayMemory, no {type(value)}\")\n",
    "        self._replay_memory = value        \n",
    "        \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red neuronal convolucional para DDQN.\n",
    "        La arquitectura es estándar para juegos de Atari:\n",
    "        - 3 capas convolucionales (32, 64, 64 filtros) con kernels 8x8, 4x4, 3x3.\n",
    "        - 1 capa densa de 512 unidades.\n",
    "        - Capa de salida con `action_size` unidades (valores Q).\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Identificador para nombrar las capas ('main' o 'target').\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        # - Conv2D: Extrae características espaciales de los frames.\n",
    "        # - strides: Reducen la dimensionalidad (downsampling).\n",
    "        # - padding='same': Mantiene el tamaño espacial.\n",
    "        # - activation='elu': Mitiga problemas de gradientes (mejor que ReLU).\n",
    "        # - glorot_uniform: Inicialización estándar para redes profundas.\n",
    "        # - Flatten: Convierte la salida convolucional en un vector.\n",
    "        # - Dense(512): Combina características para aprendizaje complejo.\n",
    "        # - Dense(action_size): Salida lineal para valores Q por acción.        \n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='elu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True - se usa \n",
    "                             principalmente para entrenamiento, donde se calculan \n",
    "                             gradientes, pero se puede sobrescribir a False para inferencia\n",
    "                            (e.g., selección de acciones en `simple_train`).\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "\n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size).\n",
    "        training : bool    - Modo entrenamiento (True) o inferencia (False).\n",
    "                             (default: False - Esto asegura que la red objetivo opere \n",
    "                             en modo inferencia, proporcionando valores Q estables \n",
    "                             sin calcular gradientes.        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size).\n",
    "\n",
    "        Con `training=False` fijo (ver tu pregunta\n",
    "        anterior). Esto asegura que la red objetivo opere en modo inferencia, proporcionando\n",
    "        valores Q estables sin calcular gradientes.\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza los pesos de la red objetivo con una actualización suave:\n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización (default: self.tau).\n",
    "              Usa `tau=1.0` al inicio para copia completa, y `tau=0.001` durante el \n",
    "              entrenamiento para actualizaciones graduales, estabilizando el aprendizaje.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una transición en la memoria de repetición. Este método es exclusivo de \n",
    "        `DDQNetworkWithReplay`. Permite guardar transiciones en `self.memory` para su uso \n",
    "        posterior en `train_from_memory`.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state : np.ndarray - Estado actual.\n",
    "        action : int       - Acción tomada.\n",
    "        reward : float     - Recompensa obtenida.\n",
    "        next_state : np.ndarray - Siguiente estado.\n",
    "        done : bool        - Indica si el episodio terminó.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory_obj = self.memory  # Usa la propiedad\n",
    "            if not hasattr(memory_obj, 'push'):\n",
    "                print(f\"[ERROR] memory no tiene método 'push'. Tipo actual: {type(memory_obj)}\")\n",
    "                print(f\"[ERROR] Métodos disponibles: {[method for method in dir(memory_obj) if not method.startswith('_')]}\")\n",
    "                print(f\"[ERROR] Atributos de self: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "                raise AttributeError(f\"memory (tipo: {type(memory_obj)}) no tiene método 'push'\")\n",
    "            \n",
    "            memory_obj.push(state, action, reward, next_state, done)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error en store_transition: {e}\")\n",
    "            print(f\"[ERROR] Tipo de self: {type(self)}\")\n",
    "            print(f\"[ERROR] Atributos de self relacionados con memory: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "            raise\n",
    "    \n",
    "    @tf.function        \n",
    "    def train_step(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        Calcula la pérdida usando la ecuación de Bellman para DDQN:\n",
    "        1. La red principal selecciona la acción óptima para next_states.\n",
    "        2. La red objetivo evalúa el valor Q de esa acción.\n",
    "        3. Target Q = reward + γ * Q_target(next_state, argmax(Q_main(next_state))).\n",
    "        Usa `@tf.function` para optimizar la ejecución compilando el método en un grafo de \n",
    "        TensorFlow, mejorando el rendimiento.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss   \n",
    "\n",
    "    def train_from_memory(self, batch_size, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Entrena la red usando un lote aleatorio muestreado de la memoria de repetición. Este \n",
    "        método es exclusivo de `DDQNetworkWithReplay`:\n",
    "        - Muestrea `batch_size` transiciones de `self.memory`.\n",
    "        - Convierte las transiciones a tensores para usarlas en `train_step`.\n",
    "        - Permite entrenar con experiencias pasadas, mejorando la estabilidad.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        batch_size : int  - Tamaño del lote a muestrear (e.g., 32).\n",
    "        gamma : float     - Factor de descuento (default: 0.99).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor or None - Pérdida calculada, o None si no hay suficientes transiciones.\n",
    "        \"\"\"\n",
    "        # Verificar que hay suficientes transiciones\n",
    "        if not self.memory.can_sample(batch_size):\n",
    "            print(f\"[WARNING] No hay suficientes transiciones en memoria. Actual: {len(self.memory)}, Requerido: {batch_size}\")\n",
    "            return None\n",
    "           \n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
    "        actions = tf.stack([tf.one_hot(a, self.action_size) for a in actions])\n",
    "        #actions = tf.convert_to_tensor(np.array([tf.one_hot(a, self.action_size) for a in actions]), dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "        loss = self.train_step(states, actions, rewards, next_states, dones, gamma)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNetworkWithReplay(DDQNetworkWithReplay):\n",
    "    \"\"\"\n",
    "    Dueling Double Deep Q-Network (Dueling DDQN) con memoria de repetición para aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase extiende `DDQNetworkWithReplay` para implementar una arquitectura dueling, que separa la estimación\n",
    "    del valor del estado (`V(s)`) y la ventaja de las acciones (`A(s,a)`) en la red neuronal, mejorando la precisión\n",
    "    de los valores Q. Está diseñada para el entorno *SpaceInvaders-v0* de OpenAI Gym, integrándose con el pipeline\n",
    "    de entrenamiento (`simple_train`, `crear_modelo`, `AtariProcessor`). La memoria de repetición y el mecanismo\n",
    "    de Double DQN aseguran estabilidad y eficiencia en el aprendizaje.\n",
    "    \n",
    "    Características principales:\n",
    "    - Arquitectura dueling: Divide la red en flujos de valor y ventaja, combinados como `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "    - Double DQN: Usa la red principal para seleccionar acciones y la red objetivo para evaluarlas, reduciendo la sobreestimación.\n",
    "    - Memoria de repetición: Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y muestrea lotes aleatorios.\n",
    "    - Actualización suave de la red objetivo con factor `tau`.\n",
    "    - Optimizada para frames preprocesados de Atari (84x84 en escala de grises, apilados).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size=5000, tau=0.001, name=\"DuelingDQNWithReplay\"):\n",
    "        \"\"\"\n",
    "        Inicializa la red Dueling DDQN con memoria de repetición.\n",
    "\n",
    "        Configura los hiperparámetros, inicializa la memoria de repetición, y crea las redes principal y objetivo\n",
    "        usando la arquitectura dueling definida en `build_model`. Hereda la funcionalidad de `DDQNetworkWithReplay`\n",
    "        para la gestión de la memoria y el entrenamiento.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state_size : tuple/list - Forma de los estados (e.g., [3, 84, 84] para una pila de 3 frames).\n",
    "        action_size : int       - Número de acciones posibles (e.g., 6 para *SpaceInvaders-v0*).\n",
    "        learning_rate : float   - Tasa de aprendizaje para el optimizador Adam (e.g., 0.00025).\n",
    "        memory_size : int       - Tamaño máximo del búfer de memoria (default: 5000).\n",
    "        tau : float             - Factor para la actualización suave de la red objetivo (default: 0.001).\n",
    "        name : str              - Identificador del modelo (default: 'DuelingDQNWithReplay').\n",
    "        \"\"\"\n",
    "        # Inicializa la clase padre `DDQNetworkWithReplay` con los parámetros proporcionados.\n",
    "        # Esto configura la memoria de repetición (`ReplayMemory`), las redes principal y objetivo,\n",
    "        # el optimizador, y la función de pérdida, heredando métodos como `train_step` y `update_target_network`.\n",
    "        super().__init__(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau, name=name)\n",
    "\n",
    "    def _crear_red(self, name):\n",
    "        \"\"\"\n",
    "        Construye la red neuronal Dueling DDQN para estimar valores Q.\n",
    "\n",
    "        Define una arquitectura dueling con capas convolucionales para procesar frames de Atari, seguida de\n",
    "        dos flujos separados: uno para el valor del estado (`V(s)`) y otro para la ventaja de las acciones (`A(s,a)`).\n",
    "        Los flujos se combinan para producir los valores Q: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "        La red es compilada con el optimizador Adam y la pérdida de error cuadrático medio (MSE).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Model - Red neuronal compilada que mapea estados a valores Q.\n",
    "        \"\"\"\n",
    "        # Define la forma de entrada basada en `state_size` (e.g., [84, 84, 3] para 3 frames apilados).\n",
    "        # La entrada espera frames de 84x84 píxeles en escala de grises con `WINDOW_LENGTH` canales.\n",
    "        input_shape = tuple(self.state_size)  # Correcto: (84, 84, 3)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Capas convolucionales para extraer características espaciales de los frames.\n",
    "        # - Conv2D: Filtros de tamaño creciente (32, 64, 64) con kernels 8x8, 4x4, 3x3.\n",
    "        # - Strides: Reducen la dimensionalidad (downsampling) para eficiencia computacional.\n",
    "        # - Activación ReLU: Introduce no linealidad para modelar patrones complejos.\n",
    "        # - Padding='valid': No agrega relleno, reduciendo el tamaño de salida.\n",
    "        x = Conv2D(16, (8, 8), strides=4, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv1')(inputs)\n",
    "        x = Conv2D(32, (4, 4), strides=2, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv2')(x)\n",
    "        x = Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv3')(x)\n",
    "        x = Flatten(name=f'Dueling_{name}_flatten')(x)  # Convierte la salida convolucional en un vector 1D.\n",
    "        \n",
    "        # Flujo de valor (`V(s)`): Estima cuán bueno es estar en un estado.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(1): Salida escalar que representa el valor del estado.\n",
    "        value_fc = Dense(256, activation='ReLU')(x)\n",
    "        value = Dense(1, activation='linear', name=f'Dueling_{name}_value')(value_fc)\n",
    "        \n",
    "        # Flujo de ventaja (`A(s,a)`): Estima la ventaja relativa de cada acción.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(action_size): Salida vectorial con una ventaja por acción.\n",
    "        advantage_fc = Dense(256, activation='ReLU')(x)\n",
    "        advantage = Dense(self.action_size, activation='linear', name=f'Dueling_{name}_advantage')(advantage_fc)\n",
    "        \n",
    "        # Combinación dueling: Calcula Q-values restando la media de las ventajas.\n",
    "        # - Lambda: Calcula la media de las ventajas sobre el eje de acciones.\n",
    "        # - Lambda: Resta la media para centrar las ventajas.\n",
    "        # - Add: Combina el valor y las ventajas centradas: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a))).\n",
    "        advantage_mean = Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(advantage)\n",
    "        q_values = Add()([value, Lambda(lambda a: a - advantage_mean)(advantage)])\n",
    "        \n",
    "        # Crea el modelo Keras que mapea estados a valores Q.\n",
    "        model = Model(inputs=inputs, outputs=q_values)\n",
    "        \n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "### 2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear clases a prefijos\n",
    "class_to_prefix = {\n",
    "        DQNetwork: \"DQN\",\n",
    "        DDQNetwork: \"DDQN\",\n",
    "        DDQNetworkWithReplay: \"DDQN_Replay\",\n",
    "        DuelingDQNetworkWithReplay: \"DuelingDQN_Replay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state, processor, frame_stack):\n",
    "    \"\"\"\n",
    "    Preprocesa una observación del entorno para generar un estado apilado de fotogramas.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------  \n",
    "        observation (np.ndarray):   Observación cruda del entorno (un fotograma RGB).\n",
    "        processor (AtariProcessor): Objeto procesador que convierte la observación a escala de grises y la redimensiona.\n",
    "        frame_stack (FrameStack):   Objeto que gestiona la pila de fotogramas para mantener el contexto temporal.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        np.ndarray: Estado preprocesado, consistente en una pila de fotogramas (shape: [84, 28, WINDOW_LENGTH]).\n",
    "    \"\"\"\n",
    "    processed_frame = processor.process_observation(state)\n",
    "    frame_stack.add_frame(processed_frame)\n",
    "    state = frame_stack.get_state()\n",
    "    return processor.process_state_batch(np.expand_dims(state, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path=None, memory_path=None, memory_size=2000):\n",
    "    \"\"\"\n",
    "    Carga los pesos del modelo y, si aplica, la memoria de repetición, con soporte para buscar el último checkpoint.\n",
    "\n",
    "    Esta función carga los pesos de un modelo de red neuronal desde un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), carga la memoria desde un archivo pickle (.pkl).\n",
    "    Si `use_latest_checkpoint` es True, busca automáticamente el checkpoint y memoria más recientes en `checkpoint_dir`\n",
    "    basándose en el número de episodio en el nombre del archivo.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para verificar si el modelo soporta memoria de repetición y para determinar el prefijo de los\n",
    "            nombres de archivo de checkpoint.\n",
    "        class_to_prefix (dict): Diccionario que mapea clases de modelos a prefijos (e.g., {DQNetwork: 'DQN'}).\n",
    "        checkpoint_path: str, opcional. Directorio donde se buscan o guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Usado cuando `use_latest_checkpoint` es True o para derivar rutas si\n",
    "            `checkpoint_path`/`memory_path` no están especificados. Por defecto es \"checkpoints\".\n",
    "        memory_path: str, opcional. Ruta completa al archivo .pkl que contiene la memoria de repetición (e.g.,\n",
    "            'checkpoints/DuelingDQN_Replay_memory_ep10.pkl'). Solo aplica a modelos con replay. Si se proporciona,\n",
    "            tiene prioridad sobre la búsqueda automática. Por defecto es None.\n",
    "        memory_size: int, opcional. Tamaño máximo de la memoria de repetición (número de transiciones almacenadas).\n",
    "            Se usa para inicializar una memoria vacía si no se carga ninguna. Por defect\n",
    "\n",
    "    Returns:\n",
    "        int: El número de episodio correspondiente al checkpoint cargado (extraído del nombre del archivo, e.g., 10\n",
    "            para 'DQN_checkpoint_ep10.h5'). Retorna 0 si no se carga ningún checkpoint o si falla la carga.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la búsqueda o carga de archivos, retornando 0 en caso de fallo.\n",
    "    \"\"\"\n",
    "    if dqnet_class not in class_to_prefix:\n",
    "        print(f\"[ERROR] - Clase {dqnet_class.__name__} no soportada\")\n",
    "        return 0\n",
    "    \n",
    "    prefijo = class_to_prefix[dqnet_class]    \n",
    "    checkpoint_pattern = re.compile(rf'^{prefijo}_checkpoint_ep(\\d+)\\.h5$')\n",
    "    memory_pattern = re.compile(rf'^{prefijo}_memory_ep(\\d+)\\.pkl$')\n",
    "    latest_checkpoint = None\n",
    "    latest_memory = None    \n",
    "    latest_episode = 0\n",
    "\n",
    "    try:\n",
    "        # Asegurar que el directorio existe\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"[INFO] - Directorio {checkpoint_path} no existe, comenzando desde cero\")\n",
    "            return 0\n",
    "\n",
    "        # Buscar en el directorio\n",
    "        for file in os.listdir(checkpoint_path):\n",
    "            # Buscar checkpoints\n",
    "            checkpoint_match = checkpoint_pattern.match(file)\n",
    "            if checkpoint_match:\n",
    "                episode = int(checkpoint_match.group(1))\n",
    "                if episode > latest_episode:\n",
    "                    latest_episode = episode\n",
    "                    latest_checkpoint = os.path.join(checkpoint_path, file)\n",
    "\n",
    "        # Buscar memoria correspondiente al mismo episodio\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay] and latest_episode > 0:\n",
    "            memory_file = f\"{prefijo}_memory_ep{latest_episode}.pkl\"\n",
    "            memory_path_candidate = os.path.join(checkpoint_path, memory_file)\n",
    "            if os.path.exists(memory_path_candidate):\n",
    "                latest_memory = memory_path_candidate\n",
    "\n",
    "        if latest_checkpoint:\n",
    "            print(f\"[INFO] - Último checkpoint encontrado: {latest_checkpoint} (episodio {latest_episode})\")\n",
    "            if latest_memory:\n",
    "                print(f\"[INFO] - Última memoria encontrada: {latest_memory} (episodio {latest_episode})\")\n",
    "        else:\n",
    "            print(f\"[INFO] - No se encontraron checkpoints para {prefijo} en {checkpoint_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] - Error buscando checkpoints: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Cargar pesos desde checkpoint si se proporciona\n",
    "    if latest_checkpoint and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            \"\"\" \n",
    "            \n",
    "            import h5py\n",
    "\n",
    "            new_path = 'checkpoints\\DuelingDQN_Replay_checkpoint_ep10_renamed.h5'\n",
    "\n",
    "            with h5py.File(latest_checkpoint, 'r') as old_f:\n",
    "                with h5py.File(new_path, 'w') as new_f:\n",
    "                    for key in old_f.keys():\n",
    "                        if key == 'model': # Si el nombre es 'model'\n",
    "                          old_f.copy(key, new_f, name='model_1') # Cópialo con el nombre 'model_1'\n",
    "                        else:\n",
    "                            old_f.copy(key, new_f) # Copia los demás objetos tal cual\n",
    "                            \"\"\"\n",
    "            # Ahora intenta cargar desde new_path            \n",
    "            dqnet.main_network.load_weights(latest_checkpoint)\n",
    "            print(f\"[INFO] - Pesos cargados desde {latest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudieron cargar, desde {latest_checkpoint}, los pesos: {e}\")\n",
    "            latest_episode = 0  # Resetear episodio si falla            \n",
    "    else:\n",
    "        print(\"[INFO] - No se proporcionó checkpoint, comenzando desde cero\")\n",
    "\n",
    "    # Cargar memoria de repetición si se proporciona y es un modelo con replay\n",
    "    if latest_memory and os.path.exists(latest_memory) and dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        try:\n",
    "            with open(latest_memory, 'rb') as f:\n",
    "                dqnet.memory = pickle.load(f)\n",
    "            print(f\"[INFO] - Memoria de repetición cargada desde {latest_memory}\")\n",
    "            print(f\"[INFO] - Tamaño de la memoria cargada: {len(dqnet.memory)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudo cargar la memoria: {e}\")\n",
    "            dqnet.memory = ReplayMemory(capacity=memory_size)  # Reiniciar si falla\n",
    "    elif dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        print(\"[INFO] - No se proporcionó memoria, inicializando memoria vacía\")\n",
    "        dqnet.memory = ReplayMemory(capacity=memory_size)  # Asegurar que la memoria esté inicializada\n",
    "        \n",
    "    return latest_episode        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path=\"checkpoints\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Guarda los pesos del modelo y, si aplica, la memoria de repetición en archivos.\n",
    "\n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay. Debe tener un método `save_weights` para guardar pesos y, si usa memoria, un\n",
    "            atributo `memory`.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para determinar el prefijo del nombre de archivo y verificar si el modelo soporta memoria de repetición.\n",
    "        prefijo (str): Cadena que identifica el tipo de modelo y se usa como prefijo en los nombres de los archivos\n",
    "                       de checkpoint y memoria (e.g., 'DQN', 'DDQN', 'DDQN_Replay', 'DuelingDQN_Replay'). \n",
    "                       Se deriva del mapeo class_to_prefix basado en dqnet_class.            \n",
    "        episode: int. Número del episodio actual del entrenamiento. Se usa para nombrar los archivos de checkpoint y\n",
    "            memoria (e.g., `checkpoint_ep10.h5` para el episodio 10).\n",
    "        checkpoint_dir: str, opcional. Directorio donde se guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Se crea el directorio si no existe. Por defecto es \"checkpoints\".\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la escritura de archivos, pero no interrumpe la ejecución.\n",
    "    \"\"\"\n",
    "    # Crear directorio de checkpoints si no existe\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    memory_path_save = None\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo\n",
    "        checkpoint_path_save = os.path.join(checkpoint_path, f'{prefijo}_checkpoint_ep{episode + 1}{suffix}.h5')\n",
    "        dqnet.main_network.save_weights(checkpoint_path_save)\n",
    "        print(f\"💾 Guardado: {checkpoint_path_save}\")\n",
    "\n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "            memory_path_save = os.path.join(checkpoint_path, f'{prefijo}_memory_ep{episode + 1}{suffix}.pkl')\n",
    "            try:\n",
    "                with open(memory_path_save, 'wb') as f:\n",
    "                    pickle.dump(dqnet.memory, f)\n",
    "                print(f\"💾 Memoria guardada: {memory_path_save}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error guardando memoria: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint: {e}\")\n",
    "\n",
    "    return checkpoint_path_save, memory_path_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(env,\n",
    "                 dqnet_class,\n",
    "                 processor,\n",
    "                 class_to_prefix,\n",
    "                 epsilon_start,\n",
    "                 total_episodios,\n",
    "                 max_steps,\n",
    "                 batch_size,\n",
    "                 gamma,\n",
    "                 tau=0.001,\n",
    "                 start_episode=0,\n",
    "                 checkpoint_path=\"checkpoints\",\n",
    "                 memory_path=None):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "\n",
    "    # Crear red\n",
    "    dqnet = dqnet_class(state_size, action_size, learning_rate, tau=tau)\n",
    "    dqnet.build(input_shape=(None, *state_size))\n",
    "\n",
    "    # Cargar pesos previos si existen\n",
    "    latest_ep = load_checkpoint_memory(\n",
    "        dqnet, dqnet_class, class_to_prefix, checkpoint_path=checkpoint_path,\n",
    "        memory_path=memory_path\n",
    "    )\n",
    "    if latest_ep > start_episode:\n",
    "        start_episode = latest_ep\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    decay = 0.995\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "\n",
    "    for ep in trange(start_episode, total_episodios, desc=\"Training\"):\n",
    "        obs = env.reset()\n",
    "        frame_stack.reset()\n",
    "        state = preprocess_state(obs, processor, frame_stack)\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "        steps = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Expandir para batch (1, 84, 84, 3)\n",
    "            state_batch = tf.expand_dims(state, axis=0)\n",
    "\n",
    "            # Epsilon-greedy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(action_size)\n",
    "            else:\n",
    "                q_vals = dqnet(state_batch, training=False)\n",
    "                action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_obs, processor, frame_stack)\n",
    "\n",
    "            # Preparar batch de entrenamiento\n",
    "            states_tf = tf.convert_to_tensor(state_batch, dtype=tf.float32)\n",
    "            next_state_tf = tf.convert_to_tensor(tf.expand_dims(next_state, 0), dtype=tf.float32)\n",
    "            actions_onehot = tf.one_hot([action], action_size, dtype=tf.float32)\n",
    "            rewards_tf = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            dones_tf = tf.convert_to_tensor([float(done)], dtype=tf.float32)\n",
    "\n",
    "            # Entrenamiento\n",
    "            loss = dqnet.train_step(states_tf, actions_onehot, rewards_tf, next_state_tf, dones_tf, gamma)\n",
    "            losses.append(tf.keras.backend.get_value(loss))\n",
    "\n",
    "            dqnet.update_target_network()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # Log optimizado con detección de degradación\n",
    "            if steps % 50 == 0:\n",
    "                print(f\"  Ep {ep:3d} Step {steps:3d}: Avg step times\")\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()                 \n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "\n",
    "        print(f\"🎯 Ep {ep} | Recompensa: {total_reward:.1f} | Loss: {np.mean(losses):.4f} | Eps: {epsilon:.3f}\")\n",
    "\n",
    "        # Guardar cada 10 episodios\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            prefix = class_to_prefix[dqnet_class]\n",
    "            save_checkpoint_memory(dqnet, dqnet_class, prefix, ep, checkpoint_path=checkpoint_path)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return dqnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train2(env,\n",
    "                 dqnet_class, \n",
    "                 processor, \n",
    "                 class_to_prefix, \n",
    "                 epsilon_start,\n",
    "                 total_episodios,\n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, memory_size=2000, \n",
    "                 tau=0.001,\n",
    "                 start_episode=0,                  # Episodio desde el cual retomar\n",
    "                 checkpoint_path=checkpoint_path,  # Ruta al archivo de checkpoint\n",
    "                 memory_path=None                  # Ruta al archivo de memoria                 \n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN en el entorno especificado con soporte para reanudar desde un checkpoint.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        env : gym.Env -          Entorno de OpenAI Gym sobre el cual entrenar (ej: 'SpaceInvaders-v0').\n",
    "        dqnet_class : class -    Clase que implementa la red neuronal (ej: DQNetwork). Debe heredar de `tf.keras.Model`.\n",
    "        processor : objeto  -    Objeto encargado de procesar las observaciones crudas del entorno \n",
    "                                  (por ejemplo, redimensionar y convertir a escala de grises).\n",
    "        class_to_prefix : dict - Diccionario que asocia el nombre de la clase de red a un prefijo identificador para guardar pesos y datos.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de tomar una acción aleatoria, exploración).\n",
    "        total_episodios : int -  Número total de episodios a entrenar.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.        \n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).      \n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "        tf.keras.Model: Modelo entrenado.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si dqnet_class no está en class_to_prefix.\n",
    "        Exception: Captura errores durante el entrenamiento o la carga de archivos.\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Asegurar eager execution y comportamiento NumPy para TensorFlow en este loop de entrenamiento.\n",
    "    import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "    # tf.config.run_functions_eagerly(True)\n",
    "    np_config.enable_numpy_behavior()\n",
    "    tf.keras.backend.clear_session() # Mantener esto para asegurar un estado limpio para cada modelo   \n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    tf.config.optimizer.set_jit(True)       # XLA compilation    \n",
    "    print(\"[INFO] - Eager execution deshabilitado\")\n",
    "    print(f\"[INFO] Configuración optimizada:\")\n",
    "    print(f\"  - Eager execution: {tf.executing_eagerly()}\")\n",
    "    print(f\"  - XLA habilitado: {tf.config.optimizer.get_jit()}\")\n",
    "    print(f\"  - Estado de memoria limpio\")    \n",
    "\n",
    "    # Limpiar cualquier grafo anterior\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()    \n",
    "    print(f\"[INFO] - Eager execution habilitado para {dqnet_class.__name__} (verificado: {tf.executing_eagerly()})\")    \n",
    "    \n",
    "    print(f\"[DEBUG] - Configuración optimizada:\")\n",
    "    print(f\"  - Eager execution: {tf.executing_eagerly()}\")\n",
    "    print(f\"  - XLA habilitado: {tf.config.optimizer.get_jit()}\")\n",
    "    print(f\"  - Estado de memoria limpio\")  \n",
    "    print(f\"  - State size: {state_size}\")\n",
    "    print(f\"  - Action size: {action_size}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Gamma: {gamma}\")\n",
    "    print(f\"  - TensorFlow eager: {tf.executing_eagerly()}\")\n",
    "\n",
    "    # Paso condicional de memory_size solo para DDQNetworkWithReplay\n",
    "    if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)       \n",
    "    elif dqnet_class in [DDQNetwork]:  # si esta clase sí necesita tau\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, tau=tau)\n",
    "    else:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate)    \n",
    "            \n",
    "    # Construir explícitamente el modelo con la forma de entrada esperada\n",
    "    # La forma de entrada al método 'call' del modelo es (batch_size, 84, 84, 4)\n",
    "    input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "    dqnet.build(input_shape)\n",
    "\n",
    "    print(\"[DEBUG] - Información de la red:\\n\")  # DEBUG\n",
    "    print(dqnet.summary())    \n",
    "    # -----------------------\n",
    "    print(f\"[DEBUG] - Red creada correctamente\\n\\n\")  # DEBUG\n",
    " \n",
    "    # Cargar pesos y memoria\n",
    "    start_episode_actual = load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path, memory_path, memory_size)     \n",
    "\n",
    "    scores = []\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)    \n",
    "    # Calcular epsilon inicial basado en el start_episode_actual de inicio\n",
    "    epsilon = max(epsilon_stop, epsilon_start * (epsilon_decay ** start_episode_actual))\n",
    "    print(f\"[INFO] - Epsilon inicial ajustado a {epsilon:.3f} para episodio {start_episode_actual}\")\n",
    "\n",
    "    # CONFIGURACIÓN ANTI-MEMORY LEAK\n",
    "    TARGET_UPDATE_FREQUENCY = 50    # Menos frecuente para evitar acumulación\n",
    "    MEMORY_CLEANUP_FREQUENCY = 100  # Limpieza de memoria cada 100 steps    \n",
    "    \n",
    "    # El bucle de `tqdm` debe ir desde `start_episode_actual` ===\n",
    "    # para que el contador interno de tqdm y el `episode` en el bucle se correspondan con el episodio real.\n",
    "    for episode in trange(start_episode_actual, total_episodios, desc=\"Training\"): # Iterar usando la barra de tqdm\n",
    "        observation = env.reset()\n",
    "        frame_stack.reset()\n",
    "        state = preprocess_state(observation, processor, frame_stack)       \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # Monitoreo de tiempo para debug\n",
    "        episode_start_time = time.time()\n",
    "        step_times = []            \n",
    "        \n",
    "        while steps < max_steps:     \n",
    "            step_start_time = time.time()\n",
    "            \n",
    "            # Selección epsilon-greedy\n",
    "            if np.random.random() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Método más robusto para obtener acción\n",
    "                state_input = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "                q_values = dqnet(state_input)  ######### , training=False)\n",
    "                # action = tf.argmax(q_values[0]).numpy()        \n",
    "                action = tf.keras.backend.get_value(tf.argmax(q_values[0]))                \n",
    "                # CRÍTICO: Limpiar referencia del tensor\n",
    "                del state_input, q_values \n",
    "                                \n",
    "            # Ejecutar acción\n",
    "            next_observation, reward, done, _ = env.step(action)            \n",
    "            # Procesar siguiente estado       \n",
    "            next_state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            # Procesar reward (clip entre -1 y 1)\n",
    "            reward = processor.process_reward(reward)     \n",
    "\n",
    "            # Calcular Q-value Objetivo\n",
    "            next_state_tf = tf.convert_to_tensor(np.expand_dims(next_state, axis=0), dtype=tf.float32)   \n",
    "            # Preparar tensores para entrenamiento\n",
    "            states_tf = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            actions_onehot = tf.one_hot([action], action_size)\n",
    "            loss = None\n",
    "            \n",
    "            actions = tf.convert_to_tensor([action], dtype=tf.float32)\n",
    "            reward_tf = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            done_tf = tf.convert_to_tensor([done], dtype=tf.bool)            \n",
    "            gamma_tf = tf.constant([gamma], dtype=tf.float32) # Or the appropriate dtype for gamma\n",
    "                     \n",
    "            \n",
    "            if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:     \n",
    "                # Almacenar en memoria\n",
    "                dqnet.store_transition(state, action, reward, next_state, done)\n",
    "                # Entrenar desde memoria si hay suficientes experiencias\n",
    "                if len(dqnet.memory) >= batch_size and steps % 4 == 0:\n",
    "                    loss = dqnet.train_from_memory(batch_size, gamma)                                     \n",
    "            elif dqnet_class == DDQNetwork:\n",
    "          #      start = time.time()                \n",
    "                # DDQN: Usar train_step interno con lógica DDQN\n",
    "                loss = dqnet.train_step(states_tf, actions_onehot, reward_tf, next_state_tf, done_tf, gamma_tf)\n",
    "          #      print(f\"⏱ Entrenamiento minibatch: {time.time() - start:.3f} s\")  \n",
    "                # Actualizar red objetivo cada 10 pasos (para optimizar tiempos)\n",
    "                if steps % 50 == 0:\n",
    "                    dqnet.update_target_network()           \n",
    "            else:\n",
    "                # DQN: Calcular Q-valor objetivo manualmente\n",
    "                next_q_vals = dqnet(next_state_tf, training=False)\n",
    "                max_next_q = tf.reduce_max(next_q_vals[0])\n",
    "                # max_next_q = tf.keras.backend.get_value(tf.reduce_max(next_q_vals[0], axis=1))           \n",
    "                target_q_value = reward + gamma * max_next_q * (1.0 - float(done))      \n",
    "                target_q_tensor = tf.convert_to_tensor([target_q_value], dtype=tf.float32)                \n",
    "                # Entrenamiento usando método interno\n",
    "                loss = dqnet.train_step(states_tf, actions_onehot, target_q_tensor)\n",
    "                del target_q_tensor, next_q_vals\n",
    "\n",
    "            # CRÍTICO: Limpiar tensores inmediatamente después del uso\n",
    "            if loss is not None: # Solo borrar 'loss' si fue asignada\n",
    "                del loss\n",
    "            del states_tf, actions_onehot, next_state_tf             \n",
    "                \n",
    "            # Actualizar estado\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Monitoreo de tiempo\n",
    "            step_time = time.time() - step_start_time\n",
    "            step_times.append(step_time)            \n",
    "            \n",
    "            if done:\n",
    "                print(f\"  [INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                \n",
    "                break\n",
    "                \n",
    "            # Log optimizado con detección de degradación\n",
    "            if steps % 50 == 0:\n",
    "                avg_step_time = np.mean(step_times[-50:])\n",
    "                print(f\"  Ep {episode:3d} Step {steps:3d}: Avg step time: {avg_step_time:.3f}s\")\n",
    "                if avg_step_time > 0.5:  # Umbral más bajo\n",
    "                    print(f\"  ⚠️ Limpieza de memoria activada ({avg_step_time:.3f}s/step)\")\n",
    "                    gc.collect()\n",
    "                    #tf.keras.backend.clear_session()             \n",
    "                \n",
    "        # Actualizar epsilon\n",
    "        if epsilon > epsilon_stop:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        avg_score = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)\n",
    "        \n",
    "        # Estadísticas de rendimiento\n",
    "        episode_time = time.time() - episode_start_time\n",
    "        avg_step_time = episode_time / steps if steps > 0 else 0             \n",
    "        \n",
    "        print(f\"\\n📊 Episodio {episode + 1}/{total_episodios}\")\n",
    "        print(f\"   Score: {total_reward:.1f} | Steps: {steps}\")\n",
    "        print(f\"   Tiempo total: {episode_time:.1f}s | Tiempo/step: {avg_step_time:.3f}s\")        \n",
    "        print(f\"   Epsilon: {epsilon:.3f} | Avg Score: {avg_score:.2f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Guardar cada 10 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            prefijo = class_to_prefix[dqnet_class]\n",
    "            save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path)\n",
    "\n",
    "        # Al final del bucle de episodios\n",
    "        gc.collect()\n",
    "\n",
    "        # Detectar y alertar sobre degradación severa\n",
    "        if avg_step_time > 2.0:\n",
    "            print(f\"🚨 DEGRADACIÓN SEVERA DETECTADA: {avg_step_time:.3f}s por step\")\n",
    "            print(\"   Ejecutando limpieza completa...\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()  \n",
    "\n",
    "    env.close()    \n",
    "    print(\"\\n🎯 Entrenamiento completado!\")\n",
    "    return dqnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple(trained_model, processor, episodes=3, render=False, max_steps=1000, record_video=False, video_dir=\"videos\"):\n",
    "    \"\"\"\n",
    "    Probar la red entrenada (DQN o DDQN) en el entorno SpaceInvaders-v0.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        trained_model: tf.keras.Model - Modelo entrenado (DQNetwork o DDQNetwork).\n",
    "        processor: AtariProcessor -     Procesador para preprocesar observaciones.\n",
    "        episodes: int -                 Número de episodios de prueba.\n",
    "        max_steps: int -                Máximo de pasos por episodio.\n",
    "        render: bool -                  Si se debe renderizar el entorno.\n",
    "    Retorna:\n",
    "    --------\n",
    "        float: Promedio de recompensa en los episodios de prueba.\n",
    "    \"\"\"\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "    scores = []    \n",
    "    \n",
    "    # Imprimir información de depuración\n",
    "    model_type = trained_model.__class__.__name__\n",
    "    print(f\"[DEBUG] - Probando modelo: {model_type}\")\n",
    "    print(f\"[DEBUG] - Episodios de prueba: {episodes}, Max pasos: {max_steps}, Render: {render}\\n\")    \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        state = preprocess_state(observation, processor, frame_stack)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Sin exploración - solo usar la red\n",
    "            state_tensor = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            q_values = trained_model(state_tensor, training=False)\n",
    "            action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "            # action = tf.argmax(q_values[0]).numpy()                 \n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"  [TEST-INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                      \n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        print(f\"[TEST] - Episodio de prueba {episode + 1}: Score = {total_reward}, Steps = {steps}\")\n",
    "    \n",
    "    avg_test_score = np.mean(scores)\n",
    "    print(f\"\\n[TEST] - Promedio de recompensa en {episodes} episodios de prueba: {avg_test_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return avg_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Función de utilidad para crear y comparar los modelos que se vayan creando\n",
    "def crear_modelo(model_type, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 total_episodios, \n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, \n",
    "                 epsilon_start, \n",
    "                 memory_size, \n",
    "                 tau, \n",
    "                 learning_rate=0.001,\n",
    "                 start_episode=0,\n",
    "                 checkpoint_path=checkpoint_path,\n",
    "                 memory_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Factory function para crear modelos DQN o DDQN.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_type : str -       Tipo de modelo a entrenar. Puede ser 'DQN' o 'DDQN'.\n",
    "        state_size : tuple -     Dimensiones del estado de entrada (por ejemplo, (84, 84, 4)).\n",
    "        action_size : int-       Número de acciones posibles en el entorno.\n",
    "        total_episodios : int-   Número total de episodios de entrenamiento.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de exploración).\n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        learning_rate : float -  Tasa de aprendizaje para el optimizador (default: 0.001).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        DQNetwork o DDQNetwork - Modelo creado según el tipo especificado\n",
    "    \"\"\"\n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()\n",
    "    \n",
    "    print(\"-\" * 60)  \n",
    "    if model_type.upper() == 'DQN':\n",
    "        print(\"Entrenando DQN simple para Space Invaders...\")\n",
    "        # Entrenar -------------------------------------\n",
    "        trained_dqn = simple_train(\n",
    "            env, DQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,           \n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DDQN':\n",
    "        print(\"Entrenando DDQN simple para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,            \n",
    "            tau=tau, start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )             \n",
    "    elif model_type.upper() == 'DDQN_REPLAY':\n",
    "        print(\"Entrenando DDQN con Replay Memory para Space Invaders...\")\n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DUELING_DQN_REPLAY':\n",
    "        print(\"Entrenando Dueling DQN con Replay Memory para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DuelingDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )   \n",
    "    else:\n",
    "        raise ValueError(\"model_type debe ser 'DQN', 'DDQN', 'DDQN_REPLAY' o 'DUELING_DQN_REPLAY' \")\n",
    "\n",
    "    print(\"-\" * 60)       \n",
    "    return trained_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIÓN AUXILIAR PARA PROCESAR CADA TIPO DE MODELO ---\n",
    "def _procesar_modelo_rl(\n",
    "    model_name_str: str,\n",
    "    model_class: type,\n",
    "    training_flag: bool,\n",
    "    # Parámetros generales para crear_modelo y simple_train\n",
    "    state_size: list, \n",
    "    action_size: int, \n",
    "    learning_rate: float,\n",
    "    tau: float, \n",
    "    memory_size: int, \n",
    "    checkpoint_path: str,\n",
    "    env, \n",
    "    processor, \n",
    "    epsilon_start, \n",
    "    total_episodios, \n",
    "    max_steps, \n",
    "    batch_size, \n",
    "    gamma\n",
    "):\n",
    "    \"\"\"\n",
    "    Función auxiliar que procesa la creación o carga de un modelo de RL (DQN/DDQN) \n",
    "    y ejecuta entrenamiento si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_name_str : str-        Nombre del modelo (usado para logs o checkpoints, por ejemplo: 'DQN', 'DDQN').\n",
    "        model_class : type -         Clase del modelo que hereda de `tf.keras.Model`, como `DQNetwork` o `DDQNetwork`.\n",
    "        training_flag : bool -       Si es True, se entrena el modelo desde cero o desde un checkpoint.\n",
    "                                        Si es False, se carga el modelo desde el checkpoint.\n",
    "        state_size : list -          Tamaño del estado de entrada, por ejemplo [84, 84, 4].\n",
    "        action_size : int -          Número de acciones posibles en el entorno.\n",
    "        learning_rate : float -      Tasa de aprendizaje para el optimizador.\n",
    "        tau : float -                Factor de actualización suave de pesos entre redes (target/main) usado en DDQN.\n",
    "        memory_size : int -          Tamaño de la memoria de repetición.\n",
    "        checkpoint_path : str -      Ruta para guardar o cargar los pesos del modelo entrenado (`.h5`).\n",
    "        env : gym.Env -              Entorno de Gym sobre el cual se entrena/evalúa el agente.\n",
    "        processor : objeto -         Objeto procesador que transforma la observación del entorno a la forma esperada por la red.\n",
    "        epsilon_start : float -      Valor inicial de epsilon (para exploración).\n",
    "        total_episodios : int -      Número total de episodios a entrenar.\n",
    "        max_steps : int -            Número máximo de pasos por episodio.\n",
    "        batch_size : int -           Tamaño de lote para el entrenamiento.\n",
    "        gamma : float -              Factor de descuento para los futuros Q-valores.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        model : tf.keras.Model -       Modelo ya entrenado o cargado desde checkpoint, listo para evaluación o inferencia.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Iniciando Entrenamiento y Carga para {model_name_str} ===\")  \n",
    "    \n",
    "    model_instance = None\n",
    "    if training_flag:\n",
    "        # Llama a la función crear_modelo existente para el entrenamiento.\n",
    "        # crear_modelo ya maneja los parámetros específicos de cada tipo de red.\n",
    "        model_instance = crear_modelo(\n",
    "            model_name_str, state_size, action_size, \n",
    "            total_episodios, max_steps, batch_size, gamma, epsilon_start, # Pasando los nuevos parámetros\n",
    "            memory_size=memory_size, tau=tau, learning_rate=learning_rate,\n",
    "            start_episode=0,checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    else:\n",
    "        tf.keras.backend.clear_session()  # Limpiar grafo antes de construir        \n",
    "        # Si no estamos entrenando, creamos una instancia vacía para cargar pesos.\n",
    "        # Necesitamos instanciar la clase de modelo directamente.\n",
    "        if model_name_str in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)\n",
    "        elif model_name_str == 'DDQN':\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, tau=tau)\n",
    "        else: # 'DQN'\n",
    "            model_instance = model_class(state_size, action_size, learning_rate)\n",
    "        \n",
    "        # Construir el modelo si no está ya construido (necesario para cargar pesos).\n",
    "        # La forma de entrada debe incluir la dimensión del batch (None).\n",
    "        model_instance.build((None,) + tuple(state_size))\n",
    "\n",
    "    # Intentar cargar los pesos del \"mejor modelo\" guardado.\n",
    "    prefijo_modelo = class_to_prefix[model_class]\n",
    "    best_model_path = os.path.join(checkpoint_path, f'{prefijo_modelo}_best_model.h5')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            model_instance.load_weights(best_model_path)\n",
    "            print(f\"Cargado el mejor modelo {model_name_str} para pruebas desde: {best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: No se pudieron cargar los pesos del mejor modelo {model_name_str}: {e}\")\n",
    "            # Si la carga falla y no estábamos entrenando, el modelo no está listo para probar.\n",
    "            if not training_flag:\n",
    "                model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "    else:\n",
    "        if training_flag and model_instance is not None:\n",
    "            print(f\"No se encontró un mejor modelo {model_name_str} guardado. Se usará la instancia recién entrenada.\")\n",
    "        else:\n",
    "            print(f\"No se encontró el mejor modelo {model_name_str} guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de {model_name_str}.\")\n",
    "            model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "\n",
    "    if model_instance is not None:\n",
    "        print(f\"{model_name_str} listo para pruebas.\")\n",
    "    return model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Eager execution habilitado\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DQN ===\n",
      "No se encontró el mejor modelo DQN guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de DQN.\n",
      "[DQN] Modelo no disponible para pruebas.\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DDQN ===\n",
      "------------------------------------------------------------\n",
      "Entrenando DDQN simple para Space Invaders...\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "[INFO] - No se encontraron checkpoints para DDQN en checkpoints\n",
      "[INFO] - No se proporcionó checkpoint, comenzando desde cero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                                                                                | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep   0 Step  50: Avg step times\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/100 [00:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"ddqn_main_conv1/kernel/Read/ReadVariableOp:0\", shape=(8, 8, 3, 16), dtype=float32) must be from the same graph as Tensor(\"ExpandDims:0\", shape=(1, 84, 84, 3), dtype=float32) (graphs are <tensorflow.python.framework.ops.Graph object at 0x000001C57ED8B610> and <tensorflow.python.framework.ops.Graph object at 0x000001C57EB7A8B0>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_class, training_specific_flag \u001b[38;5;129;01min\u001b[39;00m modelos_a_procesar:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# La bandera de entrenamiento final es la global AND la específica del modelo\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     entrenarSN \u001b[38;5;241m=\u001b[39m training_global \u001b[38;5;129;01mand\u001b[39;00m training_specific_flag    \n\u001b[1;32m---> 32\u001b[0m     current_model_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_procesar_modelo_rl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentrenarSN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_episodios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_model_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         trained_models[model_name] \u001b[38;5;241m=\u001b[39m current_model_instance\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m_procesar_modelo_rl\u001b[1;34m(model_name_str, model_class, training_flag, state_size, action_size, learning_rate, tau, memory_size, checkpoint_path, env, processor, epsilon_start, total_episodios, max_steps, batch_size, gamma)\u001b[0m\n\u001b[0;32m     51\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_flag:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Llama a la función crear_modelo existente para el entrenamiento.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# crear_modelo ya maneja los parámetros específicos de cada tipo de red.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcrear_modelo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_episodios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pasando los nuevos parámetros\u001b[39;49;00m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()  \u001b[38;5;66;03m# Limpiar grafo antes de construir        \u001b[39;00m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mcrear_modelo\u001b[1;34m(model_type, state_size, action_size, total_episodios, max_steps, batch_size, gamma, epsilon_start, memory_size, tau, learning_rate, start_episode, checkpoint_path, memory_path)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDQN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando DDQN simple para Space Invaders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[1;32m---> 55\u001b[0m     trained_dqn \u001b[38;5;241m=\u001b[39m \u001b[43msimple_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDDQNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m             \n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDQN_REPLAY\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando DDQN con Replay Memory para Space Invaders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36msimple_train\u001b[1;34m(env, dqnet_class, processor, class_to_prefix, epsilon_start, total_episodios, max_steps, batch_size, gamma, tau, start_episode, checkpoint_path, memory_path)\u001b[0m\n\u001b[0;32m     65\u001b[0m dones_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([\u001b[38;5;28mfloat\u001b[39m(done)], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdqnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mget_value(loss))\n\u001b[0;32m     71\u001b[0m dqnet\u001b[38;5;241m.\u001b[39mupdate_target_network()\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mDDQNetwork.train_step\u001b[1;34m(self, states, actions, rewards, next_states, dones, gamma)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03mRealiza un paso de entrenamiento DDQN:\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m1. Red principal selecciona la acción.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m2. Red objetivo evalúa el valor Q de esa acción.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m3. Calcula el target Q y la pérdida MSE.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Q(s, a) actual de la red principal\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_network(states, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m     current_q_action \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(current_q_values \u001b[38;5;241m*\u001b[39m actions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# Acción óptima según la red principal\u001b[39;00m\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:765\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    764\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m--> 765\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(cast_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOperatorNotAllowedInGraphError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are attempting to use Python control \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    769\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow in a layer that was not declared to be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    770\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic. Pass `dynamic=True` to the class \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    771\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstructor.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEncountered error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    772\u001b[0m                   \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\sequential.py:388\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m argspec:\n\u001b[0;32m    386\u001b[0m   kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 388\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    391\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:765\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    764\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m--> 765\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(cast_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOperatorNotAllowedInGraphError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are attempting to use Python control \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    769\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow in a layer that was not declared to be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    770\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic. Pass `dynamic=True` to the class \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    771\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstructor.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEncountered error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    772\u001b[0m                   \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\layers\\convolutional.py:248\u001b[0m, in \u001b[0;36mConv.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_causal:  \u001b[38;5;66;03m# Apply causal padding to inputs for Conv1D.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mpad(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_causal_padding(inputs))\n\u001b[1;32m--> 248\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    251\u001b[0m   output_rank \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\layers\\convolutional.py:233\u001b[0m, in \u001b[0;36mConv.convolution_op\u001b[1;34m(self, inputs, kernel)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m   tf_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tf_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"ddqn_main_conv1/kernel/Read/ReadVariableOp:0\", shape=(8, 8, 3, 16), dtype=float32) must be from the same graph as Tensor(\"ExpandDims:0\", shape=(1, 84, 84, 3), dtype=float32) (graphs are <tensorflow.python.framework.ops.Graph object at 0x000001C57ED8B610> and <tensorflow.python.framework.ops.Graph object at 0x000001C57EB7A8B0>)."
     ]
    }
   ],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":   \n",
    "    import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "    # Limpiar cualquier grafo anterior\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Enable NumPy behavior and force eager execution at the start\n",
    "    np_config.enable_numpy_behavior()\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    print(\"[INFO] - Eager execution habilitado\")\n",
    "    \n",
    "    # Control global de si se entrena o solo se carga\n",
    "    training_global = True \n",
    "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "    episode_render = False   \n",
    "    \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    \n",
    "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "    trained_models = {}\n",
    "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "    modelos_a_procesar = [\n",
    "        ('DQN', DQNetwork, False), # NO Entrenar DQN\n",
    "        ('DDQN', DDQNetwork, True), # No entrenar DDQN, solo cargar si existe\n",
    "        ('DDQN_REPLAY', DDQNetworkWithReplay, False), # No entrenar DDQN_REPLAY\n",
    "        ('DUELING_DQN_REPLAY', DuelingDQNetworkWithReplay, False) # Entrenar DuelingDQN_REPLAY\n",
    "    ]    \n",
    "\n",
    "    for model_name, model_class, training_specific_flag in modelos_a_procesar:\n",
    "        # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "        entrenarSN = training_global and training_specific_flag    \n",
    "        \n",
    "        current_model_instance = _procesar_modelo_rl(\n",
    "            model_name_str=model_name,\n",
    "            model_class=model_class,\n",
    "            training_flag=entrenarSN,\n",
    "            state_size=state_size, \n",
    "            action_size=action_size, \n",
    "            learning_rate=learning_rate,\n",
    "            tau=tau, \n",
    "            memory_size=memory_size, \n",
    "            checkpoint_path=checkpoint_path,\n",
    "            env=env, processor=processor, epsilon_start=epsilon_start, total_episodios=total_episodios,\n",
    "            max_steps=max_steps, batch_size=batch_size, gamma=gamma\n",
    "        )\n",
    "        if current_model_instance is not None:\n",
    "            trained_models[model_name] = current_model_instance\n",
    "        else:\n",
    "            print(f\"[{model_name}] Modelo no disponible para pruebas.\")\n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------ PRUEBA FINAL Y GRABACIÓN DE VIDEO --------------------------------------\n",
    "    print(\"\\n====== Resumen de Resultados y Prueba Final ======\")   \n",
    "\n",
    "    best_overall_score = -np.inf\n",
    "    best_overall_model_type = None\n",
    "    record_video = False\n",
    "    test_results = {}\n",
    "\n",
    "    for model_name, model_instance in trained_models.items():\n",
    "        print(f\"\\n=== Probando {model_name} entrenada ===\")\n",
    "        score = test_simple(\n",
    "            trained_model=model_instance, \n",
    "            processor=processor, \n",
    "            episodes=5,              # Puedes aumentar esto para una prueba más robusta\n",
    "            render=episode_render,   # No renderizar en la ventana, solo grabar\n",
    "            record_video=record_video, \n",
    "            video_dir=f'videos/{model_name}_final_game' # Directorio de video específico para cada modelo\n",
    "        )\n",
    "        test_results[model_name] = score\n",
    "        \n",
    "        # Actualizar el mejor modelo global\n",
    "        if score > best_overall_score:\n",
    "            best_overall_score = score\n",
    "            best_overall_model_type = model_name\n",
    "\n",
    "    print(\"\\n====== Resultados Promedio de Pruebas ======\")\n",
    "    for model_name, score in test_results.items():\n",
    "        print(f\"{model_name} Promedio: {score:.2f}\")\n",
    "\n",
    "    if best_overall_model_type:\n",
    "        print(f\"\\nEl MEJOR modelo general es: {best_overall_model_type} con un score promedio de {best_overall_score:.2f}\")\n",
    "        print(f\"Puedes encontrar el video de su ejecución en el directorio 'videos/{best_overall_model_type}_final_game'\")\n",
    "    else:\n",
    "        print(\"No se entrenó ningún modelo o no se pudo determinar el mejor para la prueba final.\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mghMiar08)",
   "language": "python",
   "name": "mghmiar08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
