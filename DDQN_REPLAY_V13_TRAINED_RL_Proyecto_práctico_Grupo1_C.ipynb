{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelGH1167/RL_Project_Grupo1-C/blob/main/DDQN_REPLAY_V13_TRAINED_RL_Proyecto_pr%C3%A1ctico_Grupo1_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSVPAihG4U1j"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "* Alumno 1: Benali, Abdelilah\n",
        "* Alumno 2: Cuesta Cifuentes, Jair\n",
        "* Alumno 3: González Huete, Manel\n",
        "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
        "* Alumno 5: Pascual, Guadalupe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWWcufoC7S2B"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1svUw2WiJAUy"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda update --all\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0itzNXpLeC2G"
      },
      "source": [
        "---\n",
        "### 1.2. Preparar Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBGjSR-yeC2G"
      },
      "source": [
        "<font color='red'>**IMPORTANTE:**</font><br>\n",
        "El entorno de Colab está preinstalado con una serie de librerías por defecto. Para trabajar en base a las especificaciones del ejercicio se necesitan intalar unas librerías que bajen de versión las existentes en Colab. Entre ellas tensorflow. El problema de realizar esta acción es que para que todas las versiones sean consideradas por el entorno hay que reiniciar la sesión, sino se mantienen dependencias y los import no funcionan. <br>\n",
        "Es decir tras los \"pip install\" hay que hacer un **\"Runtime > Restart runtime\"** o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\".<br>\n",
        "En este punto se ha de tener presente que se ha reiniciado y **se han perdido las variables** que se hayan establecido, por ese motivo repetiremos el código para identificar si estamos en Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Fgi_akeC2G"
      },
      "outputs": [],
      "source": [
        "# Verificamos si estamos en Colab\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byW12Ph4eC2H"
      },
      "source": [
        "#### 1.2.1. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nckWRX5HeC2H"
      },
      "source": [
        "<font color='red'>**IMPORTANTE:**</font><br>\n",
        "Ignorar los errores que puedan aparecer, son incompatibilidades con librerías avanzadas que no utilizamos ni necesitamos para nuestro código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wS6Ez-USeC2H",
        "outputId": "b812688b-4e80-40e0-d9c8-45eeb9d7af66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [INFO] - Instalando paquetes adicionales...\n",
            "============================================================\n",
            "IMPORTANTE: Ignorar los errores que aparecen:\n",
            "   Son incompatibilidades que aparecen con librería avanzadas\n",
            "   que no necesitamos ni vamos a utilizar\n",
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (2.0.2)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting future (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654677 sha256=546d5ed5b9ee3c949a92edb0fc7ef9649f41769a76fa5057f181ad85cd396d2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/b7/570cb90b10f17e85ccb291ba1f04af41ec697745104a2263eb\n",
            "Successfully built gym\n",
            "Installing collected packages: future, cloudpickle, pyglet, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "Successfully installed cloudpickle-1.6.0 future-1.0.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
            "  Cloning https://github.com/Kojoley/atari-py.git (to revision 1.2.2) to /tmp/pip-req-build-6w02lglr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-6w02lglr\n",
            "  Running command git checkout -q 78fc2cb27c5541792ca33249b68c5f853d2b5510\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 78fc2cb27c5541792ca33249b68c5f853d2b5510\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (2.0.2)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atari-py: filename=atari_py-1.2.2-cp311-cp311-linux_x86_64.whl size=4738765 sha256=0a6903baff000f476f5a82fdca3d1f3ca0e9920d47dacce4254e8bafa2321e41\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uo3_y2n3/wheels/9a/72/f3/2f69e7b1db640ff44b11a6368f427fabd4746b7d7a8a34a19e\n",
            "Successfully built atari-py\n",
            "Installing collected packages: atari-py\n",
            "Successfully installed atari-py-1.2.2\n",
            "Collecting keras-rl2==1.0.5\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
            "Collecting tensorflow (from keras-rl2==1.0.5)\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow->keras-rl2==1.0.5) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.2)\n",
            "Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, google-pasta, tensorboard, astunparse, tensorflow, keras-rl2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.31.1\n",
            "    Uninstalling protobuf-6.31.1:\n",
            "      Successfully uninstalled protobuf-6.31.1\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 keras-rl2-1.0.5 libclang-18.1.1 protobuf-5.29.5 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "db55f0b5f2f343aeb6bd19c1e2986566"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic 2.11.7 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "google-genai 1.21.1 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "optree 0.16.0 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.6.0+cpu requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.38 which is incompatible.\n",
            "orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.38 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires imageio!=2.35.0,>=2.33, but you have imageio 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.6.0\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "\n",
            "============================================================\n",
            "INSTALACIÓN COMPLETADA\n",
            "============================================================\n",
            "IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\n",
            "1. Ve a Runtime > Restart runtime\n",
            "2. Después ejecuta las importaciones\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
        "INSTALL_LOCAL = False\n",
        "#Si quremos trabajar con el entorno nativo --> false\n",
        "IN_COLAB_ENV = False\n",
        "\n",
        "if IN_COLAB:\n",
        "# =========================\n",
        "#  Entorno Colab nativo con todo lo compatible.\n",
        "#  Sólo recordar que se debe REINICIAR EL RUNTIME (al acabar)\n",
        "# =========================\n",
        "  print(\" [INFO] - Instalando paquetes adicionales...\")\n",
        "  print(\"=\"*60)\n",
        "  print(\"IMPORTANTE: Ignorar los errores que aparecen:\")\n",
        "  print(\"   Son incompatibilidades que aparecen con librería avanzadas\")\n",
        "  print(\"   que no necesitamos ni vamos a utilizar\")\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git@1.2.2\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.12.1 --quiet\n",
        "  # Instala imageio y sus dependencias para video\n",
        "  %pip install imageio==2.15.0 --quiet\n",
        "  %pip install imageio-ffmpeg\n",
        "  %pip install ffmpeg-python\n",
        "  print(\"\\n\" + \"=\"*60)\n",
        "  print(\"INSTALACIÓN COMPLETADA\")\n",
        "  print(\"=\"*60)\n",
        "  print(\"IMPORTANTE: Debes REINICIAR EL RUNTIME ahora:\")\n",
        "  print(\"1. Ve a Runtime > Restart runtime\")\n",
        "  print(\"2. Después ejecuta las importaciones\")\n",
        "  print(\"=\"*60)\n",
        "  INSTALL_LOCAL = False\n",
        "  IN_COLAB_ENV = False\n",
        "\n",
        "if IN_COLAB_ENV:\n",
        "# =========================\n",
        "#  Colab con env -->\n",
        "#    no funciona muy bien pues aunque se cree el entorno, Colab sigue\n",
        "#    utilizando el suyo con sus librería y se necesita usar %%writefile\n",
        "# =========================\n",
        "  # 1. Instalar virtualenv\n",
        "  !pip install virtualenv --quiet\n",
        "\n",
        "  # 3. Crear el entorno virtual llamado \"miar_rl\"\n",
        "  !virtualenv miar_rl\n",
        "\n",
        "  # 4. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
        "  !./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
        "  !./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
        "  !./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
        "  !./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
        "  !./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
        "\n",
        "  # 5. Librerías adicionales\n",
        "  !./miar_rl/bin/pip install Pillow\n",
        "  !./miar_rl/bin/pip install matplotlib\n",
        "  !./miar_rl/bin/pip install tqdm\n",
        "  INSTALL_LOCAL = False\n",
        "\n",
        "if INSTALL_LOCAL:\n",
        "# =========================\n",
        "#  Librería para trabajar en local, si NO se cargaron las\n",
        "#    librerías desde fichero requirements\n",
        "# =========================\n",
        "  %pip install numpy==1.23.5\n",
        "  %pip install gym==0.17\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0\n",
        "  %pip install matplotlib==3.4.3\n",
        "  %pip install tqdm\n",
        "  # Instala imageio y sus dependencias para video\n",
        "  %pip install imageio==2.15.0 --quiet\n",
        "  %pip install imageio-ffmpeg\n",
        "  %pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouO30DIAKL3"
      },
      "source": [
        "---\n",
        "### 1.3. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3sLlcmyeC2H"
      },
      "source": [
        "**IMPORTANTE:**<br>\n",
        "Recordar que antes de seguir (si hemos decidido el entorno de Colab nativo - IN_COLAB=True -)\n",
        "* Hay que hacer un <font color='red'>\"Runtime > Restart runtime\"</font> o si tienes Colab en español: \"Entorno de Ejecución/Reiniciar sesion\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "barMp48aeC2I"
      },
      "outputs": [],
      "source": [
        "#Si ya tenemos lasa librerías cargadas desde requirements.txt --> false\n",
        "INSTALL_LOCAL = False\n",
        "#Si quremos trabajar con el entorno nativo --> false\n",
        "IN_COLAB_ENV = False\n",
        "\n",
        "# Verificamos si estamos en Colab\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw5W3OopAFKN"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/2025/DDQN_REPLAY_V13\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK5sY_ybAFt8"
      },
      "source": [
        "---\n",
        "### 1.4. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lN7KLe05NSa",
        "outputId": "4089ea34-6444-41c7-bd3f-ff6949fa3d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [INFO] - Estamos ejecutando en Colab\n",
            " [INFO] - Colab: montando Google drive en:  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            " [INFO] - Colab: Asegurando que  /content/gdrive/My Drive/2025/DDQN_REPLAY_V13  existe.\n",
            "\n",
            " [INFO] - Colab: Cambiamos el directorio a:  /content/gdrive/My Drive/2025/DDQN_REPLAY_V13\n",
            "/content/gdrive/My Drive/2025/DDQN_REPLAY_V13\n",
            " [INFO] - Archivos en el directorio: \n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Cambiar al directorio en Google Drive que deseas usar\n",
        "import os\n",
        "if IN_COLAB:\n",
        "    print(\" [INFO] - Estamos ejecutando en Colab\")\n",
        "    # Montar Google Drive en el punto de montaje\n",
        "    print(\" [INFO] - Colab: montando Google drive en: \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "    # Crear drive_root si no existe\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\n [INFO] - Colab: Asegurando que \", drive_root, \" existe.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Cambiar al directorio\n",
        "    print(\"\\n [INFO] - Colab: Cambiamos el directorio a: \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verificar que estamos en el directorio de trabajo correcto\n",
        "%pwd\n",
        "print(\" [INFO] - Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihTI9TOD43ML"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIAR9zQv43MO"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas\n",
        "\n",
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4o4-N4T43MO"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gc       # Para garbage collection\n",
        "import os\n",
        "import pickle\n",
        "import re       # Para expresiones regulares en carga de checkpoints\n",
        "import gym      # Para el entorno de Atari\n",
        "import cv2      # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
        "import warnings\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import json\n",
        "import datetime\n",
        "import IPython\n",
        "import imageio\n",
        "import pandas as pd\n",
        "\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
        "from IPython.core.history import HistoryManager\n",
        "from tensorflow.keras.models import  clone_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Permute\n",
        "from tensorflow.keras.layers import Lambda, Add\n",
        "from tensorflow.keras.models import Model\n",
        "if IN_COLAB:\n",
        "  from tensorflow.keras.optimizers.legacy import Adam\n",
        "else:\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "from collections import deque\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puzS2kTzc1Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3c5983-a941-430f-db89-cb3904362e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [INFO] - Precisión mixta activada (mixed_float16)\n",
            " [INFO] - Optimización JIT activada\n"
          ]
        }
      ],
      "source": [
        "# Necesario para la grabación de video\n",
        "try:\n",
        "    import gym.wrappers\n",
        "    from gym.wrappers import Monitor\n",
        "except ImportError:\n",
        "    print(\" [WARNING] - gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
        "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n",
        "\n",
        "# Activar precisión mixta para mayor velocidad\n",
        "print(\" [INFO] - Precisión mixta activada (mixed_float16)\")\n",
        "# Activar optimización JIT\n",
        "tf.config.optimizer.set_jit(True)\n",
        "print(\" [INFO] - Optimización JIT activada\")\n",
        "\n",
        "# Hay veces que se produce un error OperationalError('database or disk is full') indica que la\n",
        "# base de datos SQLite que usa Jupyter para guardar el historial de comandos está llena.\n",
        "# Esto explica por qué no puedes guardar aunque tengas 500GB libres.\n",
        "# Desactivar guardado de historial para esta sesión\n",
        "ip = IPython.get_ipython()\n",
        "ip.history_manager.enabled = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8iRCStcc90p"
      },
      "outputs": [],
      "source": [
        "# Configurar TensorFlow para CPU (x cores)\n",
        "def optimizar_tensorflow():\n",
        "    \"\"\"Configura TensorFlow para rendimiento óptimo en CPU/GPU\"\"\"\n",
        "    # Limpiar sesión previa\n",
        "    gc.collect()\n",
        "\n",
        "    # Optimización de GPU si está disponible\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(f\" [INFO] - GPU optimizada para crecimiento adaptativo de memoria\")\n",
        "        except Exception as e:\n",
        "            print(f\" [INFO] - Error al configurar GPU: {e}\")\n",
        "\n",
        "    # Optimización de CPU\n",
        "    num_cpu_cores = os.cpu_count() or 8  # Fallback a 8 si no se puede detectar\n",
        "\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
        "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
        "    os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores // 2)\n",
        "\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(num_cpu_cores // 2)\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(max(2, num_cpu_cores // 4))\n",
        "\n",
        "    # Modo eager solo si es necesario\n",
        "    # Para entrenamiento, es mejor desactivarlo por rendimiento\n",
        "    tf.config.run_functions_eagerly(False)\n",
        "\n",
        "    # Configuración para evitar errores de guardado en Colab\n",
        "    if IN_COLAB:\n",
        "      os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "      os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reducir mensajes de TF\n",
        "      # Configuración personalizada para guardado de modelos\n",
        "      tf.keras.backend.set_learning_phase(1)  # Asegurarnos que estamos en modo entrenamiento\n",
        "      # Desactivar guardado asíncrono (la causa más común del error)\n",
        "      if hasattr(tf.config, 'experimental'):\n",
        "        tf.config.experimental.set_synchronous_execution(True)\n",
        "\n",
        "    print(f\" [INFO] - TensorFlow optimizado para {num_cpu_cores} cores CPU\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faNbnMuOdNDP"
      },
      "source": [
        "#### Crear el entorno\n",
        "Nuestro entorno es el juego Space Invaders, de Atari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WFE0sqPdLsy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Crear el entorno\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcjoyH3idqh4",
        "outputId": "02b4bf36-58d8-4267-c6cd-b5def6e6cd9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
        "print(\"El número de acciones posibles es : \", nb_actions)\n",
        "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
        "\n",
        "# Here we create an hot encoded version of our actions\n",
        "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgw7iHVRduGa"
      },
      "source": [
        "#### Definición Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TshMrqTjdxja"
      },
      "outputs": [],
      "source": [
        "### HIPERPARÁMETROS DEL MODELO\n",
        "# Hiperparámetros optimizados\n",
        "HEIGHT = 84\n",
        "WIDTH = 84\n",
        "CHANNELS = 1                    # Canal para grises\n",
        "USE_FRAMESTACK = True           # Cambiar a True si quieres detección de movimiento\n",
        "WINDOW_LENGTH = 4 if USE_FRAMESTACK else 1   # Número de fotogramas apilados          # La mayoría de implementaciones usan 4 frames\n",
        "batch_size = 32                 # Tamaño de batch óptimo\n",
        "gamma = 0.99                    # Factor de descuento (mejor que 0.95 para recompensas a largo plazo)\n",
        "learning_rate = 0.00025         # Tasa de aprendizaje estándar para DQN\n",
        "memory_size = 1000000           # Buffer de memoria grande para mejor estabilidad\n",
        "TARGET_UPDATE_INTERVAL = 10000  # Actualización de red objetivo cada 10,000 pasos\n",
        "WARMUP_STEPS = 50000            # Pasos iniciales para llenar la memoria (experiencia aleatoria)\n",
        "NUM_TRAINING_STEPS = 2000000    # Total de pasos de entrenamiento (5M para buenos resultados) = num_steps\n",
        "EPSILON_STEPS = 500000          # Total de pasos de evaluación del modelo\n",
        "INPUT_SHAPE = (HEIGHT, WIDTH)   # Dimensiones de cada frame\n",
        "\n",
        "# Single frame shape (height, width, channels)\n",
        "FRAME_SHAPE = (HEIGHT, WIDTH, CHANNELS)  # (84, 84, 1)\n",
        "MODEL_INPUT_SHAPE = (HEIGHT, WIDTH, WINDOW_LENGTH)  # Forma para el modelo (channels_last)\n",
        "SEQ_INPUT_SHAPE = (WINDOW_LENGTH,HEIGHT, WIDTH)  # Forma para el modelo (channels_last)\n",
        "\n",
        "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
        "# Definir shape consistente\n",
        "if USE_FRAMESTACK:\n",
        "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
        "else:\n",
        "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
        "\n",
        "state_size = (*INPUT_SHAPE, WINDOW_LENGTH)   # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
        "input_shape = (*INPUT_SHAPE, WINDOW_LENGTH)  # Para la API de keras-rl\n",
        "action_size = env.action_space.n       # 6 acciones posibles\n",
        "learning_rate =  0.00025               # Alfa (también conocido como tasa de aprendizaje)\n",
        "\n",
        "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
        "# total_episodios = 10    #TEST      # Episodios totales para el entrenamiento\n",
        "# max_steps = 10000       #TEST      # Máximo de pasos posibles por episodio\n",
        "total_episodios = 100                # Episodios totales para el entrenamiento\n",
        "max_steps       = 3000               # Máximo de pasos posibles por episodio\n",
        "\n",
        "# Parámetros de exploración para la estrategia epsilon-greedy\n",
        "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
        "epsilon_stop = 0.1             # Probabilidad mínima de exploración\n",
        "\n",
        "# Hiperparámetros del aprendizaje Q\n",
        "tau = 0.001\n",
        "checkpoint_path=\"checkpoints\"\n",
        "TARGET_REWARD = 20.0\n",
        "\n",
        "### HIPERPARÁMETROS DE MEMORIA\n",
        "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
        "\n",
        "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
        "episode_render = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFRyr4Ld1Hp"
      },
      "source": [
        "#### Clase \"processor\" para Atari\n",
        "\n",
        "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06wZVH5c43MP"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    \"\"\"\n",
        "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
        "\n",
        "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
        "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
        "    las recompensas.\n",
        "\n",
        "    MÉTODOS:\n",
        "    --------\n",
        "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
        "                                         y la redimensiona.\n",
        "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
        "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape=(INPUT_SHAPE)):\n",
        "        self.input_shape = input_shape\n",
        "        # Precargar una imagen negra para inicialización\n",
        "        self.black_frame = np.zeros(input_shape, dtype=np.uint8)\n",
        "\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\"\n",
        "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
        "        \"\"\"\n",
        "        # Si la observación es None, devolver un marco negro\n",
        "        if observation is None:\n",
        "            return self.black_frame\n",
        "\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        # Crop the screen (remove the part below the player)\n",
        "        # [Up: Down, Left: right]\n",
        "        cropped_img = observation[18:-12, 4:-12]\n",
        "        # Optimización: usar cv2 para redimensionar y convertir a escala de grises (más rápido que PIL)\n",
        "        resized = cv2.resize(cropped_img, self.input_shape)\n",
        "        processed_observation = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY) if len(resized.shape) == 3 else resized\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype(np.uint8)\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Normaliza un lote de estados dividiendo los valores por 255.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
        "        \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\"\n",
        "        Normaliza la recompensa al rango [-1, 1].\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            reward (float): Recompensa original del entorno.\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            float: Recompensa limitada al rango [-1, 1].\n",
        "        \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "    def process_step(self, observation, reward, done, info):\n",
        "        \"\"\"\n",
        "        Procesa un paso completo del entorno.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            observation: Observación del entorno.\n",
        "            reward: Recompensa obtenida.\n",
        "            done: Indicador de fin de episodio.\n",
        "            info: Información adicional del entorno.\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            tuple: (observación procesada, recompensa procesada, done, info)\n",
        "        \"\"\"\n",
        "        processed_observation = self.process_observation(observation)\n",
        "        processed_reward = self.process_reward(reward)\n",
        "        return processed_observation, processed_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwluQRXedZP"
      },
      "source": [
        "#### Revisar el entorno de juego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VwBXOBk43MP",
        "outputId": "c79e2270-65dc-4738-bf26-0998edc6d737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [INFO] - Numero de acciones disponibles: 6\n"
          ]
        }
      ],
      "source": [
        "print(\" [INFO] - Numero de acciones disponibles: \" + str(nb_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT6osc3H43MP",
        "outputId": "db7d3e40-f3ee-4afe-c762-1d438a89ea57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [INFO] - Formato de las observaciones:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print(\" [INFO] - Formato de las observaciones:\")\n",
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "7-BoOu_eeiAE",
        "outputId": "245e905b-61fd-4819-ea06-d604daae95fc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAGhCAYAAADY5IdbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAa+RJREFUeJzt3Xl4HNWZL/5vLb2vau2SJVle5VU2XmSxGIONF5ZAMDvJOISBkAuZC8zk5vJ7bsJy57lkyHMz82QuCSFhYDIECEzGkJjV2HgBvGFsjI13y5Zl7Uuv6rXq/P4oq+1G1bK6q7ol4ffzPPXY6qruc7r69Nunzjl1DscYYyCEEJIVfqQzQAghYxkFUUII0YCCKCGEaEBBlBBCNKAgSgghGlAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRYESD6LPPPovx48fDbDajoaEBO3fuHMnsEEJIxkYsiP7pT3/Co48+iscffxyff/456uvrsWLFCnR2do5UlgghJGPcSE1A0tDQgAULFuD//b//BwCQZRlVVVX40Y9+hP/5P//nkM+VZRmtra1wOBzgOC4f2SWEXGQYYwgEAqioqADPp69vinnMU1IsFsPu3bvx2GOPJR/jeR7Lli3Dtm3bBh0fjUYRjUaTf585cwbTp0/PS14JIRe306dPY9y4cWn3j8jlfHd3NyRJQmlpacrjpaWlaG9vH3T8008/DZfLldwogBJC8sXhcAy5f0z0zj/22GPw+XzJ7fTp0yOdJULIReJCTYYjcjlfVFQEQRDQ0dGR8nhHRwfKysoGHW8ymWAymfKVPUIIGbYRqYkajUbMmzcPGzZsSD4myzI2bNiAxsbGkcgSIYRkZURqogDw6KOPYs2aNZg/fz4WLlyIf/mXf0EoFMI999wzUlkihJCMjVgQvf3229HV1YWf/exnaG9vx5w5c/Dee+8N6mwihJDRbMTGiWrh9/vhcrlGOhuEkIuAz+eD0+lMu39M9M4TQshoRUGUEEI0oCBKCCEaUBAlhBANKIgSQogGFEQJIUQDCqKEEKIBBVFCCNFgxO5YyhVO5OCe5oZgFnR7TSYz+A77kAgmdHvNscRUaIJjwtDTgWUq7o/Dd8QHjIJbPQwih3nTPbCY9CszMmPYe8gLXzCu22uOJWVFZkyrTT9APRt9gRi+OOzFaLs96BsXRAWTgIplFTB6jLq9JpMYIt2RizaI2qptqPpWla6vGWwKwn/MDyaN/DfCYhKw+poqlHj0myksITG0dR28aIPopCo7vndTra6vebDJjy+P+iCNgjJzvm9cEJWiEs6sPwNBx1oFYwzRnqj6Tg4oXlQMS6lFt/QAoHdvL4Ing6r73DPdcE7S91c+cCKAvn19qvtCzSE0v9msa3rxQBxMHh1fhnBUwhsfNOtbE5WBjp6I6j6OA5Y3lqGy1KpbegDwyZ4uHD4ZUN23cKYHMye7dU3v4Akftn3Ro7rvaHMQ/7b2hK7peQNxyKOkzJzvGxdEWYLBf9gPqM2jygA5Iae9hOQMXNoJWOW4nDZNR60Djon6Xu6GmkNpg6i1zIqCWQW6pidFpLRBNNoXRTygXqNiMgNLpDuhAG9Qb3ZnMhsVl/IAEE8ol97p5t6NxeW0l5BGAz/k89RwAKbWOjFzkr7zPxxrDqQNotUVNiyaXahreuFIIm0Q7eqLwhuIqe6TZYZ4mjLDc4AhTZmRZYy6S3ngGzgBiWARUHNzDYzOwZfzUkzCqf86hVjf4A+XEznU3FQDc7F50D4mMzT/pRnhtrBqmiaPCbxZ3z66mDcGqV9S3WdwGiDa9f39S4QSiPvUA6VzqhMVV1eo7vMf96P1g1bVfbZqG8ZdO071hynUGsLpv54G0v825Y3NIuC+1RPhcQ0uM+GojN//13F09Q6+EjGIPL7/7VpUlgy+CklIDC+91YTmtn7VNEsKTbCY9P0Me7xRBPvVm5wKnEY47QZd0wuE4uj1qQfKOXVufHvpONW6zP5jfrz+vvqVzeQaB+6+rga8yhObzoTwh7+chJTn2uiFJiD5xtVEAYATOHDi4E+Bkzj1GuoAAarPg3osSxqqNhbzxSBH1SOFwWlI3wE2RDkZKr1EKIFESP2LJFrF9MF3iGDGcernEwA4tdJ+/vMETvWYoZ6XfxwEgYMgDP4hFAWWtshwHNI+D2BDLishSwySNPikMwA93hgiUfVCV+A0wmZRLzND1YdkWT09APAF4wikKTMOqwiXQz34DnVpzXEcRNXzAghDtJrwHCAK6leE/KgqM+d842qiAMCbeNUPgTEGOZb+cp438upfbqbUYlWfxwG1d9Sm7b0+9Z+n4DvsU91XdWMVCmaqX5a3vNOC3j29qvvKl5ajeFGx6r6OLR3o2Nqhuq+4sRjlV5er7uv5rAdn3j+juo8TuPSX5RJL39TBA4IxzRdePvtZjBJmEw9etcwAkZiU9jLSbORVv9wMQDQqQS3O8Bzw4J2TMX3i4DLMGPD8fx7D3kNe1fTuuakWC2epX5a/8vYpbP28S3Xf6mvGYdmiwUvvAMC6zWfw9pY21X3LLy3Dt5eqr3S5eVcnXntPvUYpCBxMacpMQmJpmzp4noPJyKv+cEkyQ3QEysxFVxPlRA6OCQ7VL68syQgcC0CKqPzK84C9xg7ROviUMMYQOB5IW8MLnQ6pBxIGxIfonQ23htMGp5hX/TIJACJdkbSBOdKt3pkBANHeaNrnhTvUmyoApcZsr7anzWfwlHrbrcFmgH2CHZzKVyIejCNwIjAq2kUNIofpE1wwq3QsJSQZXx71IaxSZngemDLeCbtKmZEZw4FjPtUaHgNw/HRQNZAwAL407c8AcKo1BGOaMtPtTdP5CeBMZxh7D6m3ebcPUWY6eiJpn9fSod5UAQAelxGTq9UrFj3eaNq2W5ddRF2tU7US5AvG8dVx36hrF/3G1URFm4ip909VHeIkRSQc+d0RRDoHFxrewGPy306GtWJwjymTGI6+eBShUyHtmR+DPHM9qLm5RnWf94AXTa81qe5zTnFi4ncmqjahBJuCOPbvx0bFECenTcRPH5ipOsSpPyLhH397AGc6B//ImIw8/r+/nY7xlbZB+xISwz+9cBBHTqkHi2+6y+cW4b5bJqru27W/F//v1aOq++qnuvHwd6eqtokePOHHL146lPchThddTVSKSWjb1Kba1sgSLG0vsyzJ6Py4U73NUAZivWlqhhxQNL9ItUMKAHp296St5XnqPbBWqg9z6dvfh1CzetB2TXPBUav+K+8/4of/mF91n2OCA6469R+f4KkgvAe8qvv6W/rR8k6L6r60Q78ARDojyvNUvhBx3+gZ4hSJyXhrYwus5sGffVyS4U1TZhIJhnc+boPLNrjNUGYMnb3phzhdtaAE5SodUmDAlt1dON2uXsu7dE4RascNDtoAsGNfD441q18VXDKtANMmqgeCfYe9+PKo+hXK9IlOzJ2m3uR05GQAu/arNzmdaAnhj+tOqe5rTzP0CwDOdITxytsnVa9eev0xGuKUDyzOlKE6aYY4pa35yID3K2/ajqe0w3ig1LjSjdsMnAikDaL2CXZ4ZntU94U7wmmDqK3KhqIFRar74qF42iBqKbekfR6AtEE00h1BtC9NsByiiSrmi6H7s271nQyj4lIeUIYibd/XozpUiTGlVqlGkhl2H+hVfx4AKU2Z4QDMnlqgOsSJMYaDJ/xpg+j0CU4sqlf/DFva+9MG0UnVdly1QH39Mn8wnjaIVpfb0j6PMaQNom3dYXT1qQdLeYgy0+OLYtOuTvXnMRripJshhziZBVTfWA2Dc3DtQIpKOP3X02mHOFVdX6U+xEliaHmnBeF29WBoLjGn7WWPdEUghdV7Wk2FJog29d+xaG807R1SRrdR9f0BSuBKN1TJ4DTA6Fa/kyseiKueFwBwTnaibIl6p0TgRABtG9Q7JWxVNlSsqFBt3+pv7UfLuy2jYoiT1SLgnhsnoEDlnEZiMl56qwndKj8iBpHD33yrFuVFg8tMQmb447pTaYNhZYkFVovKZ88Y2roiCIbVP/uyQjMcaYYqdfZE0t4hVVRgQoHKsD9AaaNMN1SpwGlEUYH6nVzeQEx16BcAzJ7ixreWqA+LO3jCjz9/qH5lM6najttWVKtezp9s7ccr75zKe230orucBwcIVkH1spwX0/S+n31euiFATGJDDsmRwlLamupQbX5SJE2PP4Ye3C/FJHDBNDcFDNF7KcfktIE53TAsQLkJId3QqKHmKOBEDgabQXWaG9EiggMHNgqqozzHwW4T4bQPDjKGaAJCms+e4zjYLOrPkyQZopC+zITCCSTUhjgxpQkhnf6olPacpevxBoBIVII/qB4oo7H0Y/iisfTPSzcMC1B+YNTOCwBYVJpNBogCD5fdoPrDa7NEhxyhOFK+cTVRQBlwn26I01CBSzALqsGS4ezz1MooB9TeVgv7BJXeawac+q9T8B9Rv7yuuqEK7plu1X1n3j2D3r1phjhdXY6iBvVLuo4tHej8RP1yqHhRMcquUq9R9uzuSTtonhO4tLfRypKcNgBzAqcMN1Mp+kxikIb4EuYTB6U2qjrECUB/OKE6VAkArGZBNcgyAOGIpDownOeAH94+WbWNkjHg938+ji8Oe1XTW/Ot8ViQZojTq++cwid71JtPvr10HJYuUr8sf3tzK979WP1qYlljKW66Wn2I05bPOvH6+6dV94kiB0ua4W1xiaUNwILAwWISVINlQmaqoyRy7aKriXICB2ulNe34xGwwxhA8FUx7B1G4I5x2UsFEmjtIAKWtMd2tnUMNjYr2RdM+L5bmsmxgX7rnRdNclgGAaBdhrbSqBsNsxUPxUTPaQRA41FbaYTLqd9cZY8CRUwHVO4gYlOFBgkpNlTGW9q4jAGjrjuBImuFBQw2N6upL/7yeIYZG9fliaZ/XMUSnostmUB21oEWgP4GjpwKjrl30G1cTFW0iptw/BSYdZ+RhCYajL13EQ5zmeFCzWn2IU7aCJ4I49ofRM8Tpf/1gBkoK1UdYZCORYHjm34Ye4pTuJ+lCZ2QsPO+yIYY4ZesQDXHKDzkmo/OTTl3nEwVD2k4XcEqQMat0Lmjh/cqL/jPqnRLOKU7Ya9QHv2crdDoE3yH1Htr+tn60rle/1M9WzBsbVUOc3vu4DTa1jp4sSTJLO/id44DL5xajVOcy8/lXvTjRov5DP3uKG1PG6ztJzvHmIPakGYh/qjWE//zgtK5tmN3eKA1xygc5LqN7Z5phNTlSMLNA96npon3RtEHUXmtH6aXq7VvZ6trZlTaIRjoiiHSkH9s31sXiMjbuVG9HzgUOwIKZHsyarO8sTt290bRBdNoEJ1Zept4enq0NOzrSBtGWjjBahrgL7pvkG3c5PxIs5RbV20W1iHRFEPert3GZCk1phyplK+aLIdqdvo2L6Kumwgq7Vd9ZlVo7w+jzq18xlRaa0w5VylavN4q2IW4Z/aa40OU8BVFCCBnChYIoLVRHCCEaUBAlhBANKIgSQogGFEQJIUQDCqKEEKKB7kH06aefxoIFC+BwOFBSUoKbbroJhw8fTjlmyZIlyvo7520PPPCA3lkhhJCc0z2Ibt68GQ8++CC2b9+O9evXIx6PY/ny5QiFUgcB33fffWhra0tuzzzzjN5ZIYSQnNP9jqX33nsv5e+XXnoJJSUl2L17NxYvXpx83Gq1oqxM3zsoCCEk33LeJurzKbcSejypM7j/8Y9/RFFREWbOnInHHnsM/f3pF72KRqPw+/0pGyGEjAoshyRJYtdddx277LLLUh7/7W9/y9577z22b98+9vLLL7PKykr27W9/O+3rPP744wOLSdBGG2205XXz+XxDxrmcBtEHHniA1dTUsNOnTw953IYNGxgAduzYMdX9kUiE+Xy+5Hb69OkRP7G00UbbxbFdKIjmbBanhx56COvWrcOWLVswbpz6zNgDGhoaAADHjh3DxImD5yA0mUwwmfSdPIEQQvSgexBljOFHP/oR1q5di02bNqG2tvaCz9m7dy8AoLy8XO/sEEJITukeRB988EG88soreOutt+BwONDe3g4AcLlcsFgsOH78OF555RVce+21KCwsxL59+/DII49g8eLFmD17tt7ZIYSQ3Mq2vTMdpGlXePHFFxljjDU3N7PFixczj8fDTCYTmzRpEvvxj398wXaH8/l8vhFvJ6GNNtouju1CsYnmEyWEkCHQfKKEEJJDFERHmCAAM2daMGOGBXyePo1Jk0yYO9cKs1nPZcTSq6gwYMECG9xuHRcPHEJBgYAFC2woL9d3+Y10LBYOl1xixYQJ+RlBMhJlhqRHH8EIE0UOV1/txJIlTohifoLaggV2XHutG3Z7foJaXZ0FN95YgNLS/AS18nIjbrqpAJMn67uaZjoOh4DrrivA/Pn6rrOezkiUGZLeN261z9FiyhQzZsywYMeOIFpb1Recmz/fhspKI3btCsHrlZBIZN88XVZmwKJFdhw6FMahQ+qLh02ebMbMmRacORPDwYNhBAJS1unZ7TyuvNKJrq44du5UX2GytNSAxkY7AgEJa9f2ob1d/TwMhygCV1zhBM8DmzcHVM+VzcZjyRInJIlh7do+tLSkWeZ6mBYssKG01IDNm/0IBORB+wUBWLzYCZuNx/vve9HVldCUXr7LDNEH1UR1xnGA0cihosKAWbOsKCwUYTCk1hZ4XjmmpsaEadPMOH48goMHw5AHf0+HxWDgUFgoYvZsKyoqjDAaOXDnJTmQp/JyJU/d3Qns3duPaDS7L6DBwMHhEDBjhgW1tSYYjdygy0qDgYPHo+RJkhh27w7B58suaAsCYDbzmDrVjClTzLBYOIhf+/kXRSVP06db4HQK+OyzUNZBe+DzGT/ehBkzLHA4hEGf4UCepkwxo7raiH37+tHUlN1qqSNRZoh+qHdeZ1VVRlx/vRtHj0bw1VdhNDTYUVAg4s9/7k0GkVmzLFi82InPPgvh1KkoursTWdcoHA4Bt9zigc+XwI4dQdTVWTB1qhnr1nnR3KzUxCorjfjWt9w4diyKAwf60dubQCSSXXqCANx0kwc2G48tWwIoLhaxcKEdn34awJ49/WfzxGP1ag8CARnbtwfg90uqNbnhuvpqJyZPNmPr1gBkmWHxYidOnIjgww/95+WpAHa7gC1bAvB6E+jry76WXV9vxeWXO7BrVxAdHXEsXuxEJCJj7dpeJM5WNpcscWLqVCVPXV1xdHcnkO03Kd9lhmTmQr3zdDmvM5OJQ0WFEcePR9HaGkcsxmAwcCgqEmE2K9U1l0uEIABeb0LTJS6gXOaWlRmQSDC0tsZRVWWCICi1wIGapscjQBQ59PdLaS8Th4vjOBQXi7BYeHR0xCGKShBzOoVkm6fVysNo5BGPJ9DaGs86uAxwuwWUlRnQ15dALMbA84DVei49ngdMJh6MAW1tsax/IAbYbDwqKgyIRBja2+NgjMFs5lFSYoB0Njbb7Tx4Hujujmu+jM93mSH6oiCaYx984IPdzuM73ymC262c7s8/D+H55zsRi+lfk/jssyC++KIfq1cX4IYbCgAAzc1RvPBCF2Ix/a/9TpyI4vnnO7FkiRP3318CAPD7Jbz8cjf8fklzAP263t4EXnyxCzNnWpPpSRLDa6/14MyZWNZNFOlEowyvv96LigoD/uZvipOX2e+/78X69b6cfIb5LjNEGwqiOgsEZHzxRT9aW89dSpeWirDbBVgsSq2ipMSAujoLACUAHD0aybr2FIsx7N+vXKIzBhQVGVBebkBBgZhMz+0WUVdnTrafnTwZhdeb3eUuYwxHjkRgNHKQJAanU8D48SaUlBiS6ckyw+TJZkQiSoLt7XFNtafm5hgYA8JhGSYTh8mTzRg3zgizWVlaRpIYamtNcDqV0QZer4STJ7NrnwSAri6lzbivLwFBACZMMKG83AirlU/2ho8bZ0wGtEhExtGjkWQtNVP5LjNEX9QmmmN33VWIGTOUws+d7e05/5RHIgzPP9+Jjg59LtGWLnXi6qudadMDgFdf7cH+/WFd0ps504I77ywcMr2NG/3YsEGfibRLSw24//6SZABVS+/AgTBeeaVHl/TMZg4/+EEpSkqU+oZamp2dCfz2tx26BbV8lxkyNGoTHSGTJplQV2dBebkBsRjDJ58EYDLxWLTIjubmKA4cCGPOHBvKygy48koH2tri+PTTQNa1mdJSEfPn21FdbQQA7NwZgt8v4bLLHOjvl7BjRxATJphRV2fGvHk2VFQY8cknAYRC2V3i22w8LrvMkRzQfvBgBE1NETQ02GG1CvjkkwBcLmXQ+5QpZpjNPHbtCqKzM7v2Q0EALr1USc9g4HDmTAx79/ZjxgwLqqtN2L49iFhMTh5z/fVuHDwYxvHj2ddI5861orraBIeDh88nYdu2ICorjZg1y4IvvwzjzJkYGhvtcDh4rFjhRnNzNNm5lo18lxmiDxrilCOVlUZceqkDhYVKp8++ff3JISkdHQns3BlET0/87N0nVsyYYYEgZD9wuqBARGOjHVVVJjAGHDkSxt69IcRiMvx+Cbt2hXD6tBJQJk0yn71jKfuP32LhsXChDVOnWsBxHE6fjmLXLiVwx2Iy9uwJ4ciRCBgDKiqMZ+9Yyv43WxA41NdbMWeODaLIobtbOYcdHQnIMvDVV2Hs2xdGIsFQUCBiwQI7ysuNWacHKOM2B34UQiEZn30WSg5jamqKYvfuEEIhGRYLj3nzbJgwQdvg/nyXGaIPqonmgdnM49ZbCyEISm/69OkWlJUZUFQkIhZj+MtflIHo8bg+l4McByxf7kY8LsNuF2Ay8bjnnuJkm+HGjX4cPRrJetymmoUL7airs6CkRKkp3nlnIYxGHhwH7NnTj127gpp7sc83aZIZ3/9+CTweEaIIXH+9G7KsnOtTp6L44AOfpmFOX1dUJGLNmiLYbDw4jsPll9sxd64VRUUi+vokvPVWH/r69Ht/+S4zJHsURPNAEDhUVp6rFTmdQjKgRSIyWlvjurZvcRyXcoulKCqDtAGlba2rK675bp6vKygQUVBwrjhVVZ27j9zrTSTHrOrFbhdSbls9v9bZ3y/j1Cl90zOZeFRXn3tPhYUGFCpNwfB6JZw+HdW1oyffZYZkjy7nCSFEA6qJ5hBjDKdPx9Leo15ZadTULqmmszOOri71GkphoQGlpfp+5D5fIm2t1m4Xkh1deolEZDQ1RSHLg2t9RiOP2lp9Z1KSJIampiii0cEdcDyv3Bqqp5EoM0QbCqI5tnVrAAcODB5OxHHALbd4MG2aRdf09u3rx8aN6sOJFi92YMUKfYeGNTfH8NprPaqD6qdONeO73y3SNT2vN4E33uhRvXQuKhLxgx+U6JpeLMbw9tte1Utno5HDffeV6D6TUr7LDNGGgqjOPB4R8+ZZk22Q9fVWFBeL2LEjhHBYqc1UVRkxbZoFFRVGiCKHSy+1o60tjl27ghkPV7FYOCxcaEdFhREcB0ycaIYgcNizJ4Tu7sTZPAm45BJbSp6KigzYuTOI/v7MhjjxvDKTUEWFEQaD0vZ6zTUuHDkSxsmTSo3UbObQ0KD0jg/kiedT85QJZRiTEQ6HAI4Drr7ahebmaHKs60CeysuVPJWUGLB8uQtHjkSyGnRfXW1EXZ0FZWXpP5/p0y2oqTHC6RTA80qeTp+O4ssvMx9/m+8yQ/RFQVRnBQUCLr/cAZ7nIMvAtGnKOMZ9+8LJL0RlpRFXXOFIPueSS2xobY3h889DkKTMOifMZmUcod0uQJaVADBunBEnT0aTAcvtFnH55Q4IgpKnujolT19+2Z9FEOUwZ44NVVXKZXphoYgrrnCgv19OBlGTiUdDgx0Oh5KnqiolT83N0ayC6OTJytjWAY2NdlgsfEoQra+3JZsOPB4lT+GwnFUQVft8zpyJYffuc5/P5MnmlPlDGxvtsFr5rIJovssM0RfdsaQzi0WZvOL8qegkCWhpiSWHo7hcAoqLU3+/olGGlpZYxveaiyKHceOMKVPDKRNxxJMBcjh5Gi6OG2iXS72E7e5OJG8lFUVg3DjTkHnKREmJmOyZHuD3S8mB+xynjEW1WNLnKRNqn08kwnDmzLnPp7hYhMuVmqdAQM6qxzzfZYZk5kJ3LFEQJYSQIdBCdYQQkkMURAkhRAMKooQQogEFUUII0YCCKCGEaEBBlBBCNKAgSgghGlAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRQPep8J544gk8+eSTKY9NnToVhw4dAgBEIhH8/d//PV577TVEo1GsWLECv/71r1FaWqp3VkaFyy6zY9w49dndt28P6r4W0MyZluSa5V938GAE+/Zlv6SvmnHjjLj0UnvKDEQDWlvj+PjjgK6zDLlcApYudcJgGJxgICBjwwYfolH9EjQYOCxd6hw0YxMAJBLAxo36LogH5L/MEG1yMp/ojBkz8OGHH55L5Lw50R555BG8/fbbeOONN+ByufDQQw/h5ptvxieffJKLrIwYUQQMBh4TJ5pRVzc4qDGmLDvR0RFHNMo0BxqeB0wmZVq8+nqb6jGhkIyjR8OIxZjmiXw5TkmvpEREfb0VnEoUtVoj2L1bWbY5ocNCmCYTB7dbwMyZ6ss99/QksGNHEIxJiMW0B1KDgYPdzidXMf26gaWh+/tlXQJ3vssM0YfuU+E98cQTePPNN7F3795B+3w+H4qLi/HKK6/glltuAQAcOnQI06ZNw7Zt27Bo0aJhpTEWpsJbsMCGyy5zwOkUVL/wjDH4/RL8fgmvv96Lnh5tUaa21oQbbiiAw8HDZhtcawKAUEhCMCjjnXe8OHo0oim9ggIBt91WCLdbWYVSLYhGozJ8PgnbtwexfXtQU3oGA4dbb/WgvNyIggIBPD84vUSCoa8vgaNHI1i3zqspPQBYudKFadMsKCgQVZcAYYyhr09CR0ccr7/eozmQ5rvMkOEZkanwjh49ioqKCkyYMAF33303mpubAQC7d+9GPB7HsmXLksfW1dWhuroa27ZtS/t60WgUfr8/ZRvtrFYexcUiQiEJra0xJBLnvmDBoIQzZ+IQRQ7FxQZd1ugZqBUmEspEvZHIucmPIxEZLS1KHkpKRJhM2tMTBA7FxSIMBh5nzsRTFlZLJBhaW5XF1oqLRdhs2osZxykz1judPNra4ujtTWDg958xhs7OOLq74/B4Bk+WnC2XS0BhoYju7gQ6OuIpi+P19SXQ2hqH3c7D4xFVmzMyle8yQ/ShexBtaGjASy+9hPfeew+/+c1v0NTUhCuuuAKBQADt7e0wGo1wu90pzyktLUV7e3va13z66afhcrmSW1VVld7Zzpn16/14+eVu+HzngsyBA2H87nedOHZMW21Qzeefh/DCC504ffpcu1lrawz/9m+d2LkzpHt6TU0R/P73nSltrcGghFde6cb77/t0v+Ts7ZXwhz90YdOmcz+kiQTwl7/04fXXe1N+PPQQjTL8+c89ePPN3pSgtnVrAC+91JXVcicXku8yQ7TRvU101apVyf/Pnj0bDQ0NqKmpweuvvw6LJbtVCh977DE8+uijyb/9fv+oDaROp4ApU8zJNYhqa01wOoWU2l9RkYi5c63weEQIAjBrlgVFRSIOHgxDzjAGGI0cpk9XFnLjOKCszIj6emvKchoOh4D6ehsqKpR2vUmTzDAYOBw8GFZdNXMoHKes0VRertSG3G4Rc+ZYU9oMjUYOM2ZY4XYLyeVEFiyw4ejRSFbLddTWmlBWZoDVyoPjgFmzrCgvP9fxwvPAlClmRKMMosihoEDEwoU2nD4dQ1tb5st1lJYaUFNjRGGh8vnU1VkgCFxKE0JVlXK+LRYegsBh3jwb2triOHEi8zWd8l1miL7ysjzIggULsGzZMlxzzTVYunQp+vr6UmqjNTU1ePjhh/HII48M6/VGc5vopEkmrFlTDEHgwBhLthV+/TRzHJd8jOM4nDkTw+9+15lxh0hBgYAHHiiFwyFcML2BxzmOQygk4be/7cy4JiWKHP72b4tRXW1Kyf9Q6Sn/Av/xH904fDjzmtTNNxdg/nx7Snrnv7baYxzH4b33vNiyJZBxepdeasf11xdknN7evSG8/npvxunlu8yQzFyoTTTnq30Gg0EcP34c3/3udzFv3jwYDAZs2LABq1evBgAcPnwYzc3NaGxszHVW8urw4TAOHAijocGOoiIRmzb5EQopVYaJE02YPduKzz4Lob09jsWLHRd4tQtra4th+/Yg6uosqKsz49NPg8lF04qLRVx2mQPHjkWwf38YCxfaUVCgrd0wEJCwebMfxcUGLFxow/794WRnldXK48ornfD5Eti2LYipU5U8aRGPM2zd6ocsA1de6UBbWxyffaY0T/A8cMUVDphMPDZt8qOoSERDg11Teowx7NoVQkdHHFde6YQkMWzZEkiurDl3rhVVVSZs2aLkSY/PMN9lhuhD9yD6D//wD7jhhhtQU1OD1tZWPP744xAEAXfeeSdcLhfuvfdePProo/B4PHA6nfjRj36ExsbGYffMjxUDX/IJE0ywWnkcOhRBX59S6zMaOdTVWXD8eASHD0ewYIH6kKRMeL0SPvssBJuNR22tCSdORHD8uHJpWVNjwvz5drS0xPDZZyFMnmzWHEQjERl79vSjttaE+norWlpiyXZRl0vAggV2dHcn8NlnITgcguYgKssMBw6EIUkMCxfa0NUVT6Ynihxmz7bCbGbYu7cf48ebNAdRADhxIoojR8KYO9cKSQL27+9Prr5ZWWlEUZEhmafLLtPjhzC/ZYboQ/cg2tLSgjvvvBM9PT0oLi7G5Zdfju3bt6O4uBgA8M///M/geR6rV69OGWz/TbV+vQ9ut4hVq1zJoUfHj0fwu9916j5IGwB27w7h6NEIGhsduPpqpcmjqyuOF1/sgt+vf3onTigdSzNnWnHffSUAgP5+GWvX9sLrlXLSsfTSS90YP96UTE+WGbZuDaCzM56TjqU33uhFcbEB3/1uEQRBudTesyeEl14Kors7ofkH6evyXWaINroH0ddee23I/WazGc8++yyeffZZvZMelfr6JESjDB6PiKIipfPl1KkoWluVS209hhudLxCQEQzKsNl4VFYqHRWRiIzW1lhOOiAiEYbW1jhmz0Yyvb6+BLq64ggE9E8wkWBob4+jvNxwdq12DvE4S1mHXk+MAV1dCRiNHMrKDDAalQEtO3cGs+q0Go58lxmiDd07TwghGuS8Y+liN368CYWFIk6ciCbHbkajDHPmWHHqVBT9/frW1srKDCgrM6CrK4H+fqXjpb9fRn29Fe3tcd1rT263gPHjTUgkGPbsUdJLJBimTrWgtzeR1ZCfoZjNHCZPNsPhELB3r9ImyhhQUWGEzcbjyBF9x1EKAjB5shkul4j9+8PJQfVWK4/Zsy04elTf9wfkv8wQbSiI5till9pRW2vCc891Jm/Ta2y049ZbPXjjjV4cPBjWNb0ZMyy46ion/vCH7mRAmTDBhHvuKcbHHwfQ1ubTNb1x44y49VYP3nvPhzfeUIb3uN3KsKvW1hiamvQNMi6XiJtu8uDgwXAyPVEE7r23BDYbj1OnOnVNz2DgsGKFG/E4w+9+15nsWLrppgIsWeLE739/7jG95LvMEG3ocj7H9LgdkKT6pp/Tb/r7+6ahIEoIIRpQECWEEA2oTVRn8ThDT08i2fgfCEjo7U0k73QBlCFHPT0JRKMyGFMGystydvNDSpIypGhgFqVwWHnt828FVMtTX19qnoaPweeTzs5nyRCLKa8dDp/r7Ph6nvr7lTxl23YYDMrJcyjLQG9vAsHgufGSjAE+n4R4XDmHsZg8KE+ZGPh8YrGBzyeBRCL18wmFlDwlEsr7VfKUXXr5LjNEX3m5d15vo/neeZ4HzGYe8ThDPM5gMikTV0QicrLAiyIHo5FDNCpDkpQeZwAZTwYCKO1nZjMPWWaIRhkMBg4Gw7nXHm6eMmE2c+A4IBxmEATAZOIRi7HkLEfDyVMmjEYOonguvxYLj0SCpfxQKHniEA7LyTwNvN9Mff3zsVg4MJb6+QwnT8OV7zJDMnOhe+cpiBJCyBBGZFJmQgi5WFAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRgIIoIYRoQEGUEEI0oCBKCCEaUBAlhBANKIgSQogGFER1JgiAzcbDaBz+zLoWCw+LJbuPguOUpSoGJqQYDpOJg9XKZz35r8XCZZRfg4GDzcZDyHJRTJNJeT4/zCQHPgODIbs3OJBfcZhznA18BtkuIJfvMkP0RZ+CzsaNM+L++0uwcOHw1j03GjncdpsHt9ziyepL73QKWLOmCCtXuocdFJcvd+Gee4rhdmce1UQR+Pa3PbjzzsJhB+758234wQ9KUFNjyjg9AFiyxIl77y1GUdHwotqECWY88EAJ5s61ZpXerFlWPPBAKSZNMg/reI9HxPe/X4ylS7ObFCffZYboi+YT1ZnBwMHjEWG1Du/3ieMAl0uAJGW3LATPA263mNFclna7ALdbSK6hnhkOTqcAi4UHx3EALjwJmMXCw+MRs/7C22w8CgrEYed34DMwm7OrI5jNHDweIbk88oUIAlBQIMJuz24RwHyXGaIvqokSQogGFERzrK7OjIULbSmXvuXlBlx2mR3FxfpfCFRXG3HppXYUFJy7VHe5BFx6qR3jxxt1T6+wUMRll9lRWWlIPmYycViwwIbp0y26p2e18li0yI7Jk89danMcMHu2BZdcYoMo6ls1EwRg7lwr6uutKW2yEyeasGiRHXa7/l+hfJcZog0F0RybN8+Ga65xwWY7F9TGjzfh2mvdqKzUP6hNnmzGtde6UVx8LqgVFopYtcqNqVP1D2rl5QZce60bEyacC2oWC4+lS12YP9+m++WmwyFg+XIX6uvPtXcKAtDY6MCSJY6MOmeGw2DgsHixE5dd5kgJ0LNnW7FihQsuV5a9ZUPId5kh2tDPWh6YTDyuvdaNWExptywqMlzgGdpwHLB4sSPZsWK3C8Pu2c5Wfb0VFRXK+zIYct9zPH68Cbff7gEAcByHoiIx6zWVhsPjEbB6tQeyrLQBjxuX22CW7zJDskdBVGeSpCwWd27dHWUhsqlTzeB5pSaTSDCEwzJ4noPZfG59omwWalHW/pGRSLBk4IpEGGpqTMmOGEliybV4LBb+7N8y5KxijrJuEs8zmM0cBEFZ16i42ICKCiWwDKytJEnn8hQOZ5seEIsp+TUaOciy8n7tdh6zZ1vBcRwYU9KLRpU8iaKS3sCaT5ka+HwEAWc/Hxk8L2DmTMvZzjRlMbxwWIbBwIPjlPxls74SkP8yQ/RFayzpzGDg4HYLmDFDaUfbutWPnp4Ebr+9EG638pu1d28Imzb50djoQGWlER9+6ENXVxxer5Txl4LnlZ7h6mojFi924ssv+3HgQBjf+lYBamuVIUWnT0exdm0fpk41Y84cGz7+OIBTp6JnV/zM/D0WFAjweEQsX+5CR0ccH38cwOWXOzB/vjJEx++X8Kc/9cDlErB4sRMHDvRj375++HxSVoHG4eDhcAhYtswFjgM+/NCHSZPMWL7cBY7jkEgw/PnPvQgGJVxzjQtdXQls3epHMCgnV9DMhMXCw+HgcemlDlRUGLB+vR8mE4dbb/XAYFB+FD780IdDh8JYutQFQQDWr/cjEJDg92d+QvNdZkhmLrTGEtVEdRaPM3R1JeD3K0vuer0SuroSKbWi/n4ZnZ3Ksr/RqIzu7gT6+rKIZgBkGejpScDhUGpMfr+Ezs548jIQUGpynZ1xlJcbzuYpge7uRNbvsa9P+eJGowyhkPJezg9WksTQ3R2HLLNknrq6sk8vEJARiQzUxICurgRKSlKXTO7rSySXcg6FJHR2Zp9eOKzUMkMhCdGoiO7uOKxWPiVYBQJKGuGwDFHk0NUVz7ommu8yQ/RFQTRH9u7tx5dfhlMus79u0yY/eJ7L+rLzfKdOxfD733cNuZb8F1/0Y//+sC7peb0S/uM/upNthGqam2N44YWh8zRc8TjD2rV9AJA2/z6fhJdfHjpPmdi4Ufl84nGmOoYzkWB4881eAPp8hvkuM0QfFERzRJZxwS+zJEGXAAMotbGBNdbT9YgPJ0+ZuNCa7ufnSQ/DCRx6pjeczyeRAIZzw8Fw5LvMEH3QECdCCNFA9yA6fvx4cBw3aHvwwQcBAEuWLBm074EHHtA7G4QQkhe6X87v2rUL0nldvvv378c111yDW2+9NfnYfffdh6eeeir5t9Wa3UQRY4EoKhNGJBIM0ajS+cLYuceyHfaTjiAAoshBkpBMT5IYjEalbS+b3vih8DySg9AH0ovHGQwGZahRIvv+HVUcp/Rmcxx3tiNHGeojCBwMBk7Xy/kBBoMylCsWY2BMTuZj4Jzq3Tue7zJDtMn5EKeHH34Y69atw9GjR8FxHJYsWYI5c+bgX/7lX7J+zdE8xOnrVq1yobLSiM2bAwiFlAg2caIZs2dbsX69D0eORHRNb+FCG+bPt2PbtgA6OpQJMYqLDbj0Ujv27u3Htm1BXdObONGElSvd2L+/H0ePKu/FauVx5ZVOdHTE8fbbXl2DTGGhiJtvLkB7exy7d4cAADzP4YorHOB54M9/7k2OidWD0chh9WoPeB7YsiWQbI+cO9eGykoj1q7t1TTyQE2+ywwZ2ogOcYrFYnj55Zfx6KOPJgcpA8Af//hHvPzyyygrK8MNN9yAn/70p0PWRqPRKKLRaPJvv9+fy2xrYjQqY/4G3u/AbEednXH4fMoXorjYAEEA3G4BpaXKnSjxOENfXyLjgCMIylRsA4OyHQ4BggD09ibQ2qoEUZ5XalIOx7n0ZJmhtze7caIej5AcL6nMrqQM+RlIz+Hgk++9tNQAxgDGGLze7MaJDswaBSjzAIgih2iUJdMTBKXTyW4XUFJiQDSqpBEISFmNE7ValXGpgFILNRqV9Nra4snOrbo6ZWhTYeG5cx8Oy1mNE813mSH6ymlN9PXXX8ddd92F5uZmVFRUAACef/551NTUoKKiAvv27cNPfvITLFy4EP/1X/+V9nWeeOIJPPnkk7nKpq5qa024665C8DwHxoC33urFkSORs5eCyjGCoHw5r7uuIDlJR1tbDH/4Q3fGQcbtFnDPPcWw25Uv/SefBPDppwHEYucu+3he+aIuXGjHlVcqv6j9/TJefLELvb2Z1aJEEfjud4uTtz0ePhzGX/7Sl9JUMHCpO3GiGTffXJC8q+hPf+pN1lYzccMNbsyZYwMAdHXF8eqrPejvl1Mu3Y1GJaDdfXdRMuBu3OjDJ59kXvNuaLBh+XI3AKWJ4pVXes6OvT2XnsHAwWzmcMcdRSgrU4La/v39yWFYmch3mSGZGdGa6AsvvIBVq1YlAygA3H///cn/z5o1C+Xl5Vi6dCmOHz+OiRMnqr7OY489hkcffTT5t9/vR1VVVe4yrkF/v4zDhyPJYUa9vYlkzWjAwDCV06ejyXva+/oSWQ0/iscZjh2LJOfObG+PD7qcVW6VZOjoiOPQoTAAZQD++QPyh4sx4NSpKIJBJWK2tMQGpTcwEL+3N4FDh5RzwRiSz8lUW1scZrOSb59PGhRAB95PICDhyJFIcob5np7sLrN7e6XkeYrHGfz+wTXoeJxBlhmamiLwepV0WltjWaWX7zJDdMZy5OTJk4znefbmm28OeVwwGGQA2HvvvTfs1/b5fEqPAm200UZbjjefzzdkPMrZONEXX3wRJSUluO6664Y8bu/evQCA8vLyXGWFEEJyJieX87Is48UXX8SaNWsgnrfa1/Hjx/HKK6/g2muvRWFhIfbt24dHHnkEixcvxuzZs3ORFUIIya1hX0Nn4P3332cA2OHDh1Meb25uZosXL2Yej4eZTCY2adIk9uMf//iC1eWvo8t52mijLV/bheITTYVHCCFDuFDvPN07TwghGlAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRgIIoIYRoQEGUEEI0oIXqcqy83JCcpu7r2ttjCAT0naa8sFCEx6P+sfb1aVsqWY3dzqO83Ki6LxQ6N8eoXoxGDlVVxuQcnueLx2WcPh3TdfZ+ngeqqowwGgfXN2SZoaUlNmjGJa3yXWaINhREc2zJEmdy/sev+8//7MUXX/Trml59vRVXXaV+d8XWrQF88IFP1/Rqaky4445C1X1HjkTw8svduk4aXFAg4s47C2EyDQ5qPT0JPP98Z1YTMadjNHL41rcKUFJiGLQvFmN44YVO3X8o8l1miDYURHOkpsaICRPMKC01QBAG15oYY5gxwwKnU8Dnn4cQCmn74hcWipg1y4oJE0yq6QHA+PEmXHWVE/v392te0sJq5XHJJTaMG2cEzyNl5YIBRUUirrrKiaamKJqaoiqvMnyCoCzJUV5ugMHAq75Hm43H5Zc70Noaw/79YU3pAcD06RZUVhrPrhYwOD2DAViwwI62thg+/zykeT2pfJcZog8KojnAccps5ddco9zfrzY9AWPAjBkWTJpkxtGjEU1fCI4DSkpELFvmPDs7OkumMRDbGFO+pDU1RnR3x9HdrW1ZCZuNx5IlDlitwtnXZ4PSKyoSsXSpEx995MfJk1FN6Ykih0WL7KioMKqmBwys7eTAV1+FceBAWFN6HAfMmmVBfb1tUHoDM/XzvLKmVWenCV9+GUYioe0zzGeZIfqhCUh0VlZmwNVXO1FSYkheAkYiMt57zwuzmcc117hw9GgEu3YFcdllDtTUmHDqVBStrXF88IE349qMzcZj5Uo3SkpEjBtnBMdxkGWGjz7yo68vgZUr3QgGJWzY4Me0aRbMnWvFmTNxdHXF8d57PgQCmTUgCgJwzTXKQmo1NabkSp+7d4dw6FAYS5c6YbcLeO89LzweEUuWONHTk0BnZxybNvlx5kzml76NjXZMnmzG+PGm5Az+J05E8MknASxYoOxbv96HSETGypVuxGJKW+WePSEcOJB5jbSuzoz58+2orDTA5VLqGT09cXzwgQ+1tSY0NNixY0cQTU1RLF/ugt0u4OTJKI4di2S1HEm+ywzJzIguD3IxMpt5VFcbU9rsZFlZ5Mxm48GYsoBac3MMc+bI4HnlSyTLA5fEmf2miSKX8mUf0NWlBC5JYohEGE6fjqG8XPmCFhWJMBo5GAY3810QxwGlpQaUlysLpw3wehM4fVpZKsRiYThzJp7s4HG5BJhMfHLto0x5PMoPhMFwrtoZCslobo6hrk4CY0BHRxz9/TJkmcFqVT6D48ezWxXT4RAGfYaxmHIOB85zb6+ElpYYYjFleehx44zo68uuRyvfZYboi2qiOhNFDg4Hj0WL7LjiCuXXS5aVdXp4XtkXjTL098uw2ZQvzauv9qC9PZ7VSpE8r6yGOXmyGTfeWJC8nA8GZSQSDE6nAFlmCARkmM0cLBYef/2rF4cOheH3S1mtYe5wCCgpEXHXXecWhevvlxCJMDgcPHieg98vQRQ52O08tm0L4pNPAggGB6+NNBw2Gw+7XcAddxQmV7qMRmWEQjKsVh4mE4dAQAmgTqeAY8ci+MtfvMk8Zcpk4mCz8bjuugJMm6Z08CQSymeo7BMQCkmIRpX0vN4EXn21B36/lNUldr7LDMkM1UTzLJFg6OuT0NYWR1NTBCUlBthsAtzuc6fabOZgMnHo7k6gtzeB7u7svwyyDHi9Ejo64jh5MoqCAhEFBWJyyV8AEAQOHg8PrzeB9vYoOjri8Hqz//IFAhJ4Hjh5MorCQhHFxSKsVgHnr3pdUCCiv1/CyZMxtLXFs66lAUqtMxZjaG6OIpFgKC83wGTiU2puTqeAeFxGS0sMLS2xjFcxPV80yhCNSjhzJgarlUd5uQFGI58ydMxmE2CxMLS3x9HeHkdPTyLrVTfzXWaIvqgmmiMcp7Qf3nFHIaZPtw7azxjDn/+sDFfRY1wjxym10quvdg0xxMmPDz7wQZahy7AjQQBmzLDi9ts9qr3zhw+H8corPUgkmC7pDVzG/u3fliTbRs/X3R3H73/fhWAwuxq2WnpWK4977y1J1oDPF4vJeOGFLrS26jM2Nd9lhgwP1URHCGNAIqH8G4/L+OKL/uSlZXm5ARMmmCDL0O3LwJjyWrLMwBjDV1+Fk7U/l0vAjBkWXdMDzqUHAE1NkWSnkcnEob7eevYc6BNAAaTkv7MzjiNHlDZPngdmzlSCTiLBdAmgA+klEkrmAwEJX37Zn3ztSZNMKCgQIctM188wn2WG6IOCaB7EYgybNweS66A3NtoxYYIpZ+kxBuzaFUoGmQkTTGkHb+vl0KEItm4NAADcbgFTp+Y2vZaWGN55xwsAEEWgstKYbC/MBZ9Pwvvv+5JtujfdVICCgtx9ffJdZkj26N55QgjRgIJojogiYLHwSCSAcFhOXvYCyiViOCyD5zmYzRxUmhMzxvOAxaK8UDgsQ5LOpSfLSnqAcoygflt2RjhO6ewQBA7hsJy87AWUmnAkouTBYuEh6lRhM5k4GI0cIpHUXn7GlN76aJTBbFaO0YPBwMFs5hGLyYhGU9sI4nGGSESGwaCMDtBDvssM0Qd1LOXIggU2XHqpA1u3+tHcHENfXyLZlmWx8HA4eDQ2OlBRYcDrr/cmL9uyVVtrwg03FGD//n58+WU/fD4p2VtsMHBwuwVMn27BnDk2vPOOF0ePZjeGckBBgYDbbitEV1ccH3+sDF8auGed55Xe+aoqIxYvdmLnziC2b898EPr5DAYOt97qgcHA4cMPffD75ZQbBQoKBHg8Iq65xoWWlhjWrfNqSg8AVq50YcIEE9av96O7O3WEgcPBw+EQsHSpUg5ff71H80Qk+S4zZHioY2mEWK08SkpERCJs0MxJ4bCMcFiG0cihuNiQvOtHC5OJQ0mJ8nF+/b74eJyhq0u5zbOkRNSl5iQIHIqLRfj9Ejo7U9OTZWUykKIiESUloi5tlRynDLrneaC7OzEoYPX1KWMqCwvFjO/CSsflElBYaIDPlxg0RCsQUGq+DocAg0GfmmG+ywzRB13OE0KIBlQT1ZnTKWDKFDMEgcPOnSH09aW/5DpxIopoVMbEiSYUFYk4eDCc8fAco5HD9OkWOBwCdu0K4cyZWNpj29ri2LkzBI9HxNy5Vhw8GM74jh6OA+rqLHC7Bezb14+2tvT3wnu9EnbtCgFQLlWPHo1kNci/ttaEkhIRTU1R9PfLaYf4RCIyPv88BFlWJgY5fTo2ZP7SKS01oLpauY1z9+5Qsj356ySJ4cCBftjtAurrbejqiuPEicxnq8p3mSH6opqozkpKRNx4YwFEkcNbb/UN+SXevTuE99/3Yf58G5YscWZ1iWaz8Vi1yo3x4034y1/6cPhw+rbOo0cjeOutPlRWGnHtte60E/8ORRA4XHmlA4sW2bF+vT8ZJNV0dMTx1lt9YAy48cYC1QHrwzF3rhUrV7rx+echfPSRP6UT63yhkIx33/Xh9OkYbryxAJMnm7NKb+JEE266qQDt7XG884437STIkgRs3hzArl1BLF/uwvz5tqzSy3eZIfqijiWdDdzH3t4eG9aMRYKg1OxkWbnDJ5ua6LRpFoRCEo4dG14taMIEE5xOAQcPhjPuDOE4YOpUM0SRw8GD4WEN/C4vN6CiwohjxyLw+TKviY4fb0RBgYjDhyPDmnDZ7RYwaZIZLS0xtLdnXhMtKRFRXW3CiRMR9PZeOL9mM4e6Ogt8PimreVPzXWZIZi7UsURBlBBChnChIEqX84QQogEFUUII0YCCKCGEaEBBlBBCNKAgSgghGmQcRLds2YIbbrgBFRUV4DgOb775Zsp+xhh+9rOfoby8HBaLBcuWLcPRo0dTjunt7cXdd98Np9MJt9uNe++9F8GgtnurCSFkJGQcREOhEOrr6/Hss8+q7n/mmWfwq1/9Cs899xx27NgBm82GFStWIBI5Nwj87rvvxoEDB7B+/XqsW7cOW7Zswf3335/9uyCEkJHCNADA1q5dm/xblmVWVlbGfvGLXyQf83q9zGQysVdffZUxxthXX33FALBdu3Ylj3n33XcZx3HszJkzw0rX5/MxKEsc0kYbbbTldPP5fEPGI13bRJuamtDe3o5ly5YlH3O5XGhoaMC2bdsAANu2bYPb7cb8+fOTxyxbtgw8z2PHjh2qrxuNRuH3+1M2QggZDXQNou3t7QCA0tLSlMdLS0uT+9rb21FSUpKyXxRFeDye5DFf9/TTT8PlciW3qqoqPbNNCCFZGxO984899hh8Pl9yO3369EhniRBCAOgcRMvKygAAHR0dKY93dHQk95WVlaGzszNlfyKRQG9vb/KYrzOZTHA6nSkbIYSMBrrOJ1pbW4uysjJs2LABc+bMAaBMFrJjxw788Ic/BAA0NjbC6/Vi9+7dmDdvHgBg48aNkGUZDQ0NemZnVKivt6adAu7LL4eejzMbEyeaMHGi+hRwTU1RzcuCfF1pqQGzZ1tVZ3bv6opjz55+XdOz23ksXGhXnQIuFJKwc2coZf0lrUQRWLjQrjptoCQx7NoVgt+v7xrG+S4zRJuMg2gwGMSxY8eSfzc1NWHv3r3weDyorq7Gww8/jH/8x3/E5MmTUVtbi5/+9KeoqKjATTfdBACYNm0aVq5cifvuuw/PPfcc4vE4HnroIdxxxx2oqKjQ7Y2NNI4bWA/dghkzrIP2M8bQ1RVHR0dct6nMeF6Z5m7JEvWauiD4cfx4RNf0SktFLFniAKcSRQ8fDuPLL/shSdBl7XmOAxwOAZdf7oDZPPgiqrs7jn37+nVb657jAKORx4IFdtWgFovJOHIkgmBQ0uWcjkSZIdplPBXepk2bcNVVVw16fM2aNXjppZfAGMPjjz+O559/Hl6vF5dffjl+/etfY8qUKclje3t78dBDD+Gvf/0reJ7H6tWr8atf/Qp2u31YeRgLU+HNnm3F/Pk2lJUZVGsxjDF0dMTR25vA2297B63hk6mqKiOWLnWiqMgAj0f9t7GvL4Hu7gQ++siPkyczn/fyfC6XgOuuc6OwUERZmUE1iIZCEtra4tizJ6S5RiqKHK67zo3ycgMqK40QhMHpxWIyWlpiOHEiio0btY/guPJKByZPNqOy0giTaXDQlmWGM2eUOUvfftubXBgwW/kuM2R4dF+obsmSJRgq7nIch6eeegpPPfVU2mM8Hg9eeeWVTJMeE0QRsFoFlJcbMGlS6mV1IsEQCknJWpLHI8LtFlFYKCIeZwgGM69ecJxyiVtSoqTH8+eCiywrrzmw9K7ZzGPSJBMOHBDR25vIugZls/HweERMnGiGxXIuuDDGEAqdWz7ZYOAwcaIJHR3KshmhkIREFgtUWiw87HYe48ebBtUIw+FzyxkLAofx402IxxlcLgGRiJzVCpxGIweLhUdVlQkTJqR+htGonFwuhOOA8nIjzGYebreAQEBOu5TIUPJdZoi+aFJmnY0fb8SttxbCYuEHXXK2tETx6qs9ydngr7/ejRkzLAiFZLS2xvDHP/Zk3J7ncgn47neL4HYLsFj4lBphICDhP/6jO9lm19Bgx5IlDoTDMvx+CS+/3D2smdvPJ4rAnXcWoarKCKuVTwnaksTw+us9OHVKWedp4kQTVq/2IBZT1mj/8597cfx45jXgVatcqK+3wWbjB9VA16/3YfduZYmSggLlXBgMPPr7JWzeHMhqqeb5821YutQJq5WHwZD6Ge7dG8J77/kAKD8Sd91ViJISA0IhGQcO9OOvf/VmnF6+ywzJDC2ZnGeiyMHpFFQvNxMJJbAN1MbicQaO42C3C7DZhKyW3eV5pZ3QalW7/AOCQSkZRKNRGRzHwWoVwBhSAuDwcbDZ+LTrM4VCcjK9gaU8zGYeRiOX9XpAFgsPp1M9vUjkXHoGAwfGlH9dLhFGY3bpGY3K89XEYiyZntHIQZKUGrDTKaTUyjOR7zJD9DUmxokSQshoRTXRPHK7BVx5pTPZDllWlt3ql8NlNHJobLQnl0UeP96U0/Q4TlmZs6ZGSaeoSMx5TWnyZHPyEli5/M5tghUVRlx1lXJpJwiA05nbeki+ywzJHAVRnTE2sLGUxzgOcLtFLF3qOu9xluz00dIynS49s5nHFVc4z3s8t+nxPId58+znPc6Sx2pNT5ZZMiAPpMdxyiqbdXWWlPQG3qMW56d3vnHjjBg3zphMb+DYgXxlYyTKDNEPdSzpzGbjUVVlxKxZVsyZY8WWLQF0dMSxcqU7pV2PMYZPPgmiqUkZ/B6JMJw8Gc34i2EwKD3S48ebsGSJA/v29ePLL8O46ionKiuNKcceONCPzz9XOmEkCTh5MprxsByOA2pqjCgtNWLlShfa2+PYsiWASy6xDhrb2NISw6ZN/mRwa2mJZdWbXF5uQHGxiOXL3eA44P33faiqMuLSS+0pHWl+v4T33vMme+s7OxPo6cl8OIDHI6C01IDLL3eiosKA99/3QRQ5LF/uSmnXjccZ1q/3oadHGfzu80lobc18IHy+ywzJDHUs5VkoJOPQoUhyKM7p0zE0NUWxaFEC/NkrP4OBg9HIobU1hoMHtd1BFI8zHD0agSAAjDnQ1ZXAwYNhzJplgculfAEFgYPZzKG7O6E5PcaAkyeVYChJTvj9Eg4eDKO83ICaGqXDheOUzqBgUNmn9Uve1hZHT08CixfL4PmBtdYZ6uvPBW2LhU8Ofh/O2vRD6e2V0NsrYfZsK8rKDDhxIgKeV5pGBpoLTCYOsgw0NUWGtVb8UPJdZoi+KIjmQSQi49VXe5JfiEsuseHqq3N3/z9jwNtve5Nf+OpqE265xZOz9ABg27Yg9uxRarkul4C77irKaXpHjkTwm98oczSIIofbbiuEyZS79tCurjh+//tzcz4sX+5GXZ367bV6yHeZIdmjIJoHjCmXegO01pSG4/zL5oKC3N/ZEg7LCIeV/yvtkrlNLxZjiMWU9yWKyqD0XAZRSULKHUIDTQa5MhJlhmSHhjgRQogGFEQJIUQDCqI5NpruKBlNedFiqPfxTXiP34T3cDGhNtEcu+wyB2bOVMYxdnUlsGnTudmFFi2yY+pUpXOir0/Cxo2+rCboON/MmRYUFysfazgs48MPz6VXV3euxz4aZdiwwYdAQFtbW1WVEbffrnRayTKwebM/OWyqosKI224rBKAMcdq6NaB5LkyXS8S3v+1JjpX8/PP+5JAfh0PATTcVJCdA+eKLfhw6pK0n22DgsGqVG5GIcp6OH4/is8+UDjRR5HDNNa7kpCOnTsWyulf/6/JdZog2FERzJJFgCIcZSkoMKClRhq6YzVFw3Ll9RUUGFBUp+9rbY2fHPGY3HkiSlKDpdIpwOpWPNRiUYDAEIMsM4bAMu13A5MnKlzMclrBlCw8guyDKGBAOM1gsfPI1ZZlh164golEJkchAeuazx7Oz0+FlH0QjERmMIWXS6RMnlHGS0agMSQJqa8/tO306lnVawEDnFUNV1bk7vQY67OJxhmiUYdy4c/uymTHqfPkuM0QfNNg+R6xWZfq288ViDF6vBIuFh8ORui8eV/Zl+2kok2akTkih9CgnIAgc3O7UfbKs7JOy7LgXBKCgQEwOwQGUwOr1SpAkhoICEYKQus/nk7Kec1O5e0cYdFvnwPRzBQWD9wWDsqZebadTgNmc+prhsIxAQIbDwQ+acCQSYZpmuc93mSHDc6HB9hRECSFkCBcKotSxRAghGlAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRgIIoIYRoQEGUEEI0oCBKCCEaUBAlhBANKIgSQogGNItTngjCuXkiZTn3y2fwPJKTgzCGrCcaGa3pcRxSJjjJx3Rw4nnfFknK/ZLF+S4zJDsURPPAbOZw002e5PK3+/f349NPtc87mQ7HAStWuJJTuLW2xvDOO96cfgkXLLAnV98MBiW8+WZfTtcFmjTJjKuuUiaFkCSGdeu86OjQNlfpUIqKRHzrWwXJJZO3bPFrnqt0KPkuMyR7FERzxGLhYLEICAYl8DyHiopz80C2tirzXNrtPAwGDn6/pLnmZjBwcDgEhMMyIhEZJSUGjB+vBFFJUqpMZjMHq1XJU7ZT0g3geWVVz3icIRhUpqIbSE+Zfi81TwMTF2sxMKG03y/BbudRU2MEx3GIx5VF6r6eJ63O/3xMJg7V1UYYjUp1e88eYVCetNZM811miD6oTTRH5s614YEHSlBba0p7zNVXu/D975fA49H+WzZ+vAk/+EEJ5s2zXTBPEyakz9NwFRSIuOeeYixfnn5KwupqI37wgxIsWJA+T8NlNHK45RYPbrvNA6NRff0Mt1vA975XjBUr3JrTA4Bly1z4/veLUVCg/vkYDBxWr/bg9tsL0+YpE/kuM0QfFER1ZrPxmD7dgnHjjLDZeAhC+i+XycTB4eAxZYoZEyaYUiY4Hi6DgcOUKWbU1ppgt/NDfpkNBg42G4/aWhOmTjVn9cXnOKC21oQpU8xwOASYTOkzLYpKepWVRkyfbhk04fBwVVQYMG2aBQUF4qCJkFPzxsFq5VFcLGLGDAuKirILNB6P8vySEgOsViHt58JxgNnMw+0WMG2aBZWVhqzSy3eZIfqij0BnZWUG3HFHYbJ98EIMBg7XXuvGypXuZHtbJux2Hjff7MGVVzqGdTzHcbj8cgdWrz7X3pYJQeCwfLkL11/vHnYQnjnTgjvvLERlpTHj9ACgocGO227zoKBgePmtqjLirrsKMX26Jav06urMuOuuQtTUDC+/LpeAW2/14NJLh/cZfF2+ywzRV8ZBdMuWLbjhhhtQUVEBjuPw5ptvJvfF43H85Cc/waxZs2Cz2VBRUYG/+Zu/QWtra8prjB8/HhzHpWw///nPNb+Z0YDjBjalcM+ZY8VVVzlhtZ4LANXVJixf7kJFhfG8c6A1TeV1Jk40Y/lyV7ItDVBqVtdc40qud6RneqWlBixf7kq2hwKAxcLjyiudmDvXdt6x+qRntwtYutSFmTPPBRxBABYutOPyyx0wGM6VKS0GXkMUOVx2mQOLFtlTaojTp1uwdKnz7JIs2j7DkSgzRD8ZX++EQiHU19fj+9//Pm6++eaUff39/fj888/x05/+FPX19ejr68N//+//Hd/61rfw2WefpRz71FNP4b777kv+7XBk9ys+2jCmLNg28MWYPt2S7HAY6OApLzegvNwAnlcWcBt4TrZkmSXTrKkxorramJKe0yng8ssd4Ljz08t+iI4sK6/N80BxsYgrrnCkpGcwcGhosJ83PIdpGhlwfno2G4/GRnvy8YFF2gZqcQPnVMv7U4ZoKemJInDJJefadAfe46RJZkyaZE5JL9v3OBJlhuhH0xpLHMdh7dq1uOmmm9Ies2vXLixcuBCnTp1CdXU1AKUm+vDDD+Phhx/OKt3RvMaSxcKjosKA+nor5s2zYeNGP06diqoeu3ixE9XVRrz9thft7XG0tMQy/uKLIodx44yorTVh2TIn9uzpx969IdVjZ82yYv58Gz76yI/jx6NoaYkhHs8sQY4DKiuNKC014Lrr3Ghri+Gjj/yqx44bp+Tpiy/68fnnIbS1xbMa9lRSIqKwUMS117rBcRzeftuLeHzw6zidAq6/vgDt7XFs3OhDd3cCXm/mXdgul4DiYhFXXunEuHHK5+P1Dh6IKorKZbUgKHnq6UlkNcwq32WGZOZCayzlvIvP5/OB4zi43e6Ux3/+85/jf//v/43q6mrcddddeOSRRyCK6tmJRqOIRs8VKr9f/Us7GoTDMo4fj2LcOKU22NYWx7Fj6l+IuXMlyDLQ3BzLeoxjIsFw8mQUZjMHxoDe3kTa9CoqlDy1t8fR1KR+zIUwBrS0xBCJyJBlZShRuvQGLn97exM4fjy79ACgs1MJhtEoA88znDgRUV2euLBQhCQxhEJS2jwNh88nweeTMG/ewOcTRWfn4CBqNHKIRBgMBuDEiQgikeyiWb7LDNFXToNoJBLBT37yE9x5550pkfzv/u7vcMkll8Dj8eDTTz/FY489hra2Nvzyl79UfZ2nn34aTz75ZC6zSgghWclZEI3H47jtttvAGMNvfvOblH2PPvpo8v+zZ8+G0WjED37wAzz99NMwmQaPkXvsscdSnuP3+1FVVZWrrOec2y2guNiA/n4Zx45FEIvl9n4+i4VHZaUBPM/hyJEIAoHcjtIWRaCqygSXS8DRoxH09OT2nkyOA8aNM6KgQMTJk1G0tua+hlZSIsLjEdHZGUcioa3NdzjyXWbI8OUkiA4E0FOnTmHjxo1DticAQENDAxKJBE6ePImpU6cO2m8ymVSD61hVV2fBdde58ec/9+Kdd/pz3qZVXm7Ad79bhE8/DeI//qM75194u13Abbd50NYWx8sv5z49QQBWrnTDZuPx+993IhTKfYBpbHSgvt6Kf/u3Lpw5k/t2yXyXGTJ8ugfRgQB69OhRfPTRRygsLLzgc/bu3Que51FSUqJ3dvKuoEBAfb0VPM/ho4/86Oo6VytyuQTMmWOFIHDYvNmP9va45i+D2cxh3jwbTCYemzb5U9o6jUZln8XCY+vWAE6ejGoOaDwPzJljg8slYPv2YEq7HMcpveQFBSJ27w6huzuhy62JdXVmlJcbcehQGMGgnOyxBoDJk80YN86IEyciCIdlRKNM8zkdN86IyZPN6O5OoKPDnxKUy8sNqKuzIBiUsHVrAD6f9ts9811miL4yDqLBYBDHjh1L/t3U1IS9e/fC4/GgvLwct9xyCz7//HOsW7cOkiShvb0dAODxeGA0GrFt2zbs2LEDV111FRwOB7Zt24ZHHnkE3/nOd1BQUKDfOxshhYUili51YevWAD74wJeyz+0WcPXVLuzaFcS6dV5d0rNYeCxe7ERLSwwvv9yd8gUzmZSB9b29Cbz4YpcuNUKe57BwoRKYn3uuM+WeeI4D5s2zweMR8dxzHQgE9KkRTp9uwaxZVjz/fCfa2lIv1adONWPBAjteeKETzc0xXdKrrjZi2TIn/vSnXuzb15+yb9w4Zd+bb/Zh1y71URCZyneZIfrKOIh+9tlnuOqqq5J/D7RVrlmzBk888QT+8pe/AADmzJmT8ryPPvoIS5YsgclkwmuvvYYnnngC0WgUtbW1eOSRR1LaPAkhZKzIOIguWbIEQw0tvdCw00suuQTbt2/PNNkxz2rlYTTyCIW0z6A0HMqMTbxuMyhdiNHIwWzmEYsx9PfLOb/kFEXAZOLBGBAK5X5GI55Xav08zyEUkjMeX5uNfJcZkh2aCiYPLBYed95ZiFiM4d/+rSun82wCymX1tde6UVZmwFtv9cHrTeS8c6ex0Y5LLrHhgw98aGmJ5bxzZ8oUC1atcmP79gB+97vOnI84KC5W7m8/cSKC3/ymI+efYb7LDMkeBVGdhcMyTp6Moq/v3LAexhiCQQn9/TL6+vTpbBmQSDA0N0fR1ZVIqf3198vw+ST09SV0mVtzAGMMra1xmExcym2HkYgMv19Jz+fTN6B1dydw6lQ0pTYWjzP4/cog/L4+fdPz+yWcPBlFKHTudSWJwe+XcpJevssM0Zem2z5Hymi+7RNQhtx8/d5tQUDynvVcpPf11+Z5pUaaiy+f2mtznPJ4vtPTco98OuleO9efYT7TI8M34rd9XozUAkkuaxJqr53LL57aa+dyXaV8p5futfP9GVLtc2yg+UQJIUQDCqKEEKIBBVFCCNGAgighhGhAQZQQQjSgIEoIIRpQECWEEA0oiBJCiAYURAkhRAMKooQQogEFUUII0YCCKCGEaEBBlBBCNKAgSgghGlAQJYQQDSiIEkKIBhRECSFEAwqihBCiAQVRQgjRgIIoIYRoQEGUEEI0oCBKCCEaUBAlhBANKIgSQogGFEQJIUQDCqKEEKIBBVFCCNGAgighhGhAQZQQQjTIOIhu2bIFN9xwAyoqKsBxHN58882U/d/73vfAcVzKtnLlypRjent7cffdd8PpdMLtduPee+9FMBjU9EYIIWQkZBxEQ6EQ6uvr8eyzz6Y9ZuXKlWhra0tur776asr+u+++GwcOHMD69euxbt06bNmyBffff3/muSeEkJHGNADA1q5dm/LYmjVr2I033pj2OV999RUDwHbt2pV87N1332Ucx7EzZ84MK12fz8cA0EYbbbTlfPP5fEPGo5y0iW7atAklJSWYOnUqfvjDH6Knpye5b9u2bXC73Zg/f37ysWXLloHneezYsUP19aLRKPx+f8pGCCGjge5BdOXKlfjDH/6ADRs24J/+6Z+wefNmrFq1CpIkAQDa29tRUlKS8hxRFOHxeNDe3q76mk8//TRcLldyq6qq0jvbhBCSFVHvF7zjjjuS/581axZmz56NiRMnYtOmTVi6dGlWr/nYY4/h0UcfTf7t9/spkBJCRoWcD3GaMGECioqKcOzYMQBAWVkZOjs7U45JJBLo7e1FWVmZ6muYTCY4nc6UjRBCRoOcB9GWlhb09PSgvLwcANDY2Aiv14vdu3cnj9m4cSNkWUZDQ0Ous0MIIbrK+HI+GAwma5UA0NTUhL1798Lj8cDj8eDJJ5/E6tWrUVZWhuPHj+N//I//gUmTJmHFihUAgGnTpmHlypW477778NxzzyEej+Ohhx7CHXfcgYqKCv3eGSGE5MOwxhSd56OPPlIdBrBmzRrW39/Pli9fzoqLi5nBYGA1NTXsvvvuY+3t7Smv0dPTw+68805mt9uZ0+lk99xzDwsEAsPOAw1xoo022vK1XWiIE8cYYxhj/H4/XC7XSGeDEHIR8Pl8Q/bD0L3zhBCiAQVRQgjRgIIoIYRooPtgezJ61bpMmOqxqO5r9kfxVU84zznSl1XksajCDqMwuG4QScjY1hpAVBpzXQApZhdbUWE3qu470N2P04FYnnNEKIheRC4pteH7s0pU9/31eN+YD6Jus4C/nV0Cl2lwse7qj+PLrn5EpcQI5Ew/14x34epq9U7Vf/28nYLoCKDLeUII0YCC6EWA5wCzwMHAc2mPEc8eI6Q/ZFQzChxMAo902ec4wCRyMA5xDkYzgQPMIgeBS59/I8/BLHBpzwHJDRonehGYXGDGD+eUotAioshiUD3GG02guz+BP37VhZ3toTznUBuR5/DQ3FJM8Vgwzm6EoBIo4zLDmUAUX3aF8fwXHZBHIJ9aXDHOgVumFKLUZoDDKKge09kfR3d/HP+6px3Nfrqs18uFxolSm+hFwCLymOg2QxyiFuY2iXAZhbRf0NGMA1DpMKLGaUp7jIHnMN5lRk84oTxhjFUd3CYRkwrMQx5TYjXAaRRgVulYI7lDZ5sQQjSgIEoIIRpQECWEEA0oiBJCiAYURAkhRAMKooQQogEFUUII0YCCKCGEaEBBlBBCNKAgSgghGlAQJYQQDSiIfsPxHDKamYnnxtZMThwAgQeGO3cRxwECN7ZmOhI45XMcLp7jMjqeaEOzOH2DOYw8flBfinEOEya5TeCGmEYNABhjaPbH0BaK4Xf7OtEeiucpp9m7boIbl1Y6MMlthn0Yk6f4owkc90ax6bQfH57y5SGH2lQ7jfj+zBKU2w0Y50g/wcoAmTEc7YvglC+K3+7rQCQx5r7eow7N4nQRM/A8ZhRaUWJTn/7u6ziOQ43LhGKrCIs4Ni5SxjmMmFNiG/bxTpOIuaUiDveNjVn87QYB9SVW1SVP1PAch6keC6wiD5Ebg9NVjUFj45tCCCGjFAVRQgjRgIIoIYRoQEGUEEI0oCBKCCEaUBAlhBANKIgSQogGFEQJIUQDGmx/Eenqj6MjzV1IHouICrsxzznSV1SSccIbhSQPHmBuEDhMcJlgGMPLCTPG0BKMwReRVPdX2I3wWOgrnW90xi8iW1v8+PcD3ar7rp3gxg/qS/OcI331hBN4evsZ+GKDg0yRRcQzV1aj0DJ2gygA/OfhXmw67Vfd99/mlGJFrTu/GSKZX85v2bIFN9xwAyoqKsBxHN58882U/RzHqW6/+MUvkseMHz9+0P6f//znmt8MGZrMgITMVDdZpfY21jAGxNO8v4TMvhE3QEpDvD957E2D8Y2QcU00FAqhvr4e3//+93HzzTcP2t/W1pby97vvvot7770Xq1evTnn8qaeewn333Zf82+FwZJoVMgwMymXgwP+1HjcaZTKHTvLYMfImGVI/mwseO0Y/w7Es4yC6atUqrFq1Ku3+srKylL/feustXHXVVZgwYULK4w6HY9CxRF/BmITfftEB89m57ZoDsbTH7ukM4Rc7WwEACQZ09o/+GZwAYMMpPw71KJOJ9CdkhBOy6nGBmITf7OmA6ey5ODXEuRhNzgRi+OfP2pKXjAd7I2mP/eCkD/u7+gEo5yKS5lwQnTENALC1a9em3d/e3s5EUWR//OMfUx6vqalhpaWlzOPxsDlz5rBnnnmGxePxtK8TiUSYz+dLbqdPn07+QNNGG2205XLz+XxDxsGcdiz9+7//OxwOx6DL/r/7u7/DJZdcAo/Hg08//RSPPfYY2tra8Mtf/lL1dZ5++mk8+eSTucwqIYRkJ6Oq59cAQ9dEp06dyh566KELvs4LL7zARFFkkUhEdT/VRGmjjbaR2kasJrp161YcPnwYf/rTny54bENDAxKJBE6ePImpU6cO2m8ymWAyXXhWb0IIybecDZp74YUXMG/ePNTX11/w2L1794LneZSUlOQqO4QQkhMZ10SDwSCOHTuW/LupqQl79+6Fx+NBdXU1AGUNpDfeeAP/9//+30HP37ZtG3bs2IGrrroKDocD27ZtwyOPPILvfOc7KCgo0PBWCCFkBFywwfJrPvroI9V2gzVr1iSP+e1vf8ssFgvzer2Dnr97927W0NDAXC4XM5vNbNq0aez//J//k7Y9VI3P5xvxdhLaaKPt4tgu1CZKq30SQsgQLrTa59i+kZgQQkYYBVFCCNGAgighhGhAQZQQQjSgIEoIIRpQECWEEA0oiBJCiAYURAkhRANaY4lc9HgOKHAZIZ6dsFkPjAF9/hjiiTF3LwvJEAVRctGzWUT86K4pKCrQb6awRILhX185guOng7q9JhmdKIiSix7HKYHUaTPo9prxhAyB169mS0YvahMlhBANqCZKLnqRmIx3t7bCZsns6yAIHJYsKIXHZcxRzshYQEGUXPRicRkbd3Zm/DyTkcfcugIKohc5upwnhBANqCZKLnocBzjthow7gowGHoKOw6LI2ERBlFz0kkOc3JkNceI4wGGjr9DFjkoAuejxHOCyG6htk2SF2kQJIUQDqomSi140LuPD7e0ZD3ESBQ6Xzy2G20k12IsZBVFy0YvGZLz/SXvGzzMZecyY6KIgepGjy3lCCNGAaqLkojdw73ymt7objQINcSIURAmxWUQ8dNfkzIc4AXQpTyiIEsJzQKHLhBKPeaSzQsYgahMlhBANLuqaqNMloLiMLscudmaTgNZgEH45ottryjLgqRAx0WzR7TVJfskSQ9OxC5eJizqIFhQZMHOuHRw3OjsH1BaWGJ05Hfua/H7Ar+9rlo03oGy8fhM9k/yKx2UKol8nCEDdLBssFgEAYHMIozaAAgA4wFfsRMKgfEymcBT23hAFUpIzBp7HtNICmETlO9LmD6HFFxrhXI1uYzqIigZkFAQNBg5llSY4nGPjbTNwiNjMiFnPNjlwgL2XCjTJHYHnUOG0wmpUatDheIKC6AWMjWiSxqVLCiAahh9EOQ6w2oQc5ogQcrEZ00HU4RJgMNAAA0LIyKEIRAghGmQURJ9++mksWLAADocDJSUluOmmm3D48OGUYyKRCB588EEUFhbCbrdj9erV6OjoSDmmubkZ1113HaxWK0pKSvDjH/8YiURC+7shhJA8yyiIbt68GQ8++CC2b9+O9evXIx6PY/ny5QiFzjU8P/LII/jrX/+KN954A5s3b0Zraytuvvnm5H5JknDdddchFovh008/xb//+7/jpZdews9+9jP93hUhhOQJxxhTG444LF1dXSgpKcHmzZuxePFi+Hw+FBcX45VXXsEtt9wCADh06BCmTZuGbdu2YdGiRXj33Xdx/fXXo7W1FaWlpQCA5557Dj/5yU/Q1dUFo/HCg9/9fj9cLheuu6XoG90mKnMcOscXJ3vnrb5+FLb00hAnkjNmUcCSiRXJ3vkjXV7sb+8d4VyNjHhcxtv/2Q2fzwen05n2OE0RyOfzAQA8Hg8AYPfu3YjH41i2bFnymLq6OlRXV2Pbtm0AgG3btmHWrFnJAAoAK1asgN/vx4EDB1TTiUaj8Pv9KdvFgmMMnCQrm5z17x0hw8IAJGSGhCQjIcmQqMxdUNa987Is4+GHH8Zll12GmTNnAgDa29thNBrhdrtTji0tLUV7e3vymPMD6MD+gX1qnn76aTz55JPZZnXM4hiDp7UP7Owcbbwkj3COyDddLCFhZ3MH+LPjryMJaYRzNPplXRN98MEHsX//frz22mt65kfVY489Bp/Pl9xOnz6d8zRHAw6AIZaAMRKHMRKHGJfoUp7kFAPgj8bhjcTgjcQoiA5DVjXRhx56COvWrcOWLVswbty45ONlZWWIxWLwer0ptdGOjg6UlZUlj9m5c2fK6w303g8c83UmkwkmU2ZzPRJCSD5kVBNljOGhhx7C2rVrsXHjRtTW1qbsnzdvHgwGAzZs2JB87PDhw2hubkZjYyMAoLGxEV9++SU6OzuTx6xfvx5OpxPTp0/X8l4IISTvMqqJPvjgg3jllVfw1ltvweFwJNswXS4XLBYLXC4X7r33Xjz66KPweDxwOp340Y9+hMbGRixatAgAsHz5ckyfPh3f/e538cwzz6C9vR3/63/9Lzz44INU2ySEjDkZBdHf/OY3AIAlS5akPP7iiy/ie9/7HgDgn//5n8HzPFavXo1oNIoVK1bg17/+dfJYQRCwbt06/PCHP0RjYyNsNhvWrFmDp556Sts7IYSQEaBpnOhIuVjGiRJCRk5exokSQsjFjoIoIYRoQEGUEEI0oCBKCCEaUBAlhBANKIgSQogGFEQJIUQDCqKEEKLBmFyobuD+gHicpoYjhOTGQHy50P1IYzKIBgIBAMAHb12cM24TQvInEAjA5XKl3T8mb/uUZRmHDx/G9OnTcfr06SFvySLZ8fv9qKqqovObI3R+c0uP88sYQyAQQEVFBXg+fcvnmKyJ8jyPyspKAIDT6aRCmEN0fnOLzm9uaT2/Q9VAB1DHEiGEaEBBlBBCNBizQdRkMuHxxx+niZxzhM5vbtH5za18nt8x2bFECCGjxZitiRJCyGhAQZQQQjSgIEoIIRpQECWEEA0oiBJCiAZjMog+++yzGD9+PMxmMxoaGrBz586RztKY9MQTT4DjuJStrq4uuT8SieDBBx9EYWEh7HY7Vq9ejY6OjhHM8ei2ZcsW3HDDDaioqADHcXjzzTdT9jPG8LOf/Qzl5eWwWCxYtmwZjh49mnJMb28v7r77bjidTrjdbtx7770IBoN5fBej14XO7/e+971B5XnlypUpx+Ti/I65IPqnP/0Jjz76KB5//HF8/vnnqK+vx4oVK9DZ2TnSWRuTZsyYgba2tuT28ccfJ/c98sgj+Otf/4o33ngDmzdvRmtrK26++eYRzO3oFgqFUF9fj2effVZ1/zPPPINf/epXeO6557Bjxw7YbDasWLECkUgkeczdd9+NAwcOYP369Vi3bh22bNmC+++/P19vYVS70PkFgJUrV6aU51dffTVlf07OLxtjFi5cyB588MHk35IksYqKCvb000+PYK7Gpscff5zV19er7vN6vcxgMLA33ngj+djBgwcZALZt27Y85XDsAsDWrl2b/FuWZVZWVsZ+8YtfJB/zer3MZDKxV199lTHG2FdffcUAsF27diWPeffddxnHcezMmTN5y/tY8PXzyxhja9asYTfeeGPa5+Tq/I6pmmgsFsPu3buxbNmy5GM8z2PZsmXYtm3bCOZs7Dp69CgqKiowYcIE3H333WhubgYA7N69G/F4POVc19XVobq6ms51FpqamtDe3p5yPl0uFxoaGpLnc9u2bXC73Zg/f37ymGXLloHneezYsSPveR6LNm3ahJKSEkydOhU//OEP0dPTk9yXq/M7poJod3c3JElCaWlpyuOlpaVob28foVyNXQ0NDXjppZfw3nvv4Te/+Q2amppwxRVXIBAIoL29HUajEW63O+U5dK6zM3DOhiq77e3tKCkpSdkviiI8Hg+d82FYuXIl/vCHP2DDhg34p3/6J2zevBmrVq2CJEkAcnd+x+RUeEQfq1atSv5/9uzZaGhoQE1NDV5//XVYLJYRzBkhmbvjjjuS/581axZmz56NiRMnYtOmTVi6dGnO0h1TNdGioiIIgjCoh7ijowNlZWUjlKtvDrfbjSlTpuDYsWMoKytDLBaD1+tNOYbOdXYGztlQZbesrGxQB2kikUBvby+d8yxMmDABRUVFOHbsGIDcnd8xFUSNRiPmzZuHDRs2JB+TZRkbNmxAY2PjCObsmyEYDOL48eMoLy/HvHnzYDAYUs714cOH0dzcTOc6C7W1tSgrK0s5n36/Hzt27Eiez8bGRni9XuzevTt5zMaNGyHLMhoaGvKe57GupaUFPT09KC8vB5DD85t1l9QIee2115jJZGIvvfQS++qrr9j999/P3G43a29vH+msjTl///d/zzZt2sSamprYJ598wpYtW8aKiopYZ2cnY4yxBx54gFVXV7ONGzeyzz77jDU2NrLGxsYRzvXoFQgE2J49e9iePXsYAPbLX/6S7dmzh506dYoxxtjPf/5z5na72VtvvcX27dvHbrzxRlZbW8vC4XDyNVauXMnmzp3LduzYwT7++GM2efJkduedd47UWxpVhjq/gUCA/cM//APbtm0ba2pqYh9++CG75JJL2OTJk1kkEkm+Ri7O75gLoowx9q//+q+surqaGY1GtnDhQrZ9+/aRztKYdPvtt7Py8nJmNBpZZWUlu/3229mxY8eS+8PhMPtv/+2/sYKCAma1Wtm3v/1t1tbWNoI5Ht0++ugjBmDQtmbNGsaYMszppz/9KSstLWUmk4ktXbqUHT58OOU1enp62J133snsdjtzOp3snnvuYYFAYATezegz1Pnt7+9ny5cvZ8XFxcxgMLCamhp23333Dapc5eL80nyihBCiwZhqEyWEkNGGgighhGhAQZQQQjSgIEoIIRpQECWEEA0oiBJCiAYURAkhRAMKooQQogEFUUII0YCCKCGEaEBBlBBCNPj/AaJl6p+BKz5sAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "for i in range(22):\n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, reward, done, info = env.step(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "vHJaGcAVelRY",
        "outputId": "ee2423b9-8091-4ba1-e22f-65208535f628"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n",
            "(84, 84)\n"
          ]
        }
      ],
      "source": [
        "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
        "processor = AtariProcessor()\n",
        "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
        "# Seleccionamos el primer frame y lo normalizamos\n",
        "frame = processor.process_state_batch(obs_preprocessed)\n",
        "# Visualizar en escala de grises\n",
        "plt.imshow(frame, cmap='gray')\n",
        "plt.show()\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-yCJoGjf2Fg"
      },
      "source": [
        "#### Clase ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewKKozUaf-mG"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
        "\n",
        "    def __init__(self, capacity, state_shape):\n",
        "        self.capacity = capacity\n",
        "        self.position = 0\n",
        "        self.size = 0\n",
        "\n",
        "        # Pre-asignar arrays con el tamaño exacto\n",
        "        # Usar uint8 para estados (más eficiente que float32)\n",
        "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
        "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
        "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
        "\n",
        "        print(f\" [INFO] - ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
        "        memory_size = (\n",
        "            self.states.nbytes + self.next_states.nbytes +\n",
        "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
        "        ) / (1024 * 1024)\n",
        "        print(f\" [INFO] - Memoria asignada: {memory_size:.2f} MB\")\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Añade una experiencia al buffer de forma eficiente\"\"\"\n",
        "        # Convertir a uint8 para ahorrar memoria (estados son imágenes 0-255)\n",
        "        if state.dtype != np.uint8:\n",
        "            state = (state * 255).astype(np.uint8)\n",
        "        if next_state.dtype != np.uint8:\n",
        "            next_state = (next_state * 255).astype(np.uint8)\n",
        "\n",
        "        # Almacenar directamente en el array pre-asignado\n",
        "        self.states[self.position] = state\n",
        "        self.actions[self.position] = action\n",
        "        self.rewards[self.position] = reward\n",
        "        self.next_states[self.position] = next_state\n",
        "        self.dones[self.position] = done\n",
        "\n",
        "        # Actualizar posición circular\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
        "        if self.size < batch_size:\n",
        "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
        "\n",
        "        # Generar índices aleatorios\n",
        "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
        "\n",
        "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
        "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
        "        batch_actions = self.actions[indices]\n",
        "        batch_rewards = self.rewards[indices]\n",
        "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
        "        batch_dones = self.dones[indices]\n",
        "\n",
        "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
        "        self.position = 0\n",
        "        self.size = 0\n",
        "        # No es necesario limpiar los arrays, se sobrescriben"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No-SaTPRkQoK"
      },
      "source": [
        "#### Clase PerformanceMonitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOYQASDDeC2M"
      },
      "source": [
        "Actualmente no se usa, si se necesitase mayor detalle de la evolución de los entrenamientos se podría incluir en el Callback antes del entrenamiento. De momento no se usa!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf2A5kDokNdS"
      },
      "outputs": [],
      "source": [
        "# Clase para monitoreo de memoria y rendimiento\n",
        "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_path='diagnosticos'):\n",
        "        self.save_path = save_path\n",
        "        self.episode_times = []\n",
        "        self.memory_usage = []\n",
        "        self.current_episode = 0\n",
        "        self.episode_start_time = None\n",
        "        self.episode_start_memory = None\n",
        "\n",
        "    def on_episode_begin(self, episode, logs={}):\n",
        "        self.episode_start_time = time.time()\n",
        "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "        self.current_episode = episode\n",
        "        print(f\" [INFO] - Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        end_time = time.time()\n",
        "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "        episode_duration = end_time - self.episode_start_time\n",
        "\n",
        "        self.episode_times.append(episode_duration)\n",
        "        self.memory_usage.append(final_memory)\n",
        "\n",
        "        print(f\" [INFO] - Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
        "        print(f\" [INFO] - Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
        "\n",
        "        # Guardar diagnóstico cada 5 episodios\n",
        "        if (episode + 1) % 5 == 0:\n",
        "            self.save_diagnostics(episode)\n",
        "\n",
        "        # Forzar recolección de basura\n",
        "        gc.collect()\n",
        "\n",
        "    def save_diagnostics(self, episode):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.episode_times)\n",
        "        plt.title('Tiempo por episodio')\n",
        "        plt.ylabel('Segundos')\n",
        "        plt.xlabel('Episodio')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.memory_usage)\n",
        "        plt.title('Uso de memoria')\n",
        "        plt.ylabel('MB')\n",
        "        plt.xlabel('Episodio')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTgDOJoCgISN"
      },
      "source": [
        "### 1. Implementación de la red neuronal\n",
        "\n",
        "#### Definición de las redes neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAzP0UigPVg"
      },
      "source": [
        "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ0dGSAUgP0a"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP2vNpKnzkTl"
      },
      "outputs": [],
      "source": [
        "def create_dqn_model(input_shape, nb_actions, memory_size):\n",
        "    \"\"\"\n",
        "    Crea un modelo DQN usando SOLO Keras estándar. Base común para redes DQN y DDQN\n",
        "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
        "    Construye un modelo que acepta channels_first y convierte internamente\n",
        "\n",
        "    Esta función implementa una red convolucional que recibe un estado (conjunto de frames)\n",
        "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
        "    de capas totalmente conectadas, con activación RELU.\n",
        "    Esto evita completamente los problemas de grafos múltiples\n",
        "    \"\"\"\n",
        "    print(f\"🏗️ Creando modelo DQN estándar: input_shape={input_shape}, actions={nb_actions}\")\n",
        "\n",
        "    # Input en formato channels_first (como viene de SequentialMemory)\n",
        "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
        "\n",
        "    # Convertir a channels_last usando Permute\n",
        "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
        "    # Red convolucional estándar\n",
        "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
        "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
        "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(512, activation='relu', name='dense1')(x)\n",
        "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DQN_Model')\n",
        "    memory = None\n",
        "\n",
        "    print(\"✅ Modelo creado exitosamente\")\n",
        "    print(f\"📊 Resumen del modelo:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model, memory, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJbsXLvceC2R"
      },
      "outputs": [],
      "source": [
        "def create_ddqn_models(input_shape, nb_actions, memory_size):\n",
        "    \"\"\"\n",
        "    Crea modelos para Double DQN (principal y objetivo)\n",
        "    \"\"\"\n",
        "    print(f\"🏗️ Creando modelos DDQN: input_shape={input_shape}, actions={nb_actions}\")\n",
        "\n",
        "    # Modelo principal\n",
        "    main_model, memory, _ = create_dqn_model(input_shape, nb_actions, memory_size)\n",
        "    main_model._name = 'DDQN_Main_Model'\n",
        "\n",
        "    # Modelo objetivo (copia exacta)\n",
        "    target_model = tf.keras.models.clone_model(main_model)\n",
        "    target_model.set_weights(main_model.get_weights())\n",
        "    target_model._name = 'DDQN_Target_Model'\n",
        "\n",
        "    print(\"✅ Modelos DDQN creados exitosamente\")\n",
        "    return main_model, memory, target_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItIFlubUeC2R"
      },
      "outputs": [],
      "source": [
        "def create_ddqn_replay_model(input_shape, nb_actions, memory_size):\n",
        "    print(f\"🏗️ Creando modelos DDQN_replay: input_shape={input_shape}, actions={nb_actions}\")\n",
        "\n",
        "    # Input en formato channels_first (como viene de SequentialMemory)\n",
        "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
        "\n",
        "    # Convertir a channels_last usando Permute\n",
        "    x = Permute((2, 3, 1), name='convert_to_channels_last')(inputs)  # (84, 84, 4)\n",
        "    # Red convolucional estándar\n",
        "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', name='conv1', input_shape=input_shape)(x)\n",
        "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid', name='conv2')(x)\n",
        "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid', name='conv3')(x)\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(512, activation='relu', name='dense1')(x)\n",
        "    outputs = Dense(nb_actions, activation='linear', name='q_values')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DDQN_replay_Main_Model')\n",
        "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
        "    target_model = clone_model(model)  # Create target model for DDQN\n",
        "    target_model.set_weights(model.get_weights())  # Initialize with same weights\n",
        "    target_model._name = 'DDQN_replay_Target_Model'\n",
        "\n",
        "    print(\"✅ Modelo creado exitosamente\")\n",
        "    print(f\"📊 Resumen del modelo:\")\n",
        "    model.summary()\n",
        "    return model, memory, target_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csPlTb9CeC2R"
      },
      "outputs": [],
      "source": [
        "def create_dueling_dqn_replay_model(input_shape, action_size, memory_size):\n",
        "    \"\"\"\n",
        "    Crea un modelo Dueling DQN con replay.\n",
        "    \"\"\"\n",
        "    print(f\"🏗️ Creando modelo DUELING_DQN_REPLAY: input_shape={input_shape}, actions={action_size}\")\n",
        "    # Input en formato channels_first (como viene de SequentialMemory)\n",
        "    inputs = Input(shape=SEQ_INPUT_SHAPE, name='input_channels_first')  # (4, 84, 84)\n",
        "    x = Permute((2, 3, 1))(inputs)\n",
        "    x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid', input_shape=input_shape)(x)\n",
        "    x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
        "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
        "    x = Flatten()(x)\n",
        "    # Value stream\n",
        "    value = Dense(512, activation='relu')(x)\n",
        "    value = Dense(1, activation='linear')(value)\n",
        "    # Advantage stream\n",
        "    advantage = Dense(512, activation='relu')(x)\n",
        "    advantage = Dense(action_size, activation='linear')(advantage)\n",
        "    # Combine streams\n",
        "    outputs = Add()([value, Lambda(lambda a: a - K.mean(a, axis=1, keepdims=True))(advantage)])\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='DuelingDQNReplay_Main_Model')\n",
        "    memory = SequentialMemory(limit=memory_size, window_length=WINDOW_LENGTH)\n",
        "    target_model = clone_model(model)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "    target_model._name = 'DuelingDQNReplay_Target_Model'\n",
        "\n",
        "    print(\"✅ Modelo creado exitosamente\")\n",
        "    print(f\"📊 Resumen del modelo:\")\n",
        "    model.summary()\n",
        "    return model, memory, target_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0DHvKNshvQo"
      },
      "source": [
        "### 2. Implementación de la solución DQN\n",
        "\n",
        "#### Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy5xJ3RleC2S"
      },
      "outputs": [],
      "source": [
        "def load_model_checkpoint(model, memory, target_model, model_name, checkpoint_dir=\"checkpoints\", suffix=\"lastest\"):\n",
        "    \"\"\"\n",
        "    Carga pesos y estado para componentes separados (modelo, memoria, target_model).\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "        model: El modelo principal\n",
        "        memory: La memoria de repetición\n",
        "        target_model: El modelo target (puede ser None)\n",
        "        model_name: Tipo del modelo ('DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.)\n",
        "        checkpoint_dir: Directorio donde buscar los checkpoints\n",
        "        suffix: Tipo de checkpoint a cargar (\"lastest\", \"best\", o \"epXXX\")\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "        tuple: (episode, steps, epsilon) - El episodio, pasos y epsilon desde donde continuar\n",
        "               Si no se encuentra el checkpoint, devuelve (0, 0, 0.1)\n",
        "    \"\"\"\n",
        "    # Valores predeterminados\n",
        "    episode = 0\n",
        "    steps = 0\n",
        "    epsilon = 0.1  # Valor por defecto\n",
        "\n",
        "    # Definir las rutas de los archivos\n",
        "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
        "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
        "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
        "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"\n",
        "\n",
        "    try:\n",
        "        # Cargar estado del entrenamiento\n",
        "        if not os.path.exists(state_path):\n",
        "            print(f\"⚠️ No se encontró el checkpoint {suffix} para {model_name}\")\n",
        "            return episode, steps, epsilon\n",
        "        else:\n",
        "            print(f\"📂 Se cargó: {state_path}\")\n",
        "\n",
        "        with open(state_path, 'r') as f:\n",
        "            state = json.load(f)\n",
        "\n",
        "        # Extraer información básica\n",
        "        episode = state.get('episode', 0)\n",
        "        steps = state.get('global_steps', 0)\n",
        "        epsilon = state.get('epsilon', 0.1)\n",
        "\n",
        "        print(f\"📂 Cargando checkpoint {suffix} (episodio: {episode}, pasos: {steps}, epsilon: {epsilon})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error al cargar el archivo de estado: {e}\")\n",
        "\n",
        "    # Cargar modelo principal\n",
        "    if os.path.exists(main_model_path):\n",
        "        try:\n",
        "            model.load_weights(main_model_path)\n",
        "            print(f\"📂 Modelo principal cargado: {main_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ [ERROR] - Error al cargar el modelo principal: {e}\")\n",
        "            return 0, 0, 0.1  # Si falla la carga del modelo principal, mejor empezar desde cero\n",
        "    else:\n",
        "        print(f\"❌ [ERROR] - No se encontró el archivo del modelo principal: {main_model_path}\")\n",
        "        return 0, 0, 0.1\n",
        "\n",
        "    # Cargar modelo target si existe y se proporcionó\n",
        "    if target_model is not None:\n",
        "        if os.path.exists(target_model_path):\n",
        "            try:\n",
        "                target_model.load_weights(target_model_path)\n",
        "                print(f\"📂 Modelo target cargado: {target_model_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ [WARNING] - Error al cargar el modelo target: {e}\")\n",
        "                # Si hay error, sincronizar con el principal\n",
        "                print(\"🔄 Sincronizando red target desde la principal...\")\n",
        "                target_model.set_weights(model.get_weights())\n",
        "        else:\n",
        "            # Si no existe el archivo, sincronizar con el principal\n",
        "            print(\"🔄 No se encontró archivo target, sincronizando desde la principal...\")\n",
        "            target_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Cargar memoria si corresponde al tipo de modelo\n",
        "    if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and memory is not None and os.path.exists(memory_path):\n",
        "        try:\n",
        "            with open(memory_path, 'rb') as f:\n",
        "                loaded_memory = pickle.load(f)\n",
        "\n",
        "            # Transferir contenido no reemplazar objeto\n",
        "            memory_loaded = False\n",
        "            # Estructura SequentialMemory (Keras-RL)\n",
        "            if hasattr(loaded_memory, 'observations') and hasattr(memory, 'observations'):\n",
        "                memory.observations = loaded_memory.observations\n",
        "                memory.actions = loaded_memory.actions\n",
        "                memory.rewards = loaded_memory.rewards\n",
        "                memory.terminals = loaded_memory.terminals\n",
        "                if hasattr(loaded_memory, 'observations_'):\n",
        "                    memory.observations_ = loaded_memory.observations_\n",
        "                memory_loaded = True\n",
        "            elif hasattr(loaded_memory, 'buffer') and hasattr(memory, 'buffer'):\n",
        "                memory.buffer = loaded_memory.buffer\n",
        "                memory.position = loaded_memory.position\n",
        "                if hasattr(loaded_memory, 'size'):\n",
        "                    memory.size = loaded_memory.size\n",
        "                memory_loaded = True\n",
        "\n",
        "            if memory_loaded:\n",
        "                print(f\"📂 Memoria cargada correctamente: {memory_path}\")\n",
        "            else:\n",
        "                print(f\"⚠️ Estructura de memoria desconocida, no se pudo cargar\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error al cargar la memoria: {e}\")\n",
        "\n",
        "    return episode, steps, epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26L72dkSeC2S"
      },
      "outputs": [],
      "source": [
        "# Función auxiliar para convertir objetos no serializables a JSON\n",
        "def convert_to_json_serializable(obj):\n",
        "    \"\"\"Convierte tipos numpy a tipos Python nativos para serialización JSON\"\"\"\n",
        "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, np.bool_):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, (datetime.datetime, datetime.date)):\n",
        "        return obj.isoformat()\n",
        "    else:\n",
        "        # Para cualquier otro tipo no reconocido\n",
        "        return str(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etldjzpgeC2S"
      },
      "outputs": [],
      "source": [
        "def save_model_checkpoint(agent, model_name, episode=0, steps=0,\n",
        "                          checkpoint_dir=\"checkpoints\", suffix=\"lastest\", epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Guarda el modelo, la memoria de repetición y el estado del entrenamiento.\n",
        "\n",
        "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
        "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
        "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
        "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "        model: El modelo principal\n",
        "        memory: La memoria de repetición\n",
        "        target_model: El modelo target (puede ser None)\n",
        "        model_name: Nombre base del modelo (ej: 'DQN', 'DDQN_REPLAY')\n",
        "        episode: Número del episodio actual\n",
        "        steps: Pasos globales acumulados\n",
        "        checkpoint_dir: Directorio donde guardar los checkpoints\n",
        "        suffix: Tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\")\n",
        "\n",
        "        model : tensorflow.keras.Model-  El modelo principal de red neuronal que se desea guardar.\n",
        "        memory : objeto de memoria-      La memoria de repetición utilizada para almacenar experiencias de entrenamiento.\n",
        "                                         Puede ser SequentialMemory, ReplayBuffer u otra implementación compatible.\n",
        "        target_model : tensorflow.keras.Model-         El modelo target utilizado en algoritmos como DDQN. Es una copia del modelo principal\n",
        "            que se actualiza periódicamente durante el entrenamiento.\n",
        "        model_name : str-                Nombre identificativo del modelo ('DQN', 'DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY', etc.).\n",
        "                                         Se utiliza para nombrar los archivos de checkpoint.\n",
        "        episode : int, opcional-         Número del episodio actual de entrenamiento. Por defecto es 0.\n",
        "        steps : int, opcional-           Número total de pasos (interacciones con el entorno) realizados. Por defecto es 0.\n",
        "\n",
        "        checkpoint_dir : str, opcional-  Directorio donde se guardarán los archivos de checkpoint. Por defecto es \"checkpoints\".\n",
        "\n",
        "        suffix : str, opcional-          Sufijo para identificar el tipo de checkpoint (\"lastest\", \"best\", o \"epXXX\"). Por defecto es \"lastest\".\n",
        "\n",
        "        epsilon : float, opcional-       Valor actual de epsilon para la política epsilon-greedy. Por defecto es 0.1.\n",
        "        force_override: Si es True, sobrescribe incluso los checkpoints 'best'\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Asegurar que existe el directorio\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    # Puedes intentar limpiar la memoria de Python no usada explícitamente antes de medir\n",
        "    gc.collect()\n",
        "\n",
        "    # Definir las rutas de los archivos\n",
        "    main_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_model.h5\"\n",
        "    target_model_path = f\"{checkpoint_dir}/{model_name}_{suffix}_target.h5\"\n",
        "    memory_path = f\"{checkpoint_dir}/{model_name}_{suffix}_memory.pkl\"\n",
        "    state_path = f\"{checkpoint_dir}/{model_name}_{suffix}_state.json\"\n",
        "\n",
        "    # PROTECCIÓN DE CHECKPOINTS EXISTENTES\n",
        "    # Si es un checkpoint \"best\" y estamos intentando guardarlo con episodio 0\n",
        "    if (\"best\" in suffix or \"lastest\" in suffix) and episode == 0:\n",
        "        # Verificar si ya existe un mejor checkpoint con episodio > 0\n",
        "        state_path = f\"{checkpoint_dir}/{model_name}_best_state.json\"\n",
        "        if os.path.exists(state_path):\n",
        "            try:\n",
        "                with open(state_path, 'r') as f:\n",
        "                    existing_state = json.load(f)\n",
        "                existing_episode = existing_state.get('episode', 0)\n",
        "\n",
        "                if existing_episode > 0:\n",
        "                    print(f\"🛡️ Protegiendo checkpoint '{suffix}' existente (ep: {existing_episode})\")\n",
        "                    print(f\"❌ NO se guardará un nuevo checkpoint con episodio 0\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error verificando checkpoint existente: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Guardar pesos del modelo principal\n",
        "        if hasattr(agent, 'model'):\n",
        "            agent.model.save_weights(main_model_path)\n",
        "            print(f\"💾 Guardado modelo principal {suffix}: {main_model_path}\")\n",
        "        else:\n",
        "            print(f\"⚠️ El agente no tiene el atributo 'model'\")\n",
        "\n",
        "        # Guardar pesos del modelo target si existe\n",
        "        if hasattr(agent, 'target_model') and agent.target_model is not None:\n",
        "            agent.target_model.save_weights(target_model_path)\n",
        "            print(f\"💾 Guardado modelo target {suffix}: {target_model_path}\")\n",
        "        else:\n",
        "            print(f\"⚠️ El agente no tiene el atributo 'target_model' o es None\")\n",
        "\n",
        "        # Preparar el estado del entrenamiento - Con conversión a tipos Python nativos\n",
        "        state = {\n",
        "            'episode': int(episode),\n",
        "            'global_steps': int(steps),\n",
        "            'epsilon': float(epsilon),\n",
        "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        # Si hay una política específica en el agente\n",
        "        if hasattr(agent, 'policy'):\n",
        "            policy_state = {}\n",
        "\n",
        "            # Intentar guardar el estado de la política\n",
        "            if hasattr(agent.policy, 'eps'):\n",
        "                policy_state['eps'] = float(agent.policy.eps)  # Convertir a float Python\n",
        "\n",
        "            # Si la política tiene más atributos relevantes\n",
        "            for attr in ['value_max', 'value_min', 'value_test', 'nb_steps']:\n",
        "                if hasattr(agent.policy, attr):\n",
        "                    value = getattr(agent.policy, attr)\n",
        "                    if isinstance(value, (np.integer, np.floating, np.bool_)):\n",
        "                        value = value.item()  # Convierte cualquier tipo numpy a su equivalente Python\n",
        "                    policy_state[attr] = value\n",
        "\n",
        "            if policy_state:\n",
        "                state['policy'] = policy_state\n",
        "\n",
        "        # Guardar memoria de repetición para modelos con replay\n",
        "        if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY'] and hasattr(agent, 'memory') and agent.memory is not None:\n",
        "            try:\n",
        "                # Usar compresión máxima\n",
        "                with open(memory_path, 'wb') as f:\n",
        "                    pickle.dump(agent.memory, f)\n",
        "                print(f\"💾 Memoria {suffix} guardada: {memory_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ No se pudo guardar la memoria: {e}\")\n",
        "\n",
        "        # Guardar el estado\n",
        "        with open(state_path, 'w') as f:\n",
        "            json.dump(state, f, indent=2, default=convert_to_json_serializable)\n",
        "\n",
        "        print(f\"💾 Checkpoint {suffix} guardado (ep: {episode}, pasos: {steps})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error guardando checkpoint {suffix}: {e}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93iueQ3FeC2T"
      },
      "outputs": [],
      "source": [
        "class SimpleProgressCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback personalizado para monitorear el progreso del entrenamiento de un agente DQN.\n",
        "\n",
        "    Registra el avance en términos de pasos completados, porcentaje, velocidad de entrenamiento\n",
        "    (pasos por segundo) y tiempo estimado de finalización (ETA).\n",
        "\n",
        "    Atributos:\n",
        "        total_steps (int): Número total de pasos de entrenamiento.\n",
        "        print_interval (int): Intervalo de pasos para imprimir el progreso (por defecto: 10,000).\n",
        "        start_time (float): Tiempo de inicio del entrenamiento (en segundos).\n",
        "        last_step (int): Último paso registrado (inicializado en 0).\n",
        "    \"\"\"\n",
        "    def __init__(self, total_steps, print_interval=100):\n",
        "        \"\"\"\n",
        "        Inicializa el callback.\n",
        "\n",
        "        Args:\n",
        "            total_steps (int): Número total de pasos de entrenamiento.\n",
        "            print_interval (int): Intervalo de pasos para imprimir el progreso.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.total_steps = total_steps\n",
        "        self.print_interval = print_interval\n",
        "        self.step_counter = 0\n",
        "        self.start_time = time.time()\n",
        "        self.episode_rewards = []  # Store clipped episode rewards\n",
        "        self.episode_steps = []\n",
        "        self.current_episode_reward = 0.0  # Track clipped reward for current episode\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        \"\"\"\n",
        "        Se ejecuta al inicio del entrenamiento.\n",
        "\n",
        "        Inicializa el tiempo de inicio y muestra un mensaje de comienzo.\n",
        "\n",
        "        Args:\n",
        "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
        "        \"\"\"\n",
        "        self.start_time = time.time()\n",
        "        print(f\"🚀 Entrenamiento iniciado: {self.total_steps:,} pasos\")\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        \"\"\"\n",
        "        Se ejecuta al final de cada paso de entrenamiento.\n",
        "\n",
        "        Calcula y muestra el progreso, incluyendo porcentaje completado, velocidad\n",
        "        (pasos por segundo) y tiempo estimado de finalización (ETA) en horas.\n",
        "\n",
        "        Args:\n",
        "            step (int): Número del paso actual.\n",
        "            logs (dict): Diccionario de métricas (no utilizado aquí).\n",
        "        \"\"\"\n",
        "        self.step_counter += 1\n",
        "        raw_reward = logs.get('reward', 0.0)\n",
        "        clipped_reward = np.clip(raw_reward, -1.0, 1.0)  # Match AtariProcessor clipping\n",
        "        self.current_episode_reward += clipped_reward\n",
        "        if self.step_counter % self.print_interval == 0:\n",
        "            progress = (self.step_counter / self.total_steps) * 100\n",
        "            elapsed_time = (time.time() - self.start_time)\n",
        "            steps_per_sec = self.step_counter / elapsed_time\n",
        "            eta_hours = (self.total_steps - self.step_counter) / steps_per_sec / 3600\n",
        "            memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
        "            print(f\"📊 Paso {self.step_counter:,}/{self.total_steps:,} ({progress:.1f}%) - \"\n",
        "                  f\"{steps_per_sec:.1f} pasos/seg - ETA: {eta_hours:.1f}h - Memoria: {memory_usage:.2f} MB\")\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        nb_steps = logs.get('nb_episode_steps', 1)\n",
        "        self.episode_rewards.append(self.current_episode_reward)\n",
        "        self.episode_steps.append(nb_steps)\n",
        "        mean_reward = self.current_episode_reward / nb_steps if nb_steps > 0 else 0\n",
        "        print(f\"📈 Episodio {episode+1}: Recompensa total (clipped): {self.current_episode_reward:.3f}, \"\n",
        "              f\"Pasos: {nb_steps}, Mean Reward Calculado: {mean_reward:.6f} (Recompensa/Pasos)\")\n",
        "        # Reset for next episode\n",
        "        self.current_episode_reward = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErMJ6b2deC2T"
      },
      "outputs": [],
      "source": [
        "class TargetRewardTracker(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback que monitorea el progreso hacia una media de episode_reward objetivo\n",
        "    e integra con nuestro sistema de checkpoints.\n",
        "    \"\"\"\n",
        "    def __init__(self, dqn, target_avg_reward=20.0, name_model=None, window_size=100, save_best=True, checkpoint_dir=checkpoint_path):\n",
        "        super().__init__()\n",
        "        self.target_avg_reward = target_avg_reward\n",
        "        self.window_size = window_size\n",
        "        self.save_best = save_best\n",
        "        # Obtener el nombre del modelo sin ruta\n",
        "        self.model = dqn\n",
        "        self.model_name = name_model\n",
        "        self.episode_count = 0\n",
        "        self.episode_rewards = []\n",
        "        self.best_avg_reward = float('-inf')\n",
        "        self.episodes_at_target = 0\n",
        "        self.consecutive_target_episodes = 0\n",
        "        self.checkpoint_dir =  f\"{checkpoint_path}/{self.model_name}\"\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"🎯 OBJETIVO: Media de episode_reward = {target_avg_reward}\")\n",
        "        print(f\"📊 Ventana de evaluación: {window_size} episodios\")\n",
        "\n",
        "    def on_episode_end(self, episode, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.episode_count += 1\n",
        "        episode_reward = logs.get('episode_reward', 0)\n",
        "        # Convert NumPy types to Python types\n",
        "        if isinstance(episode_reward, np.floating):\n",
        "            episode_reward = episode_reward.item()\n",
        "        self.episode_rewards.append(episode_reward)\n",
        "\n",
        "        # Calcular media móvil\n",
        "        if len(self.episode_rewards) >= self.window_size or self.episode_count % 10 == 0:\n",
        "            recent_rewards = self.episode_rewards[-self.window_size:]\n",
        "            current_avg = np.mean(recent_rewards)\n",
        "\n",
        "            # Verificar si alcanzamos el objetivo\n",
        "            target_reached = current_avg >= self.target_avg_reward\n",
        "\n",
        "            if target_reached:\n",
        "                self.episodes_at_target += 1\n",
        "                self.consecutive_target_episodes += 1\n",
        "            else:\n",
        "                self.consecutive_target_episodes = 0\n",
        "\n",
        "            # Guardar si es el mejor promedio\n",
        "            if current_avg > self.best_avg_reward or self.episode_count % 10 == 0:\n",
        "                self.best_avg_reward = current_avg\n",
        "                if self.save_best and hasattr(self, 'model') and self.episode_count % 50 == 0:\n",
        "                    # Formato del sufijo para el mejor modelo con su promedio\n",
        "                    best_suffix = f\"best_avg{current_avg:.1f}\"\n",
        "                    try:\n",
        "                        epsilon = self.model.policy.eps if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'eps') else 0.1\n",
        "\n",
        "                        # Usar nuestro sistema de checkpoint para guardar el mejor modelo\n",
        "                        if hasattr(self.model, 'model'): # Por si el agente es un DQNAgent con un .model\n",
        "# AVG                            save_model_checkpoint(   self.model,   self.model_name,\n",
        "# AVG                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
        "# AVG                                checkpoint_dir=self.checkpoint_dir,   suffix=best_suffix,\n",
        "# AVG                                epsilon=epsilon\n",
        "# AVG                            )\n",
        "                            # También actualizar el checkpoint \"best\" general\n",
        "                            save_model_checkpoint(   self.model,    self.model_name,\n",
        "                                episode=self.episode_count,     steps=getattr(self.model, 'step', 0),\n",
        "                                checkpoint_dir=self.checkpoint_dir,   suffix=\"best\",\n",
        "                                epsilon=epsilon\n",
        "                            )\n",
        "                        else:\n",
        "                            # Si no es un agente completo, guardar solo los pesos\n",
        "                            best_filename = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_model.h5\"\n",
        "                            self.model.save_weights(best_filename, overwrite=True)\n",
        "\n",
        "                        # Guardar métricas en JSON\n",
        "                        metrics = {\n",
        "                            \"episode\": int(self.episode_count),\n",
        "                            \"avg_reward\": float(current_avg),\n",
        "                            \"best_avg_reward\": float(self.best_avg_reward),\n",
        "                            \"timestamp\": int(time.time()),\n",
        "                            \"consecutive_target_episodes\": int(self.consecutive_target_episodes),\n",
        "                            \"episodes_at_target\": int(self.episodes_at_target)\n",
        "                        }\n",
        "                        metrics_file = f\"{self.checkpoint_dir}/{self.model_name}_{best_suffix}_metrics.json\"\n",
        "                        with open(metrics_file, 'w') as f:\n",
        "                            json.dump(metrics, f, indent=2, default=str)\n",
        "\n",
        "                        print(f\"💾 NUEVO MEJOR PROMEDIO: {current_avg:.2f} - Guardado en {self.checkpoint_dir}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ [ERROR] - Error guardando mejor modelo: {e}\")\n",
        "\n",
        "            # Mostrar progreso cada 50 episodios\n",
        "            if self.episode_count % 50 == 0:\n",
        "                progress_pct = (current_avg / self.target_avg_reward) * 100\n",
        "                target_status = \"🎯 OBJETIVO ALCANZADO!\" if target_reached else f\"📈 {progress_pct:.1f}% del objetivo\"\n",
        "\n",
        "                print(f\"\\n📊 EPISODIO {self.episode_count} - PROGRESO HACIA OBJETIVO\")\n",
        "                print(f\"   Reward actual: {episode_reward:.2f}\")\n",
        "                print(f\"   Media últimos {self.window_size}: {current_avg:.2f} / {self.target_avg_reward}\")\n",
        "                print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
        "                print(f\"   Estado: {target_status}\")\n",
        "                print(f\"   Episodios en objetivo: {self.episodes_at_target}\")\n",
        "                print(f\"   Episodios consecutivos en objetivo: {self.consecutive_target_episodes}\")\n",
        "\n",
        "                if self.consecutive_target_episodes >= 50:\n",
        "                    print(f\"🏆 ¡MODELO ESTABLE EN OBJETIVO! {self.consecutive_target_episodes} episodios consecutivos\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        \"\"\"Resumen final del entrenamiento\"\"\"\n",
        "        logs = logs or {}\n",
        "        if len(self.episode_rewards) >= self.window_size:\n",
        "            final_avg = np.mean(self.episode_rewards[-self.window_size:])\n",
        "            objetivo_alcanzado = final_avg >= self.target_avg_reward\n",
        "\n",
        "            print(f\"\\n🏁 RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
        "            print(f\"   Total episodios: {self.episode_count}\")\n",
        "            print(f\"   Media final últimos {self.window_size}: {final_avg:.2f}\")\n",
        "            print(f\"   Objetivo ({self.target_avg_reward}): {'✅ ALCANZADO' if objetivo_alcanzado else '❌ NO ALCANZADO'}\")\n",
        "            print(f\"   Mejor promedio histórico: {self.best_avg_reward:.2f}\")\n",
        "            print(f\"   Episodios que alcanzaron objetivo: {self.episodes_at_target}\")\n",
        "\n",
        "            # Save final metrics to JSON\n",
        "            final_metrics = {\n",
        "                \"total_episodes\": int(self.episode_count),\n",
        "                \"final_avg_reward\": float(final_avg),\n",
        "                \"target_reached\": bool(objetivo_alcanzado),\n",
        "                \"best_avg_reward\": float(self.best_avg_reward),\n",
        "                \"episodes_at_target\": int(self.episodes_at_target),\n",
        "                \"consecutive_target_episodes\": int(self.consecutive_target_episodes)\n",
        "            }\n",
        "            final_log_path = f\"{self.checkpoint_dir}/final_metrics.json\"\n",
        "            try:\n",
        "                with open(final_log_path, 'w') as f:\n",
        "                    json.dump(final_metrics, f, indent=2, default=str)\n",
        "                print(f\"💾 Métricas finales guardadas en: {final_log_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error al guardar métricas finales: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah5oKeR-eC2T"
      },
      "outputs": [],
      "source": [
        "class CustomFileLogger(FileLogger):\n",
        "    def __init__(self, filepath, interval=100):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.step = 0\n",
        "        self.filepath = filepath\n",
        "        self.interval = interval\n",
        "        self.current_episode_reward = 0.0\n",
        "        self.logs = {}\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.step += 1\n",
        "        if self.step % self.interval == 0:\n",
        "            episode_logs = {}\n",
        "        raw_reward = logs.get('reward', 0.0)\n",
        "        self.current_episode_reward += np.clip(raw_reward, -1.0, 1.0)\n",
        "\n",
        "    def on_episode_end(self, episode, logs):\n",
        "        metrics = logs.copy()\n",
        "        metrics['episode_reward'] = self.current_episode_reward\n",
        "        metrics['mean_reward_step'] = self.current_episode_reward / metrics.get('nb_episode_steps', 1)\n",
        "        metrics = {k: float(v) if isinstance(v, np.floating) else v for k, v in metrics.items()}\n",
        "        self.metrics[int(episode)] = metrics\n",
        "        if self.step % self.interval == 0:\n",
        "            with open(self.filepath, 'w') as f:\n",
        "                json.dump(self.metrics, f, indent=2, default=str)\n",
        "        self.current_episode_reward = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFeFvSybeC2T"
      },
      "outputs": [],
      "source": [
        "class EpisodeCheckpointCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback para guardar checkpoints episódicamente usando nuestras funciones personalizadas.\n",
        "    \"\"\"\n",
        "    def __init__(self, dqnet, checkpoint_path, save_freq=100, model_name='DQN'):\n",
        "        \"\"\"\n",
        "        Inicializa el callback.\n",
        "\n",
        "        Parametros:\n",
        "            dqnet: El agente DQN a guardar\n",
        "            checkpoint_path: Ruta donde guardar los checkpoints\n",
        "            save_freq: Frecuencia de episodios para guardar\n",
        "            model_name: Nombre base para los archivos de checkpoint\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_freq = save_freq\n",
        "        self.model_name = model_name\n",
        "        self.checkpoint_path = f\"{checkpoint_path}/{self.model_name}\"\n",
        "        self.episode_counter = 0\n",
        "        self.dqnet = dqnet\n",
        "        os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        \"\"\"\n",
        "        Guarda checkpoint al final de ciertos episodios.\n",
        "        \"\"\"\n",
        "        self.episode_counter += 1\n",
        "        if self.episode_counter % self.save_freq == 0:\n",
        "            try:\n",
        "                epsilon = self.dqnet.policy.eps if hasattr(self.dqnet, 'policy') and hasattr(self.dqnet.policy, 'eps') else 0.1\n",
        "                # También actualizar el checkpoint \"lastest\"\n",
        "                save_model_checkpoint(\n",
        "                    self.dqnet,\n",
        "                    self.model_name,\n",
        "                    episode=self.episode_counter,\n",
        "                    steps=self.dqnet.step,\n",
        "                    checkpoint_dir=self.checkpoint_path,\n",
        "                    suffix=\"lastest\",\n",
        "                    epsilon=epsilon\n",
        "                )\n",
        "                print(f\"✅ Checkpoint guardado para episodio {self.episode_counter}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ [ERROR] - Error al guardar checkpoint para episodio {self.episode_counter}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pXZJCPyeC2U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgbzJyUjmTzs"
      },
      "source": [
        "#### **ENTRENAMIENTO** ***********"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0HggwR1eC2U"
      },
      "outputs": [],
      "source": [
        "def entrenar_modelo(env,  model_name,  model, memory, target_model, model_instance=False,\n",
        "        start_episode=0, start_steps=0,\n",
        "        batch_size=batch_size, learning_rate=learning_rate, checkpoint_path='checkpoints',\n",
        "        input_shape=MODEL_INPUT_SHAPE, memoria_tamano=memory_size, warmup_steps=WARMUP_STEPS,\n",
        "        target_update_interval=TARGET_UPDATE_INTERVAL, target_update_tau=tau, epsilon_start=epsilon_start,\n",
        "        epsilon_min=0.1, epsilon_steps=EPSILON_STEPS, num_steps=NUM_TRAINING_STEPS, target_reward=TARGET_REWARD,\n",
        "        enable_double_dqn = False):\n",
        "    \"\"\"\n",
        "    Entrena un modelo DQN con el entorno proporcionado\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "        env: El entorno de Gym\n",
        "        model_name: Nombre identificador del modelo ('DQN', 'DDQN', etc.)\n",
        "        model: El modelo principal\n",
        "        memory: La memoria de repetición\n",
        "        target_model: El modelo target (puede ser None para DQN)\n",
        "        model_instance: Flag que determina si existe modelo cargado (si False, se crea uno nuevo)\n",
        "        checkpoint_path: Ruta donde guardar checkpoints\n",
        "        start_episode: Episodio desde donde continuar el entrenamiento\n",
        "        start_steps: Pasos desde donde continuar el entrenamiento\n",
        "        batch_size: Tamaño del lote para el entrenamiento\n",
        "        learning_rate: Tasa de aprendizaje del optimizador\n",
        "        input_shape: Forma de la entrada para el modelo\n",
        "        memoria_tamano: Tamaño de la memoria de repetición\n",
        "        warmup_steps: Pasos de calentamiento antes del entrenamiento\n",
        "        target_update_interval: Intervalo para actualizar la red objetivo\n",
        "        target_update_tau: Factor de actualización suave para la red objetivo\n",
        "        epsilon_start: Valor inicial de epsilon para exploración\n",
        "        epsilon_min: Valor mínimo de epsilon para exploración\n",
        "        epsilon_steps: Número de pasos para decrementar epsilon\n",
        "        num_steps: Número total de pasos de entrenamiento\n",
        "        target_reward: Recompensa objetivo para considerar resuelto el entorno\n",
        "        enable_double_dqn: Si es True, usa DDQN; si no, usa DQN estándar\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "        tuple: (agente_entrenado, éxito) - Modelo entrenado y booleano indicando éxito\n",
        "    \"\"\"\n",
        "    # Nombre del modelo para logs y checkpoints\n",
        "    name_model = model_name.upper()\n",
        "    print(f\"🤖 {'Continuando' if model_instance else 'Creando'} entrenamiento para {name_model}...\")\n",
        "\n",
        "    # Inicializar dqn desde el principio para evitar referencia antes de asignación\n",
        "    dqn = None\n",
        "    save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"\n",
        "    # Inicializar callbacks al principio para asegurarnos de que siempre esté definido\n",
        "    callbacks = []\n",
        "    # Verificar target_model solo si estamos usando DDQN\n",
        "    if enable_double_dqn and target_model is None:\n",
        "        raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
        "\n",
        "    # Crear el procesador Atari\n",
        "    processor = AtariProcessor()\n",
        "    try:\n",
        "        # Verificar que tenemos un modelo válido\n",
        "        if model is None:\n",
        "            raise ValueError(\"El modelo principal no puede ser None.\")\n",
        "        # Verificar target_model solo si estamos usando DDQN\n",
        "        if enable_double_dqn and target_model is None:\n",
        "            raise ValueError(\"Para DDQN, el target_model no puede ser None.\")\n",
        "\n",
        "        # Verificar si la memoria ya se creó o necesitamos crearla\n",
        "        if memory is None:\n",
        "            print(\"Creando nueva memoria de experiencia...\")\n",
        "            memory = SequentialMemory(limit=memoria_tamano, window_length=WINDOW_LENGTH)\n",
        "\n",
        "        # Política de exploración\n",
        "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
        "                                     attr='eps',\n",
        "                                     value_max=epsilon_start,\n",
        "                                     value_min=epsilon_min,\n",
        "                                     value_test=.05,\n",
        "                                     nb_steps=epsilon_steps)\n",
        "        # Crear agente DQN\n",
        "        dqn = DQNAgent(\n",
        "            model=model,\n",
        "            nb_actions=env.action_space.n,\n",
        "            memory=memory,\n",
        "            processor=processor,\n",
        "            nb_steps_warmup=warmup_steps,\n",
        "            target_model_update=target_update_interval if enable_double_dqn else 10000,\n",
        "            enable_double_dqn=enable_double_dqn,\n",
        "            policy=policy,\n",
        "            gamma=0.99,\n",
        "            train_interval=4,\n",
        "            delta_clip=1.0,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        # Después de crear el agente, reemplazar el target_model si estamos usando DDQN\n",
        "        if enable_double_dqn and target_model is not None:\n",
        "            dqn.target_model = target_model\n",
        "\n",
        "        # Compilar el agente\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al crear o compilar el agente: {str(e)}\")\n",
        "        return None, False\n",
        "\n",
        "    # Verificar que tenemos un agente válido antes de continuar\n",
        "    if dqn is None:\n",
        "        print(f\"❌ Error: No se pudo inicializar el agente DQN\")\n",
        "        return None, False\n",
        "    try:\n",
        "        # DEBUG --------------------\n",
        "        # callbacks = [\n",
        "        #     log_filename = f'{checkpoint_path}/{name_model}_log.json'\n",
        "        #     progress_callback,\n",
        "        #     ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
        "        #     CustomFileLogger(log_filename, interval=1000),\n",
        "        #     PerformanceMonitor(save_path='diagnosticos')\n",
        "        # ]\n",
        "        # Callbacks optimizados para el objetivo\n",
        "        callbacks = [\n",
        "            SimpleProgressCallback(num_steps, print_interval=20000),\n",
        "            TargetRewardTracker(dqn,target_avg_reward=target_reward, name_model=name_model, window_size=100,\n",
        "                                save_best=True, checkpoint_dir=checkpoint_path),\n",
        "            EpisodeCheckpointCallback(dqn, checkpoint_path=checkpoint_path, save_freq=1000, model_name=name_model)\n",
        "        ]\n",
        "\n",
        "         # Ajustar el número de pasos restantes si estamos continuando el entrenamiento\n",
        "        adjusted_steps = max(0, num_steps - start_steps)\n",
        "        if start_steps > 0:\n",
        "            print(f\"Continuando desde el paso {start_steps} (quedan {adjusted_steps} pasos)\")\n",
        "\n",
        "        print(f\"Iniciando entrenamiento de {name_model} por {adjusted_steps} pasos...\")\n",
        "        start_time = time.time()\n",
        "        # Fit del agente al entorno\n",
        "        history = dqn.fit(env, nb_steps=adjusted_steps, callbacks=callbacks, verbose=2)\n",
        "\n",
        "        training_time = (time.time() - start_time) / 60\n",
        "        print(f\"Entrenamiento completado en {training_time:.2f} minutos\")\n",
        "\n",
        "        # Guardar checkpoint final\n",
        "        # Verificar correctamente las claves disponibles y usar la adecuada\n",
        "        if hasattr(history, 'history'):\n",
        "            if 'episode' in history.history and history.history['episode']:\n",
        "                episode = start_episode + history.history['episode'][-1]\n",
        "            elif 'nb_episode' in history.history and history.history['nb_episode']:\n",
        "                episode = start_episode + history.history['nb_episode'][-1]\n",
        "            else:\n",
        "                # Intentar obtener el episodio del agente directamente\n",
        "                episode = getattr(dqn, 'episode', start_episode)\n",
        "        else:\n",
        "            episode = start_episode\n",
        "        steps = start_steps + adjusted_steps\n",
        "        epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
        "        try:\n",
        "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
        "                                checkpoint_dir=save_checkpoint_path, suffix=\"lastest\",\n",
        "                                epsilon=epsilon)\n",
        "\n",
        "            # También guardar como \"best\" si no hay un checkpoint \"best\" previo\n",
        "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
        "                                    checkpoint_dir=save_checkpoint_path, suffix=\"best\",\n",
        "                                    epsilon=epsilon)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error al guardar el modelo: {str(e)}\")\n",
        "\n",
        "        return dqn, True\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nEntrenamiento interrumpido por el usuario\")\n",
        "        # Guardar pesos de emergencia\n",
        "        try:\n",
        "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
        "            episode = start_episode  # No podemos saber el episodio exacto después de la interrupción\n",
        "            steps = start_steps + dqn.step if hasattr(dqn, 'step') else start_steps\n",
        "            save_model_checkpoint(dqn, name_model, episode=episode, steps=steps,\n",
        "                                checkpoint_dir=save_checkpoint_path, suffix=\"emergency\",\n",
        "                                epsilon=epsilon)\n",
        "            print(\"✅ Modelo guardado en estado de emergencia\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error al guardar el modelo de emergencia: {str(e)}\")\n",
        "        return dqn, True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ [ERROR] - Error durante el entrenamiento: {str(e)}\")\n",
        "        # Intentar guardar en estado de error\n",
        "        try:\n",
        "            epsilon = dqn.policy.eps if hasattr(dqn, 'policy') and hasattr(dqn.policy, 'eps') else 0.1\n",
        "            save_model_checkpoint(dqn, name_model, episode=start_episode,\n",
        "                                steps=start_steps + (dqn.step if hasattr(dqn, 'step') else 0),\n",
        "                                checkpoint_dir=save_checkpoint_path, suffix=\"error_recovery\",\n",
        "                                epsilon=epsilon)\n",
        "            print(\"✅ Modelo guardado en estado de recuperación de error\")\n",
        "        except Exception as e2:\n",
        "            print(f\"⚠️ Error al guardar modelo de recuperación: {str(e2)}\")\n",
        "        return None, False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N93fvkPleC2U"
      },
      "source": [
        "#### **EVALUACION** ***********"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GZzkrK72yj1"
      },
      "outputs": [],
      "source": [
        "# Función para evaluar el modelo\n",
        "def evaluar_modelo(agent, model_name, env, num_episodes=200, render=True, record_video=False):\n",
        "    \"\"\"\n",
        "    Evalúa un modelo DQN o DDQN y si el modelo alcanza el objetivo de media\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "        agent: Agente DQN entrenado\n",
        "        model_name: Nobre del agente\n",
        "        env: Entorno de gym\n",
        "        num_episodes: Número de episodios para evaluar\n",
        "        render: Si se debe mostrar la visualización\n",
        "        record_video: Si se debe grabar video\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "        Lista de recompensas por episodio\n",
        "    \"\"\"\n",
        "    print(f\"🎯 EVALUANDO MODELO {model_name}\")\n",
        "    print(f\"📊 Evaluando por {num_episodes} episodios...\")\n",
        "    rewards = []\n",
        "    rewards_clip = []\n",
        "\n",
        "    # Setup for frame stacking\n",
        "    window_length = 4  # As specified in your model input shape\n",
        "\n",
        "    try:\n",
        "        for episode in range(num_episodes):\n",
        "            # Reset environment\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            total_reward_clip = 0\n",
        "            step = 0\n",
        "\n",
        "            # Initialize the frame buffer with the initial observation\n",
        "            if hasattr(agent, 'processor') and agent.processor is not None:\n",
        "                processed_obs = agent.processor.process_observation(observation)\n",
        "            else:\n",
        "                processed_obs = observation\n",
        "\n",
        "            # Create a frame stack buffer of the right shape\n",
        "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
        "\n",
        "            # Fill buffer with the first observation\n",
        "            for i in range(window_length):\n",
        "                frame_buffer[i] = processed_obs\n",
        "\n",
        "            while not done and step < 2000:\n",
        "                try:\n",
        "                    # Prepare input in the format expected by the model: (batch, channels=window_length, h, w)\n",
        "                    # Format directly to channels_first, correcting for the specific shape expected\n",
        "                    input_data = np.expand_dims(frame_buffer, axis=0)  # Add batch dimension\n",
        "\n",
        "                    # Get Q-values directly from the model\n",
        "                    if hasattr(agent, 'model'):\n",
        "                        q_values = agent.model.predict(input_data)\n",
        "\n",
        "                        # Handle the output format for Dueling DQN\n",
        "                        if isinstance(q_values, list):\n",
        "                            q_values = q_values[0]  # Take first output for value function\n",
        "\n",
        "                        action = np.argmax(q_values)\n",
        "                    else:\n",
        "                        # Fallback to random action\n",
        "                        print(f\"⚠️ No se encuentra ningún modelo: {model_name}, utilizando una acción aleatoria\")\n",
        "                        action = env.action_space.sample()\n",
        "\n",
        "                    # Execute action\n",
        "                    observation, reward, done, info = env.step(action)\n",
        "\n",
        "                    # Render if enabled\n",
        "                    if render:\n",
        "                        env.render()\n",
        "\n",
        "                    # Process new observation\n",
        "                    if hasattr(agent, 'processor') and agent.processor is not None:\n",
        "                        processed_obs = agent.processor.process_observation(observation)\n",
        "                    else:\n",
        "                        processed_obs = observation\n",
        "\n",
        "                    # Update frame buffer - shift frames\n",
        "                    for i in range(window_length-1):\n",
        "                        frame_buffer[i] = frame_buffer[i+1]\n",
        "                    frame_buffer[window_length-1] = processed_obs\n",
        "\n",
        "                    # Aplica clipping de la recompensa entre -1 y 1\n",
        "                    reward_clip = np.clip(reward, -1.0, 1.0)\n",
        "                    # Update counters\n",
        "                    total_reward += reward\n",
        "                    total_reward_clip += reward_clip\n",
        "                    step += 1\n",
        "\n",
        "                except Exception as step_error:\n",
        "                    print(f\"⚠️ Error en el paso {step}: {step_error}\")\n",
        "                    break\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "            rewards_clip.append(total_reward_clip)\n",
        "            print(f\"   Episodio {episode + 1}/{num_episodes}: reward (clip) = {total_reward_clip:.1f}: reward (real) = {total_reward:.1f}\")\n",
        "\n",
        "        # Análisis final\n",
        "        if len(rewards_clip) > 0:\n",
        "            avg_reward = np.mean(rewards_clip)\n",
        "            std_reward = np.std(rewards_clip)\n",
        "            max_reward = np.max(rewards_clip)\n",
        "            min_reward = np.min(rewards_clip)\n",
        "\n",
        "            objetivo_alcanzado = avg_reward >= TARGET_REWARD\n",
        "\n",
        "            print(f\"\\n📊 RESULTADOS DE EVALUACIÓN:\")\n",
        "            print(f\"   Media: {avg_reward:.2f} {'✅' if objetivo_alcanzado else '❌'}\")\n",
        "            print(f\"   Desviación: ±{std_reward:.2f}\")\n",
        "            print(f\"   Máximo: {max_reward:.2f}\")\n",
        "            print(f\"   Mínimo: {min_reward:.2f}\")\n",
        "            print(f\"   Episodios sobre {TARGET_REWARD}: {sum(1 for r in rewards_clip if r >= TARGET_REWARD)} / {len(rewards_clip)}\")\n",
        "\n",
        "            if objetivo_alcanzado:\n",
        "                print(f\"🏆 ¡OBJETIVO ALCANZADO! El modelo tiene una media de {avg_reward:.2f}\")\n",
        "            else:\n",
        "                print(f\"📈 Progreso: {(avg_reward/TARGET_REWARD)*100:.1f}% del objetivo\")\n",
        "        else:\n",
        "            print(\"❌ No se completaron episodios correctamente\")\n",
        "            avg_reward = float('nan')\n",
        "            objetivo_alcanzado = False\n",
        "\n",
        "        # Limpiar entorno si es necesario\n",
        "        if hasattr(env, 'close'):\n",
        "            env.close()\n",
        "        return rewards_clip, objetivo_alcanzado\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ [ERROR] - Error durante evaluación: {e}\")\n",
        "        # Limpiar entorno si es necesario\n",
        "        if hasattr(env, 'close'):\n",
        "            env.close()\n",
        "        return [], False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP4CEvcaeC2V"
      },
      "outputs": [],
      "source": [
        "def grabar_video_del_modelo(model, model_name, env, num_episodes=1, video_dir=\"checkpoints/videos\", fps=30, max_steps=10000):\n",
        "    \"\"\"\n",
        "    Graba un video del comportamiento del modelo en el entorno.\n",
        "    Los frames se capturan manualmente y se guardan directamente en checkpoints/videos.\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "    model: El agente entrenado (DQN, DDQN, etc.) con un método .predict() y un .processor.\n",
        "    model_name: Nombre del modelo para identificar el archivo de video.\n",
        "    env: El entorno de Gym. Debe ser capaz de renderizar en 'rgb_array'.\n",
        "         Asegúrate de que este entorno NO tiene wrappers de FrameStack si tu agente\n",
        "         ya maneja el apilamiento de frames con su procesador.\n",
        "    num_episodes: Número de episodios a grabar.\n",
        "    fps: Frames por segundo para el video.\n",
        "    max_steps: Número máximo de pasos a ejecutar por episodio.\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "    str: Ruta al archivo de video grabado, o None si hubo un error.\n",
        "    \"\"\"\n",
        "    # Crear directorio para videos\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "    # Nombre del archivo MP4 directamente en el directorio de videos\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    video_path = os.path.join(video_dir, f\"{model_name}_{timestamp}.mp4\")\n",
        "\n",
        "    frames = [] # Lista para almacenar los frames RGB capturados\n",
        "\n",
        "    # Obtener window_length del agente (para el frame stacking de la lógica del agente)\n",
        "    window_length = 4 # Valor por defecto\n",
        "    if hasattr(model, 'processor') and hasattr(model.processor, 'window_length'):\n",
        "        # Ajuste si el processor está anidado, como en keras-rl\n",
        "        if hasattr(model.processor, 'processor') and hasattr(model.processor.processor, 'window_length'):\n",
        "            window_length = model.processor.processor.window_length\n",
        "        else:\n",
        "            window_length = model.processor.window_length\n",
        "\n",
        "    try:\n",
        "        print(f\"📹 Grabando video del modelo {model_name} en: {video_path}...\")\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            # Inicializar episodio\n",
        "            observation_raw = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            steps = 0\n",
        "\n",
        "            # Preprocesar la observación inicial para el agente y llenar el buffer\n",
        "            processed_obs = model.processor.process_observation(observation_raw)\n",
        "\n",
        "            frame_buffer = np.zeros((window_length, *processed_obs.shape), dtype=np.float32)\n",
        "            for k in range(window_length):\n",
        "                frame_buffer[k] = processed_obs\n",
        "\n",
        "            while not done and steps < max_steps:\n",
        "                # Capturar frame RGB para el video\n",
        "                # env.render() DEBE devolver un array RGB (altura, anchura, 3)\n",
        "                current_frame_rgb = env.render(mode='rgb_array')\n",
        "\n",
        "                if current_frame_rgb is not None:\n",
        "                    frames.append(current_frame_rgb)\n",
        "                else:\n",
        "                    print(\"Advertencia: env.render() devolvió None. No se pudieron capturar frames para el video.\")\n",
        "                    # Si no se capturan frames, el video no se generará.\n",
        "                    # Puedes optar por romper el bucle o continuar, pero el video estará vacío.\n",
        "\n",
        "                # Preparar la entrada para el modelo del agente\n",
        "                input_data_for_model = np.expand_dims(frame_buffer, axis=0)\n",
        "\n",
        "                # Obtener acción del modelo\n",
        "                action = None\n",
        "                if hasattr(model, 'model'): # Asumo que tu agente tiene un atributo 'model' que es un modelo Keras\n",
        "                    try:\n",
        "                        q_values = model.model.predict(input_data_for_model, verbose=0)\n",
        "                        if isinstance(q_values, list):\n",
        "                            q_values = q_values[0]\n",
        "                        action = np.argmax(q_values)\n",
        "                    except Exception as predict_error:\n",
        "                        print(f\"⚠️ Error al predecir la acción: {predict_error}. Usando acción aleatoria.\")\n",
        "                        action = env.action_space.sample()\n",
        "                else: # Fallback a acción aleatoria\n",
        "                    action = env.action_space.sample()\n",
        "\n",
        "                # Ejecutar acción en el entorno\n",
        "                observation_next_raw, reward, done, info = env.step(action)\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                # Preprocesar la nueva observación para actualizar el buffer de frames\n",
        "                processed_next_obs = model.processor.process_observation(observation_next_raw)\n",
        "                # Actualizar el buffer de frames\n",
        "                frame_buffer[:window_length-1] = frame_buffer[1:]\n",
        "                frame_buffer[window_length-1] = processed_next_obs\n",
        "\n",
        "            print(f\"  Episodio {episode+1}: Recompensa Total = {total_reward} (Pasos: {steps})\")\n",
        "\n",
        "        # Guardar video con imageio\n",
        "        if frames:\n",
        "            print(f\"💾 Guardando video con {len(frames)} frames en {video_path}...\")\n",
        "            # Usa fps aquí para controlar la velocidad del video\n",
        "            imageio.mimsave(video_path, frames, fps=fps)\n",
        "            print(f\"✅ Video guardado en: {video_path}\")\n",
        "        else:\n",
        "            print(\"❌ No se generaron frames para el video. El archivo MP4 no se creará.\")\n",
        "            video_path = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al grabar el video: {e}\")\n",
        "        video_path = None\n",
        "\n",
        "    finally:\n",
        "        # Cerrar entorno SIEMPRE al final\n",
        "        env.close()\n",
        "\n",
        "    return video_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c15Z8D2neC2V"
      },
      "source": [
        "### ¡¡¡¡¡¡¡ **EJECUCION - MAIN** !!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "jOau8678eC2V"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    tf.keras.backend.clear_session()\n",
        "    # Control global de si se entrena o solo se carga\n",
        "    training_global = True\n",
        "    # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
        "    episode_render = False\n",
        "    # Asegurar que existe el directorio\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    # Optimizar configuración de TensorFlow\n",
        "    optimizar_tensorflow()\n",
        "    tf.config.run_functions_eagerly(False)\n",
        "    gc.collect()\n",
        "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
        "    # Ejecutar prueba\n",
        "    print(\"🚀 EJECUTANDO SOLUCIÓN...\")\n",
        "    print(f\"🎯 OBJETIVO: Conseguir media de episode_reward = {TARGET_REWARD} (con clipping)\")\n",
        "\n",
        "    # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
        "    trained_models = {}\n",
        "    # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
        "    modelos_a_procesar = [\n",
        "        ('DQN', create_dqn_model, False),\n",
        "        ('DDQN', create_ddqn_models, False),\n",
        "        ('DDQN_REPLAY', create_ddqn_replay_model, True),\n",
        "        ('DUELING_DQN_REPLAY', create_dueling_dqn_replay_model, False)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        for model_name, model_process, training_specific_flag in modelos_a_procesar:\n",
        "            # La bandera de entrenamiento final es la global AND la específica del modelo\n",
        "            # Verificar el tipo de modelo y establecer enable_double_dqn correctamente\n",
        "            enable_double_dqn = model_name in ['DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']\n",
        "            entrenarSN = training_global and training_specific_flag\n",
        "            model_instance = False\n",
        "            agent = None\n",
        "            save_checkpoint_path = f\"{checkpoint_path}/{model_name}\"\n",
        "            if entrenarSN:\n",
        "                # Crear una nueva sesión para cada modelo\n",
        "                tf.keras.backend.clear_session()\n",
        "                # Intentar cargar un modelo previamente guardado (independientemente de si entrenaremos o no)\n",
        "                try:\n",
        "                    # Crear una instancia del modelo - crear primero la arquitectura antes de poder cargar los pesos en ella\n",
        "                    if enable_double_dqn:\n",
        "                        model, memory, target_model = model_process(input_shape, env.action_space.n, memory_size)\n",
        "                    else:\n",
        "                        model, memory, _ = model_process(input_shape, env.action_space.n, memory_size)\n",
        "                        target_model = None\n",
        "\n",
        "                    # Intentar cargar el mejor checkpoint (o lastest si best no existe)\n",
        "                    start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name,\n",
        "                                                                                 checkpoint_dir=save_checkpoint_path, suffix=\"best\")\n",
        "                    if start_episode == 0:\n",
        "                        # Si no encontró el mejor, intentar con el último guardado\n",
        "                        start_episode, global_steps, epsilon = load_model_checkpoint(model, memory, target_model, model_name,\n",
        "                                                                                     checkpoint_dir=save_checkpoint_path, suffix=\"lastest\")\n",
        "                    if start_episode > 0:\n",
        "                        model_instance = True\n",
        "                        print(f\"✅ Modelo {model_name} cargado exitosamente desde el episodio {start_episode}\")\n",
        "                        # Si se debe entrenar, continuar desde donde quedó\n",
        "                        if entrenarSN:\n",
        "                            print(f\"⏩ Continuando entrenamiento de {model_name} desde episodio {start_episode+1}\")\n",
        "                            # Establecer parámetros para continuar el entrenamiento\n",
        "                            epsilon_actual = epsilon\n",
        "\n",
        "                            # Entrenar el modelo desde donde quedó\n",
        "                            agent, success = entrenar_modelo(\n",
        "                                env=env,\n",
        "                                model_name=model_name,\n",
        "                                model=model,\n",
        "                                memory=memory,\n",
        "                                target_model=target_model,\n",
        "                                model_instance=model_instance,\n",
        "                                checkpoint_path=checkpoint_path,\n",
        "                                start_episode=start_episode+1,\n",
        "                                start_steps=global_steps,\n",
        "                                epsilon_start=epsilon_actual,  # Usar el epsilon guardado\n",
        "                                epsilon_min=epsilon_stop,\n",
        "                                epsilon_steps=EPSILON_STEPS,\n",
        "                                num_steps=NUM_TRAINING_STEPS,\n",
        "                                warmup_steps=WARMUP_STEPS,\n",
        "                                target_update_interval=TARGET_UPDATE_INTERVAL,\n",
        "                                target_update_tau=tau,\n",
        "                                enable_double_dqn = enable_double_dqn\n",
        "                            )\n",
        "                    else:\n",
        "                        print(f\"🆕 Creando y entrenando un nuevo modelo {model_name}\")\n",
        "                        model_instance = False\n",
        "                        # Entrenar el modelo desde cero\n",
        "                        agent, success = entrenar_modelo(\n",
        "                            env=env,\n",
        "                            model_name=model_name,\n",
        "                            model=model,\n",
        "                            memory=memory,\n",
        "                            target_model=target_model,\n",
        "                            model_instance=model_instance,\n",
        "                            checkpoint_path=checkpoint_path,\n",
        "                            start_episode=0,  # Añadido: Especificar episodio inicial\n",
        "                            start_steps=0,    # Añadido: Especificar pasos iniciales\n",
        "                            epsilon_start=epsilon_start,\n",
        "                            epsilon_min=epsilon_stop,\n",
        "                            epsilon_steps=EPSILON_STEPS,\n",
        "                            num_steps=NUM_TRAINING_STEPS,\n",
        "                            warmup_steps=WARMUP_STEPS,\n",
        "                            target_update_interval=TARGET_UPDATE_INTERVAL,\n",
        "                            target_update_tau=tau,\n",
        "                            enable_double_dqn = enable_double_dqn\n",
        "                        )\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ [ERROR] - Error al cargar o entrenar modelo {model_name}: {e}\")\n",
        "\n",
        "            # Si tenemos un modelo válido (cargado o entrenado), evaluarlo y guardarlo en el diccionario\n",
        "            # Evaluación rápida con 10 episodios para:\n",
        "            # - Comprobar el rendimiento básico del modelo\n",
        "            # - Decidir si vale la pena guardarlo como \"best\"\n",
        "            # - Mostrar un feedback rápido sobre su desempeño\n",
        "            if agent:\n",
        "                rewards, _ = evaluar_modelo(agent, model_name, env, num_episodes=10, render=False, record_video=False)\n",
        "                avg_reward = np.mean(rewards)\n",
        "                trained_models[model_name] = agent\n",
        "                print(f\"📊 Recompensa promedio para {model_name}: {avg_reward:.2f}\")\n",
        "                # Guardar el modelo como \"best\" si supera umbral de evaluación\n",
        "                if avg_reward >= TARGET_REWARD * 0.8:  # 80% del objetivo como umbral mínimo\n",
        "                    save_model_checkpoint(agent, model_name, episode=start_episode, steps=global_steps,  # Corregido: Usar el episodio y pasos actuales\n",
        "                                         checkpoint_dir=save_checkpoint_path, suffix=\"best_reward\",\n",
        "                                         epsilon=epsilon_start)\n",
        "                    print(f\"🏆 Modelo {model_name} guardado como 'best_reward' con recompensa {avg_reward:.2f}\")\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # Búsqueda del mejor modelo entre todos los entrenados/cargados: comparar su rendimiento\n",
        "        # con el mismo número de episodios (10) para mantener una comparación justa\n",
        "        # Ayuda a determinar cuál es el mejor modelo para la evaluación final\n",
        "        best_model_name = None\n",
        "        best_reward = -float('inf')\n",
        "        for name, model in trained_models.items():\n",
        "            rewards, objetivo_conseguido = evaluar_modelo(model, model_name, env, num_episodes=10, render=False, record_video=False)\n",
        "            avg_reward = np.mean(rewards)\n",
        "            if avg_reward > best_reward:\n",
        "                best_reward = avg_reward\n",
        "                best_model_name = name\n",
        "\n",
        "        if best_model_name:\n",
        "            print(\"\\n✅ SOLUCIÓN EXITOSA - Entrenamiento completado\")\n",
        "            print(f\"\\n🥇 El mejor modelo es {best_model_name} con recompensa promedio {best_reward:.2f}\")\n",
        "            best_model = trained_models[best_model_name]\n",
        "\n",
        "            # Evaluación final y más exhaustiva del mejor modelo: Se hace con muchos más episodios (200)\n",
        "            #   para obtener resultados estadísticamente más significativos. Es la evaluación definitiva\n",
        "            #   para determinar si se cumple el objetivo --> conclusión final del proceso\n",
        "            #   print(f\"\\n🎯 EVALUACIÓN FINAL DEL OBJETIVO\")\n",
        "            #   rewards_eval, objetivo_conseguido = evaluar_modelo(best_model, env, num_episodes=200)\n",
        "\n",
        "            # Grabar video del mejor modelo\n",
        "            video_path = grabar_video_del_modelo(best_model, best_model_name, env)\n",
        "            if video_path:\n",
        "                print(f\"🎬 Se ha grabado una demostración del modelo en: {video_path}\")\n",
        "\n",
        "            if objetivo_conseguido:\n",
        "                print(f\"🏆 ¡FELICIDADES EQUIPO! El modelo alcanzó el objetivo de media {TARGET_REWARD}\")\n",
        "            else:\n",
        "                print(f\"📈 El modelo necesita más entrenamiento para alcanzar media {TARGET_REWARD}\")\n",
        "        else:\n",
        "            print(\"❌ [ERROR] - No se pudo entrenar ningún modelo correctamente\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\n⚠️ Entrenamiento interrumpido por el usuario\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\n❌ Error inesperado: {e}\")\n",
        "    finally:\n",
        "        # Asegurar que siempre se cierra el entorno\n",
        "        env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdLm-013eC2W",
        "outputId": "bcb982d9-5aa1-4e53-9e00-b265092f3513"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " [INFO] - TensorFlow optimizado para 96 cores CPU\n",
            "🚀 EJECUTANDO SOLUCIÓN...\n",
            "🎯 OBJETIVO: Conseguir media de episode_reward = 20.0 (con clipping)\n",
            "🏗️ Creando modelos DDQN_replay: input_shape=(84, 84, 4), actions=6\n",
            "✅ Modelo creado exitosamente\n",
            "📊 Resumen del modelo:\n",
            "Model: \"DDQN_replay_Main_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_channels_first (Input  [(None, 4, 84, 84)]      0         \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            " convert_to_channels_last (P  (None, 84, 84, 4)        0         \n",
            " ermute)                                                         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (None, 512)               1606144   \n",
            "                                                                 \n",
            " q_values (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,687,206\n",
            "Trainable params: 1,687,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "⚠️ No se encontró el checkpoint best para DDQN_REPLAY\n",
            "⚠️ No se encontró el checkpoint lastest para DDQN_REPLAY\n",
            "🆕 Creando y entrenando un nuevo modelo DDQN_REPLAY\n",
            "🤖 Creando entrenamiento para DDQN_REPLAY...\n",
            "🎯 OBJETIVO: Media de episode_reward = 20.0\n",
            "📊 Ventana de evaluación: 100 episodios\n",
            "Iniciando entrenamiento de DDQN_REPLAY por 2000000 pasos...\n",
            "🚀 Entrenamiento iniciado: 2,000,000 pasos\n",
            "Training for 2000000 steps ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📈 Episodio 1: Recompensa total (clipped): 8.000, Pasos: 685, Mean Reward Calculado: 0.011679 (Recompensa/Pasos)\n",
            "     685/2000000: episode: 1, duration: 2.823s, episode steps: 685, steps per second: 243, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 2: Recompensa total (clipped): 9.000, Pasos: 769, Mean Reward Calculado: 0.011704 (Recompensa/Pasos)\n",
            "    1454/2000000: episode: 2, duration: 3.022s, episode steps: 769, steps per second: 254, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 3: Recompensa total (clipped): 6.000, Pasos: 755, Mean Reward Calculado: 0.007947 (Recompensa/Pasos)\n",
            "    2209/2000000: episode: 3, duration: 2.931s, episode steps: 755, steps per second: 258, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 4: Recompensa total (clipped): 12.000, Pasos: 1122, Mean Reward Calculado: 0.010695 (Recompensa/Pasos)\n",
            "    3331/2000000: episode: 4, duration: 4.340s, episode steps: 1122, steps per second: 259, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 5: Recompensa total (clipped): 11.000, Pasos: 620, Mean Reward Calculado: 0.017742 (Recompensa/Pasos)\n",
            "    3951/2000000: episode: 5, duration: 2.441s, episode steps: 620, steps per second: 254, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 6: Recompensa total (clipped): 16.000, Pasos: 1242, Mean Reward Calculado: 0.012882 (Recompensa/Pasos)\n",
            "    5193/2000000: episode: 6, duration: 4.902s, episode steps: 1242, steps per second: 253, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 7: Recompensa total (clipped): 6.000, Pasos: 610, Mean Reward Calculado: 0.009836 (Recompensa/Pasos)\n",
            "    5803/2000000: episode: 7, duration: 2.372s, episode steps: 610, steps per second: 257, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 8: Recompensa total (clipped): 12.000, Pasos: 782, Mean Reward Calculado: 0.015345 (Recompensa/Pasos)\n",
            "    6585/2000000: episode: 8, duration: 3.048s, episode steps: 782, steps per second: 257, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 9: Recompensa total (clipped): 9.000, Pasos: 792, Mean Reward Calculado: 0.011364 (Recompensa/Pasos)\n",
            "    7377/2000000: episode: 9, duration: 3.130s, episode steps: 792, steps per second: 253, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 10: Recompensa total (clipped): 5.000, Pasos: 501, Mean Reward Calculado: 0.009980 (Recompensa/Pasos)\n",
            "    7878/2000000: episode: 10, duration: 1.955s, episode steps: 501, steps per second: 256, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 11: Recompensa total (clipped): 8.000, Pasos: 661, Mean Reward Calculado: 0.012103 (Recompensa/Pasos)\n",
            "    8539/2000000: episode: 11, duration: 2.595s, episode steps: 661, steps per second: 255, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 12: Recompensa total (clipped): 8.000, Pasos: 512, Mean Reward Calculado: 0.015625 (Recompensa/Pasos)\n",
            "    9051/2000000: episode: 12, duration: 2.019s, episode steps: 512, steps per second: 254, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 13: Recompensa total (clipped): 6.000, Pasos: 446, Mean Reward Calculado: 0.013453 (Recompensa/Pasos)\n",
            "    9497/2000000: episode: 13, duration: 1.763s, episode steps: 446, steps per second: 253, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 14: Recompensa total (clipped): 8.000, Pasos: 379, Mean Reward Calculado: 0.021108 (Recompensa/Pasos)\n",
            "    9876/2000000: episode: 14, duration: 1.468s, episode steps: 379, steps per second: 258, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 15: Recompensa total (clipped): 10.000, Pasos: 758, Mean Reward Calculado: 0.013193 (Recompensa/Pasos)\n",
            "   10634/2000000: episode: 15, duration: 3.029s, episode steps: 758, steps per second: 250, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 16: Recompensa total (clipped): 8.000, Pasos: 552, Mean Reward Calculado: 0.014493 (Recompensa/Pasos)\n",
            "   11186/2000000: episode: 16, duration: 2.152s, episode steps: 552, steps per second: 256, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 17: Recompensa total (clipped): 6.000, Pasos: 645, Mean Reward Calculado: 0.009302 (Recompensa/Pasos)\n",
            "   11831/2000000: episode: 17, duration: 2.489s, episode steps: 645, steps per second: 259, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 18: Recompensa total (clipped): 10.000, Pasos: 628, Mean Reward Calculado: 0.015924 (Recompensa/Pasos)\n",
            "   12459/2000000: episode: 18, duration: 2.421s, episode steps: 628, steps per second: 259, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 19: Recompensa total (clipped): 12.000, Pasos: 624, Mean Reward Calculado: 0.019231 (Recompensa/Pasos)\n",
            "   13083/2000000: episode: 19, duration: 2.543s, episode steps: 624, steps per second: 245, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 20: Recompensa total (clipped): 8.000, Pasos: 693, Mean Reward Calculado: 0.011544 (Recompensa/Pasos)\n",
            "   13776/2000000: episode: 20, duration: 2.791s, episode steps: 693, steps per second: 248, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 21: Recompensa total (clipped): 7.000, Pasos: 770, Mean Reward Calculado: 0.009091 (Recompensa/Pasos)\n",
            "   14546/2000000: episode: 21, duration: 3.016s, episode steps: 770, steps per second: 255, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 22: Recompensa total (clipped): 7.000, Pasos: 627, Mean Reward Calculado: 0.011164 (Recompensa/Pasos)\n",
            "   15173/2000000: episode: 22, duration: 2.450s, episode steps: 627, steps per second: 256, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 23: Recompensa total (clipped): 16.000, Pasos: 1139, Mean Reward Calculado: 0.014047 (Recompensa/Pasos)\n",
            "   16312/2000000: episode: 23, duration: 4.439s, episode steps: 1139, steps per second: 257, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 24: Recompensa total (clipped): 5.000, Pasos: 421, Mean Reward Calculado: 0.011876 (Recompensa/Pasos)\n",
            "   16733/2000000: episode: 24, duration: 1.682s, episode steps: 421, steps per second: 250, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 25: Recompensa total (clipped): 11.000, Pasos: 806, Mean Reward Calculado: 0.013648 (Recompensa/Pasos)\n",
            "   17539/2000000: episode: 25, duration: 3.151s, episode steps: 806, steps per second: 256, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 26: Recompensa total (clipped): 9.000, Pasos: 597, Mean Reward Calculado: 0.015075 (Recompensa/Pasos)\n",
            "   18136/2000000: episode: 26, duration: 2.323s, episode steps: 597, steps per second: 257, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 27: Recompensa total (clipped): 13.000, Pasos: 773, Mean Reward Calculado: 0.016818 (Recompensa/Pasos)\n",
            "   18909/2000000: episode: 27, duration: 2.980s, episode steps: 773, steps per second: 259, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 28: Recompensa total (clipped): 12.000, Pasos: 847, Mean Reward Calculado: 0.014168 (Recompensa/Pasos)\n",
            "   19756/2000000: episode: 28, duration: 3.283s, episode steps: 847, steps per second: 258, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📊 Paso 20,000/2,000,000 (1.0%) - 254.5 pasos/seg - ETA: 2.2h - Memoria: 848.75 MB\n",
            "📈 Episodio 29: Recompensa total (clipped): 11.000, Pasos: 789, Mean Reward Calculado: 0.013942 (Recompensa/Pasos)\n",
            "   20545/2000000: episode: 29, duration: 3.116s, episode steps: 789, steps per second: 253, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 30: Recompensa total (clipped): 9.000, Pasos: 669, Mean Reward Calculado: 0.013453 (Recompensa/Pasos)\n",
            "   21214/2000000: episode: 30, duration: 2.608s, episode steps: 669, steps per second: 257, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 31: Recompensa total (clipped): 9.000, Pasos: 551, Mean Reward Calculado: 0.016334 (Recompensa/Pasos)\n",
            "   21765/2000000: episode: 31, duration: 2.123s, episode steps: 551, steps per second: 259, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 32: Recompensa total (clipped): 9.000, Pasos: 575, Mean Reward Calculado: 0.015652 (Recompensa/Pasos)\n",
            "   22340/2000000: episode: 32, duration: 2.217s, episode steps: 575, steps per second: 259, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 33: Recompensa total (clipped): 5.000, Pasos: 499, Mean Reward Calculado: 0.010020 (Recompensa/Pasos)\n",
            "   22839/2000000: episode: 33, duration: 1.934s, episode steps: 499, steps per second: 258, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 34: Recompensa total (clipped): 7.000, Pasos: 504, Mean Reward Calculado: 0.013889 (Recompensa/Pasos)\n",
            "   23343/2000000: episode: 34, duration: 1.986s, episode steps: 504, steps per second: 254, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 35: Recompensa total (clipped): 5.000, Pasos: 542, Mean Reward Calculado: 0.009225 (Recompensa/Pasos)\n",
            "   23885/2000000: episode: 35, duration: 2.121s, episode steps: 542, steps per second: 256, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 36: Recompensa total (clipped): 7.000, Pasos: 541, Mean Reward Calculado: 0.012939 (Recompensa/Pasos)\n",
            "   24426/2000000: episode: 36, duration: 2.087s, episode steps: 541, steps per second: 259, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 37: Recompensa total (clipped): 7.000, Pasos: 643, Mean Reward Calculado: 0.010886 (Recompensa/Pasos)\n",
            "   25069/2000000: episode: 37, duration: 2.510s, episode steps: 643, steps per second: 256, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 38: Recompensa total (clipped): 9.000, Pasos: 906, Mean Reward Calculado: 0.009934 (Recompensa/Pasos)\n",
            "   25975/2000000: episode: 38, duration: 3.507s, episode steps: 906, steps per second: 258, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 39: Recompensa total (clipped): 5.000, Pasos: 379, Mean Reward Calculado: 0.013193 (Recompensa/Pasos)\n",
            "   26354/2000000: episode: 39, duration: 1.513s, episode steps: 379, steps per second: 250, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 40: Recompensa total (clipped): 8.000, Pasos: 719, Mean Reward Calculado: 0.011127 (Recompensa/Pasos)\n",
            "   27073/2000000: episode: 40, duration: 2.841s, episode steps: 719, steps per second: 253, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 41: Recompensa total (clipped): 6.000, Pasos: 409, Mean Reward Calculado: 0.014670 (Recompensa/Pasos)\n",
            "   27482/2000000: episode: 41, duration: 1.603s, episode steps: 409, steps per second: 255, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 42: Recompensa total (clipped): 3.000, Pasos: 649, Mean Reward Calculado: 0.004622 (Recompensa/Pasos)\n",
            "   28131/2000000: episode: 42, duration: 2.575s, episode steps: 649, steps per second: 252, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 43: Recompensa total (clipped): 12.000, Pasos: 810, Mean Reward Calculado: 0.014815 (Recompensa/Pasos)\n",
            "   28941/2000000: episode: 43, duration: 3.186s, episode steps: 810, steps per second: 254, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 44: Recompensa total (clipped): 26.000, Pasos: 1569, Mean Reward Calculado: 0.016571 (Recompensa/Pasos)\n",
            "   30510/2000000: episode: 44, duration: 6.166s, episode steps: 1569, steps per second: 254, episode reward: 26.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 45: Recompensa total (clipped): 8.000, Pasos: 473, Mean Reward Calculado: 0.016913 (Recompensa/Pasos)\n",
            "   30983/2000000: episode: 45, duration: 1.872s, episode steps: 473, steps per second: 253, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 46: Recompensa total (clipped): 8.000, Pasos: 689, Mean Reward Calculado: 0.011611 (Recompensa/Pasos)\n",
            "   31672/2000000: episode: 46, duration: 2.688s, episode steps: 689, steps per second: 256, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 47: Recompensa total (clipped): 12.000, Pasos: 777, Mean Reward Calculado: 0.015444 (Recompensa/Pasos)\n",
            "   32449/2000000: episode: 47, duration: 3.045s, episode steps: 777, steps per second: 255, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 48: Recompensa total (clipped): 12.000, Pasos: 727, Mean Reward Calculado: 0.016506 (Recompensa/Pasos)\n",
            "   33176/2000000: episode: 48, duration: 2.879s, episode steps: 727, steps per second: 252, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 49: Recompensa total (clipped): 15.000, Pasos: 704, Mean Reward Calculado: 0.021307 (Recompensa/Pasos)\n",
            "   33880/2000000: episode: 49, duration: 2.785s, episode steps: 704, steps per second: 253, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 50: Recompensa total (clipped): 3.000, Pasos: 484, Mean Reward Calculado: 0.006198 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 50, pasos: 34364)\n",
            "💾 NUEVO MEJOR PROMEDIO: 9.08 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 50 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 3.00\n",
            "   Media últimos 100: 9.08 / 20.0\n",
            "   Mejor promedio histórico: 9.08\n",
            "   Estado: 📈 45.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "   34364/2000000: episode: 50, duration: 3.458s, episode steps: 484, steps per second: 140, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 51: Recompensa total (clipped): 8.000, Pasos: 570, Mean Reward Calculado: 0.014035 (Recompensa/Pasos)\n",
            "   34934/2000000: episode: 51, duration: 2.261s, episode steps: 570, steps per second: 252, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 52: Recompensa total (clipped): 13.000, Pasos: 882, Mean Reward Calculado: 0.014739 (Recompensa/Pasos)\n",
            "   35816/2000000: episode: 52, duration: 3.490s, episode steps: 882, steps per second: 253, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 53: Recompensa total (clipped): 13.000, Pasos: 794, Mean Reward Calculado: 0.016373 (Recompensa/Pasos)\n",
            "   36610/2000000: episode: 53, duration: 3.065s, episode steps: 794, steps per second: 259, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 54: Recompensa total (clipped): 15.000, Pasos: 1487, Mean Reward Calculado: 0.010087 (Recompensa/Pasos)\n",
            "   38097/2000000: episode: 54, duration: 5.782s, episode steps: 1487, steps per second: 257, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 55: Recompensa total (clipped): 2.000, Pasos: 543, Mean Reward Calculado: 0.003683 (Recompensa/Pasos)\n",
            "   38640/2000000: episode: 55, duration: 2.167s, episode steps: 543, steps per second: 251, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 56: Recompensa total (clipped): 8.000, Pasos: 676, Mean Reward Calculado: 0.011834 (Recompensa/Pasos)\n",
            "   39316/2000000: episode: 56, duration: 2.656s, episode steps: 676, steps per second: 255, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📊 Paso 40,000/2,000,000 (2.0%) - 252.3 pasos/seg - ETA: 2.2h - Memoria: 1003.36 MB\n",
            "📈 Episodio 57: Recompensa total (clipped): 15.000, Pasos: 867, Mean Reward Calculado: 0.017301 (Recompensa/Pasos)\n",
            "   40183/2000000: episode: 57, duration: 3.399s, episode steps: 867, steps per second: 255, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 58: Recompensa total (clipped): 6.000, Pasos: 545, Mean Reward Calculado: 0.011009 (Recompensa/Pasos)\n",
            "   40728/2000000: episode: 58, duration: 2.146s, episode steps: 545, steps per second: 254, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 59: Recompensa total (clipped): 17.000, Pasos: 1008, Mean Reward Calculado: 0.016865 (Recompensa/Pasos)\n",
            "   41736/2000000: episode: 59, duration: 3.941s, episode steps: 1008, steps per second: 256, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 60: Recompensa total (clipped): 10.000, Pasos: 785, Mean Reward Calculado: 0.012739 (Recompensa/Pasos)\n",
            "   42521/2000000: episode: 60, duration: 3.087s, episode steps: 785, steps per second: 254, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 61: Recompensa total (clipped): 17.000, Pasos: 1055, Mean Reward Calculado: 0.016114 (Recompensa/Pasos)\n",
            "   43576/2000000: episode: 61, duration: 4.089s, episode steps: 1055, steps per second: 258, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 62: Recompensa total (clipped): 13.000, Pasos: 1012, Mean Reward Calculado: 0.012846 (Recompensa/Pasos)\n",
            "   44588/2000000: episode: 62, duration: 3.912s, episode steps: 1012, steps per second: 259, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 63: Recompensa total (clipped): 12.000, Pasos: 792, Mean Reward Calculado: 0.015152 (Recompensa/Pasos)\n",
            "   45380/2000000: episode: 63, duration: 3.175s, episode steps: 792, steps per second: 249, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 64: Recompensa total (clipped): 5.000, Pasos: 362, Mean Reward Calculado: 0.013812 (Recompensa/Pasos)\n",
            "   45742/2000000: episode: 64, duration: 1.454s, episode steps: 362, steps per second: 249, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 65: Recompensa total (clipped): 3.000, Pasos: 464, Mean Reward Calculado: 0.006466 (Recompensa/Pasos)\n",
            "   46206/2000000: episode: 65, duration: 1.847s, episode steps: 464, steps per second: 251, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 66: Recompensa total (clipped): 11.000, Pasos: 799, Mean Reward Calculado: 0.013767 (Recompensa/Pasos)\n",
            "   47005/2000000: episode: 66, duration: 3.197s, episode steps: 799, steps per second: 250, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 67: Recompensa total (clipped): 6.000, Pasos: 726, Mean Reward Calculado: 0.008264 (Recompensa/Pasos)\n",
            "   47731/2000000: episode: 67, duration: 2.889s, episode steps: 726, steps per second: 251, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 68: Recompensa total (clipped): 5.000, Pasos: 325, Mean Reward Calculado: 0.015385 (Recompensa/Pasos)\n",
            "   48056/2000000: episode: 68, duration: 1.324s, episode steps: 325, steps per second: 245, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 69: Recompensa total (clipped): 6.000, Pasos: 615, Mean Reward Calculado: 0.009756 (Recompensa/Pasos)\n",
            "   48671/2000000: episode: 69, duration: 2.473s, episode steps: 615, steps per second: 249, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 70: Recompensa total (clipped): 7.000, Pasos: 484, Mean Reward Calculado: 0.014463 (Recompensa/Pasos)\n",
            "   49155/2000000: episode: 70, duration: 1.885s, episode steps: 484, steps per second: 257, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "📈 Episodio 71: Recompensa total (clipped): 6.000, Pasos: 773, Mean Reward Calculado: 0.007762 (Recompensa/Pasos)\n",
            "   49928/2000000: episode: 71, duration: 3.005s, episode steps: 773, steps per second: 257, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "  295965/2000000: episode: 427, duration: 21.831s, episode steps: 702, steps per second:  32, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.008406, mae: 0.399567, mean_q: 0.498961, mean_eps: 0.467895\n",
            "📈 Episodio 428: Recompensa total (clipped): 5.000, Pasos: 478, Mean Reward Calculado: 0.010460 (Recompensa/Pasos)\n",
            "  296443/2000000: episode: 428, duration: 14.767s, episode steps: 478, steps per second:  32, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009105, mae: 0.406486, mean_q: 0.506909, mean_eps: 0.466833\n",
            "📈 Episodio 429: Recompensa total (clipped): 6.000, Pasos: 523, Mean Reward Calculado: 0.011472 (Recompensa/Pasos)\n",
            "  296966/2000000: episode: 429, duration: 16.249s, episode steps: 523, steps per second:  32, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.008765, mae: 0.408958, mean_q: 0.511841, mean_eps: 0.465933\n",
            "📈 Episodio 430: Recompensa total (clipped): 12.000, Pasos: 512, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            "  297478/2000000: episode: 430, duration: 15.900s, episode steps: 512, steps per second:  32, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009515, mae: 0.412480, mean_q: 0.515736, mean_eps: 0.465000\n",
            "📈 Episodio 431: Recompensa total (clipped): 3.000, Pasos: 344, Mean Reward Calculado: 0.008721 (Recompensa/Pasos)\n",
            "  297822/2000000: episode: 431, duration: 10.616s, episode steps: 344, steps per second:  32, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.007863, mae: 0.401067, mean_q: 0.502076, mean_eps: 0.464230\n",
            "📈 Episodio 432: Recompensa total (clipped): 6.000, Pasos: 396, Mean Reward Calculado: 0.015152 (Recompensa/Pasos)\n",
            "  298218/2000000: episode: 432, duration: 12.348s, episode steps: 396, steps per second:  32, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.009641, mae: 0.406850, mean_q: 0.508274, mean_eps: 0.463564\n",
            "📈 Episodio 433: Recompensa total (clipped): 11.000, Pasos: 595, Mean Reward Calculado: 0.018487 (Recompensa/Pasos)\n",
            "  298813/2000000: episode: 433, duration: 18.673s, episode steps: 595, steps per second:  32, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.008390, mae: 0.407064, mean_q: 0.506844, mean_eps: 0.462671\n",
            "📈 Episodio 434: Recompensa total (clipped): 8.000, Pasos: 539, Mean Reward Calculado: 0.014842 (Recompensa/Pasos)\n",
            "  299352/2000000: episode: 434, duration: 16.859s, episode steps: 539, steps per second:  32, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.009565, mae: 0.406130, mean_q: 0.506702, mean_eps: 0.461652\n",
            "📈 Episodio 435: Recompensa total (clipped): 6.000, Pasos: 392, Mean Reward Calculado: 0.015306 (Recompensa/Pasos)\n",
            "  299744/2000000: episode: 435, duration: 12.225s, episode steps: 392, steps per second:  32, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.008112, mae: 0.406759, mean_q: 0.508887, mean_eps: 0.460817\n",
            "📊 Paso 300,000/2,000,000 (15.0%) - 38.9 pasos/seg - ETA: 12.1h - Memoria: 4479.56 MB\n",
            "📈 Episodio 436: Recompensa total (clipped): 8.000, Pasos: 481, Mean Reward Calculado: 0.016632 (Recompensa/Pasos)\n",
            "  300225/2000000: episode: 436, duration: 14.797s, episode steps: 481, steps per second:  33, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.009153, mae: 0.405549, mean_q: 0.506584, mean_eps: 0.460029\n",
            "📈 Episodio 437: Recompensa total (clipped): 16.000, Pasos: 1246, Mean Reward Calculado: 0.012841 (Recompensa/Pasos)\n",
            "  301471/2000000: episode: 437, duration: 38.326s, episode steps: 1246, steps per second:  33, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008508, mae: 0.412458, mean_q: 0.513501, mean_eps: 0.458474\n",
            "📈 Episodio 438: Recompensa total (clipped): 5.000, Pasos: 516, Mean Reward Calculado: 0.009690 (Recompensa/Pasos)\n",
            "  301987/2000000: episode: 438, duration: 16.092s, episode steps: 516, steps per second:  32, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.009151, mae: 0.412038, mean_q: 0.514791, mean_eps: 0.456890\n",
            "📈 Episodio 439: Recompensa total (clipped): 14.000, Pasos: 738, Mean Reward Calculado: 0.018970 (Recompensa/Pasos)\n",
            "  302725/2000000: episode: 439, duration: 22.793s, episode steps: 738, steps per second:  32, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008247, mae: 0.408680, mean_q: 0.511428, mean_eps: 0.455759\n",
            "📈 Episodio 440: Recompensa total (clipped): 18.000, Pasos: 800, Mean Reward Calculado: 0.022500 (Recompensa/Pasos)\n",
            "  303525/2000000: episode: 440, duration: 24.669s, episode steps: 800, steps per second:  32, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.009158, mae: 0.414931, mean_q: 0.518490, mean_eps: 0.454373\n",
            "📈 Episodio 441: Recompensa total (clipped): 29.000, Pasos: 1711, Mean Reward Calculado: 0.016949 (Recompensa/Pasos)\n",
            "  305236/2000000: episode: 441, duration: 52.975s, episode steps: 1711, steps per second:  32, episode reward: 29.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.008591, mae: 0.408709, mean_q: 0.510317, mean_eps: 0.452116\n",
            "📈 Episodio 442: Recompensa total (clipped): 13.000, Pasos: 806, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            "  306042/2000000: episode: 442, duration: 25.280s, episode steps: 806, steps per second:  32, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.008722, mae: 0.413533, mean_q: 0.514807, mean_eps: 0.449852\n",
            "📈 Episodio 443: Recompensa total (clipped): 16.000, Pasos: 798, Mean Reward Calculado: 0.020050 (Recompensa/Pasos)\n",
            "  306840/2000000: episode: 443, duration: 24.989s, episode steps: 798, steps per second:  32, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.008445, mae: 0.404686, mean_q: 0.504640, mean_eps: 0.448408\n",
            "📈 Episodio 444: Recompensa total (clipped): 20.000, Pasos: 957, Mean Reward Calculado: 0.020899 (Recompensa/Pasos)\n",
            "  307797/2000000: episode: 444, duration: 29.743s, episode steps: 957, steps per second:  32, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.008595, mae: 0.409254, mean_q: 0.510417, mean_eps: 0.446828\n",
            "📈 Episodio 445: Recompensa total (clipped): 5.000, Pasos: 515, Mean Reward Calculado: 0.009709 (Recompensa/Pasos)\n",
            "  308312/2000000: episode: 445, duration: 16.044s, episode steps: 515, steps per second:  32, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.008567, mae: 0.411422, mean_q: 0.511605, mean_eps: 0.445503\n",
            "📈 Episodio 446: Recompensa total (clipped): 14.000, Pasos: 889, Mean Reward Calculado: 0.015748 (Recompensa/Pasos)\n",
            "  309201/2000000: episode: 446, duration: 27.921s, episode steps: 889, steps per second:  32, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.008231, mae: 0.407786, mean_q: 0.508492, mean_eps: 0.444239\n",
            "📈 Episodio 447: Recompensa total (clipped): 6.000, Pasos: 469, Mean Reward Calculado: 0.012793 (Recompensa/Pasos)\n",
            "  309670/2000000: episode: 447, duration: 14.507s, episode steps: 469, steps per second:  32, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.007777, mae: 0.411892, mean_q: 0.513315, mean_eps: 0.443015\n",
            "📈 Episodio 448: Recompensa total (clipped): 7.000, Pasos: 473, Mean Reward Calculado: 0.014799 (Recompensa/Pasos)\n",
            "  310143/2000000: episode: 448, duration: 14.645s, episode steps: 473, steps per second:  32, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.008284, mae: 0.410303, mean_q: 0.512778, mean_eps: 0.442169\n",
            "📈 Episodio 449: Recompensa total (clipped): 12.000, Pasos: 553, Mean Reward Calculado: 0.021700 (Recompensa/Pasos)\n",
            "  310696/2000000: episode: 449, duration: 17.382s, episode steps: 553, steps per second:  32, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.008621, mae: 0.447030, mean_q: 0.557641, mean_eps: 0.441248\n",
            "📈 Episodio 450: Recompensa total (clipped): 12.000, Pasos: 642, Mean Reward Calculado: 0.018692 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 450, pasos: 311338)\n",
            "💾 NUEVO MEJOR PROMEDIO: 11.56 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 450 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 12.00\n",
            "   Media últimos 100: 11.56 / 20.0\n",
            "   Mejor promedio histórico: 11.56\n",
            "   Estado: 📈 57.8% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  311338/2000000: episode: 450, duration: 51.931s, episode steps: 642, steps per second:  12, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009872, mae: 0.448442, mean_q: 0.560116, mean_eps: 0.440171\n",
            "📈 Episodio 451: Recompensa total (clipped): 12.000, Pasos: 674, Mean Reward Calculado: 0.017804 (Recompensa/Pasos)\n",
            "  312012/2000000: episode: 451, duration: 21.157s, episode steps: 674, steps per second:  32, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.008264, mae: 0.439825, mean_q: 0.549804, mean_eps: 0.438987\n",
            "📈 Episodio 452: Recompensa total (clipped): 11.000, Pasos: 587, Mean Reward Calculado: 0.018739 (Recompensa/Pasos)\n",
            "  312599/2000000: episode: 452, duration: 18.599s, episode steps: 587, steps per second:  32, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.008565, mae: 0.438486, mean_q: 0.547821, mean_eps: 0.437853\n",
            "📈 Episodio 453: Recompensa total (clipped): 10.000, Pasos: 664, Mean Reward Calculado: 0.015060 (Recompensa/Pasos)\n",
            "  313263/2000000: episode: 453, duration: 20.821s, episode steps: 664, steps per second:  32, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.009274, mae: 0.439058, mean_q: 0.548397, mean_eps: 0.436726\n",
            "📈 Episodio 454: Recompensa total (clipped): 16.000, Pasos: 1004, Mean Reward Calculado: 0.015936 (Recompensa/Pasos)\n",
            "  314267/2000000: episode: 454, duration: 31.452s, episode steps: 1004, steps per second:  32, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.008455, mae: 0.437240, mean_q: 0.546431, mean_eps: 0.435225\n",
            "📈 Episodio 455: Recompensa total (clipped): 25.000, Pasos: 1079, Mean Reward Calculado: 0.023170 (Recompensa/Pasos)\n",
            "  315346/2000000: episode: 455, duration: 33.836s, episode steps: 1079, steps per second:  32, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.007842, mae: 0.438577, mean_q: 0.546656, mean_eps: 0.433349\n",
            "📈 Episodio 456: Recompensa total (clipped): 9.000, Pasos: 622, Mean Reward Calculado: 0.014469 (Recompensa/Pasos)\n",
            "  315968/2000000: episode: 456, duration: 19.569s, episode steps: 622, steps per second:  32, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.008915, mae: 0.441626, mean_q: 0.552525, mean_eps: 0.431819\n",
            "📈 Episodio 457: Recompensa total (clipped): 9.000, Pasos: 787, Mean Reward Calculado: 0.011436 (Recompensa/Pasos)\n",
            "  316755/2000000: episode: 457, duration: 24.679s, episode steps: 787, steps per second:  32, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009266, mae: 0.442965, mean_q: 0.553603, mean_eps: 0.430552\n",
            "📈 Episodio 458: Recompensa total (clipped): 6.000, Pasos: 456, Mean Reward Calculado: 0.013158 (Recompensa/Pasos)\n",
            "  317211/2000000: episode: 458, duration: 14.368s, episode steps: 456, steps per second:  32, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.009101, mae: 0.440919, mean_q: 0.551301, mean_eps: 0.429432\n",
            "📈 Episodio 459: Recompensa total (clipped): 7.000, Pasos: 538, Mean Reward Calculado: 0.013011 (Recompensa/Pasos)\n",
            "  317749/2000000: episode: 459, duration: 17.083s, episode steps: 538, steps per second:  31, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009340, mae: 0.435079, mean_q: 0.543905, mean_eps: 0.428536\n",
            "📈 Episodio 460: Recompensa total (clipped): 11.000, Pasos: 629, Mean Reward Calculado: 0.017488 (Recompensa/Pasos)\n",
            "  318378/2000000: episode: 460, duration: 19.675s, episode steps: 629, steps per second:  32, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008295, mae: 0.435137, mean_q: 0.543866, mean_eps: 0.427485\n",
            "📈 Episodio 461: Recompensa total (clipped): 12.000, Pasos: 824, Mean Reward Calculado: 0.014563 (Recompensa/Pasos)\n",
            "  319202/2000000: episode: 461, duration: 25.852s, episode steps: 824, steps per second:  32, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.008661, mae: 0.440918, mean_q: 0.549656, mean_eps: 0.426178\n",
            "📊 Paso 320,000/2,000,000 (16.0%) - 38.3 pasos/seg - ETA: 12.2h - Memoria: 4502.31 MB\n",
            "📈 Episodio 462: Recompensa total (clipped): 19.000, Pasos: 1003, Mean Reward Calculado: 0.018943 (Recompensa/Pasos)\n",
            "  320205/2000000: episode: 462, duration: 31.405s, episode steps: 1003, steps per second:  32, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.008678, mae: 0.444920, mean_q: 0.556052, mean_eps: 0.424533\n",
            "📈 Episodio 463: Recompensa total (clipped): 12.000, Pasos: 646, Mean Reward Calculado: 0.018576 (Recompensa/Pasos)\n",
            "  320851/2000000: episode: 463, duration: 20.320s, episode steps: 646, steps per second:  32, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.026 [0.000, 5.000],  loss: 0.009099, mae: 0.479491, mean_q: 0.597911, mean_eps: 0.423050\n",
            "📈 Episodio 464: Recompensa total (clipped): 11.000, Pasos: 508, Mean Reward Calculado: 0.021654 (Recompensa/Pasos)\n",
            "  321359/2000000: episode: 464, duration: 16.138s, episode steps: 508, steps per second:  31, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.010319, mae: 0.476156, mean_q: 0.592411, mean_eps: 0.422013\n",
            "📈 Episodio 465: Recompensa total (clipped): 7.000, Pasos: 491, Mean Reward Calculado: 0.014257 (Recompensa/Pasos)\n",
            "  321850/2000000: episode: 465, duration: 15.531s, episode steps: 491, steps per second:  32, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.008583, mae: 0.481600, mean_q: 0.600431, mean_eps: 0.421113\n",
            "📈 Episodio 466: Recompensa total (clipped): 9.000, Pasos: 568, Mean Reward Calculado: 0.015845 (Recompensa/Pasos)\n",
            "  322418/2000000: episode: 466, duration: 17.803s, episode steps: 568, steps per second:  32, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.009565, mae: 0.465731, mean_q: 0.579854, mean_eps: 0.420159\n",
            "📈 Episodio 467: Recompensa total (clipped): 12.000, Pasos: 636, Mean Reward Calculado: 0.018868 (Recompensa/Pasos)\n",
            "  323054/2000000: episode: 467, duration: 19.862s, episode steps: 636, steps per second:  32, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.008024, mae: 0.473004, mean_q: 0.592254, mean_eps: 0.419075\n",
            "📈 Episodio 468: Recompensa total (clipped): 7.000, Pasos: 369, Mean Reward Calculado: 0.018970 (Recompensa/Pasos)\n",
            "  323423/2000000: episode: 468, duration: 11.555s, episode steps: 369, steps per second:  32, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.008233, mae: 0.459356, mean_q: 0.575516, mean_eps: 0.418172\n",
            "📈 Episodio 469: Recompensa total (clipped): 8.000, Pasos: 462, Mean Reward Calculado: 0.017316 (Recompensa/Pasos)\n",
            "  323885/2000000: episode: 469, duration: 14.596s, episode steps: 462, steps per second:  32, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.007542, mae: 0.459765, mean_q: 0.574565, mean_eps: 0.417423\n",
            "📈 Episodio 470: Recompensa total (clipped): 7.000, Pasos: 441, Mean Reward Calculado: 0.015873 (Recompensa/Pasos)\n",
            "  324326/2000000: episode: 470, duration: 13.940s, episode steps: 441, steps per second:  32, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.008647, mae: 0.453185, mean_q: 0.567373, mean_eps: 0.416609\n",
            "📈 Episodio 471: Recompensa total (clipped): 12.000, Pasos: 676, Mean Reward Calculado: 0.017751 (Recompensa/Pasos)\n",
            "  325002/2000000: episode: 471, duration: 21.469s, episode steps: 676, steps per second:  31, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.009483, mae: 0.465479, mean_q: 0.580347, mean_eps: 0.415605\n",
            "📈 Episodio 472: Recompensa total (clipped): 22.000, Pasos: 1011, Mean Reward Calculado: 0.021761 (Recompensa/Pasos)\n",
            "  326013/2000000: episode: 472, duration: 32.081s, episode steps: 1011, steps per second:  32, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.008975, mae: 0.462807, mean_q: 0.578075, mean_eps: 0.414086\n",
            "📈 Episodio 473: Recompensa total (clipped): 7.000, Pasos: 592, Mean Reward Calculado: 0.011824 (Recompensa/Pasos)\n",
            "  326605/2000000: episode: 473, duration: 18.707s, episode steps: 592, steps per second:  32, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.009731, mae: 0.467148, mean_q: 0.584443, mean_eps: 0.412642\n",
            "📈 Episodio 474: Recompensa total (clipped): 15.000, Pasos: 831, Mean Reward Calculado: 0.018051 (Recompensa/Pasos)\n",
            "  327436/2000000: episode: 474, duration: 26.147s, episode steps: 831, steps per second:  32, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.009151, mae: 0.469306, mean_q: 0.585193, mean_eps: 0.411364\n",
            "📈 Episodio 475: Recompensa total (clipped): 9.000, Pasos: 452, Mean Reward Calculado: 0.019912 (Recompensa/Pasos)\n",
            "  327888/2000000: episode: 475, duration: 14.394s, episode steps: 452, steps per second:  31, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.009620, mae: 0.473773, mean_q: 0.592500, mean_eps: 0.410212\n",
            "📈 Episodio 476: Recompensa total (clipped): 13.000, Pasos: 700, Mean Reward Calculado: 0.018571 (Recompensa/Pasos)\n",
            "  328588/2000000: episode: 476, duration: 22.277s, episode steps: 700, steps per second:  31, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.009680, mae: 0.473274, mean_q: 0.589705, mean_eps: 0.409175\n",
            "📈 Episodio 477: Recompensa total (clipped): 9.000, Pasos: 387, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            "  328975/2000000: episode: 477, duration: 12.364s, episode steps: 387, steps per second:  31, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.007686, mae: 0.462172, mean_q: 0.578766, mean_eps: 0.408196\n",
            "📈 Episodio 478: Recompensa total (clipped): 11.000, Pasos: 702, Mean Reward Calculado: 0.015670 (Recompensa/Pasos)\n",
            "  329677/2000000: episode: 478, duration: 22.464s, episode steps: 702, steps per second:  31, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.009118, mae: 0.467737, mean_q: 0.583483, mean_eps: 0.407213\n",
            "📈 Episodio 479: Recompensa total (clipped): 13.000, Pasos: 725, Mean Reward Calculado: 0.017931 (Recompensa/Pasos)\n",
            "  330402/2000000: episode: 479, duration: 22.685s, episode steps: 725, steps per second:  32, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.008959, mae: 0.482152, mean_q: 0.601588, mean_eps: 0.405928\n",
            "📈 Episodio 480: Recompensa total (clipped): 17.000, Pasos: 740, Mean Reward Calculado: 0.022973 (Recompensa/Pasos)\n",
            "  331142/2000000: episode: 480, duration: 23.333s, episode steps: 740, steps per second:  32, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.008796, mae: 0.509200, mean_q: 0.635768, mean_eps: 0.404610\n",
            "📈 Episodio 481: Recompensa total (clipped): 9.000, Pasos: 520, Mean Reward Calculado: 0.017308 (Recompensa/Pasos)\n",
            "  331662/2000000: episode: 481, duration: 16.525s, episode steps: 520, steps per second:  31, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.010447, mae: 0.504311, mean_q: 0.630502, mean_eps: 0.403476\n",
            "📈 Episodio 482: Recompensa total (clipped): 14.000, Pasos: 931, Mean Reward Calculado: 0.015038 (Recompensa/Pasos)\n",
            "  332593/2000000: episode: 482, duration: 29.250s, episode steps: 931, steps per second:  32, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.009032, mae: 0.501820, mean_q: 0.626380, mean_eps: 0.402170\n",
            "📈 Episodio 483: Recompensa total (clipped): 5.000, Pasos: 344, Mean Reward Calculado: 0.014535 (Recompensa/Pasos)\n",
            "  332937/2000000: episode: 483, duration: 10.801s, episode steps: 344, steps per second:  32, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.015 [0.000, 5.000],  loss: 0.007605, mae: 0.504780, mean_q: 0.629449, mean_eps: 0.401021\n",
            "📈 Episodio 484: Recompensa total (clipped): 11.000, Pasos: 611, Mean Reward Calculado: 0.018003 (Recompensa/Pasos)\n",
            "  333548/2000000: episode: 484, duration: 19.263s, episode steps: 611, steps per second:  32, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.009653, mae: 0.508963, mean_q: 0.633477, mean_eps: 0.400164\n",
            "📈 Episodio 485: Recompensa total (clipped): 21.000, Pasos: 955, Mean Reward Calculado: 0.021990 (Recompensa/Pasos)\n",
            "  334503/2000000: episode: 485, duration: 30.178s, episode steps: 955, steps per second:  32, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.008915, mae: 0.500336, mean_q: 0.623749, mean_eps: 0.398757\n",
            "📈 Episodio 486: Recompensa total (clipped): 19.000, Pasos: 1021, Mean Reward Calculado: 0.018609 (Recompensa/Pasos)\n",
            "  335524/2000000: episode: 486, duration: 32.426s, episode steps: 1021, steps per second:  31, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008829, mae: 0.501826, mean_q: 0.625336, mean_eps: 0.396978\n",
            "📈 Episodio 487: Recompensa total (clipped): 12.000, Pasos: 803, Mean Reward Calculado: 0.014944 (Recompensa/Pasos)\n",
            "  336327/2000000: episode: 487, duration: 25.532s, episode steps: 803, steps per second:  31, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.008975, mae: 0.503159, mean_q: 0.628477, mean_eps: 0.395337\n",
            "📈 Episodio 488: Recompensa total (clipped): 11.000, Pasos: 718, Mean Reward Calculado: 0.015320 (Recompensa/Pasos)\n",
            "  337045/2000000: episode: 488, duration: 23.034s, episode steps: 718, steps per second:  31, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009522, mae: 0.503461, mean_q: 0.627040, mean_eps: 0.393965\n",
            "📈 Episodio 489: Recompensa total (clipped): 7.000, Pasos: 615, Mean Reward Calculado: 0.011382 (Recompensa/Pasos)\n",
            "  337660/2000000: episode: 489, duration: 19.659s, episode steps: 615, steps per second:  31, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010346, mae: 0.504086, mean_q: 0.628229, mean_eps: 0.392766\n",
            "📈 Episodio 490: Recompensa total (clipped): 12.000, Pasos: 690, Mean Reward Calculado: 0.017391 (Recompensa/Pasos)\n",
            "  338350/2000000: episode: 490, duration: 21.944s, episode steps: 690, steps per second:  31, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.009781, mae: 0.500456, mean_q: 0.624634, mean_eps: 0.391593\n",
            "📈 Episodio 491: Recompensa total (clipped): 12.000, Pasos: 675, Mean Reward Calculado: 0.017778 (Recompensa/Pasos)\n",
            "  339025/2000000: episode: 491, duration: 21.518s, episode steps: 675, steps per second:  31, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.008536, mae: 0.496783, mean_q: 0.619522, mean_eps: 0.390362\n",
            "📈 Episodio 492: Recompensa total (clipped): 15.000, Pasos: 735, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            "  339760/2000000: episode: 492, duration: 23.299s, episode steps: 735, steps per second:  32, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.008441, mae: 0.493057, mean_q: 0.614450, mean_eps: 0.389094\n",
            "📊 Paso 340,000/2,000,000 (17.0%) - 37.8 pasos/seg - ETA: 12.2h - Memoria: 4596.86 MB\n",
            "📈 Episodio 493: Recompensa total (clipped): 20.000, Pasos: 979, Mean Reward Calculado: 0.020429 (Recompensa/Pasos)\n",
            "  340739/2000000: episode: 493, duration: 30.914s, episode steps: 979, steps per second:  32, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.009071, mae: 0.517031, mean_q: 0.645196, mean_eps: 0.387554\n",
            "📈 Episodio 494: Recompensa total (clipped): 1.000, Pasos: 532, Mean Reward Calculado: 0.001880 (Recompensa/Pasos)\n",
            "  341271/2000000: episode: 494, duration: 16.867s, episode steps: 532, steps per second:  32, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.010581, mae: 0.521014, mean_q: 0.649681, mean_eps: 0.386193\n",
            "📈 Episodio 495: Recompensa total (clipped): 7.000, Pasos: 534, Mean Reward Calculado: 0.013109 (Recompensa/Pasos)\n",
            "  341805/2000000: episode: 495, duration: 17.038s, episode steps: 534, steps per second:  31, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.008763, mae: 0.512266, mean_q: 0.641900, mean_eps: 0.385232\n",
            "📈 Episodio 496: Recompensa total (clipped): 7.000, Pasos: 423, Mean Reward Calculado: 0.016548 (Recompensa/Pasos)\n",
            "  342228/2000000: episode: 496, duration: 13.417s, episode steps: 423, steps per second:  32, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.137 [0.000, 5.000],  loss: 0.008618, mae: 0.507723, mean_q: 0.633597, mean_eps: 0.384371\n",
            "📈 Episodio 497: Recompensa total (clipped): 25.000, Pasos: 971, Mean Reward Calculado: 0.025747 (Recompensa/Pasos)\n",
            "  343199/2000000: episode: 497, duration: 30.750s, episode steps: 971, steps per second:  32, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.009130, mae: 0.516635, mean_q: 0.644486, mean_eps: 0.383118\n",
            "📈 Episodio 498: Recompensa total (clipped): 15.000, Pasos: 768, Mean Reward Calculado: 0.019531 (Recompensa/Pasos)\n",
            "  343967/2000000: episode: 498, duration: 24.345s, episode steps: 768, steps per second:  32, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009027, mae: 0.517115, mean_q: 0.645395, mean_eps: 0.381552\n",
            "📈 Episodio 499: Recompensa total (clipped): 16.000, Pasos: 699, Mean Reward Calculado: 0.022890 (Recompensa/Pasos)\n",
            "  344666/2000000: episode: 499, duration: 22.222s, episode steps: 699, steps per second:  31, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.009466, mae: 0.523902, mean_q: 0.653604, mean_eps: 0.380231\n",
            "📈 Episodio 500: Recompensa total (clipped): 13.000, Pasos: 734, Mean Reward Calculado: 0.017711 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 500, pasos: 345400)\n",
            "💾 NUEVO MEJOR PROMEDIO: 12.07 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 500 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 13.00\n",
            "   Media últimos 100: 12.07 / 20.0\n",
            "   Mejor promedio histórico: 12.07\n",
            "   Estado: 📈 60.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  345400/2000000: episode: 500, duration: 36.922s, episode steps: 734, steps per second:  20, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009761, mae: 0.512082, mean_q: 0.639422, mean_eps: 0.378942\n",
            "📈 Episodio 501: Recompensa total (clipped): 23.000, Pasos: 1101, Mean Reward Calculado: 0.020890 (Recompensa/Pasos)\n",
            "  346501/2000000: episode: 501, duration: 35.660s, episode steps: 1101, steps per second:  31, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.010275, mae: 0.517945, mean_q: 0.645933, mean_eps: 0.377290\n",
            "📈 Episodio 502: Recompensa total (clipped): 9.000, Pasos: 618, Mean Reward Calculado: 0.014563 (Recompensa/Pasos)\n",
            "  347119/2000000: episode: 502, duration: 19.647s, episode steps: 618, steps per second:  31, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.009193, mae: 0.515020, mean_q: 0.642493, mean_eps: 0.375742\n",
            "📈 Episodio 503: Recompensa total (clipped): 6.000, Pasos: 362, Mean Reward Calculado: 0.016575 (Recompensa/Pasos)\n",
            "  347481/2000000: episode: 503, duration: 11.646s, episode steps: 362, steps per second:  31, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.961 [0.000, 5.000],  loss: 0.009510, mae: 0.518483, mean_q: 0.648812, mean_eps: 0.374860\n",
            "📈 Episodio 504: Recompensa total (clipped): 4.000, Pasos: 517, Mean Reward Calculado: 0.007737 (Recompensa/Pasos)\n",
            "  347998/2000000: episode: 504, duration: 16.402s, episode steps: 517, steps per second:  32, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.010521, mae: 0.517894, mean_q: 0.646681, mean_eps: 0.374068\n",
            "📈 Episodio 505: Recompensa total (clipped): 10.000, Pasos: 542, Mean Reward Calculado: 0.018450 (Recompensa/Pasos)\n",
            "  348540/2000000: episode: 505, duration: 17.588s, episode steps: 542, steps per second:  31, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.010150, mae: 0.513618, mean_q: 0.641440, mean_eps: 0.373118\n",
            "📈 Episodio 506: Recompensa total (clipped): 23.000, Pasos: 1023, Mean Reward Calculado: 0.022483 (Recompensa/Pasos)\n",
            "  349563/2000000: episode: 506, duration: 33.247s, episode steps: 1023, steps per second:  31, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.010082, mae: 0.517407, mean_q: 0.645766, mean_eps: 0.371710\n",
            "📈 Episodio 507: Recompensa total (clipped): 7.000, Pasos: 523, Mean Reward Calculado: 0.013384 (Recompensa/Pasos)\n",
            "  350086/2000000: episode: 507, duration: 16.672s, episode steps: 523, steps per second:  31, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.010090, mae: 0.527670, mean_q: 0.658785, mean_eps: 0.370317\n",
            "📈 Episodio 508: Recompensa total (clipped): 13.000, Pasos: 744, Mean Reward Calculado: 0.017473 (Recompensa/Pasos)\n",
            "  350830/2000000: episode: 508, duration: 23.449s, episode steps: 744, steps per second:  32, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.009737, mae: 0.559378, mean_q: 0.696092, mean_eps: 0.369176\n",
            "📈 Episodio 509: Recompensa total (clipped): 7.000, Pasos: 512, Mean Reward Calculado: 0.013672 (Recompensa/Pasos)\n",
            "  351342/2000000: episode: 509, duration: 16.151s, episode steps: 512, steps per second:  32, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.011098, mae: 0.550264, mean_q: 0.686706, mean_eps: 0.368045\n",
            "📈 Episodio 510: Recompensa total (clipped): 19.000, Pasos: 985, Mean Reward Calculado: 0.019289 (Recompensa/Pasos)\n",
            "  352327/2000000: episode: 510, duration: 31.409s, episode steps: 985, steps per second:  31, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.010102, mae: 0.549384, mean_q: 0.683682, mean_eps: 0.366699\n",
            "📈 Episodio 511: Recompensa total (clipped): 15.000, Pasos: 955, Mean Reward Calculado: 0.015707 (Recompensa/Pasos)\n",
            "  353282/2000000: episode: 511, duration: 30.571s, episode steps: 955, steps per second:  31, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.008727, mae: 0.539557, mean_q: 0.670828, mean_eps: 0.364953\n",
            "📈 Episodio 512: Recompensa total (clipped): 24.000, Pasos: 956, Mean Reward Calculado: 0.025105 (Recompensa/Pasos)\n",
            "  354238/2000000: episode: 512, duration: 30.601s, episode steps: 956, steps per second:  31, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009731, mae: 0.541629, mean_q: 0.675074, mean_eps: 0.363232\n",
            "📈 Episodio 513: Recompensa total (clipped): 19.000, Pasos: 852, Mean Reward Calculado: 0.022300 (Recompensa/Pasos)\n",
            "  355090/2000000: episode: 513, duration: 26.963s, episode steps: 852, steps per second:  32, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.010242, mae: 0.551600, mean_q: 0.687616, mean_eps: 0.361605\n",
            "📈 Episodio 514: Recompensa total (clipped): 16.000, Pasos: 628, Mean Reward Calculado: 0.025478 (Recompensa/Pasos)\n",
            "  355718/2000000: episode: 514, duration: 20.056s, episode steps: 628, steps per second:  31, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.009071, mae: 0.545410, mean_q: 0.679718, mean_eps: 0.360273\n",
            "📈 Episodio 515: Recompensa total (clipped): 16.000, Pasos: 738, Mean Reward Calculado: 0.021680 (Recompensa/Pasos)\n",
            "  356456/2000000: episode: 515, duration: 23.828s, episode steps: 738, steps per second:  31, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.009827, mae: 0.544329, mean_q: 0.677227, mean_eps: 0.359045\n",
            "📈 Episodio 516: Recompensa total (clipped): 11.000, Pasos: 551, Mean Reward Calculado: 0.019964 (Recompensa/Pasos)\n",
            "  357007/2000000: episode: 516, duration: 17.674s, episode steps: 551, steps per second:  31, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.008638, mae: 0.538533, mean_q: 0.669320, mean_eps: 0.357886\n",
            "📈 Episodio 517: Recompensa total (clipped): 21.000, Pasos: 835, Mean Reward Calculado: 0.025150 (Recompensa/Pasos)\n",
            "  357842/2000000: episode: 517, duration: 26.675s, episode steps: 835, steps per second:  31, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.010068, mae: 0.544907, mean_q: 0.675917, mean_eps: 0.356637\n",
            "📈 Episodio 518: Recompensa total (clipped): 18.000, Pasos: 703, Mean Reward Calculado: 0.025605 (Recompensa/Pasos)\n",
            "  358545/2000000: episode: 518, duration: 22.521s, episode steps: 703, steps per second:  31, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008892, mae: 0.545600, mean_q: 0.678819, mean_eps: 0.355251\n",
            "📈 Episodio 519: Recompensa total (clipped): 8.000, Pasos: 430, Mean Reward Calculado: 0.018605 (Recompensa/Pasos)\n",
            "  358975/2000000: episode: 519, duration: 13.805s, episode steps: 430, steps per second:  31, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.011579, mae: 0.548490, mean_q: 0.681118, mean_eps: 0.354232\n",
            "📈 Episodio 520: Recompensa total (clipped): 10.000, Pasos: 404, Mean Reward Calculado: 0.024752 (Recompensa/Pasos)\n",
            "  359379/2000000: episode: 520, duration: 13.022s, episode steps: 404, steps per second:  31, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.009873, mae: 0.543243, mean_q: 0.673767, mean_eps: 0.353483\n",
            "📈 Episodio 521: Recompensa total (clipped): 9.000, Pasos: 500, Mean Reward Calculado: 0.018000 (Recompensa/Pasos)\n",
            "  359879/2000000: episode: 521, duration: 15.947s, episode steps: 500, steps per second:  31, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.009548, mae: 0.546664, mean_q: 0.679685, mean_eps: 0.352670\n",
            "📊 Paso 360,000/2,000,000 (18.0%) - 37.3 pasos/seg - ETA: 12.2h - Memoria: 4808.88 MB\n",
            "📈 Episodio 522: Recompensa total (clipped): 10.000, Pasos: 690, Mean Reward Calculado: 0.014493 (Recompensa/Pasos)\n",
            "  360569/2000000: episode: 522, duration: 22.111s, episode steps: 690, steps per second:  31, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.009218, mae: 0.556252, mean_q: 0.692861, mean_eps: 0.351597\n",
            "📈 Episodio 523: Recompensa total (clipped): 9.000, Pasos: 665, Mean Reward Calculado: 0.013534 (Recompensa/Pasos)\n",
            "  361234/2000000: episode: 523, duration: 21.270s, episode steps: 665, steps per second:  31, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.009437, mae: 0.543439, mean_q: 0.677842, mean_eps: 0.350376\n",
            "📈 Episodio 524: Recompensa total (clipped): 23.000, Pasos: 1331, Mean Reward Calculado: 0.017280 (Recompensa/Pasos)\n",
            "  362565/2000000: episode: 524, duration: 42.582s, episode steps: 1331, steps per second:  31, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009747, mae: 0.544778, mean_q: 0.679574, mean_eps: 0.348580\n",
            "📈 Episodio 525: Recompensa total (clipped): 8.000, Pasos: 515, Mean Reward Calculado: 0.015534 (Recompensa/Pasos)\n",
            "  363080/2000000: episode: 525, duration: 16.566s, episode steps: 515, steps per second:  31, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.009899, mae: 0.543519, mean_q: 0.678145, mean_eps: 0.346920\n",
            "📈 Episodio 526: Recompensa total (clipped): 5.000, Pasos: 510, Mean Reward Calculado: 0.009804 (Recompensa/Pasos)\n",
            "  363590/2000000: episode: 526, duration: 16.468s, episode steps: 510, steps per second:  31, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008860, mae: 0.535852, mean_q: 0.666541, mean_eps: 0.345999\n",
            "📈 Episodio 527: Recompensa total (clipped): 13.000, Pasos: 648, Mean Reward Calculado: 0.020062 (Recompensa/Pasos)\n",
            "  364238/2000000: episode: 527, duration: 20.937s, episode steps: 648, steps per second:  31, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.009831, mae: 0.552017, mean_q: 0.688964, mean_eps: 0.344955\n",
            "📈 Episodio 528: Recompensa total (clipped): 18.000, Pasos: 943, Mean Reward Calculado: 0.019088 (Recompensa/Pasos)\n",
            "  365181/2000000: episode: 528, duration: 30.505s, episode steps: 943, steps per second:  31, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.011201, mae: 0.562421, mean_q: 0.699074, mean_eps: 0.343522\n",
            "📈 Episodio 529: Recompensa total (clipped): 24.000, Pasos: 1066, Mean Reward Calculado: 0.022514 (Recompensa/Pasos)\n",
            "  366247/2000000: episode: 529, duration: 34.435s, episode steps: 1066, steps per second:  31, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010109, mae: 0.553697, mean_q: 0.689702, mean_eps: 0.341715\n",
            "📈 Episodio 530: Recompensa total (clipped): 23.000, Pasos: 897, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            "  367144/2000000: episode: 530, duration: 28.976s, episode steps: 897, steps per second:  31, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.009519, mae: 0.547736, mean_q: 0.682630, mean_eps: 0.339951\n",
            "📈 Episodio 531: Recompensa total (clipped): 15.000, Pasos: 690, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            "  367834/2000000: episode: 531, duration: 22.264s, episode steps: 690, steps per second:  31, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.010204, mae: 0.551218, mean_q: 0.685455, mean_eps: 0.338522\n",
            "📈 Episodio 532: Recompensa total (clipped): 8.000, Pasos: 365, Mean Reward Calculado: 0.021918 (Recompensa/Pasos)\n",
            "  368199/2000000: episode: 532, duration: 11.718s, episode steps: 365, steps per second:  31, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.009011, mae: 0.537601, mean_q: 0.668503, mean_eps: 0.337571\n",
            "📈 Episodio 533: Recompensa total (clipped): 10.000, Pasos: 681, Mean Reward Calculado: 0.014684 (Recompensa/Pasos)\n",
            "  368880/2000000: episode: 533, duration: 21.996s, episode steps: 681, steps per second:  31, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.009379, mae: 0.550039, mean_q: 0.683019, mean_eps: 0.336632\n",
            "📈 Episodio 534: Recompensa total (clipped): 14.000, Pasos: 1108, Mean Reward Calculado: 0.012635 (Recompensa/Pasos)\n",
            "  369988/2000000: episode: 534, duration: 35.814s, episode steps: 1108, steps per second:  31, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.009373, mae: 0.552671, mean_q: 0.686656, mean_eps: 0.335022\n",
            "📈 Episodio 535: Recompensa total (clipped): 21.000, Pasos: 741, Mean Reward Calculado: 0.028340 (Recompensa/Pasos)\n",
            "  370729/2000000: episode: 535, duration: 24.042s, episode steps: 741, steps per second:  31, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.010361, mae: 0.584313, mean_q: 0.726752, mean_eps: 0.333356\n",
            "📈 Episodio 536: Recompensa total (clipped): 18.000, Pasos: 1007, Mean Reward Calculado: 0.017875 (Recompensa/Pasos)\n",
            "  371736/2000000: episode: 536, duration: 32.603s, episode steps: 1007, steps per second:  31, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.009018, mae: 0.589623, mean_q: 0.732171, mean_eps: 0.331782\n",
            "📈 Episodio 537: Recompensa total (clipped): 10.000, Pasos: 533, Mean Reward Calculado: 0.018762 (Recompensa/Pasos)\n",
            "  372269/2000000: episode: 537, duration: 17.308s, episode steps: 533, steps per second:  31, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010900, mae: 0.581973, mean_q: 0.722251, mean_eps: 0.330396\n",
            "📈 Episodio 538: Recompensa total (clipped): 9.000, Pasos: 576, Mean Reward Calculado: 0.015625 (Recompensa/Pasos)\n",
            "  372845/2000000: episode: 538, duration: 18.543s, episode steps: 576, steps per second:  31, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.010819, mae: 0.587534, mean_q: 0.729740, mean_eps: 0.329396\n",
            "📈 Episodio 539: Recompensa total (clipped): 18.000, Pasos: 749, Mean Reward Calculado: 0.024032 (Recompensa/Pasos)\n",
            "  373594/2000000: episode: 539, duration: 24.041s, episode steps: 749, steps per second:  31, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.010394, mae: 0.590706, mean_q: 0.734973, mean_eps: 0.328204\n",
            "📈 Episodio 540: Recompensa total (clipped): 21.000, Pasos: 1170, Mean Reward Calculado: 0.017949 (Recompensa/Pasos)\n",
            "  374764/2000000: episode: 540, duration: 38.006s, episode steps: 1170, steps per second:  31, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.010290, mae: 0.591393, mean_q: 0.734988, mean_eps: 0.326480\n",
            "📈 Episodio 541: Recompensa total (clipped): 14.000, Pasos: 764, Mean Reward Calculado: 0.018325 (Recompensa/Pasos)\n",
            "  375528/2000000: episode: 541, duration: 25.040s, episode steps: 764, steps per second:  31, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.009237, mae: 0.579173, mean_q: 0.721352, mean_eps: 0.324741\n",
            "📈 Episodio 542: Recompensa total (clipped): 18.000, Pasos: 837, Mean Reward Calculado: 0.021505 (Recompensa/Pasos)\n",
            "  376365/2000000: episode: 542, duration: 27.361s, episode steps: 837, steps per second:  31, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.009547, mae: 0.578012, mean_q: 0.721188, mean_eps: 0.323297\n",
            "📈 Episodio 543: Recompensa total (clipped): 26.000, Pasos: 1102, Mean Reward Calculado: 0.023593 (Recompensa/Pasos)\n",
            "  377467/2000000: episode: 543, duration: 35.382s, episode steps: 1102, steps per second:  31, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.010922, mae: 0.588812, mean_q: 0.732505, mean_eps: 0.321551\n",
            "📈 Episodio 544: Recompensa total (clipped): 18.000, Pasos: 762, Mean Reward Calculado: 0.023622 (Recompensa/Pasos)\n",
            "  378229/2000000: episode: 544, duration: 24.738s, episode steps: 762, steps per second:  31, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.009344, mae: 0.576092, mean_q: 0.715233, mean_eps: 0.319874\n",
            "📈 Episodio 545: Recompensa total (clipped): 10.000, Pasos: 664, Mean Reward Calculado: 0.015060 (Recompensa/Pasos)\n",
            "  378893/2000000: episode: 545, duration: 21.460s, episode steps: 664, steps per second:  31, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.010104, mae: 0.584869, mean_q: 0.725648, mean_eps: 0.318588\n",
            "📊 Paso 380,000/2,000,000 (19.0%) - 36.9 pasos/seg - ETA: 12.2h - Memoria: 4963.19 MB\n",
            "📈 Episodio 546: Recompensa total (clipped): 25.000, Pasos: 1108, Mean Reward Calculado: 0.022563 (Recompensa/Pasos)\n",
            "  380001/2000000: episode: 546, duration: 35.738s, episode steps: 1108, steps per second:  31, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.009746, mae: 0.586985, mean_q: 0.728008, mean_eps: 0.316994\n",
            "📈 Episodio 547: Recompensa total (clipped): 23.000, Pasos: 967, Mean Reward Calculado: 0.023785 (Recompensa/Pasos)\n",
            "  380968/2000000: episode: 547, duration: 30.944s, episode steps: 967, steps per second:  31, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.010218, mae: 0.619181, mean_q: 0.768415, mean_eps: 0.315129\n",
            "📈 Episodio 548: Recompensa total (clipped): 22.000, Pasos: 881, Mean Reward Calculado: 0.024972 (Recompensa/Pasos)\n",
            "  381849/2000000: episode: 548, duration: 28.260s, episode steps: 881, steps per second:  31, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010852, mae: 0.614642, mean_q: 0.761130, mean_eps: 0.313466\n",
            "📈 Episodio 549: Recompensa total (clipped): 26.000, Pasos: 1961, Mean Reward Calculado: 0.013259 (Recompensa/Pasos)\n",
            "  383810/2000000: episode: 549, duration: 64.014s, episode steps: 1961, steps per second:  31, episode reward: 26.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.010506, mae: 0.613060, mean_q: 0.761367, mean_eps: 0.310906\n",
            "📈 Episodio 550: Recompensa total (clipped): 15.000, Pasos: 709, Mean Reward Calculado: 0.021157 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 550, pasos: 384519)\n",
            "💾 NUEVO MEJOR PROMEDIO: 13.62 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 550 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 15.00\n",
            "   Media últimos 100: 13.62 / 20.0\n",
            "   Mejor promedio histórico: 13.62\n",
            "   Estado: 📈 68.1% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  384519/2000000: episode: 550, duration: 40.159s, episode steps: 709, steps per second:  18, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.009960, mae: 0.616950, mean_q: 0.766588, mean_eps: 0.308505\n",
            "📈 Episodio 551: Recompensa total (clipped): 15.000, Pasos: 794, Mean Reward Calculado: 0.018892 (Recompensa/Pasos)\n",
            "  385313/2000000: episode: 551, duration: 26.178s, episode steps: 794, steps per second:  30, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.009731, mae: 0.609411, mean_q: 0.756817, mean_eps: 0.307151\n",
            "📈 Episodio 552: Recompensa total (clipped): 17.000, Pasos: 644, Mean Reward Calculado: 0.026398 (Recompensa/Pasos)\n",
            "  385957/2000000: episode: 552, duration: 21.150s, episode steps: 644, steps per second:  30, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.010553, mae: 0.618340, mean_q: 0.768037, mean_eps: 0.305855\n",
            "📈 Episodio 553: Recompensa total (clipped): 11.000, Pasos: 518, Mean Reward Calculado: 0.021236 (Recompensa/Pasos)\n",
            "  386475/2000000: episode: 553, duration: 16.983s, episode steps: 518, steps per second:  31, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.009457, mae: 0.609466, mean_q: 0.755943, mean_eps: 0.304811\n",
            "📈 Episodio 554: Recompensa total (clipped): 11.000, Pasos: 372, Mean Reward Calculado: 0.029570 (Recompensa/Pasos)\n",
            "  386847/2000000: episode: 554, duration: 12.273s, episode steps: 372, steps per second:  30, episode reward: 11.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.009000, mae: 0.608611, mean_q: 0.757590, mean_eps: 0.304012\n",
            "📈 Episodio 555: Recompensa total (clipped): 7.000, Pasos: 490, Mean Reward Calculado: 0.014286 (Recompensa/Pasos)\n",
            "  387337/2000000: episode: 555, duration: 16.150s, episode steps: 490, steps per second:  30, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011605, mae: 0.621542, mean_q: 0.772363, mean_eps: 0.303234\n",
            "📈 Episodio 556: Recompensa total (clipped): 17.000, Pasos: 1027, Mean Reward Calculado: 0.016553 (Recompensa/Pasos)\n",
            "  388364/2000000: episode: 556, duration: 33.624s, episode steps: 1027, steps per second:  31, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010647, mae: 0.608959, mean_q: 0.757098, mean_eps: 0.301870\n",
            "📈 Episodio 557: Recompensa total (clipped): 17.000, Pasos: 843, Mean Reward Calculado: 0.020166 (Recompensa/Pasos)\n",
            "  389207/2000000: episode: 557, duration: 27.523s, episode steps: 843, steps per second:  31, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.010576, mae: 0.615644, mean_q: 0.765659, mean_eps: 0.300189\n",
            "📈 Episodio 558: Recompensa total (clipped): 25.000, Pasos: 1151, Mean Reward Calculado: 0.021720 (Recompensa/Pasos)\n",
            "  390358/2000000: episode: 558, duration: 37.748s, episode steps: 1151, steps per second:  30, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.009739, mae: 0.617611, mean_q: 0.766771, mean_eps: 0.298392\n",
            "📈 Episodio 559: Recompensa total (clipped): 11.000, Pasos: 568, Mean Reward Calculado: 0.019366 (Recompensa/Pasos)\n",
            "  390926/2000000: episode: 559, duration: 18.646s, episode steps: 568, steps per second:  30, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.009498, mae: 0.627852, mean_q: 0.779035, mean_eps: 0.296844\n",
            "📈 Episodio 560: Recompensa total (clipped): 14.000, Pasos: 662, Mean Reward Calculado: 0.021148 (Recompensa/Pasos)\n",
            "  391588/2000000: episode: 560, duration: 21.667s, episode steps: 662, steps per second:  31, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.010819, mae: 0.631225, mean_q: 0.783118, mean_eps: 0.295739\n",
            "📈 Episodio 561: Recompensa total (clipped): 12.000, Pasos: 552, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            "  392140/2000000: episode: 561, duration: 18.153s, episode steps: 552, steps per second:  30, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011216, mae: 0.625428, mean_q: 0.773946, mean_eps: 0.294648\n",
            "📈 Episodio 562: Recompensa total (clipped): 16.000, Pasos: 723, Mean Reward Calculado: 0.022130 (Recompensa/Pasos)\n",
            "  392863/2000000: episode: 562, duration: 23.484s, episode steps: 723, steps per second:  31, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.010806, mae: 0.628526, mean_q: 0.779760, mean_eps: 0.293500\n",
            "📈 Episodio 563: Recompensa total (clipped): 14.000, Pasos: 667, Mean Reward Calculado: 0.020990 (Recompensa/Pasos)\n",
            "  393530/2000000: episode: 563, duration: 21.900s, episode steps: 667, steps per second:  30, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.009175, mae: 0.627149, mean_q: 0.777058, mean_eps: 0.292247\n",
            "📈 Episodio 564: Recompensa total (clipped): 13.000, Pasos: 679, Mean Reward Calculado: 0.019146 (Recompensa/Pasos)\n",
            "  394209/2000000: episode: 564, duration: 22.121s, episode steps: 679, steps per second:  31, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.011518, mae: 0.645773, mean_q: 0.802355, mean_eps: 0.291034\n",
            "📈 Episodio 565: Recompensa total (clipped): 12.000, Pasos: 824, Mean Reward Calculado: 0.014563 (Recompensa/Pasos)\n",
            "  395033/2000000: episode: 565, duration: 26.858s, episode steps: 824, steps per second:  31, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.009938, mae: 0.630934, mean_q: 0.783837, mean_eps: 0.289680\n",
            "📈 Episodio 566: Recompensa total (clipped): 12.000, Pasos: 635, Mean Reward Calculado: 0.018898 (Recompensa/Pasos)\n",
            "  395668/2000000: episode: 566, duration: 20.914s, episode steps: 635, steps per second:  30, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.011903, mae: 0.632830, mean_q: 0.786063, mean_eps: 0.288370\n",
            "📈 Episodio 567: Recompensa total (clipped): 7.000, Pasos: 503, Mean Reward Calculado: 0.013917 (Recompensa/Pasos)\n",
            "  396171/2000000: episode: 567, duration: 16.454s, episode steps: 503, steps per second:  31, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.012170, mae: 0.631321, mean_q: 0.782204, mean_eps: 0.287348\n",
            "📈 Episodio 568: Recompensa total (clipped): 16.000, Pasos: 611, Mean Reward Calculado: 0.026187 (Recompensa/Pasos)\n",
            "  396782/2000000: episode: 568, duration: 19.980s, episode steps: 611, steps per second:  31, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009490, mae: 0.629481, mean_q: 0.781159, mean_eps: 0.286343\n",
            "📈 Episodio 569: Recompensa total (clipped): 19.000, Pasos: 706, Mean Reward Calculado: 0.026912 (Recompensa/Pasos)\n",
            "  397488/2000000: episode: 569, duration: 23.215s, episode steps: 706, steps per second:  30, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.011168, mae: 0.619616, mean_q: 0.769720, mean_eps: 0.285159\n",
            "📈 Episodio 570: Recompensa total (clipped): 17.000, Pasos: 825, Mean Reward Calculado: 0.020606 (Recompensa/Pasos)\n",
            "  398313/2000000: episode: 570, duration: 27.187s, episode steps: 825, steps per second:  30, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.010900, mae: 0.633583, mean_q: 0.786306, mean_eps: 0.283780\n",
            "📈 Episodio 571: Recompensa total (clipped): 24.000, Pasos: 1024, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            "  399337/2000000: episode: 571, duration: 33.529s, episode steps: 1024, steps per second:  31, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.009297, mae: 0.620737, mean_q: 0.772561, mean_eps: 0.282113\n",
            "📈 Episodio 572: Recompensa total (clipped): 10.000, Pasos: 626, Mean Reward Calculado: 0.015974 (Recompensa/Pasos)\n",
            "  399963/2000000: episode: 572, duration: 20.456s, episode steps: 626, steps per second:  31, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.859 [0.000, 5.000],  loss: 0.010170, mae: 0.626559, mean_q: 0.777328, mean_eps: 0.280630\n",
            "📊 Paso 400,000/2,000,000 (20.0%) - 36.5 pasos/seg - ETA: 12.2h - Memoria: 5128.36 MB\n",
            "📈 Episodio 573: Recompensa total (clipped): 10.000, Pasos: 550, Mean Reward Calculado: 0.018182 (Recompensa/Pasos)\n",
            "  400513/2000000: episode: 573, duration: 18.103s, episode steps: 550, steps per second:  30, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.010299, mae: 0.641620, mean_q: 0.797700, mean_eps: 0.279572\n",
            "📈 Episodio 574: Recompensa total (clipped): 11.000, Pasos: 556, Mean Reward Calculado: 0.019784 (Recompensa/Pasos)\n",
            "  401069/2000000: episode: 574, duration: 18.260s, episode steps: 556, steps per second:  30, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.010570, mae: 0.642229, mean_q: 0.797931, mean_eps: 0.278574\n",
            "📈 Episodio 575: Recompensa total (clipped): 13.000, Pasos: 811, Mean Reward Calculado: 0.016030 (Recompensa/Pasos)\n",
            "  401880/2000000: episode: 575, duration: 26.582s, episode steps: 811, steps per second:  31, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.012425, mae: 0.654209, mean_q: 0.813569, mean_eps: 0.277347\n",
            "📈 Episodio 576: Recompensa total (clipped): 18.000, Pasos: 844, Mean Reward Calculado: 0.021327 (Recompensa/Pasos)\n",
            "  402724/2000000: episode: 576, duration: 27.844s, episode steps: 844, steps per second:  30, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.010797, mae: 0.637997, mean_q: 0.792424, mean_eps: 0.275860\n",
            "📈 Episodio 577: Recompensa total (clipped): 15.000, Pasos: 740, Mean Reward Calculado: 0.020270 (Recompensa/Pasos)\n",
            "  403464/2000000: episode: 577, duration: 24.479s, episode steps: 740, steps per second:  30, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.010380, mae: 0.653457, mean_q: 0.813814, mean_eps: 0.274434\n",
            "📈 Episodio 578: Recompensa total (clipped): 7.000, Pasos: 390, Mean Reward Calculado: 0.017949 (Recompensa/Pasos)\n",
            "  403854/2000000: episode: 578, duration: 12.842s, episode steps: 390, steps per second:  30, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.913 [0.000, 5.000],  loss: 0.012061, mae: 0.631981, mean_q: 0.784827, mean_eps: 0.273416\n",
            "📈 Episodio 579: Recompensa total (clipped): 20.000, Pasos: 942, Mean Reward Calculado: 0.021231 (Recompensa/Pasos)\n",
            "  404796/2000000: episode: 579, duration: 30.965s, episode steps: 942, steps per second:  30, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.011436, mae: 0.644826, mean_q: 0.801051, mean_eps: 0.272217\n",
            "📈 Episodio 580: Recompensa total (clipped): 15.000, Pasos: 772, Mean Reward Calculado: 0.019430 (Recompensa/Pasos)\n",
            "  405568/2000000: episode: 580, duration: 25.406s, episode steps: 772, steps per second:  30, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.010816, mae: 0.652242, mean_q: 0.810160, mean_eps: 0.270676\n",
            "📈 Episodio 581: Recompensa total (clipped): 14.000, Pasos: 765, Mean Reward Calculado: 0.018301 (Recompensa/Pasos)\n",
            "  406333/2000000: episode: 581, duration: 25.309s, episode steps: 765, steps per second:  30, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.010821, mae: 0.640236, mean_q: 0.795561, mean_eps: 0.269290\n",
            "📈 Episodio 582: Recompensa total (clipped): 13.000, Pasos: 621, Mean Reward Calculado: 0.020934 (Recompensa/Pasos)\n",
            "  406954/2000000: episode: 582, duration: 20.535s, episode steps: 621, steps per second:  30, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012234, mae: 0.641382, mean_q: 0.796336, mean_eps: 0.268041\n",
            "📈 Episodio 583: Recompensa total (clipped): 17.000, Pasos: 688, Mean Reward Calculado: 0.024709 (Recompensa/Pasos)\n",
            "  407642/2000000: episode: 583, duration: 22.595s, episode steps: 688, steps per second:  30, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.009895, mae: 0.649427, mean_q: 0.806938, mean_eps: 0.266864\n",
            "📈 Episodio 584: Recompensa total (clipped): 14.000, Pasos: 591, Mean Reward Calculado: 0.023689 (Recompensa/Pasos)\n",
            "  408233/2000000: episode: 584, duration: 19.227s, episode steps: 591, steps per second:  31, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.011164, mae: 0.650088, mean_q: 0.809046, mean_eps: 0.265712\n",
            "📈 Episodio 585: Recompensa total (clipped): 14.000, Pasos: 677, Mean Reward Calculado: 0.020679 (Recompensa/Pasos)\n",
            "  408910/2000000: episode: 585, duration: 22.197s, episode steps: 677, steps per second:  31, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.010759, mae: 0.646683, mean_q: 0.805538, mean_eps: 0.264570\n",
            "📈 Episodio 586: Recompensa total (clipped): 9.000, Pasos: 499, Mean Reward Calculado: 0.018036 (Recompensa/Pasos)\n",
            "  409409/2000000: episode: 586, duration: 16.416s, episode steps: 499, steps per second:  30, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.012890, mae: 0.650877, mean_q: 0.809736, mean_eps: 0.263512\n",
            "📈 Episodio 587: Recompensa total (clipped): 13.000, Pasos: 726, Mean Reward Calculado: 0.017906 (Recompensa/Pasos)\n",
            "  410135/2000000: episode: 587, duration: 23.856s, episode steps: 726, steps per second:  30, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.009755, mae: 0.652191, mean_q: 0.811205, mean_eps: 0.262410\n",
            "📈 Episodio 588: Recompensa total (clipped): 20.000, Pasos: 853, Mean Reward Calculado: 0.023447 (Recompensa/Pasos)\n",
            "  410988/2000000: episode: 588, duration: 28.031s, episode steps: 853, steps per second:  30, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.010697, mae: 0.666467, mean_q: 0.825989, mean_eps: 0.260992\n",
            "📈 Episodio 589: Recompensa total (clipped): 24.000, Pasos: 1176, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            "  412164/2000000: episode: 589, duration: 38.528s, episode steps: 1176, steps per second:  31, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.010906, mae: 0.676030, mean_q: 0.838454, mean_eps: 0.259167\n",
            "📈 Episodio 590: Recompensa total (clipped): 12.000, Pasos: 694, Mean Reward Calculado: 0.017291 (Recompensa/Pasos)\n",
            "  412858/2000000: episode: 590, duration: 22.904s, episode steps: 694, steps per second:  30, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.010368, mae: 0.674217, mean_q: 0.834079, mean_eps: 0.257482\n",
            "📈 Episodio 591: Recompensa total (clipped): 24.000, Pasos: 1038, Mean Reward Calculado: 0.023121 (Recompensa/Pasos)\n",
            "  413896/2000000: episode: 591, duration: 34.479s, episode steps: 1038, steps per second:  30, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.011829, mae: 0.673364, mean_q: 0.832500, mean_eps: 0.255923\n",
            "📈 Episodio 592: Recompensa total (clipped): 10.000, Pasos: 532, Mean Reward Calculado: 0.018797 (Recompensa/Pasos)\n",
            "  414428/2000000: episode: 592, duration: 17.632s, episode steps: 532, steps per second:  30, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.118 [0.000, 5.000],  loss: 0.010444, mae: 0.670728, mean_q: 0.833334, mean_eps: 0.254512\n",
            "📈 Episodio 593: Recompensa total (clipped): 12.000, Pasos: 643, Mean Reward Calculado: 0.018663 (Recompensa/Pasos)\n",
            "  415071/2000000: episode: 593, duration: 21.262s, episode steps: 643, steps per second:  30, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.010783, mae: 0.667654, mean_q: 0.828895, mean_eps: 0.253454\n",
            "📈 Episodio 594: Recompensa total (clipped): 5.000, Pasos: 396, Mean Reward Calculado: 0.012626 (Recompensa/Pasos)\n",
            "  415467/2000000: episode: 594, duration: 12.930s, episode steps: 396, steps per second:  31, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.010989, mae: 0.670063, mean_q: 0.832151, mean_eps: 0.252518\n",
            "📈 Episodio 595: Recompensa total (clipped): 14.000, Pasos: 679, Mean Reward Calculado: 0.020619 (Recompensa/Pasos)\n",
            "  416146/2000000: episode: 595, duration: 22.236s, episode steps: 679, steps per second:  31, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.011555, mae: 0.671490, mean_q: 0.832582, mean_eps: 0.251549\n",
            "📈 Episodio 596: Recompensa total (clipped): 12.000, Pasos: 700, Mean Reward Calculado: 0.017143 (Recompensa/Pasos)\n",
            "  416846/2000000: episode: 596, duration: 23.282s, episode steps: 700, steps per second:  30, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012200, mae: 0.662930, mean_q: 0.820571, mean_eps: 0.250307\n",
            "📈 Episodio 597: Recompensa total (clipped): 15.000, Pasos: 700, Mean Reward Calculado: 0.021429 (Recompensa/Pasos)\n",
            "  417546/2000000: episode: 597, duration: 23.262s, episode steps: 700, steps per second:  30, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.010798, mae: 0.668582, mean_q: 0.828772, mean_eps: 0.249047\n",
            "📈 Episodio 598: Recompensa total (clipped): 11.000, Pasos: 650, Mean Reward Calculado: 0.016923 (Recompensa/Pasos)\n",
            "  418196/2000000: episode: 598, duration: 21.498s, episode steps: 650, steps per second:  30, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.010956, mae: 0.666065, mean_q: 0.825387, mean_eps: 0.247834\n",
            "📈 Episodio 599: Recompensa total (clipped): 12.000, Pasos: 566, Mean Reward Calculado: 0.021201 (Recompensa/Pasos)\n",
            "  418762/2000000: episode: 599, duration: 18.686s, episode steps: 566, steps per second:  30, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.011688, mae: 0.666550, mean_q: 0.826350, mean_eps: 0.246740\n",
            "📈 Episodio 600: Recompensa total (clipped): 17.000, Pasos: 934, Mean Reward Calculado: 0.018201 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 600, pasos: 419696)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.70 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 600 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 17.00\n",
            "   Media últimos 100: 14.70 / 20.0\n",
            "   Mejor promedio histórico: 14.70\n",
            "   Estado: 📈 73.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  419696/2000000: episode: 600, duration: 48.725s, episode steps: 934, steps per second:  19, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.009890, mae: 0.661799, mean_q: 0.821331, mean_eps: 0.245390\n",
            "📊 Paso 420,000/2,000,000 (21.0%) - 36.1 pasos/seg - ETA: 12.2h - Memoria: 5285.92 MB\n",
            "📈 Episodio 601: Recompensa total (clipped): 20.000, Pasos: 959, Mean Reward Calculado: 0.020855 (Recompensa/Pasos)\n",
            "  420655/2000000: episode: 601, duration: 32.065s, episode steps: 959, steps per second:  30, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.009656, mae: 0.690985, mean_q: 0.857447, mean_eps: 0.243687\n",
            "📈 Episodio 602: Recompensa total (clipped): 7.000, Pasos: 484, Mean Reward Calculado: 0.014463 (Recompensa/Pasos)\n",
            "  421139/2000000: episode: 602, duration: 16.098s, episode steps: 484, steps per second:  30, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.009822, mae: 0.708123, mean_q: 0.878756, mean_eps: 0.242387\n",
            "📈 Episodio 603: Recompensa total (clipped): 17.000, Pasos: 757, Mean Reward Calculado: 0.022457 (Recompensa/Pasos)\n",
            "  421896/2000000: episode: 603, duration: 25.278s, episode steps: 757, steps per second:  30, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.011765, mae: 0.718890, mean_q: 0.893058, mean_eps: 0.241271\n",
            "📈 Episodio 604: Recompensa total (clipped): 8.000, Pasos: 383, Mean Reward Calculado: 0.020888 (Recompensa/Pasos)\n",
            "  422279/2000000: episode: 604, duration: 12.811s, episode steps: 383, steps per second:  30, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009914, mae: 0.714567, mean_q: 0.886960, mean_eps: 0.240245\n",
            "📈 Episodio 605: Recompensa total (clipped): 7.000, Pasos: 413, Mean Reward Calculado: 0.016949 (Recompensa/Pasos)\n",
            "  422692/2000000: episode: 605, duration: 13.902s, episode steps: 413, steps per second:  30, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.011563, mae: 0.718019, mean_q: 0.889358, mean_eps: 0.239529\n",
            "📈 Episodio 606: Recompensa total (clipped): 23.000, Pasos: 875, Mean Reward Calculado: 0.026286 (Recompensa/Pasos)\n",
            "  423567/2000000: episode: 606, duration: 29.352s, episode steps: 875, steps per second:  30, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.010864, mae: 0.703274, mean_q: 0.872713, mean_eps: 0.238370\n",
            "📈 Episodio 607: Recompensa total (clipped): 13.000, Pasos: 1065, Mean Reward Calculado: 0.012207 (Recompensa/Pasos)\n",
            "  424632/2000000: episode: 607, duration: 35.821s, episode steps: 1065, steps per second:  30, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.011124, mae: 0.709527, mean_q: 0.879533, mean_eps: 0.236624\n",
            "📈 Episodio 608: Recompensa total (clipped): 17.000, Pasos: 915, Mean Reward Calculado: 0.018579 (Recompensa/Pasos)\n",
            "  425547/2000000: episode: 608, duration: 30.549s, episode steps: 915, steps per second:  30, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.011627, mae: 0.695997, mean_q: 0.863774, mean_eps: 0.234842\n",
            "📈 Episodio 609: Recompensa total (clipped): 19.000, Pasos: 911, Mean Reward Calculado: 0.020856 (Recompensa/Pasos)\n",
            "  426458/2000000: episode: 609, duration: 30.302s, episode steps: 911, steps per second:  30, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.010225, mae: 0.697901, mean_q: 0.865087, mean_eps: 0.233196\n",
            "📈 Episodio 610: Recompensa total (clipped): 7.000, Pasos: 395, Mean Reward Calculado: 0.017722 (Recompensa/Pasos)\n",
            "  426853/2000000: episode: 610, duration: 13.057s, episode steps: 395, steps per second:  30, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.965 [0.000, 5.000],  loss: 0.011456, mae: 0.703508, mean_q: 0.873009, mean_eps: 0.232019\n",
            "📈 Episodio 611: Recompensa total (clipped): 14.000, Pasos: 691, Mean Reward Calculado: 0.020260 (Recompensa/Pasos)\n",
            "  427544/2000000: episode: 611, duration: 22.964s, episode steps: 691, steps per second:  30, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.010804, mae: 0.700443, mean_q: 0.868688, mean_eps: 0.231044\n",
            "📈 Episodio 612: Recompensa total (clipped): 20.000, Pasos: 904, Mean Reward Calculado: 0.022124 (Recompensa/Pasos)\n",
            "  428448/2000000: episode: 612, duration: 30.315s, episode steps: 904, steps per second:  30, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.012811, mae: 0.707326, mean_q: 0.877157, mean_eps: 0.229611\n",
            "📈 Episodio 613: Recompensa total (clipped): 15.000, Pasos: 852, Mean Reward Calculado: 0.017606 (Recompensa/Pasos)\n",
            "  429300/2000000: episode: 613, duration: 28.509s, episode steps: 852, steps per second:  30, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.012428, mae: 0.707169, mean_q: 0.876994, mean_eps: 0.228030\n",
            "📈 Episodio 614: Recompensa total (clipped): 16.000, Pasos: 760, Mean Reward Calculado: 0.021053 (Recompensa/Pasos)\n",
            "  430060/2000000: episode: 614, duration: 25.390s, episode steps: 760, steps per second:  30, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.012401, mae: 0.707433, mean_q: 0.877000, mean_eps: 0.226580\n",
            "📈 Episodio 615: Recompensa total (clipped): 16.000, Pasos: 667, Mean Reward Calculado: 0.023988 (Recompensa/Pasos)\n",
            "  430727/2000000: episode: 615, duration: 22.375s, episode steps: 667, steps per second:  30, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.011118, mae: 0.739209, mean_q: 0.915976, mean_eps: 0.225294\n",
            "📈 Episodio 616: Recompensa total (clipped): 21.000, Pasos: 802, Mean Reward Calculado: 0.026185 (Recompensa/Pasos)\n",
            "  431529/2000000: episode: 616, duration: 26.894s, episode steps: 802, steps per second:  30, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010563, mae: 0.726252, mean_q: 0.900960, mean_eps: 0.223970\n",
            "📈 Episodio 617: Recompensa total (clipped): 12.000, Pasos: 565, Mean Reward Calculado: 0.021239 (Recompensa/Pasos)\n",
            "  432094/2000000: episode: 617, duration: 18.770s, episode steps: 565, steps per second:  30, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.011131, mae: 0.743410, mean_q: 0.920042, mean_eps: 0.222738\n",
            "📈 Episodio 618: Recompensa total (clipped): 14.000, Pasos: 883, Mean Reward Calculado: 0.015855 (Recompensa/Pasos)\n",
            "  432977/2000000: episode: 618, duration: 29.526s, episode steps: 883, steps per second:  30, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.011448, mae: 0.741132, mean_q: 0.918817, mean_eps: 0.221435\n",
            "📈 Episodio 619: Recompensa total (clipped): 26.000, Pasos: 1068, Mean Reward Calculado: 0.024345 (Recompensa/Pasos)\n",
            "  434045/2000000: episode: 619, duration: 35.707s, episode steps: 1068, steps per second:  30, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.011031, mae: 0.730587, mean_q: 0.905291, mean_eps: 0.219678\n",
            "📈 Episodio 620: Recompensa total (clipped): 20.000, Pasos: 979, Mean Reward Calculado: 0.020429 (Recompensa/Pasos)\n",
            "  435024/2000000: episode: 620, duration: 32.652s, episode steps: 979, steps per second:  30, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.011523, mae: 0.742285, mean_q: 0.917185, mean_eps: 0.217839\n",
            "📈 Episodio 621: Recompensa total (clipped): 16.000, Pasos: 894, Mean Reward Calculado: 0.017897 (Recompensa/Pasos)\n",
            "  435918/2000000: episode: 621, duration: 29.729s, episode steps: 894, steps per second:  30, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011149, mae: 0.741174, mean_q: 0.916589, mean_eps: 0.216154\n",
            "📈 Episodio 622: Recompensa total (clipped): 14.000, Pasos: 878, Mean Reward Calculado: 0.015945 (Recompensa/Pasos)\n",
            "  436796/2000000: episode: 622, duration: 29.291s, episode steps: 878, steps per second:  30, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.012392, mae: 0.736529, mean_q: 0.912475, mean_eps: 0.214559\n",
            "📈 Episodio 623: Recompensa total (clipped): 14.000, Pasos: 642, Mean Reward Calculado: 0.021807 (Recompensa/Pasos)\n",
            "  437438/2000000: episode: 623, duration: 21.351s, episode steps: 642, steps per second:  30, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.011607, mae: 0.727831, mean_q: 0.901494, mean_eps: 0.213191\n",
            "📈 Episodio 624: Recompensa total (clipped): 25.000, Pasos: 984, Mean Reward Calculado: 0.025407 (Recompensa/Pasos)\n",
            "  438422/2000000: episode: 624, duration: 32.879s, episode steps: 984, steps per second:  30, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.010745, mae: 0.731480, mean_q: 0.906469, mean_eps: 0.211726\n",
            "📈 Episodio 625: Recompensa total (clipped): 8.000, Pasos: 558, Mean Reward Calculado: 0.014337 (Recompensa/Pasos)\n",
            "  438980/2000000: episode: 625, duration: 18.729s, episode steps: 558, steps per second:  30, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.010965, mae: 0.732122, mean_q: 0.905656, mean_eps: 0.210340\n",
            "📈 Episodio 626: Recompensa total (clipped): 15.000, Pasos: 695, Mean Reward Calculado: 0.021583 (Recompensa/Pasos)\n",
            "  439675/2000000: episode: 626, duration: 23.396s, episode steps: 695, steps per second:  30, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.010274, mae: 0.736336, mean_q: 0.911446, mean_eps: 0.209213\n",
            "📊 Paso 440,000/2,000,000 (22.0%) - 35.7 pasos/seg - ETA: 12.1h - Memoria: 5382.15 MB\n",
            "📈 Episodio 627: Recompensa total (clipped): 17.000, Pasos: 752, Mean Reward Calculado: 0.022606 (Recompensa/Pasos)\n",
            "  440427/2000000: episode: 627, duration: 25.064s, episode steps: 752, steps per second:  30, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.012649, mae: 0.759559, mean_q: 0.940543, mean_eps: 0.207910\n",
            "📈 Episodio 628: Recompensa total (clipped): 14.000, Pasos: 709, Mean Reward Calculado: 0.019746 (Recompensa/Pasos)\n",
            "  441136/2000000: episode: 628, duration: 23.721s, episode steps: 709, steps per second:  30, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.011688, mae: 0.777407, mean_q: 0.961569, mean_eps: 0.206596\n",
            "📈 Episodio 629: Recompensa total (clipped): 9.000, Pasos: 509, Mean Reward Calculado: 0.017682 (Recompensa/Pasos)\n",
            "  441645/2000000: episode: 629, duration: 17.211s, episode steps: 509, steps per second:  30, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.011229, mae: 0.788449, mean_q: 0.977290, mean_eps: 0.205498\n",
            "📈 Episodio 630: Recompensa total (clipped): 12.000, Pasos: 667, Mean Reward Calculado: 0.017991 (Recompensa/Pasos)\n",
            "  442312/2000000: episode: 630, duration: 22.491s, episode steps: 667, steps per second:  30, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.011732, mae: 0.785250, mean_q: 0.972669, mean_eps: 0.204440\n",
            "📈 Episodio 631: Recompensa total (clipped): 16.000, Pasos: 767, Mean Reward Calculado: 0.020860 (Recompensa/Pasos)\n",
            "  443079/2000000: episode: 631, duration: 25.626s, episode steps: 767, steps per second:  30, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012190, mae: 0.777659, mean_q: 0.962773, mean_eps: 0.203151\n",
            "📈 Episodio 632: Recompensa total (clipped): 11.000, Pasos: 605, Mean Reward Calculado: 0.018182 (Recompensa/Pasos)\n",
            "  443684/2000000: episode: 632, duration: 20.265s, episode steps: 605, steps per second:  30, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.012493, mae: 0.784883, mean_q: 0.970594, mean_eps: 0.201916\n",
            "📈 Episodio 633: Recompensa total (clipped): 13.000, Pasos: 575, Mean Reward Calculado: 0.022609 (Recompensa/Pasos)\n",
            "  444259/2000000: episode: 633, duration: 19.491s, episode steps: 575, steps per second:  30, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.083 [0.000, 5.000],  loss: 0.013533, mae: 0.800229, mean_q: 0.992175, mean_eps: 0.200854\n",
            "📈 Episodio 634: Recompensa total (clipped): 9.000, Pasos: 508, Mean Reward Calculado: 0.017717 (Recompensa/Pasos)\n",
            "  444767/2000000: episode: 634, duration: 16.966s, episode steps: 508, steps per second:  30, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014386, mae: 0.787911, mean_q: 0.976605, mean_eps: 0.199878\n",
            "📈 Episodio 635: Recompensa total (clipped): 13.000, Pasos: 725, Mean Reward Calculado: 0.017931 (Recompensa/Pasos)\n",
            "  445492/2000000: episode: 635, duration: 24.536s, episode steps: 725, steps per second:  30, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.011691, mae: 0.773409, mean_q: 0.960249, mean_eps: 0.198770\n",
            "📈 Episodio 636: Recompensa total (clipped): 18.000, Pasos: 1113, Mean Reward Calculado: 0.016173 (Recompensa/Pasos)\n",
            "  446605/2000000: episode: 636, duration: 37.444s, episode steps: 1113, steps per second:  30, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.012191, mae: 0.776192, mean_q: 0.960121, mean_eps: 0.197114\n",
            "📈 Episodio 637: Recompensa total (clipped): 8.000, Pasos: 494, Mean Reward Calculado: 0.016194 (Recompensa/Pasos)\n",
            "  447099/2000000: episode: 637, duration: 16.457s, episode steps: 494, steps per second:  30, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.010909, mae: 0.782232, mean_q: 0.969045, mean_eps: 0.195666\n",
            "📈 Episodio 638: Recompensa total (clipped): 9.000, Pasos: 503, Mean Reward Calculado: 0.017893 (Recompensa/Pasos)\n",
            "  447602/2000000: episode: 638, duration: 16.925s, episode steps: 503, steps per second:  30, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.012594, mae: 0.775057, mean_q: 0.959514, mean_eps: 0.194770\n",
            "📈 Episodio 639: Recompensa total (clipped): 22.000, Pasos: 953, Mean Reward Calculado: 0.023085 (Recompensa/Pasos)\n",
            "  448555/2000000: episode: 639, duration: 32.046s, episode steps: 953, steps per second:  30, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012197, mae: 0.778751, mean_q: 0.961576, mean_eps: 0.193460\n",
            "📈 Episodio 640: Recompensa total (clipped): 10.000, Pasos: 566, Mean Reward Calculado: 0.017668 (Recompensa/Pasos)\n",
            "  449121/2000000: episode: 640, duration: 19.324s, episode steps: 566, steps per second:  29, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.012141, mae: 0.783726, mean_q: 0.967419, mean_eps: 0.192092\n",
            "📈 Episodio 641: Recompensa total (clipped): 23.000, Pasos: 942, Mean Reward Calculado: 0.024416 (Recompensa/Pasos)\n",
            "  450063/2000000: episode: 641, duration: 31.790s, episode steps: 942, steps per second:  30, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.011639, mae: 0.783703, mean_q: 0.967007, mean_eps: 0.190734\n",
            "📈 Episodio 642: Recompensa total (clipped): 8.000, Pasos: 535, Mean Reward Calculado: 0.014953 (Recompensa/Pasos)\n",
            "  450598/2000000: episode: 642, duration: 18.133s, episode steps: 535, steps per second:  30, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.011510, mae: 0.820578, mean_q: 1.014900, mean_eps: 0.189406\n",
            "📈 Episodio 643: Recompensa total (clipped): 18.000, Pasos: 617, Mean Reward Calculado: 0.029173 (Recompensa/Pasos)\n",
            "  451215/2000000: episode: 643, duration: 20.732s, episode steps: 617, steps per second:  30, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.012847, mae: 0.808434, mean_q: 1.000207, mean_eps: 0.188369\n",
            "📈 Episodio 644: Recompensa total (clipped): 21.000, Pasos: 922, Mean Reward Calculado: 0.022777 (Recompensa/Pasos)\n",
            "  452137/2000000: episode: 644, duration: 31.103s, episode steps: 922, steps per second:  30, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.011273, mae: 0.811252, mean_q: 1.003008, mean_eps: 0.186983\n",
            "📈 Episodio 645: Recompensa total (clipped): 23.000, Pasos: 1225, Mean Reward Calculado: 0.018776 (Recompensa/Pasos)\n",
            "  453362/2000000: episode: 645, duration: 41.207s, episode steps: 1225, steps per second:  30, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.011855, mae: 0.814440, mean_q: 1.006390, mean_eps: 0.185050\n",
            "📈 Episodio 646: Recompensa total (clipped): 14.000, Pasos: 750, Mean Reward Calculado: 0.018667 (Recompensa/Pasos)\n",
            "  454112/2000000: episode: 646, duration: 25.281s, episode steps: 750, steps per second:  30, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.012658, mae: 0.809333, mean_q: 0.998573, mean_eps: 0.183275\n",
            "📈 Episodio 647: Recompensa total (clipped): 15.000, Pasos: 756, Mean Reward Calculado: 0.019841 (Recompensa/Pasos)\n",
            "  454868/2000000: episode: 647, duration: 25.867s, episode steps: 756, steps per second:  29, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.012445, mae: 0.811965, mean_q: 1.002599, mean_eps: 0.181922\n",
            "📈 Episodio 648: Recompensa total (clipped): 12.000, Pasos: 716, Mean Reward Calculado: 0.016760 (Recompensa/Pasos)\n",
            "  455584/2000000: episode: 648, duration: 24.215s, episode steps: 716, steps per second:  30, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011931, mae: 0.806393, mean_q: 0.995661, mean_eps: 0.180597\n",
            "📈 Episodio 649: Recompensa total (clipped): 13.000, Pasos: 674, Mean Reward Calculado: 0.019288 (Recompensa/Pasos)\n",
            "  456258/2000000: episode: 649, duration: 22.718s, episode steps: 674, steps per second:  30, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.010855, mae: 0.808289, mean_q: 0.998115, mean_eps: 0.179344\n",
            "📈 Episodio 650: Recompensa total (clipped): 28.000, Pasos: 1229, Mean Reward Calculado: 0.022783 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 650, pasos: 457487)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.68 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 650 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 28.00\n",
            "   Media últimos 100: 14.68 / 20.0\n",
            "   Mejor promedio histórico: 14.68\n",
            "   Estado: 📈 73.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  457487/2000000: episode: 650, duration: 59.724s, episode steps: 1229, steps per second:  21, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.012550, mae: 0.813160, mean_q: 1.002970, mean_eps: 0.177630\n",
            "📈 Episodio 651: Recompensa total (clipped): 17.000, Pasos: 752, Mean Reward Calculado: 0.022606 (Recompensa/Pasos)\n",
            "  458239/2000000: episode: 651, duration: 26.184s, episode steps: 752, steps per second:  29, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.013340, mae: 0.818733, mean_q: 1.009971, mean_eps: 0.175848\n",
            "📈 Episodio 652: Recompensa total (clipped): 11.000, Pasos: 538, Mean Reward Calculado: 0.020446 (Recompensa/Pasos)\n",
            "  458777/2000000: episode: 652, duration: 18.341s, episode steps: 538, steps per second:  29, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.012463, mae: 0.811214, mean_q: 1.000846, mean_eps: 0.174686\n",
            "📈 Episodio 653: Recompensa total (clipped): 17.000, Pasos: 818, Mean Reward Calculado: 0.020782 (Recompensa/Pasos)\n",
            "  459595/2000000: episode: 653, duration: 27.771s, episode steps: 818, steps per second:  29, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.012533, mae: 0.815704, mean_q: 1.006055, mean_eps: 0.173465\n",
            "📊 Paso 460,000/2,000,000 (23.0%) - 35.4 pasos/seg - ETA: 12.1h - Memoria: 5521.16 MB\n",
            "📈 Episodio 654: Recompensa total (clipped): 12.000, Pasos: 623, Mean Reward Calculado: 0.019262 (Recompensa/Pasos)\n",
            "  460218/2000000: episode: 654, duration: 21.095s, episode steps: 623, steps per second:  30, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.011065, mae: 0.815513, mean_q: 1.008195, mean_eps: 0.172169\n",
            "📈 Episodio 655: Recompensa total (clipped): 18.000, Pasos: 776, Mean Reward Calculado: 0.023196 (Recompensa/Pasos)\n",
            "  460994/2000000: episode: 655, duration: 26.219s, episode steps: 776, steps per second:  30, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.011323, mae: 0.846138, mean_q: 1.047468, mean_eps: 0.170909\n",
            "📈 Episodio 656: Recompensa total (clipped): 15.000, Pasos: 679, Mean Reward Calculado: 0.022091 (Recompensa/Pasos)\n",
            "  461673/2000000: episode: 656, duration: 23.111s, episode steps: 679, steps per second:  29, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.011509, mae: 0.829232, mean_q: 1.023887, mean_eps: 0.169599\n",
            "📈 Episodio 657: Recompensa total (clipped): 19.000, Pasos: 764, Mean Reward Calculado: 0.024869 (Recompensa/Pasos)\n",
            "  462437/2000000: episode: 657, duration: 25.921s, episode steps: 764, steps per second:  29, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.012084, mae: 0.834084, mean_q: 1.031827, mean_eps: 0.168299\n",
            "📈 Episodio 658: Recompensa total (clipped): 10.000, Pasos: 453, Mean Reward Calculado: 0.022075 (Recompensa/Pasos)\n",
            "  462890/2000000: episode: 658, duration: 15.368s, episode steps: 453, steps per second:  29, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.011711, mae: 0.842929, mean_q: 1.040137, mean_eps: 0.167205\n",
            "📈 Episodio 659: Recompensa total (clipped): 8.000, Pasos: 354, Mean Reward Calculado: 0.022599 (Recompensa/Pasos)\n",
            "  463244/2000000: episode: 659, duration: 12.086s, episode steps: 354, steps per second:  29, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.011860, mae: 0.815502, mean_q: 1.006403, mean_eps: 0.166481\n",
            "📈 Episodio 660: Recompensa total (clipped): 19.000, Pasos: 1108, Mean Reward Calculado: 0.017148 (Recompensa/Pasos)\n",
            "  464352/2000000: episode: 660, duration: 37.936s, episode steps: 1108, steps per second:  29, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.012792, mae: 0.827989, mean_q: 1.023680, mean_eps: 0.165167\n",
            "📈 Episodio 661: Recompensa total (clipped): 13.000, Pasos: 691, Mean Reward Calculado: 0.018813 (Recompensa/Pasos)\n",
            "  465043/2000000: episode: 661, duration: 23.566s, episode steps: 691, steps per second:  29, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012067, mae: 0.835472, mean_q: 1.029935, mean_eps: 0.163547\n",
            "📈 Episodio 662: Recompensa total (clipped): 17.000, Pasos: 875, Mean Reward Calculado: 0.019429 (Recompensa/Pasos)\n",
            "  465918/2000000: episode: 662, duration: 29.907s, episode steps: 875, steps per second:  29, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.009 [0.000, 5.000],  loss: 0.012193, mae: 0.830618, mean_q: 1.023558, mean_eps: 0.162136\n",
            "📈 Episodio 663: Recompensa total (clipped): 12.000, Pasos: 569, Mean Reward Calculado: 0.021090 (Recompensa/Pasos)\n",
            "  466487/2000000: episode: 663, duration: 19.486s, episode steps: 569, steps per second:  29, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.012065, mae: 0.839574, mean_q: 1.037487, mean_eps: 0.160836\n",
            "📈 Episodio 664: Recompensa total (clipped): 11.000, Pasos: 473, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            "  466960/2000000: episode: 664, duration: 16.409s, episode steps: 473, steps per second:  29, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.012345, mae: 0.827079, mean_q: 1.020517, mean_eps: 0.159900\n",
            "📈 Episodio 665: Recompensa total (clipped): 10.000, Pasos: 458, Mean Reward Calculado: 0.021834 (Recompensa/Pasos)\n",
            "  467418/2000000: episode: 665, duration: 15.530s, episode steps: 458, steps per second:  29, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.012909, mae: 0.835394, mean_q: 1.030768, mean_eps: 0.159062\n",
            "📈 Episodio 666: Recompensa total (clipped): 12.000, Pasos: 546, Mean Reward Calculado: 0.021978 (Recompensa/Pasos)\n",
            "  467964/2000000: episode: 666, duration: 18.652s, episode steps: 546, steps per second:  29, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.013818, mae: 0.846926, mean_q: 1.044855, mean_eps: 0.158158\n",
            "📈 Episodio 667: Recompensa total (clipped): 28.000, Pasos: 1060, Mean Reward Calculado: 0.026415 (Recompensa/Pasos)\n",
            "  469024/2000000: episode: 667, duration: 35.985s, episode steps: 1060, steps per second:  29, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.012143, mae: 0.831202, mean_q: 1.027908, mean_eps: 0.156714\n",
            "📈 Episodio 668: Recompensa total (clipped): 15.000, Pasos: 681, Mean Reward Calculado: 0.022026 (Recompensa/Pasos)\n",
            "  469705/2000000: episode: 668, duration: 23.156s, episode steps: 681, steps per second:  29, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.012493, mae: 0.835657, mean_q: 1.032805, mean_eps: 0.155145\n",
            "📈 Episodio 669: Recompensa total (clipped): 7.000, Pasos: 430, Mean Reward Calculado: 0.016279 (Recompensa/Pasos)\n",
            "  470135/2000000: episode: 669, duration: 14.429s, episode steps: 430, steps per second:  30, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.012616, mae: 0.848202, mean_q: 1.045486, mean_eps: 0.154144\n",
            "📈 Episodio 670: Recompensa total (clipped): 11.000, Pasos: 492, Mean Reward Calculado: 0.022358 (Recompensa/Pasos)\n",
            "  470627/2000000: episode: 670, duration: 16.705s, episode steps: 492, steps per second:  29, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.010120, mae: 0.879625, mean_q: 1.085660, mean_eps: 0.153316\n",
            "📈 Episodio 671: Recompensa total (clipped): 13.000, Pasos: 773, Mean Reward Calculado: 0.016818 (Recompensa/Pasos)\n",
            "  471400/2000000: episode: 671, duration: 26.526s, episode steps: 773, steps per second:  29, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.012040, mae: 0.887727, mean_q: 1.093512, mean_eps: 0.152178\n",
            "📈 Episodio 672: Recompensa total (clipped): 15.000, Pasos: 752, Mean Reward Calculado: 0.019947 (Recompensa/Pasos)\n",
            "  472152/2000000: episode: 672, duration: 25.577s, episode steps: 752, steps per second:  29, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.011441, mae: 0.893418, mean_q: 1.102308, mean_eps: 0.150807\n",
            "📈 Episodio 673: Recompensa total (clipped): 21.000, Pasos: 864, Mean Reward Calculado: 0.024306 (Recompensa/Pasos)\n",
            "  473016/2000000: episode: 673, duration: 29.714s, episode steps: 864, steps per second:  29, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.012624, mae: 0.890151, mean_q: 1.098222, mean_eps: 0.149352\n",
            "📈 Episodio 674: Recompensa total (clipped): 8.000, Pasos: 453, Mean Reward Calculado: 0.017660 (Recompensa/Pasos)\n",
            "  473469/2000000: episode: 674, duration: 15.478s, episode steps: 453, steps per second:  29, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012448, mae: 0.877128, mean_q: 1.080212, mean_eps: 0.148164\n",
            "📈 Episodio 675: Recompensa total (clipped): 20.000, Pasos: 861, Mean Reward Calculado: 0.023229 (Recompensa/Pasos)\n",
            "  474330/2000000: episode: 675, duration: 29.243s, episode steps: 861, steps per second:  29, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.011376, mae: 0.882928, mean_q: 1.088966, mean_eps: 0.146980\n",
            "📈 Episodio 676: Recompensa total (clipped): 23.000, Pasos: 859, Mean Reward Calculado: 0.026775 (Recompensa/Pasos)\n",
            "  475189/2000000: episode: 676, duration: 29.374s, episode steps: 859, steps per second:  29, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.013019, mae: 0.880254, mean_q: 1.086508, mean_eps: 0.145432\n",
            "📈 Episodio 677: Recompensa total (clipped): 11.000, Pasos: 628, Mean Reward Calculado: 0.017516 (Recompensa/Pasos)\n",
            "  475817/2000000: episode: 677, duration: 21.651s, episode steps: 628, steps per second:  29, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.011442, mae: 0.876919, mean_q: 1.083072, mean_eps: 0.144093\n",
            "📈 Episodio 678: Recompensa total (clipped): 11.000, Pasos: 483, Mean Reward Calculado: 0.022774 (Recompensa/Pasos)\n",
            "  476300/2000000: episode: 678, duration: 16.630s, episode steps: 483, steps per second:  29, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012250, mae: 0.876568, mean_q: 1.081762, mean_eps: 0.143096\n",
            "📈 Episodio 679: Recompensa total (clipped): 7.000, Pasos: 482, Mean Reward Calculado: 0.014523 (Recompensa/Pasos)\n",
            "  476782/2000000: episode: 679, duration: 16.497s, episode steps: 482, steps per second:  29, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.012022, mae: 0.881215, mean_q: 1.085222, mean_eps: 0.142228\n",
            "📈 Episodio 680: Recompensa total (clipped): 10.000, Pasos: 526, Mean Reward Calculado: 0.019011 (Recompensa/Pasos)\n",
            "  477308/2000000: episode: 680, duration: 18.099s, episode steps: 526, steps per second:  29, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.011344, mae: 0.876644, mean_q: 1.078109, mean_eps: 0.141321\n",
            "📈 Episodio 681: Recompensa total (clipped): 13.000, Pasos: 773, Mean Reward Calculado: 0.016818 (Recompensa/Pasos)\n",
            "  478081/2000000: episode: 681, duration: 26.612s, episode steps: 773, steps per second:  29, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.013643, mae: 0.892823, mean_q: 1.100016, mean_eps: 0.140151\n",
            "📈 Episodio 682: Recompensa total (clipped): 11.000, Pasos: 587, Mean Reward Calculado: 0.018739 (Recompensa/Pasos)\n",
            "  478668/2000000: episode: 682, duration: 20.162s, episode steps: 587, steps per second:  29, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013156, mae: 0.889817, mean_q: 1.094755, mean_eps: 0.138927\n",
            "📈 Episodio 683: Recompensa total (clipped): 11.000, Pasos: 393, Mean Reward Calculado: 0.027990 (Recompensa/Pasos)\n",
            "  479061/2000000: episode: 683, duration: 13.604s, episode steps: 393, steps per second:  29, episode reward: 11.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.012242, mae: 0.885223, mean_q: 1.088768, mean_eps: 0.138045\n",
            "📊 Paso 480,000/2,000,000 (24.0%) - 35.1 pasos/seg - ETA: 12.0h - Memoria: 5675.16 MB\n",
            "📈 Episodio 684: Recompensa total (clipped): 21.000, Pasos: 1065, Mean Reward Calculado: 0.019718 (Recompensa/Pasos)\n",
            "  480126/2000000: episode: 684, duration: 36.351s, episode steps: 1065, steps per second:  29, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.012701, mae: 0.871595, mean_q: 1.071907, mean_eps: 0.136731\n",
            "📈 Episodio 685: Recompensa total (clipped): 8.000, Pasos: 500, Mean Reward Calculado: 0.016000 (Recompensa/Pasos)\n",
            "  480626/2000000: episode: 685, duration: 17.175s, episode steps: 500, steps per second:  29, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011412, mae: 0.880891, mean_q: 1.085983, mean_eps: 0.135323\n",
            "📈 Episodio 686: Recompensa total (clipped): 24.000, Pasos: 807, Mean Reward Calculado: 0.029740 (Recompensa/Pasos)\n",
            "  481433/2000000: episode: 686, duration: 27.563s, episode steps: 807, steps per second:  29, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.073 [0.000, 5.000],  loss: 0.011747, mae: 0.897772, mean_q: 1.107502, mean_eps: 0.134146\n",
            "📈 Episodio 687: Recompensa total (clipped): 15.000, Pasos: 677, Mean Reward Calculado: 0.022157 (Recompensa/Pasos)\n",
            "  482110/2000000: episode: 687, duration: 23.197s, episode steps: 677, steps per second:  29, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.011981, mae: 0.882113, mean_q: 1.084931, mean_eps: 0.132810\n",
            "📈 Episodio 688: Recompensa total (clipped): 15.000, Pasos: 620, Mean Reward Calculado: 0.024194 (Recompensa/Pasos)\n",
            "  482730/2000000: episode: 688, duration: 21.340s, episode steps: 620, steps per second:  29, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.012268, mae: 0.898457, mean_q: 1.107179, mean_eps: 0.131644\n",
            "📈 Episodio 689: Recompensa total (clipped): 6.000, Pasos: 567, Mean Reward Calculado: 0.010582 (Recompensa/Pasos)\n",
            "  483297/2000000: episode: 689, duration: 19.469s, episode steps: 567, steps per second:  29, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.012961, mae: 0.883791, mean_q: 1.086117, mean_eps: 0.130575\n",
            "📈 Episodio 690: Recompensa total (clipped): 16.000, Pasos: 732, Mean Reward Calculado: 0.021858 (Recompensa/Pasos)\n",
            "  484029/2000000: episode: 690, duration: 24.896s, episode steps: 732, steps per second:  29, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.012701, mae: 0.886312, mean_q: 1.089374, mean_eps: 0.129405\n",
            "📈 Episodio 691: Recompensa total (clipped): 7.000, Pasos: 426, Mean Reward Calculado: 0.016432 (Recompensa/Pasos)\n",
            "  484455/2000000: episode: 691, duration: 14.487s, episode steps: 426, steps per second:  29, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.011963, mae: 0.902078, mean_q: 1.108357, mean_eps: 0.128364\n",
            "📈 Episodio 692: Recompensa total (clipped): 27.000, Pasos: 973, Mean Reward Calculado: 0.027749 (Recompensa/Pasos)\n",
            "  485428/2000000: episode: 692, duration: 33.482s, episode steps: 973, steps per second:  29, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.013064, mae: 0.885733, mean_q: 1.088351, mean_eps: 0.127108\n",
            "📈 Episodio 693: Recompensa total (clipped): 15.000, Pasos: 859, Mean Reward Calculado: 0.017462 (Recompensa/Pasos)\n",
            "  486287/2000000: episode: 693, duration: 29.504s, episode steps: 859, steps per second:  29, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.013240, mae: 0.890818, mean_q: 1.095391, mean_eps: 0.125459\n",
            "📈 Episodio 694: Recompensa total (clipped): 19.000, Pasos: 987, Mean Reward Calculado: 0.019250 (Recompensa/Pasos)\n",
            "  487274/2000000: episode: 694, duration: 33.882s, episode steps: 987, steps per second:  29, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.013279, mae: 0.880885, mean_q: 1.083636, mean_eps: 0.123796\n",
            "📈 Episodio 695: Recompensa total (clipped): 18.000, Pasos: 971, Mean Reward Calculado: 0.018538 (Recompensa/Pasos)\n",
            "  488245/2000000: episode: 695, duration: 33.345s, episode steps: 971, steps per second:  29, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.012363, mae: 0.891296, mean_q: 1.095436, mean_eps: 0.122032\n",
            "📈 Episodio 696: Recompensa total (clipped): 10.000, Pasos: 599, Mean Reward Calculado: 0.016694 (Recompensa/Pasos)\n",
            "  488844/2000000: episode: 696, duration: 20.629s, episode steps: 599, steps per second:  29, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.011666, mae: 0.875391, mean_q: 1.074865, mean_eps: 0.120621\n",
            "📈 Episodio 697: Recompensa total (clipped): 22.000, Pasos: 946, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            "  489790/2000000: episode: 697, duration: 32.652s, episode steps: 946, steps per second:  29, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.012043, mae: 0.885123, mean_q: 1.087656, mean_eps: 0.119231\n",
            "📈 Episodio 698: Recompensa total (clipped): 15.000, Pasos: 898, Mean Reward Calculado: 0.016704 (Recompensa/Pasos)\n",
            "  490688/2000000: episode: 698, duration: 30.744s, episode steps: 898, steps per second:  29, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011790, mae: 0.909336, mean_q: 1.118828, mean_eps: 0.117572\n",
            "📈 Episodio 699: Recompensa total (clipped): 6.000, Pasos: 400, Mean Reward Calculado: 0.015000 (Recompensa/Pasos)\n",
            "  491088/2000000: episode: 699, duration: 13.850s, episode steps: 400, steps per second:  29, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.010859, mae: 0.905405, mean_q: 1.115884, mean_eps: 0.116405\n",
            "📈 Episodio 700: Recompensa total (clipped): 14.000, Pasos: 767, Mean Reward Calculado: 0.018253 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 700, pasos: 491855)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.74 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 700 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 14.00\n",
            "   Media últimos 100: 14.74 / 20.0\n",
            "   Mejor promedio histórico: 14.74\n",
            "   Estado: 📈 73.7% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  491855/2000000: episode: 700, duration: 44.116s, episode steps: 767, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.012165, mae: 0.907018, mean_q: 1.119067, mean_eps: 0.115354\n",
            "📈 Episodio 701: Recompensa total (clipped): 23.000, Pasos: 1094, Mean Reward Calculado: 0.021024 (Recompensa/Pasos)\n",
            "  492949/2000000: episode: 701, duration: 38.492s, episode steps: 1094, steps per second:  28, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.012656, mae: 0.895992, mean_q: 1.102413, mean_eps: 0.113676\n",
            "📈 Episodio 702: Recompensa total (clipped): 19.000, Pasos: 882, Mean Reward Calculado: 0.021542 (Recompensa/Pasos)\n",
            "  493831/2000000: episode: 702, duration: 30.971s, episode steps: 882, steps per second:  28, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.011656, mae: 0.906732, mean_q: 1.116978, mean_eps: 0.111898\n",
            "📈 Episodio 703: Recompensa total (clipped): 17.000, Pasos: 743, Mean Reward Calculado: 0.022880 (Recompensa/Pasos)\n",
            "  494574/2000000: episode: 703, duration: 25.950s, episode steps: 743, steps per second:  29, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.012685, mae: 0.902989, mean_q: 1.110047, mean_eps: 0.110436\n",
            "📈 Episodio 704: Recompensa total (clipped): 21.000, Pasos: 1201, Mean Reward Calculado: 0.017485 (Recompensa/Pasos)\n",
            "  495775/2000000: episode: 704, duration: 42.233s, episode steps: 1201, steps per second:  28, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.012560, mae: 0.917542, mean_q: 1.130128, mean_eps: 0.108687\n",
            "📈 Episodio 705: Recompensa total (clipped): 12.000, Pasos: 598, Mean Reward Calculado: 0.020067 (Recompensa/Pasos)\n",
            "  496373/2000000: episode: 705, duration: 21.771s, episode steps: 598, steps per second:  27, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.012180, mae: 0.908558, mean_q: 1.118703, mean_eps: 0.107067\n",
            "📈 Episodio 706: Recompensa total (clipped): 9.000, Pasos: 492, Mean Reward Calculado: 0.018293 (Recompensa/Pasos)\n",
            "  496865/2000000: episode: 706, duration: 17.970s, episode steps: 492, steps per second:  27, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.011996, mae: 0.889966, mean_q: 1.094333, mean_eps: 0.106084\n",
            "📈 Episodio 707: Recompensa total (clipped): 13.000, Pasos: 617, Mean Reward Calculado: 0.021070 (Recompensa/Pasos)\n",
            "  497482/2000000: episode: 707, duration: 21.494s, episode steps: 617, steps per second:  29, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013184, mae: 0.921243, mean_q: 1.135793, mean_eps: 0.105087\n",
            "📈 Episodio 708: Recompensa total (clipped): 4.000, Pasos: 356, Mean Reward Calculado: 0.011236 (Recompensa/Pasos)\n",
            "  497838/2000000: episode: 708, duration: 12.398s, episode steps: 356, steps per second:  29, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.011292, mae: 0.893494, mean_q: 1.098724, mean_eps: 0.104212\n",
            "📈 Episodio 709: Recompensa total (clipped): 9.000, Pasos: 530, Mean Reward Calculado: 0.016981 (Recompensa/Pasos)\n",
            "  498368/2000000: episode: 709, duration: 18.386s, episode steps: 530, steps per second:  29, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.014241, mae: 0.906199, mean_q: 1.111762, mean_eps: 0.103416\n",
            "📈 Episodio 710: Recompensa total (clipped): 12.000, Pasos: 662, Mean Reward Calculado: 0.018127 (Recompensa/Pasos)\n",
            "  499030/2000000: episode: 710, duration: 23.249s, episode steps: 662, steps per second:  28, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011995, mae: 0.899584, mean_q: 1.107356, mean_eps: 0.102344\n",
            "📈 Episodio 711: Recompensa total (clipped): 10.000, Pasos: 436, Mean Reward Calculado: 0.022936 (Recompensa/Pasos)\n",
            "  499466/2000000: episode: 711, duration: 14.998s, episode steps: 436, steps per second:  29, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.012620, mae: 0.903991, mean_q: 1.111470, mean_eps: 0.101354\n",
            "📊 Paso 500,000/2,000,000 (25.0%) - 34.7 pasos/seg - ETA: 12.0h - Memoria: 5887.26 MB\n",
            "📈 Episodio 712: Recompensa total (clipped): 16.000, Pasos: 642, Mean Reward Calculado: 0.024922 (Recompensa/Pasos)\n",
            "  500108/2000000: episode: 712, duration: 22.391s, episode steps: 642, steps per second:  29, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.012745, mae: 0.912398, mean_q: 1.123562, mean_eps: 0.100401\n",
            "📈 Episodio 713: Recompensa total (clipped): 8.000, Pasos: 356, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            "  500464/2000000: episode: 713, duration: 12.377s, episode steps: 356, steps per second:  29, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.012714, mae: 0.973292, mean_q: 1.200081, mean_eps: 0.100000\n",
            "📈 Episodio 714: Recompensa total (clipped): 17.000, Pasos: 939, Mean Reward Calculado: 0.018104 (Recompensa/Pasos)\n",
            "  501403/2000000: episode: 714, duration: 32.508s, episode steps: 939, steps per second:  29, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.012070, mae: 0.961153, mean_q: 1.184508, mean_eps: 0.100000\n",
            "📈 Episodio 715: Recompensa total (clipped): 19.000, Pasos: 790, Mean Reward Calculado: 0.024051 (Recompensa/Pasos)\n",
            "  502193/2000000: episode: 715, duration: 27.237s, episode steps: 790, steps per second:  29, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.012810, mae: 0.964490, mean_q: 1.186569, mean_eps: 0.100000\n",
            "📈 Episodio 716: Recompensa total (clipped): 6.000, Pasos: 474, Mean Reward Calculado: 0.012658 (Recompensa/Pasos)\n",
            "  502667/2000000: episode: 716, duration: 16.394s, episode steps: 474, steps per second:  29, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.014408, mae: 0.983691, mean_q: 1.208591, mean_eps: 0.100000\n",
            "📈 Episodio 717: Recompensa total (clipped): 9.000, Pasos: 634, Mean Reward Calculado: 0.014196 (Recompensa/Pasos)\n",
            "  503301/2000000: episode: 717, duration: 22.110s, episode steps: 634, steps per second:  29, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.013347, mae: 0.959883, mean_q: 1.179632, mean_eps: 0.100000\n",
            "📈 Episodio 718: Recompensa total (clipped): 13.000, Pasos: 689, Mean Reward Calculado: 0.018868 (Recompensa/Pasos)\n",
            "  503990/2000000: episode: 718, duration: 23.819s, episode steps: 689, steps per second:  29, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.012257, mae: 0.969695, mean_q: 1.192161, mean_eps: 0.100000\n",
            "📈 Episodio 719: Recompensa total (clipped): 28.000, Pasos: 1156, Mean Reward Calculado: 0.024221 (Recompensa/Pasos)\n",
            "  505146/2000000: episode: 719, duration: 40.287s, episode steps: 1156, steps per second:  29, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.013317, mae: 0.972306, mean_q: 1.197538, mean_eps: 0.100000\n",
            "📈 Episodio 720: Recompensa total (clipped): 12.000, Pasos: 500, Mean Reward Calculado: 0.024000 (Recompensa/Pasos)\n",
            "  505646/2000000: episode: 720, duration: 17.476s, episode steps: 500, steps per second:  29, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.013793, mae: 0.954062, mean_q: 1.174985, mean_eps: 0.100000\n",
            "📈 Episodio 721: Recompensa total (clipped): 15.000, Pasos: 727, Mean Reward Calculado: 0.020633 (Recompensa/Pasos)\n",
            "  506373/2000000: episode: 721, duration: 25.420s, episode steps: 727, steps per second:  29, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013206, mae: 0.959520, mean_q: 1.179158, mean_eps: 0.100000\n",
            "📈 Episodio 722: Recompensa total (clipped): 10.000, Pasos: 459, Mean Reward Calculado: 0.021786 (Recompensa/Pasos)\n",
            "  506832/2000000: episode: 722, duration: 15.987s, episode steps: 459, steps per second:  29, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.012073, mae: 0.954949, mean_q: 1.174219, mean_eps: 0.100000\n",
            "📈 Episodio 723: Recompensa total (clipped): 20.000, Pasos: 864, Mean Reward Calculado: 0.023148 (Recompensa/Pasos)\n",
            "  507696/2000000: episode: 723, duration: 30.340s, episode steps: 864, steps per second:  28, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.012802, mae: 0.964915, mean_q: 1.188654, mean_eps: 0.100000\n",
            "📈 Episodio 724: Recompensa total (clipped): 14.000, Pasos: 485, Mean Reward Calculado: 0.028866 (Recompensa/Pasos)\n",
            "  508181/2000000: episode: 724, duration: 17.180s, episode steps: 485, steps per second:  28, episode reward: 14.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.014383, mae: 0.971532, mean_q: 1.193524, mean_eps: 0.100000\n",
            "📈 Episodio 725: Recompensa total (clipped): 14.000, Pasos: 547, Mean Reward Calculado: 0.025594 (Recompensa/Pasos)\n",
            "  508728/2000000: episode: 725, duration: 19.290s, episode steps: 547, steps per second:  28, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012652, mae: 0.944040, mean_q: 1.158843, mean_eps: 0.100000\n",
            "📈 Episodio 726: Recompensa total (clipped): 16.000, Pasos: 974, Mean Reward Calculado: 0.016427 (Recompensa/Pasos)\n",
            "  509702/2000000: episode: 726, duration: 33.851s, episode steps: 974, steps per second:  29, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012956, mae: 0.965441, mean_q: 1.188167, mean_eps: 0.100000\n",
            "📈 Episodio 727: Recompensa total (clipped): 8.000, Pasos: 481, Mean Reward Calculado: 0.016632 (Recompensa/Pasos)\n",
            "  510183/2000000: episode: 727, duration: 16.507s, episode steps: 481, steps per second:  29, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.011255, mae: 0.962113, mean_q: 1.183668, mean_eps: 0.100000\n",
            "📈 Episodio 728: Recompensa total (clipped): 21.000, Pasos: 1182, Mean Reward Calculado: 0.017766 (Recompensa/Pasos)\n",
            "  511365/2000000: episode: 728, duration: 41.170s, episode steps: 1182, steps per second:  29, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.011785, mae: 0.976487, mean_q: 1.200072, mean_eps: 0.100000\n",
            "📈 Episodio 729: Recompensa total (clipped): 10.000, Pasos: 605, Mean Reward Calculado: 0.016529 (Recompensa/Pasos)\n",
            "  511970/2000000: episode: 729, duration: 20.954s, episode steps: 605, steps per second:  29, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012112, mae: 0.991414, mean_q: 1.217236, mean_eps: 0.100000\n",
            "📈 Episodio 730: Recompensa total (clipped): 10.000, Pasos: 709, Mean Reward Calculado: 0.014104 (Recompensa/Pasos)\n",
            "  512679/2000000: episode: 730, duration: 24.608s, episode steps: 709, steps per second:  29, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.013008, mae: 0.999587, mean_q: 1.227245, mean_eps: 0.100000\n",
            "📈 Episodio 731: Recompensa total (clipped): 10.000, Pasos: 650, Mean Reward Calculado: 0.015385 (Recompensa/Pasos)\n",
            "  513329/2000000: episode: 731, duration: 22.811s, episode steps: 650, steps per second:  28, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.012324, mae: 0.986522, mean_q: 1.210596, mean_eps: 0.100000\n",
            "📈 Episodio 732: Recompensa total (clipped): 8.000, Pasos: 518, Mean Reward Calculado: 0.015444 (Recompensa/Pasos)\n",
            "  513847/2000000: episode: 732, duration: 17.971s, episode steps: 518, steps per second:  29, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012991, mae: 0.991659, mean_q: 1.219378, mean_eps: 0.100000\n",
            "📈 Episodio 733: Recompensa total (clipped): 13.000, Pasos: 535, Mean Reward Calculado: 0.024299 (Recompensa/Pasos)\n",
            "  514382/2000000: episode: 733, duration: 18.778s, episode steps: 535, steps per second:  28, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.013401, mae: 0.991289, mean_q: 1.216998, mean_eps: 0.100000\n",
            "📈 Episodio 734: Recompensa total (clipped): 18.000, Pasos: 911, Mean Reward Calculado: 0.019759 (Recompensa/Pasos)\n",
            "  515293/2000000: episode: 734, duration: 32.009s, episode steps: 911, steps per second:  28, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.013886, mae: 0.990400, mean_q: 1.215967, mean_eps: 0.100000\n",
            "📈 Episodio 735: Recompensa total (clipped): 17.000, Pasos: 788, Mean Reward Calculado: 0.021574 (Recompensa/Pasos)\n",
            "  516081/2000000: episode: 735, duration: 27.551s, episode steps: 788, steps per second:  29, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.011379, mae: 0.990106, mean_q: 1.216900, mean_eps: 0.100000\n",
            "📈 Episodio 736: Recompensa total (clipped): 7.000, Pasos: 531, Mean Reward Calculado: 0.013183 (Recompensa/Pasos)\n",
            "  516612/2000000: episode: 736, duration: 18.509s, episode steps: 531, steps per second:  29, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.492 [0.000, 5.000],  loss: 0.012502, mae: 1.000372, mean_q: 1.229808, mean_eps: 0.100000\n",
            "📈 Episodio 737: Recompensa total (clipped): 15.000, Pasos: 703, Mean Reward Calculado: 0.021337 (Recompensa/Pasos)\n",
            "  517315/2000000: episode: 737, duration: 24.588s, episode steps: 703, steps per second:  29, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.011801, mae: 0.975371, mean_q: 1.198580, mean_eps: 0.100000\n",
            "📈 Episodio 738: Recompensa total (clipped): 20.000, Pasos: 1077, Mean Reward Calculado: 0.018570 (Recompensa/Pasos)\n",
            "  518392/2000000: episode: 738, duration: 38.119s, episode steps: 1077, steps per second:  28, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.013202, mae: 0.995538, mean_q: 1.221290, mean_eps: 0.100000\n",
            "📈 Episodio 739: Recompensa total (clipped): 4.000, Pasos: 399, Mean Reward Calculado: 0.010025 (Recompensa/Pasos)\n",
            "  518791/2000000: episode: 739, duration: 14.032s, episode steps: 399, steps per second:  28, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.012817, mae: 0.987105, mean_q: 1.213034, mean_eps: 0.100000\n",
            "📈 Episodio 740: Recompensa total (clipped): 12.000, Pasos: 687, Mean Reward Calculado: 0.017467 (Recompensa/Pasos)\n",
            "  519478/2000000: episode: 740, duration: 24.099s, episode steps: 687, steps per second:  29, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.012076, mae: 0.981851, mean_q: 1.206708, mean_eps: 0.100000\n",
            "📊 Paso 520,000/2,000,000 (26.0%) - 34.4 pasos/seg - ETA: 11.9h - Memoria: 6018.28 MB\n",
            "📈 Episodio 741: Recompensa total (clipped): 23.000, Pasos: 1127, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            "  520605/2000000: episode: 741, duration: 40.040s, episode steps: 1127, steps per second:  28, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013918, mae: 1.002087, mean_q: 1.231451, mean_eps: 0.100000\n",
            "📈 Episodio 742: Recompensa total (clipped): 9.000, Pasos: 486, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
            "  521091/2000000: episode: 742, duration: 17.000s, episode steps: 486, steps per second:  29, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.012869, mae: 1.001613, mean_q: 1.227344, mean_eps: 0.100000\n",
            "📈 Episodio 743: Recompensa total (clipped): 15.000, Pasos: 687, Mean Reward Calculado: 0.021834 (Recompensa/Pasos)\n",
            "  521778/2000000: episode: 743, duration: 24.160s, episode steps: 687, steps per second:  28, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013575, mae: 1.001860, mean_q: 1.230237, mean_eps: 0.100000\n",
            "📈 Episodio 744: Recompensa total (clipped): 27.000, Pasos: 993, Mean Reward Calculado: 0.027190 (Recompensa/Pasos)\n",
            "  522771/2000000: episode: 744, duration: 35.000s, episode steps: 993, steps per second:  28, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.012564, mae: 0.998581, mean_q: 1.225204, mean_eps: 0.100000\n",
            "📈 Episodio 745: Recompensa total (clipped): 16.000, Pasos: 903, Mean Reward Calculado: 0.017719 (Recompensa/Pasos)\n",
            "  523674/2000000: episode: 745, duration: 31.597s, episode steps: 903, steps per second:  29, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.013492, mae: 1.007400, mean_q: 1.235136, mean_eps: 0.100000\n",
            "📈 Episodio 746: Recompensa total (clipped): 18.000, Pasos: 743, Mean Reward Calculado: 0.024226 (Recompensa/Pasos)\n",
            "  524417/2000000: episode: 746, duration: 26.315s, episode steps: 743, steps per second:  28, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.013414, mae: 1.002388, mean_q: 1.232021, mean_eps: 0.100000\n",
            "📈 Episodio 747: Recompensa total (clipped): 15.000, Pasos: 656, Mean Reward Calculado: 0.022866 (Recompensa/Pasos)\n",
            "  525073/2000000: episode: 747, duration: 23.102s, episode steps: 656, steps per second:  28, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.012300, mae: 0.993084, mean_q: 1.221083, mean_eps: 0.100000\n",
            "📈 Episodio 748: Recompensa total (clipped): 15.000, Pasos: 661, Mean Reward Calculado: 0.022693 (Recompensa/Pasos)\n",
            "  525734/2000000: episode: 748, duration: 23.209s, episode steps: 661, steps per second:  28, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.013044, mae: 1.003002, mean_q: 1.232506, mean_eps: 0.100000\n",
            "📈 Episodio 749: Recompensa total (clipped): 21.000, Pasos: 900, Mean Reward Calculado: 0.023333 (Recompensa/Pasos)\n",
            "  526634/2000000: episode: 749, duration: 31.527s, episode steps: 900, steps per second:  29, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.011761, mae: 1.000481, mean_q: 1.229319, mean_eps: 0.100000\n",
            "📈 Episodio 750: Recompensa total (clipped): 8.000, Pasos: 406, Mean Reward Calculado: 0.019704 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 750, pasos: 527040)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.20 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 750 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 8.00\n",
            "   Media últimos 100: 14.20 / 20.0\n",
            "   Mejor promedio histórico: 14.20\n",
            "   Estado: 📈 71.0% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  527040/2000000: episode: 750, duration: 60.868s, episode steps: 406, steps per second:   7, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.012731, mae: 0.996057, mean_q: 1.223532, mean_eps: 0.100000\n",
            "📈 Episodio 751: Recompensa total (clipped): 12.000, Pasos: 488, Mean Reward Calculado: 0.024590 (Recompensa/Pasos)\n",
            "  527528/2000000: episode: 751, duration: 17.402s, episode steps: 488, steps per second:  28, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013580, mae: 1.002234, mean_q: 1.231511, mean_eps: 0.100000\n",
            "📈 Episodio 752: Recompensa total (clipped): 10.000, Pasos: 535, Mean Reward Calculado: 0.018692 (Recompensa/Pasos)\n",
            "  528063/2000000: episode: 752, duration: 18.737s, episode steps: 535, steps per second:  29, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.012721, mae: 0.995028, mean_q: 1.220876, mean_eps: 0.100000\n",
            "📈 Episodio 753: Recompensa total (clipped): 7.000, Pasos: 373, Mean Reward Calculado: 0.018767 (Recompensa/Pasos)\n",
            "  528436/2000000: episode: 753, duration: 13.211s, episode steps: 373, steps per second:  28, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.011896, mae: 0.995442, mean_q: 1.221704, mean_eps: 0.100000\n",
            "📈 Episodio 754: Recompensa total (clipped): 7.000, Pasos: 459, Mean Reward Calculado: 0.015251 (Recompensa/Pasos)\n",
            "  528895/2000000: episode: 754, duration: 16.157s, episode steps: 459, steps per second:  28, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.010969, mae: 1.005017, mean_q: 1.233070, mean_eps: 0.100000\n",
            "📈 Episodio 755: Recompensa total (clipped): 9.000, Pasos: 368, Mean Reward Calculado: 0.024457 (Recompensa/Pasos)\n",
            "  529263/2000000: episode: 755, duration: 13.039s, episode steps: 368, steps per second:  28, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.012393, mae: 0.999375, mean_q: 1.226415, mean_eps: 0.100000\n",
            "📈 Episodio 756: Recompensa total (clipped): 9.000, Pasos: 506, Mean Reward Calculado: 0.017787 (Recompensa/Pasos)\n",
            "  529769/2000000: episode: 756, duration: 17.829s, episode steps: 506, steps per second:  28, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012958, mae: 1.003234, mean_q: 1.231256, mean_eps: 0.100000\n",
            "📈 Episodio 757: Recompensa total (clipped): 31.000, Pasos: 1014, Mean Reward Calculado: 0.030572 (Recompensa/Pasos)\n",
            "  530783/2000000: episode: 757, duration: 35.512s, episode steps: 1014, steps per second:  29, episode reward: 31.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.011667, mae: 1.034869, mean_q: 1.272570, mean_eps: 0.100000\n",
            "📈 Episodio 758: Recompensa total (clipped): 14.000, Pasos: 655, Mean Reward Calculado: 0.021374 (Recompensa/Pasos)\n",
            "  531438/2000000: episode: 758, duration: 23.020s, episode steps: 655, steps per second:  28, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012754, mae: 1.047861, mean_q: 1.289508, mean_eps: 0.100000\n",
            "📈 Episodio 759: Recompensa total (clipped): 16.000, Pasos: 628, Mean Reward Calculado: 0.025478 (Recompensa/Pasos)\n",
            "  532066/2000000: episode: 759, duration: 22.097s, episode steps: 628, steps per second:  28, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.013833, mae: 1.059981, mean_q: 1.303541, mean_eps: 0.100000\n",
            "📈 Episodio 760: Recompensa total (clipped): 10.000, Pasos: 620, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            "  532686/2000000: episode: 760, duration: 21.973s, episode steps: 620, steps per second:  28, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.014085, mae: 1.041452, mean_q: 1.277704, mean_eps: 0.100000\n",
            "📈 Episodio 761: Recompensa total (clipped): 16.000, Pasos: 685, Mean Reward Calculado: 0.023358 (Recompensa/Pasos)\n",
            "  533371/2000000: episode: 761, duration: 24.136s, episode steps: 685, steps per second:  28, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.012735, mae: 1.044829, mean_q: 1.283213, mean_eps: 0.100000\n",
            "📈 Episodio 762: Recompensa total (clipped): 25.000, Pasos: 1002, Mean Reward Calculado: 0.024950 (Recompensa/Pasos)\n",
            "  534373/2000000: episode: 762, duration: 35.590s, episode steps: 1002, steps per second:  28, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.012551, mae: 1.047106, mean_q: 1.285757, mean_eps: 0.100000\n",
            "📈 Episodio 763: Recompensa total (clipped): 18.000, Pasos: 932, Mean Reward Calculado: 0.019313 (Recompensa/Pasos)\n",
            "  535305/2000000: episode: 763, duration: 32.739s, episode steps: 932, steps per second:  28, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.013066, mae: 1.041523, mean_q: 1.276352, mean_eps: 0.100000\n",
            "📈 Episodio 764: Recompensa total (clipped): 13.000, Pasos: 545, Mean Reward Calculado: 0.023853 (Recompensa/Pasos)\n",
            "  535850/2000000: episode: 764, duration: 19.059s, episode steps: 545, steps per second:  29, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.013545, mae: 1.050018, mean_q: 1.288908, mean_eps: 0.100000\n",
            "📈 Episodio 765: Recompensa total (clipped): 3.000, Pasos: 335, Mean Reward Calculado: 0.008955 (Recompensa/Pasos)\n",
            "  536185/2000000: episode: 765, duration: 11.806s, episode steps: 335, steps per second:  28, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.012945, mae: 1.056489, mean_q: 1.296773, mean_eps: 0.100000\n",
            "📈 Episodio 766: Recompensa total (clipped): 17.000, Pasos: 915, Mean Reward Calculado: 0.018579 (Recompensa/Pasos)\n",
            "  537100/2000000: episode: 766, duration: 32.557s, episode steps: 915, steps per second:  28, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.013616, mae: 1.043068, mean_q: 1.279584, mean_eps: 0.100000\n",
            "📈 Episodio 767: Recompensa total (clipped): 30.000, Pasos: 1135, Mean Reward Calculado: 0.026432 (Recompensa/Pasos)\n",
            "  538235/2000000: episode: 767, duration: 39.949s, episode steps: 1135, steps per second:  28, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.012496, mae: 1.046430, mean_q: 1.284160, mean_eps: 0.100000\n",
            "📈 Episodio 768: Recompensa total (clipped): 13.000, Pasos: 676, Mean Reward Calculado: 0.019231 (Recompensa/Pasos)\n",
            "  538911/2000000: episode: 768, duration: 23.904s, episode steps: 676, steps per second:  28, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.012080, mae: 1.047651, mean_q: 1.286779, mean_eps: 0.100000\n",
            "📈 Episodio 769: Recompensa total (clipped): 9.000, Pasos: 489, Mean Reward Calculado: 0.018405 (Recompensa/Pasos)\n",
            "  539400/2000000: episode: 769, duration: 17.420s, episode steps: 489, steps per second:  28, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.013809, mae: 1.060381, mean_q: 1.300864, mean_eps: 0.100000\n",
            "📊 Paso 540,000/2,000,000 (27.0%) - 34.1 pasos/seg - ETA: 11.9h - Memoria: 6165.13 MB\n",
            "📈 Episodio 770: Recompensa total (clipped): 14.000, Pasos: 624, Mean Reward Calculado: 0.022436 (Recompensa/Pasos)\n",
            "  540024/2000000: episode: 770, duration: 22.389s, episode steps: 624, steps per second:  28, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.012566, mae: 1.043128, mean_q: 1.281368, mean_eps: 0.100000\n",
            "📈 Episodio 771: Recompensa total (clipped): 34.000, Pasos: 1295, Mean Reward Calculado: 0.026255 (Recompensa/Pasos)\n",
            "  541319/2000000: episode: 771, duration: 45.774s, episode steps: 1295, steps per second:  28, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011879, mae: 1.045990, mean_q: 1.283730, mean_eps: 0.100000\n",
            "📈 Episodio 772: Recompensa total (clipped): 10.000, Pasos: 540, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
            "  541859/2000000: episode: 772, duration: 19.215s, episode steps: 540, steps per second:  28, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.012372, mae: 1.058667, mean_q: 1.298014, mean_eps: 0.100000\n",
            "📈 Episodio 773: Recompensa total (clipped): 12.000, Pasos: 509, Mean Reward Calculado: 0.023576 (Recompensa/Pasos)\n",
            "  542368/2000000: episode: 773, duration: 18.276s, episode steps: 509, steps per second:  28, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.012965, mae: 1.061585, mean_q: 1.301113, mean_eps: 0.100000\n",
            "📈 Episodio 774: Recompensa total (clipped): 15.000, Pasos: 639, Mean Reward Calculado: 0.023474 (Recompensa/Pasos)\n",
            "  543007/2000000: episode: 774, duration: 22.959s, episode steps: 639, steps per second:  28, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.842 [0.000, 5.000],  loss: 0.010929, mae: 1.035476, mean_q: 1.270391, mean_eps: 0.100000\n",
            "📈 Episodio 775: Recompensa total (clipped): 11.000, Pasos: 497, Mean Reward Calculado: 0.022133 (Recompensa/Pasos)\n",
            "  543504/2000000: episode: 775, duration: 17.886s, episode steps: 497, steps per second:  28, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.013122, mae: 1.039128, mean_q: 1.273268, mean_eps: 0.100000\n",
            "📈 Episodio 776: Recompensa total (clipped): 14.000, Pasos: 521, Mean Reward Calculado: 0.026871 (Recompensa/Pasos)\n",
            "  544025/2000000: episode: 776, duration: 18.684s, episode steps: 521, steps per second:  28, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.011850, mae: 1.043527, mean_q: 1.278978, mean_eps: 0.100000\n",
            "📈 Episodio 777: Recompensa total (clipped): 10.000, Pasos: 500, Mean Reward Calculado: 0.020000 (Recompensa/Pasos)\n",
            "  544525/2000000: episode: 777, duration: 17.708s, episode steps: 500, steps per second:  28, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.011504, mae: 1.051920, mean_q: 1.289482, mean_eps: 0.100000\n",
            "📈 Episodio 778: Recompensa total (clipped): 9.000, Pasos: 458, Mean Reward Calculado: 0.019651 (Recompensa/Pasos)\n",
            "  544983/2000000: episode: 778, duration: 16.433s, episode steps: 458, steps per second:  28, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.013668, mae: 1.048917, mean_q: 1.288880, mean_eps: 0.100000\n",
            "📈 Episodio 779: Recompensa total (clipped): 7.000, Pasos: 504, Mean Reward Calculado: 0.013889 (Recompensa/Pasos)\n",
            "  545487/2000000: episode: 779, duration: 18.380s, episode steps: 504, steps per second:  27, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012422, mae: 1.061104, mean_q: 1.302680, mean_eps: 0.100000\n",
            "📈 Episodio 780: Recompensa total (clipped): 14.000, Pasos: 622, Mean Reward Calculado: 0.022508 (Recompensa/Pasos)\n",
            "  546109/2000000: episode: 780, duration: 21.996s, episode steps: 622, steps per second:  28, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.013064, mae: 1.042556, mean_q: 1.278027, mean_eps: 0.100000\n",
            "📈 Episodio 781: Recompensa total (clipped): 14.000, Pasos: 492, Mean Reward Calculado: 0.028455 (Recompensa/Pasos)\n",
            "  546601/2000000: episode: 781, duration: 17.601s, episode steps: 492, steps per second:  28, episode reward: 14.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.010715, mae: 1.050047, mean_q: 1.287882, mean_eps: 0.100000\n",
            "📈 Episodio 782: Recompensa total (clipped): 9.000, Pasos: 350, Mean Reward Calculado: 0.025714 (Recompensa/Pasos)\n",
            "  546951/2000000: episode: 782, duration: 12.402s, episode steps: 350, steps per second:  28, episode reward:  9.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.012219, mae: 1.046415, mean_q: 1.283082, mean_eps: 0.100000\n",
            "📈 Episodio 783: Recompensa total (clipped): 27.000, Pasos: 943, Mean Reward Calculado: 0.028632 (Recompensa/Pasos)\n",
            "  547894/2000000: episode: 783, duration: 33.436s, episode steps: 943, steps per second:  28, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.012493, mae: 1.045840, mean_q: 1.281297, mean_eps: 0.100000\n",
            "📈 Episodio 784: Recompensa total (clipped): 13.000, Pasos: 570, Mean Reward Calculado: 0.022807 (Recompensa/Pasos)\n",
            "  548464/2000000: episode: 784, duration: 20.414s, episode steps: 570, steps per second:  28, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.014108, mae: 1.045319, mean_q: 1.277217, mean_eps: 0.100000\n",
            "📈 Episodio 785: Recompensa total (clipped): 32.000, Pasos: 1266, Mean Reward Calculado: 0.025276 (Recompensa/Pasos)\n",
            "  549730/2000000: episode: 785, duration: 44.893s, episode steps: 1266, steps per second:  28, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012552, mae: 1.047985, mean_q: 1.283759, mean_eps: 0.100000\n",
            "📈 Episodio 786: Recompensa total (clipped): 19.000, Pasos: 830, Mean Reward Calculado: 0.022892 (Recompensa/Pasos)\n",
            "  550560/2000000: episode: 786, duration: 29.653s, episode steps: 830, steps per second:  28, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.012839, mae: 1.080552, mean_q: 1.324393, mean_eps: 0.100000\n",
            "📈 Episodio 787: Recompensa total (clipped): 17.000, Pasos: 693, Mean Reward Calculado: 0.024531 (Recompensa/Pasos)\n",
            "  551253/2000000: episode: 787, duration: 24.760s, episode steps: 693, steps per second:  28, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013002, mae: 1.098238, mean_q: 1.345769, mean_eps: 0.100000\n",
            "📈 Episodio 788: Recompensa total (clipped): 4.000, Pasos: 374, Mean Reward Calculado: 0.010695 (Recompensa/Pasos)\n",
            "  551627/2000000: episode: 788, duration: 13.199s, episode steps: 374, steps per second:  28, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.011227, mae: 1.102619, mean_q: 1.353417, mean_eps: 0.100000\n",
            "📈 Episodio 789: Recompensa total (clipped): 20.000, Pasos: 927, Mean Reward Calculado: 0.021575 (Recompensa/Pasos)\n",
            "  552554/2000000: episode: 789, duration: 33.053s, episode steps: 927, steps per second:  28, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012555, mae: 1.099972, mean_q: 1.347928, mean_eps: 0.100000\n",
            "📈 Episodio 790: Recompensa total (clipped): 17.000, Pasos: 802, Mean Reward Calculado: 0.021197 (Recompensa/Pasos)\n",
            "  553356/2000000: episode: 790, duration: 28.780s, episode steps: 802, steps per second:  28, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.012013, mae: 1.107019, mean_q: 1.358366, mean_eps: 0.100000\n",
            "📈 Episodio 791: Recompensa total (clipped): 11.000, Pasos: 501, Mean Reward Calculado: 0.021956 (Recompensa/Pasos)\n",
            "  553857/2000000: episode: 791, duration: 17.978s, episode steps: 501, steps per second:  28, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.012392, mae: 1.082228, mean_q: 1.325925, mean_eps: 0.100000\n",
            "📈 Episodio 792: Recompensa total (clipped): 23.000, Pasos: 876, Mean Reward Calculado: 0.026256 (Recompensa/Pasos)\n",
            "  554733/2000000: episode: 792, duration: 31.096s, episode steps: 876, steps per second:  28, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.012008, mae: 1.089372, mean_q: 1.334795, mean_eps: 0.100000\n",
            "📈 Episodio 793: Recompensa total (clipped): 7.000, Pasos: 379, Mean Reward Calculado: 0.018470 (Recompensa/Pasos)\n",
            "  555112/2000000: episode: 793, duration: 13.670s, episode steps: 379, steps per second:  28, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.013134, mae: 1.107859, mean_q: 1.357487, mean_eps: 0.100000\n",
            "📈 Episodio 794: Recompensa total (clipped): 20.000, Pasos: 1090, Mean Reward Calculado: 0.018349 (Recompensa/Pasos)\n",
            "  556202/2000000: episode: 794, duration: 39.253s, episode steps: 1090, steps per second:  28, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.831 [0.000, 5.000],  loss: 0.013402, mae: 1.087608, mean_q: 1.330969, mean_eps: 0.100000\n",
            "📈 Episodio 795: Recompensa total (clipped): 9.000, Pasos: 474, Mean Reward Calculado: 0.018987 (Recompensa/Pasos)\n",
            "  556676/2000000: episode: 795, duration: 17.071s, episode steps: 474, steps per second:  28, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012314, mae: 1.100274, mean_q: 1.346000, mean_eps: 0.100000\n",
            "📈 Episodio 796: Recompensa total (clipped): 19.000, Pasos: 794, Mean Reward Calculado: 0.023929 (Recompensa/Pasos)\n",
            "  557470/2000000: episode: 796, duration: 28.601s, episode steps: 794, steps per second:  28, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.013395, mae: 1.101543, mean_q: 1.349490, mean_eps: 0.100000\n",
            "📈 Episodio 797: Recompensa total (clipped): 18.000, Pasos: 883, Mean Reward Calculado: 0.020385 (Recompensa/Pasos)\n",
            "  558353/2000000: episode: 797, duration: 31.592s, episode steps: 883, steps per second:  28, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.012401, mae: 1.099188, mean_q: 1.345019, mean_eps: 0.100000\n",
            "📈 Episodio 798: Recompensa total (clipped): 5.000, Pasos: 350, Mean Reward Calculado: 0.014286 (Recompensa/Pasos)\n",
            "  558703/2000000: episode: 798, duration: 12.516s, episode steps: 350, steps per second:  28, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.012406, mae: 1.089032, mean_q: 1.331869, mean_eps: 0.100000\n",
            "📈 Episodio 799: Recompensa total (clipped): 20.000, Pasos: 804, Mean Reward Calculado: 0.024876 (Recompensa/Pasos)\n",
            "  559507/2000000: episode: 799, duration: 28.844s, episode steps: 804, steps per second:  28, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.012810, mae: 1.086511, mean_q: 1.329893, mean_eps: 0.100000\n",
            "📊 Paso 560,000/2,000,000 (28.0%) - 33.8 pasos/seg - ETA: 11.8h - Memoria: 6282.30 MB\n",
            "📈 Episodio 800: Recompensa total (clipped): 23.000, Pasos: 900, Mean Reward Calculado: 0.025556 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 800, pasos: 560407)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.52 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 800 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 23.00\n",
            "   Media últimos 100: 14.52 / 20.0\n",
            "   Mejor promedio histórico: 14.52\n",
            "   Estado: 📈 72.6% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  560407/2000000: episode: 800, duration: 71.848s, episode steps: 900, steps per second:  13, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.012089, mae: 1.104247, mean_q: 1.352333, mean_eps: 0.100000\n",
            "📈 Episodio 801: Recompensa total (clipped): 28.000, Pasos: 999, Mean Reward Calculado: 0.028028 (Recompensa/Pasos)\n",
            "  561406/2000000: episode: 801, duration: 36.289s, episode steps: 999, steps per second:  28, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.011184, mae: 1.116097, mean_q: 1.367164, mean_eps: 0.100000\n",
            "📈 Episodio 802: Recompensa total (clipped): 14.000, Pasos: 535, Mean Reward Calculado: 0.026168 (Recompensa/Pasos)\n",
            "  561941/2000000: episode: 802, duration: 19.330s, episode steps: 535, steps per second:  28, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013741, mae: 1.121939, mean_q: 1.371859, mean_eps: 0.100000\n",
            "📈 Episodio 803: Recompensa total (clipped): 9.000, Pasos: 380, Mean Reward Calculado: 0.023684 (Recompensa/Pasos)\n",
            "  562321/2000000: episode: 803, duration: 13.746s, episode steps: 380, steps per second:  28, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.011439, mae: 1.109492, mean_q: 1.355801, mean_eps: 0.100000\n",
            "📈 Episodio 804: Recompensa total (clipped): 15.000, Pasos: 670, Mean Reward Calculado: 0.022388 (Recompensa/Pasos)\n",
            "  562991/2000000: episode: 804, duration: 24.091s, episode steps: 670, steps per second:  28, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013941, mae: 1.121699, mean_q: 1.370511, mean_eps: 0.100000\n",
            "📈 Episodio 805: Recompensa total (clipped): 18.000, Pasos: 705, Mean Reward Calculado: 0.025532 (Recompensa/Pasos)\n",
            "  563696/2000000: episode: 805, duration: 25.672s, episode steps: 705, steps per second:  27, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.013286, mae: 1.109749, mean_q: 1.354521, mean_eps: 0.100000\n",
            "📈 Episodio 806: Recompensa total (clipped): 1.000, Pasos: 397, Mean Reward Calculado: 0.002519 (Recompensa/Pasos)\n",
            "  564093/2000000: episode: 806, duration: 14.818s, episode steps: 397, steps per second:  27, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.012983, mae: 1.119086, mean_q: 1.365828, mean_eps: 0.100000\n",
            "📈 Episodio 807: Recompensa total (clipped): 13.000, Pasos: 632, Mean Reward Calculado: 0.020570 (Recompensa/Pasos)\n",
            "  564725/2000000: episode: 807, duration: 23.149s, episode steps: 632, steps per second:  27, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.012408, mae: 1.126387, mean_q: 1.376135, mean_eps: 0.100000\n",
            "📈 Episodio 808: Recompensa total (clipped): 8.000, Pasos: 529, Mean Reward Calculado: 0.015123 (Recompensa/Pasos)\n",
            "  565254/2000000: episode: 808, duration: 19.156s, episode steps: 529, steps per second:  28, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.014251, mae: 1.125835, mean_q: 1.375868, mean_eps: 0.100000\n",
            "📈 Episodio 809: Recompensa total (clipped): 3.000, Pasos: 328, Mean Reward Calculado: 0.009146 (Recompensa/Pasos)\n",
            "  565582/2000000: episode: 809, duration: 11.933s, episode steps: 328, steps per second:  27, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013028, mae: 1.120868, mean_q: 1.369692, mean_eps: 0.100000\n",
            "📈 Episodio 810: Recompensa total (clipped): 18.000, Pasos: 1013, Mean Reward Calculado: 0.017769 (Recompensa/Pasos)\n",
            "  566595/2000000: episode: 810, duration: 36.910s, episode steps: 1013, steps per second:  27, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.012738, mae: 1.112402, mean_q: 1.359455, mean_eps: 0.100000\n",
            "📈 Episodio 811: Recompensa total (clipped): 21.000, Pasos: 780, Mean Reward Calculado: 0.026923 (Recompensa/Pasos)\n",
            "  567375/2000000: episode: 811, duration: 28.238s, episode steps: 780, steps per second:  28, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013116, mae: 1.110433, mean_q: 1.356932, mean_eps: 0.100000\n",
            "📈 Episodio 812: Recompensa total (clipped): 18.000, Pasos: 567, Mean Reward Calculado: 0.031746 (Recompensa/Pasos)\n",
            "  567942/2000000: episode: 812, duration: 20.429s, episode steps: 567, steps per second:  28, episode reward: 18.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.013290, mae: 1.098955, mean_q: 1.343459, mean_eps: 0.100000\n",
            "📈 Episodio 813: Recompensa total (clipped): 19.000, Pasos: 807, Mean Reward Calculado: 0.023544 (Recompensa/Pasos)\n",
            "  568749/2000000: episode: 813, duration: 28.976s, episode steps: 807, steps per second:  28, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011377, mae: 1.110112, mean_q: 1.355803, mean_eps: 0.100000\n",
            "📈 Episodio 814: Recompensa total (clipped): 23.000, Pasos: 839, Mean Reward Calculado: 0.027414 (Recompensa/Pasos)\n",
            "  569588/2000000: episode: 814, duration: 30.201s, episode steps: 839, steps per second:  28, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.012413, mae: 1.106273, mean_q: 1.352181, mean_eps: 0.100000\n",
            "📈 Episodio 815: Recompensa total (clipped): 24.000, Pasos: 866, Mean Reward Calculado: 0.027714 (Recompensa/Pasos)\n",
            "  570454/2000000: episode: 815, duration: 31.390s, episode steps: 866, steps per second:  28, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.012081, mae: 1.130525, mean_q: 1.384641, mean_eps: 0.100000\n",
            "📈 Episodio 816: Recompensa total (clipped): 5.000, Pasos: 383, Mean Reward Calculado: 0.013055 (Recompensa/Pasos)\n",
            "  570837/2000000: episode: 816, duration: 13.844s, episode steps: 383, steps per second:  28, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.011552, mae: 1.131393, mean_q: 1.385562, mean_eps: 0.100000\n",
            "📈 Episodio 817: Recompensa total (clipped): 10.000, Pasos: 369, Mean Reward Calculado: 0.027100 (Recompensa/Pasos)\n",
            "  571206/2000000: episode: 817, duration: 13.380s, episode steps: 369, steps per second:  28, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.010596, mae: 1.111508, mean_q: 1.360002, mean_eps: 0.100000\n",
            "📈 Episodio 818: Recompensa total (clipped): 14.000, Pasos: 582, Mean Reward Calculado: 0.024055 (Recompensa/Pasos)\n",
            "  571788/2000000: episode: 818, duration: 20.943s, episode steps: 582, steps per second:  28, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.012244, mae: 1.141656, mean_q: 1.399856, mean_eps: 0.100000\n",
            "📈 Episodio 819: Recompensa total (clipped): 7.000, Pasos: 488, Mean Reward Calculado: 0.014344 (Recompensa/Pasos)\n",
            "  572276/2000000: episode: 819, duration: 17.859s, episode steps: 488, steps per second:  27, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011507, mae: 1.128897, mean_q: 1.380913, mean_eps: 0.100000\n",
            "📈 Episodio 820: Recompensa total (clipped): 16.000, Pasos: 744, Mean Reward Calculado: 0.021505 (Recompensa/Pasos)\n",
            "  573020/2000000: episode: 820, duration: 27.046s, episode steps: 744, steps per second:  28, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.011670, mae: 1.130333, mean_q: 1.383736, mean_eps: 0.100000\n",
            "📈 Episodio 821: Recompensa total (clipped): 12.000, Pasos: 551, Mean Reward Calculado: 0.021779 (Recompensa/Pasos)\n",
            "  573571/2000000: episode: 821, duration: 20.037s, episode steps: 551, steps per second:  27, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.013025, mae: 1.138779, mean_q: 1.392609, mean_eps: 0.100000\n",
            "📈 Episodio 822: Recompensa total (clipped): 13.000, Pasos: 607, Mean Reward Calculado: 0.021417 (Recompensa/Pasos)\n",
            "  574178/2000000: episode: 822, duration: 22.065s, episode steps: 607, steps per second:  28, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014559, mae: 1.147025, mean_q: 1.401584, mean_eps: 0.100000\n",
            "📈 Episodio 823: Recompensa total (clipped): 10.000, Pasos: 528, Mean Reward Calculado: 0.018939 (Recompensa/Pasos)\n",
            "  574706/2000000: episode: 823, duration: 19.136s, episode steps: 528, steps per second:  28, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.012868, mae: 1.137290, mean_q: 1.389354, mean_eps: 0.100000\n",
            "📈 Episodio 824: Recompensa total (clipped): 15.000, Pasos: 650, Mean Reward Calculado: 0.023077 (Recompensa/Pasos)\n",
            "  575356/2000000: episode: 824, duration: 23.514s, episode steps: 650, steps per second:  28, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.012436, mae: 1.119589, mean_q: 1.366436, mean_eps: 0.100000\n",
            "📈 Episodio 825: Recompensa total (clipped): 18.000, Pasos: 1188, Mean Reward Calculado: 0.015152 (Recompensa/Pasos)\n",
            "  576544/2000000: episode: 825, duration: 43.047s, episode steps: 1188, steps per second:  28, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.013632, mae: 1.130251, mean_q: 1.381052, mean_eps: 0.100000\n",
            "📈 Episodio 826: Recompensa total (clipped): 9.000, Pasos: 555, Mean Reward Calculado: 0.016216 (Recompensa/Pasos)\n",
            "  577099/2000000: episode: 826, duration: 20.026s, episode steps: 555, steps per second:  28, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.012809, mae: 1.135927, mean_q: 1.387709, mean_eps: 0.100000\n",
            "📈 Episodio 827: Recompensa total (clipped): 21.000, Pasos: 1012, Mean Reward Calculado: 0.020751 (Recompensa/Pasos)\n",
            "  578111/2000000: episode: 827, duration: 36.760s, episode steps: 1012, steps per second:  28, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.012990, mae: 1.127294, mean_q: 1.377284, mean_eps: 0.100000\n",
            "📈 Episodio 828: Recompensa total (clipped): 12.000, Pasos: 782, Mean Reward Calculado: 0.015345 (Recompensa/Pasos)\n",
            "  578893/2000000: episode: 828, duration: 28.457s, episode steps: 782, steps per second:  27, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 0.013882, mae: 1.123083, mean_q: 1.371862, mean_eps: 0.100000\n",
            "📈 Episodio 829: Recompensa total (clipped): 4.000, Pasos: 519, Mean Reward Calculado: 0.007707 (Recompensa/Pasos)\n",
            "  579412/2000000: episode: 829, duration: 18.904s, episode steps: 519, steps per second:  27, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013296, mae: 1.133275, mean_q: 1.384826, mean_eps: 0.100000\n",
            "📊 Paso 580,000/2,000,000 (29.0%) - 33.5 pasos/seg - ETA: 11.8h - Memoria: 6488.69 MB\n",
            "📈 Episodio 830: Recompensa total (clipped): 13.000, Pasos: 643, Mean Reward Calculado: 0.020218 (Recompensa/Pasos)\n",
            "  580055/2000000: episode: 830, duration: 23.340s, episode steps: 643, steps per second:  28, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.014066, mae: 1.146383, mean_q: 1.400843, mean_eps: 0.100000\n",
            "📈 Episodio 831: Recompensa total (clipped): 20.000, Pasos: 1176, Mean Reward Calculado: 0.017007 (Recompensa/Pasos)\n",
            "  581231/2000000: episode: 831, duration: 42.856s, episode steps: 1176, steps per second:  27, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012211, mae: 1.194090, mean_q: 1.460464, mean_eps: 0.100000\n",
            "📈 Episodio 832: Recompensa total (clipped): 3.000, Pasos: 361, Mean Reward Calculado: 0.008310 (Recompensa/Pasos)\n",
            "  581592/2000000: episode: 832, duration: 13.192s, episode steps: 361, steps per second:  27, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.025 [0.000, 5.000],  loss: 0.014343, mae: 1.184861, mean_q: 1.448751, mean_eps: 0.100000\n",
            "📈 Episodio 833: Recompensa total (clipped): 22.000, Pasos: 949, Mean Reward Calculado: 0.023182 (Recompensa/Pasos)\n",
            "  582541/2000000: episode: 833, duration: 34.404s, episode steps: 949, steps per second:  28, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.013845, mae: 1.192249, mean_q: 1.457786, mean_eps: 0.100000\n",
            "📈 Episodio 834: Recompensa total (clipped): 6.000, Pasos: 374, Mean Reward Calculado: 0.016043 (Recompensa/Pasos)\n",
            "  582915/2000000: episode: 834, duration: 13.384s, episode steps: 374, steps per second:  28, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.014515, mae: 1.166656, mean_q: 1.424603, mean_eps: 0.100000\n",
            "📈 Episodio 835: Recompensa total (clipped): 19.000, Pasos: 833, Mean Reward Calculado: 0.022809 (Recompensa/Pasos)\n",
            "  583748/2000000: episode: 835, duration: 30.473s, episode steps: 833, steps per second:  27, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013360, mae: 1.201798, mean_q: 1.468992, mean_eps: 0.100000\n",
            "📈 Episodio 836: Recompensa total (clipped): 6.000, Pasos: 555, Mean Reward Calculado: 0.010811 (Recompensa/Pasos)\n",
            "  584303/2000000: episode: 836, duration: 20.315s, episode steps: 555, steps per second:  27, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.013067, mae: 1.171972, mean_q: 1.433698, mean_eps: 0.100000\n",
            "📈 Episodio 837: Recompensa total (clipped): 15.000, Pasos: 910, Mean Reward Calculado: 0.016484 (Recompensa/Pasos)\n",
            "  585213/2000000: episode: 837, duration: 33.536s, episode steps: 910, steps per second:  27, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.014253, mae: 1.183632, mean_q: 1.443617, mean_eps: 0.100000\n",
            "📈 Episodio 838: Recompensa total (clipped): 13.000, Pasos: 489, Mean Reward Calculado: 0.026585 (Recompensa/Pasos)\n",
            "  585702/2000000: episode: 838, duration: 17.932s, episode steps: 489, steps per second:  27, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.013197, mae: 1.171266, mean_q: 1.427783, mean_eps: 0.100000\n",
            "📈 Episodio 839: Recompensa total (clipped): 23.000, Pasos: 983, Mean Reward Calculado: 0.023398 (Recompensa/Pasos)\n",
            "  586685/2000000: episode: 839, duration: 36.070s, episode steps: 983, steps per second:  27, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013754, mae: 1.198819, mean_q: 1.464711, mean_eps: 0.100000\n",
            "📈 Episodio 840: Recompensa total (clipped): 4.000, Pasos: 386, Mean Reward Calculado: 0.010363 (Recompensa/Pasos)\n",
            "  587071/2000000: episode: 840, duration: 14.200s, episode steps: 386, steps per second:  27, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.011826, mae: 1.170342, mean_q: 1.432813, mean_eps: 0.100000\n",
            "📈 Episodio 841: Recompensa total (clipped): 16.000, Pasos: 654, Mean Reward Calculado: 0.024465 (Recompensa/Pasos)\n",
            "  587725/2000000: episode: 841, duration: 24.022s, episode steps: 654, steps per second:  27, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.013556, mae: 1.188315, mean_q: 1.452785, mean_eps: 0.100000\n",
            "📈 Episodio 842: Recompensa total (clipped): 12.000, Pasos: 499, Mean Reward Calculado: 0.024048 (Recompensa/Pasos)\n",
            "  588224/2000000: episode: 842, duration: 18.192s, episode steps: 499, steps per second:  27, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.012885, mae: 1.172483, mean_q: 1.436607, mean_eps: 0.100000\n",
            "📈 Episodio 843: Recompensa total (clipped): 7.000, Pasos: 496, Mean Reward Calculado: 0.014113 (Recompensa/Pasos)\n",
            "  588720/2000000: episode: 843, duration: 18.307s, episode steps: 496, steps per second:  27, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.013823, mae: 1.201608, mean_q: 1.467760, mean_eps: 0.100000\n",
            "📈 Episodio 844: Recompensa total (clipped): 9.000, Pasos: 556, Mean Reward Calculado: 0.016187 (Recompensa/Pasos)\n",
            "  589276/2000000: episode: 844, duration: 20.414s, episode steps: 556, steps per second:  27, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013538, mae: 1.205833, mean_q: 1.473160, mean_eps: 0.100000\n",
            "📈 Episodio 845: Recompensa total (clipped): 14.000, Pasos: 661, Mean Reward Calculado: 0.021180 (Recompensa/Pasos)\n",
            "  589937/2000000: episode: 845, duration: 24.092s, episode steps: 661, steps per second:  27, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012515, mae: 1.181561, mean_q: 1.443619, mean_eps: 0.100000\n",
            "📈 Episodio 846: Recompensa total (clipped): 9.000, Pasos: 645, Mean Reward Calculado: 0.013953 (Recompensa/Pasos)\n",
            "  590582/2000000: episode: 846, duration: 23.431s, episode steps: 645, steps per second:  28, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.012522, mae: 1.212119, mean_q: 1.479509, mean_eps: 0.100000\n",
            "📈 Episodio 847: Recompensa total (clipped): 13.000, Pasos: 691, Mean Reward Calculado: 0.018813 (Recompensa/Pasos)\n",
            "  591273/2000000: episode: 847, duration: 25.401s, episode steps: 691, steps per second:  27, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.012121, mae: 1.203007, mean_q: 1.467800, mean_eps: 0.100000\n",
            "📈 Episodio 848: Recompensa total (clipped): 8.000, Pasos: 526, Mean Reward Calculado: 0.015209 (Recompensa/Pasos)\n",
            "  591799/2000000: episode: 848, duration: 19.155s, episode steps: 526, steps per second:  27, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013652, mae: 1.209279, mean_q: 1.476720, mean_eps: 0.100000\n",
            "📈 Episodio 849: Recompensa total (clipped): 23.000, Pasos: 1059, Mean Reward Calculado: 0.021719 (Recompensa/Pasos)\n",
            "  592858/2000000: episode: 849, duration: 38.806s, episode steps: 1059, steps per second:  27, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.012879, mae: 1.210479, mean_q: 1.478458, mean_eps: 0.100000\n",
            "📈 Episodio 850: Recompensa total (clipped): 8.000, Pasos: 381, Mean Reward Calculado: 0.020997 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 850, pasos: 593239)\n",
            "💾 NUEVO MEJOR PROMEDIO: 14.07 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 850 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 8.00\n",
            "   Media últimos 100: 14.07 / 20.0\n",
            "   Mejor promedio histórico: 14.07\n",
            "   Estado: 📈 70.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  593239/2000000: episode: 850, duration: 35.083s, episode steps: 381, steps per second:  11, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.013480, mae: 1.208039, mean_q: 1.474409, mean_eps: 0.100000\n",
            "📈 Episodio 851: Recompensa total (clipped): 15.000, Pasos: 707, Mean Reward Calculado: 0.021216 (Recompensa/Pasos)\n",
            "  593946/2000000: episode: 851, duration: 26.232s, episode steps: 707, steps per second:  27, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.013231, mae: 1.211944, mean_q: 1.480864, mean_eps: 0.100000\n",
            "📈 Episodio 852: Recompensa total (clipped): 7.000, Pasos: 329, Mean Reward Calculado: 0.021277 (Recompensa/Pasos)\n",
            "  594275/2000000: episode: 852, duration: 12.199s, episode steps: 329, steps per second:  27, episode reward:  7.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012186, mae: 1.184625, mean_q: 1.445741, mean_eps: 0.100000\n",
            "📈 Episodio 853: Recompensa total (clipped): 25.000, Pasos: 1216, Mean Reward Calculado: 0.020559 (Recompensa/Pasos)\n",
            "  595491/2000000: episode: 853, duration: 44.971s, episode steps: 1216, steps per second:  27, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.012775, mae: 1.213037, mean_q: 1.480999, mean_eps: 0.100000\n",
            "📈 Episodio 854: Recompensa total (clipped): 20.000, Pasos: 867, Mean Reward Calculado: 0.023068 (Recompensa/Pasos)\n",
            "  596358/2000000: episode: 854, duration: 31.778s, episode steps: 867, steps per second:  27, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.013208, mae: 1.209718, mean_q: 1.474606, mean_eps: 0.100000\n",
            "📈 Episodio 855: Recompensa total (clipped): 5.000, Pasos: 491, Mean Reward Calculado: 0.010183 (Recompensa/Pasos)\n",
            "  596849/2000000: episode: 855, duration: 18.055s, episode steps: 491, steps per second:  27, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.919 [0.000, 5.000],  loss: 0.013274, mae: 1.207174, mean_q: 1.472736, mean_eps: 0.100000\n",
            "📈 Episodio 856: Recompensa total (clipped): 7.000, Pasos: 434, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            "  597283/2000000: episode: 856, duration: 15.822s, episode steps: 434, steps per second:  27, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013498, mae: 1.207620, mean_q: 1.471850, mean_eps: 0.100000\n",
            "📈 Episodio 857: Recompensa total (clipped): 5.000, Pasos: 385, Mean Reward Calculado: 0.012987 (Recompensa/Pasos)\n",
            "  597668/2000000: episode: 857, duration: 14.262s, episode steps: 385, steps per second:  27, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.012088, mae: 1.204007, mean_q: 1.469030, mean_eps: 0.100000\n",
            "📈 Episodio 858: Recompensa total (clipped): 23.000, Pasos: 1119, Mean Reward Calculado: 0.020554 (Recompensa/Pasos)\n",
            "  598787/2000000: episode: 858, duration: 41.127s, episode steps: 1119, steps per second:  27, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.013451, mae: 1.215425, mean_q: 1.483363, mean_eps: 0.100000\n",
            "📈 Episodio 859: Recompensa total (clipped): 26.000, Pasos: 963, Mean Reward Calculado: 0.026999 (Recompensa/Pasos)\n",
            "  599750/2000000: episode: 859, duration: 35.573s, episode steps: 963, steps per second:  27, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.012442, mae: 1.217325, mean_q: 1.484778, mean_eps: 0.100000\n",
            "📊 Paso 600,000/2,000,000 (30.0%) - 33.2 pasos/seg - ETA: 11.7h - Memoria: 6625.70 MB\n",
            "📈 Episodio 860: Recompensa total (clipped): 19.000, Pasos: 757, Mean Reward Calculado: 0.025099 (Recompensa/Pasos)\n",
            "  600507/2000000: episode: 860, duration: 27.868s, episode steps: 757, steps per second:  27, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.012853, mae: 1.227516, mean_q: 1.499429, mean_eps: 0.100000\n",
            "📈 Episodio 861: Recompensa total (clipped): 21.000, Pasos: 966, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            "  601473/2000000: episode: 861, duration: 35.586s, episode steps: 966, steps per second:  27, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.013179, mae: 1.242822, mean_q: 1.517639, mean_eps: 0.100000\n",
            "📈 Episodio 862: Recompensa total (clipped): 8.000, Pasos: 426, Mean Reward Calculado: 0.018779 (Recompensa/Pasos)\n",
            "  601899/2000000: episode: 862, duration: 15.664s, episode steps: 426, steps per second:  27, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013337, mae: 1.241780, mean_q: 1.517712, mean_eps: 0.100000\n",
            "📈 Episodio 863: Recompensa total (clipped): 8.000, Pasos: 384, Mean Reward Calculado: 0.020833 (Recompensa/Pasos)\n",
            "  602283/2000000: episode: 863, duration: 14.136s, episode steps: 384, steps per second:  27, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013590, mae: 1.223071, mean_q: 1.495825, mean_eps: 0.100000\n",
            "📈 Episodio 864: Recompensa total (clipped): 17.000, Pasos: 984, Mean Reward Calculado: 0.017276 (Recompensa/Pasos)\n",
            "  603267/2000000: episode: 864, duration: 36.467s, episode steps: 984, steps per second:  27, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.012853, mae: 1.227825, mean_q: 1.498432, mean_eps: 0.100000\n",
            "📈 Episodio 865: Recompensa total (clipped): 6.000, Pasos: 369, Mean Reward Calculado: 0.016260 (Recompensa/Pasos)\n",
            "  603636/2000000: episode: 865, duration: 13.821s, episode steps: 369, steps per second:  27, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.084 [0.000, 5.000],  loss: 0.012666, mae: 1.235909, mean_q: 1.508662, mean_eps: 0.100000\n",
            "📈 Episodio 866: Recompensa total (clipped): 15.000, Pasos: 933, Mean Reward Calculado: 0.016077 (Recompensa/Pasos)\n",
            "  604569/2000000: episode: 866, duration: 34.120s, episode steps: 933, steps per second:  27, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.013227, mae: 1.230510, mean_q: 1.503415, mean_eps: 0.100000\n",
            "📈 Episodio 867: Recompensa total (clipped): 15.000, Pasos: 951, Mean Reward Calculado: 0.015773 (Recompensa/Pasos)\n",
            "  605520/2000000: episode: 867, duration: 35.196s, episode steps: 951, steps per second:  27, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.583 [0.000, 5.000],  loss: 0.013774, mae: 1.240049, mean_q: 1.513517, mean_eps: 0.100000\n",
            "📈 Episodio 868: Recompensa total (clipped): 25.000, Pasos: 1254, Mean Reward Calculado: 0.019936 (Recompensa/Pasos)\n",
            "  606774/2000000: episode: 868, duration: 46.406s, episode steps: 1254, steps per second:  27, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.013804, mae: 1.234383, mean_q: 1.507627, mean_eps: 0.100000\n",
            "📈 Episodio 869: Recompensa total (clipped): 14.000, Pasos: 710, Mean Reward Calculado: 0.019718 (Recompensa/Pasos)\n",
            "  607484/2000000: episode: 869, duration: 25.912s, episode steps: 710, steps per second:  27, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013387, mae: 1.229553, mean_q: 1.503597, mean_eps: 0.100000\n",
            "📈 Episodio 870: Recompensa total (clipped): 15.000, Pasos: 586, Mean Reward Calculado: 0.025597 (Recompensa/Pasos)\n",
            "  608070/2000000: episode: 870, duration: 21.487s, episode steps: 586, steps per second:  27, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.012800, mae: 1.243122, mean_q: 1.517549, mean_eps: 0.100000\n",
            "📈 Episodio 871: Recompensa total (clipped): 11.000, Pasos: 687, Mean Reward Calculado: 0.016012 (Recompensa/Pasos)\n",
            "  608757/2000000: episode: 871, duration: 25.481s, episode steps: 687, steps per second:  27, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013324, mae: 1.232910, mean_q: 1.504673, mean_eps: 0.100000\n",
            "📈 Episodio 872: Recompensa total (clipped): 11.000, Pasos: 689, Mean Reward Calculado: 0.015965 (Recompensa/Pasos)\n",
            "  609446/2000000: episode: 872, duration: 25.396s, episode steps: 689, steps per second:  27, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.013718, mae: 1.224724, mean_q: 1.494891, mean_eps: 0.100000\n",
            "📈 Episodio 873: Recompensa total (clipped): 10.000, Pasos: 590, Mean Reward Calculado: 0.016949 (Recompensa/Pasos)\n",
            "  610036/2000000: episode: 873, duration: 22.056s, episode steps: 590, steps per second:  27, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.012828, mae: 1.250055, mean_q: 1.524844, mean_eps: 0.100000\n",
            "📈 Episodio 874: Recompensa total (clipped): 8.000, Pasos: 505, Mean Reward Calculado: 0.015842 (Recompensa/Pasos)\n",
            "  610541/2000000: episode: 874, duration: 18.725s, episode steps: 505, steps per second:  27, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.013411, mae: 1.307688, mean_q: 1.595864, mean_eps: 0.100000\n",
            "📈 Episodio 875: Recompensa total (clipped): 20.000, Pasos: 814, Mean Reward Calculado: 0.024570 (Recompensa/Pasos)\n",
            "  611355/2000000: episode: 875, duration: 29.861s, episode steps: 814, steps per second:  27, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.012515, mae: 1.295822, mean_q: 1.580615, mean_eps: 0.100000\n",
            "📈 Episodio 876: Recompensa total (clipped): 17.000, Pasos: 916, Mean Reward Calculado: 0.018559 (Recompensa/Pasos)\n",
            "  612271/2000000: episode: 876, duration: 33.935s, episode steps: 916, steps per second:  27, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.013748, mae: 1.286033, mean_q: 1.568184, mean_eps: 0.100000\n",
            "📈 Episodio 877: Recompensa total (clipped): 10.000, Pasos: 583, Mean Reward Calculado: 0.017153 (Recompensa/Pasos)\n",
            "  612854/2000000: episode: 877, duration: 21.434s, episode steps: 583, steps per second:  27, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.014592, mae: 1.285700, mean_q: 1.569601, mean_eps: 0.100000\n",
            "📈 Episodio 878: Recompensa total (clipped): 14.000, Pasos: 703, Mean Reward Calculado: 0.019915 (Recompensa/Pasos)\n",
            "  613557/2000000: episode: 878, duration: 25.935s, episode steps: 703, steps per second:  27, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.014140, mae: 1.298532, mean_q: 1.582784, mean_eps: 0.100000\n",
            "📈 Episodio 879: Recompensa total (clipped): 13.000, Pasos: 645, Mean Reward Calculado: 0.020155 (Recompensa/Pasos)\n",
            "  614202/2000000: episode: 879, duration: 23.931s, episode steps: 645, steps per second:  27, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.013219, mae: 1.298727, mean_q: 1.584262, mean_eps: 0.100000\n",
            "📈 Episodio 880: Recompensa total (clipped): 21.000, Pasos: 834, Mean Reward Calculado: 0.025180 (Recompensa/Pasos)\n",
            "  615036/2000000: episode: 880, duration: 31.093s, episode steps: 834, steps per second:  27, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.013909, mae: 1.297491, mean_q: 1.581656, mean_eps: 0.100000\n",
            "📈 Episodio 881: Recompensa total (clipped): 6.000, Pasos: 524, Mean Reward Calculado: 0.011450 (Recompensa/Pasos)\n",
            "  615560/2000000: episode: 881, duration: 19.655s, episode steps: 524, steps per second:  27, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013391, mae: 1.281629, mean_q: 1.560299, mean_eps: 0.100000\n",
            "📈 Episodio 882: Recompensa total (clipped): 14.000, Pasos: 789, Mean Reward Calculado: 0.017744 (Recompensa/Pasos)\n",
            "  616349/2000000: episode: 882, duration: 29.459s, episode steps: 789, steps per second:  27, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014028, mae: 1.301028, mean_q: 1.582749, mean_eps: 0.100000\n",
            "📈 Episodio 883: Recompensa total (clipped): 26.000, Pasos: 1104, Mean Reward Calculado: 0.023551 (Recompensa/Pasos)\n",
            "  617453/2000000: episode: 883, duration: 41.109s, episode steps: 1104, steps per second:  27, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.014691, mae: 1.294196, mean_q: 1.578195, mean_eps: 0.100000\n",
            "📈 Episodio 884: Recompensa total (clipped): 7.000, Pasos: 380, Mean Reward Calculado: 0.018421 (Recompensa/Pasos)\n",
            "  617833/2000000: episode: 884, duration: 14.079s, episode steps: 380, steps per second:  27, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.014603, mae: 1.279647, mean_q: 1.559085, mean_eps: 0.100000\n",
            "📈 Episodio 885: Recompensa total (clipped): 15.000, Pasos: 816, Mean Reward Calculado: 0.018382 (Recompensa/Pasos)\n",
            "  618649/2000000: episode: 885, duration: 30.550s, episode steps: 816, steps per second:  27, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.014192, mae: 1.286619, mean_q: 1.567433, mean_eps: 0.100000\n",
            "📈 Episodio 886: Recompensa total (clipped): 16.000, Pasos: 872, Mean Reward Calculado: 0.018349 (Recompensa/Pasos)\n",
            "  619521/2000000: episode: 886, duration: 32.252s, episode steps: 872, steps per second:  27, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.014745, mae: 1.287114, mean_q: 1.568859, mean_eps: 0.100000\n",
            "📊 Paso 620,000/2,000,000 (31.0%) - 32.9 pasos/seg - ETA: 11.6h - Memoria: 6780.48 MB\n",
            "📈 Episodio 887: Recompensa total (clipped): 13.000, Pasos: 653, Mean Reward Calculado: 0.019908 (Recompensa/Pasos)\n",
            "  620174/2000000: episode: 887, duration: 24.246s, episode steps: 653, steps per second:  27, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.103 [0.000, 5.000],  loss: 0.014158, mae: 1.293105, mean_q: 1.577274, mean_eps: 0.100000\n",
            "📈 Episodio 888: Recompensa total (clipped): 9.000, Pasos: 592, Mean Reward Calculado: 0.015203 (Recompensa/Pasos)\n",
            "  620766/2000000: episode: 888, duration: 21.943s, episode steps: 592, steps per second:  27, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.013517, mae: 1.304089, mean_q: 1.591291, mean_eps: 0.100000\n",
            "📈 Episodio 889: Recompensa total (clipped): 25.000, Pasos: 972, Mean Reward Calculado: 0.025720 (Recompensa/Pasos)\n",
            "  621738/2000000: episode: 889, duration: 36.144s, episode steps: 972, steps per second:  27, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013852, mae: 1.333580, mean_q: 1.625583, mean_eps: 0.100000\n",
            "📈 Episodio 890: Recompensa total (clipped): 19.000, Pasos: 1026, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
            "  622764/2000000: episode: 890, duration: 38.028s, episode steps: 1026, steps per second:  27, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.013909, mae: 1.335678, mean_q: 1.627216, mean_eps: 0.100000\n",
            "📈 Episodio 891: Recompensa total (clipped): 12.000, Pasos: 874, Mean Reward Calculado: 0.013730 (Recompensa/Pasos)\n",
            "  623638/2000000: episode: 891, duration: 32.612s, episode steps: 874, steps per second:  27, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.014438, mae: 1.333230, mean_q: 1.627492, mean_eps: 0.100000\n",
            "📈 Episodio 892: Recompensa total (clipped): 15.000, Pasos: 667, Mean Reward Calculado: 0.022489 (Recompensa/Pasos)\n",
            "  624305/2000000: episode: 892, duration: 24.957s, episode steps: 667, steps per second:  27, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.014081, mae: 1.322171, mean_q: 1.612951, mean_eps: 0.100000\n",
            "📈 Episodio 893: Recompensa total (clipped): 11.000, Pasos: 681, Mean Reward Calculado: 0.016153 (Recompensa/Pasos)\n",
            "  624986/2000000: episode: 893, duration: 25.154s, episode steps: 681, steps per second:  27, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.014153, mae: 1.314413, mean_q: 1.605346, mean_eps: 0.100000\n",
            "📈 Episodio 894: Recompensa total (clipped): 13.000, Pasos: 738, Mean Reward Calculado: 0.017615 (Recompensa/Pasos)\n",
            "  625724/2000000: episode: 894, duration: 27.317s, episode steps: 738, steps per second:  27, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014902, mae: 1.333143, mean_q: 1.625402, mean_eps: 0.100000\n",
            "📈 Episodio 895: Recompensa total (clipped): 15.000, Pasos: 676, Mean Reward Calculado: 0.022189 (Recompensa/Pasos)\n",
            "  626400/2000000: episode: 895, duration: 24.992s, episode steps: 676, steps per second:  27, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.014259, mae: 1.337828, mean_q: 1.631321, mean_eps: 0.100000\n",
            "📈 Episodio 896: Recompensa total (clipped): 7.000, Pasos: 501, Mean Reward Calculado: 0.013972 (Recompensa/Pasos)\n",
            "  626901/2000000: episode: 896, duration: 18.587s, episode steps: 501, steps per second:  27, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.013660, mae: 1.342892, mean_q: 1.638970, mean_eps: 0.100000\n",
            "📈 Episodio 897: Recompensa total (clipped): 13.000, Pasos: 637, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            "  627538/2000000: episode: 897, duration: 23.643s, episode steps: 637, steps per second:  27, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.015400, mae: 1.336322, mean_q: 1.629225, mean_eps: 0.100000\n",
            "📈 Episodio 898: Recompensa total (clipped): 5.000, Pasos: 332, Mean Reward Calculado: 0.015060 (Recompensa/Pasos)\n",
            "  627870/2000000: episode: 898, duration: 12.302s, episode steps: 332, steps per second:  27, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015617, mae: 1.343476, mean_q: 1.639538, mean_eps: 0.100000\n",
            "📈 Episodio 899: Recompensa total (clipped): 7.000, Pasos: 662, Mean Reward Calculado: 0.010574 (Recompensa/Pasos)\n",
            "  628532/2000000: episode: 899, duration: 24.648s, episode steps: 662, steps per second:  27, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.016464, mae: 1.338721, mean_q: 1.632821, mean_eps: 0.100000\n",
            "📈 Episodio 900: Recompensa total (clipped): 33.000, Pasos: 1284, Mean Reward Calculado: 0.025701 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 900, pasos: 629816)\n",
            "💾 NUEVO MEJOR PROMEDIO: 13.73 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 900 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 33.00\n",
            "   Media últimos 100: 13.73 / 20.0\n",
            "   Mejor promedio histórico: 13.73\n",
            "   Estado: 📈 68.7% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  629816/2000000: episode: 900, duration: 97.115s, episode steps: 1284, steps per second:  13, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.014568, mae: 1.329416, mean_q: 1.622058, mean_eps: 0.100000\n",
            "📈 Episodio 901: Recompensa total (clipped): 35.000, Pasos: 1805, Mean Reward Calculado: 0.019391 (Recompensa/Pasos)\n",
            "  631621/2000000: episode: 901, duration: 66.933s, episode steps: 1805, steps per second:  27, episode reward: 35.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.014606, mae: 1.350820, mean_q: 1.647103, mean_eps: 0.100000\n",
            "📈 Episodio 902: Recompensa total (clipped): 11.000, Pasos: 661, Mean Reward Calculado: 0.016641 (Recompensa/Pasos)\n",
            "  632282/2000000: episode: 902, duration: 24.543s, episode steps: 661, steps per second:  27, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.013139, mae: 1.339271, mean_q: 1.635605, mean_eps: 0.100000\n",
            "📈 Episodio 903: Recompensa total (clipped): 15.000, Pasos: 943, Mean Reward Calculado: 0.015907 (Recompensa/Pasos)\n",
            "  633225/2000000: episode: 903, duration: 35.246s, episode steps: 943, steps per second:  27, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.013838, mae: 1.353800, mean_q: 1.649925, mean_eps: 0.100000\n",
            "📈 Episodio 904: Recompensa total (clipped): 14.000, Pasos: 732, Mean Reward Calculado: 0.019126 (Recompensa/Pasos)\n",
            "  633957/2000000: episode: 904, duration: 27.455s, episode steps: 732, steps per second:  27, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013346, mae: 1.341711, mean_q: 1.634592, mean_eps: 0.100000\n",
            "📈 Episodio 905: Recompensa total (clipped): 12.000, Pasos: 684, Mean Reward Calculado: 0.017544 (Recompensa/Pasos)\n",
            "  634641/2000000: episode: 905, duration: 25.769s, episode steps: 684, steps per second:  27, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.014472, mae: 1.325545, mean_q: 1.615226, mean_eps: 0.100000\n",
            "📈 Episodio 906: Recompensa total (clipped): 11.000, Pasos: 632, Mean Reward Calculado: 0.017405 (Recompensa/Pasos)\n",
            "  635273/2000000: episode: 906, duration: 23.868s, episode steps: 632, steps per second:  26, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.014471, mae: 1.344565, mean_q: 1.639144, mean_eps: 0.100000\n",
            "📈 Episodio 907: Recompensa total (clipped): 21.000, Pasos: 1014, Mean Reward Calculado: 0.020710 (Recompensa/Pasos)\n",
            "  636287/2000000: episode: 907, duration: 38.166s, episode steps: 1014, steps per second:  27, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.014464, mae: 1.339084, mean_q: 1.633553, mean_eps: 0.100000\n",
            "📈 Episodio 908: Recompensa total (clipped): 13.000, Pasos: 499, Mean Reward Calculado: 0.026052 (Recompensa/Pasos)\n",
            "  636786/2000000: episode: 908, duration: 18.581s, episode steps: 499, steps per second:  27, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015574, mae: 1.338821, mean_q: 1.631441, mean_eps: 0.100000\n",
            "📈 Episodio 909: Recompensa total (clipped): 14.000, Pasos: 725, Mean Reward Calculado: 0.019310 (Recompensa/Pasos)\n",
            "  637511/2000000: episode: 909, duration: 27.069s, episode steps: 725, steps per second:  27, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.013488, mae: 1.336628, mean_q: 1.629248, mean_eps: 0.100000\n",
            "📈 Episodio 910: Recompensa total (clipped): 29.000, Pasos: 1072, Mean Reward Calculado: 0.027052 (Recompensa/Pasos)\n",
            "  638583/2000000: episode: 910, duration: 40.077s, episode steps: 1072, steps per second:  27, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014033, mae: 1.335905, mean_q: 1.627469, mean_eps: 0.100000\n",
            "📈 Episodio 911: Recompensa total (clipped): 30.000, Pasos: 1377, Mean Reward Calculado: 0.021786 (Recompensa/Pasos)\n",
            "  639960/2000000: episode: 911, duration: 52.032s, episode steps: 1377, steps per second:  26, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.014529, mae: 1.356797, mean_q: 1.653563, mean_eps: 0.100000\n",
            "📊 Paso 640,000/2,000,000 (32.0%) - 32.6 pasos/seg - ETA: 11.6h - Memoria: 10480.70 MB\n",
            "📈 Episodio 912: Recompensa total (clipped): 2.000, Pasos: 497, Mean Reward Calculado: 0.004024 (Recompensa/Pasos)\n",
            "  640457/2000000: episode: 912, duration: 18.747s, episode steps: 497, steps per second:  27, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.015029, mae: 1.378979, mean_q: 1.681599, mean_eps: 0.100000\n",
            "📈 Episodio 913: Recompensa total (clipped): 19.000, Pasos: 875, Mean Reward Calculado: 0.021714 (Recompensa/Pasos)\n",
            "  641332/2000000: episode: 913, duration: 32.614s, episode steps: 875, steps per second:  27, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.013258, mae: 1.393864, mean_q: 1.697613, mean_eps: 0.100000\n",
            "📈 Episodio 914: Recompensa total (clipped): 27.000, Pasos: 955, Mean Reward Calculado: 0.028272 (Recompensa/Pasos)\n",
            "  642287/2000000: episode: 914, duration: 35.654s, episode steps: 955, steps per second:  27, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.014747, mae: 1.381509, mean_q: 1.683290, mean_eps: 0.100000\n",
            "📈 Episodio 915: Recompensa total (clipped): 15.000, Pasos: 974, Mean Reward Calculado: 0.015400 (Recompensa/Pasos)\n",
            "  643261/2000000: episode: 915, duration: 36.446s, episode steps: 974, steps per second:  27, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.012553, mae: 1.375416, mean_q: 1.678652, mean_eps: 0.100000\n",
            "📈 Episodio 916: Recompensa total (clipped): 20.000, Pasos: 742, Mean Reward Calculado: 0.026954 (Recompensa/Pasos)\n",
            "  644003/2000000: episode: 916, duration: 27.805s, episode steps: 742, steps per second:  27, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014710, mae: 1.388580, mean_q: 1.692779, mean_eps: 0.100000\n",
            "📈 Episodio 917: Recompensa total (clipped): 22.000, Pasos: 1277, Mean Reward Calculado: 0.017228 (Recompensa/Pasos)\n",
            "  645280/2000000: episode: 917, duration: 48.161s, episode steps: 1277, steps per second:  27, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.014035, mae: 1.378234, mean_q: 1.679306, mean_eps: 0.100000\n",
            "📈 Episodio 918: Recompensa total (clipped): 18.000, Pasos: 872, Mean Reward Calculado: 0.020642 (Recompensa/Pasos)\n",
            "  646152/2000000: episode: 918, duration: 32.739s, episode steps: 872, steps per second:  27, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014502, mae: 1.379281, mean_q: 1.681459, mean_eps: 0.100000\n",
            "📈 Episodio 919: Recompensa total (clipped): 21.000, Pasos: 1043, Mean Reward Calculado: 0.020134 (Recompensa/Pasos)\n",
            "  647195/2000000: episode: 919, duration: 39.142s, episode steps: 1043, steps per second:  27, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.014463, mae: 1.371586, mean_q: 1.670747, mean_eps: 0.100000\n",
            "📈 Episodio 920: Recompensa total (clipped): 19.000, Pasos: 797, Mean Reward Calculado: 0.023839 (Recompensa/Pasos)\n",
            "  647992/2000000: episode: 920, duration: 30.149s, episode steps: 797, steps per second:  26, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.014319, mae: 1.374762, mean_q: 1.673749, mean_eps: 0.100000\n",
            "📈 Episodio 921: Recompensa total (clipped): 12.000, Pasos: 987, Mean Reward Calculado: 0.012158 (Recompensa/Pasos)\n",
            "  648979/2000000: episode: 921, duration: 37.060s, episode steps: 987, steps per second:  27, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.013638, mae: 1.381310, mean_q: 1.682144, mean_eps: 0.100000\n",
            "📈 Episodio 922: Recompensa total (clipped): 16.000, Pasos: 678, Mean Reward Calculado: 0.023599 (Recompensa/Pasos)\n",
            "  649657/2000000: episode: 922, duration: 25.360s, episode steps: 678, steps per second:  27, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014779, mae: 1.377786, mean_q: 1.676739, mean_eps: 0.100000\n",
            "📈 Episodio 923: Recompensa total (clipped): 16.000, Pasos: 808, Mean Reward Calculado: 0.019802 (Recompensa/Pasos)\n",
            "  650465/2000000: episode: 923, duration: 29.889s, episode steps: 808, steps per second:  27, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.013048, mae: 1.394216, mean_q: 1.699104, mean_eps: 0.100000\n",
            "📈 Episodio 924: Recompensa total (clipped): 16.000, Pasos: 731, Mean Reward Calculado: 0.021888 (Recompensa/Pasos)\n",
            "  651196/2000000: episode: 924, duration: 27.018s, episode steps: 731, steps per second:  27, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.014141, mae: 1.404030, mean_q: 1.709076, mean_eps: 0.100000\n",
            "📈 Episodio 925: Recompensa total (clipped): 26.000, Pasos: 952, Mean Reward Calculado: 0.027311 (Recompensa/Pasos)\n",
            "  652148/2000000: episode: 925, duration: 35.654s, episode steps: 952, steps per second:  27, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.015813, mae: 1.412145, mean_q: 1.720974, mean_eps: 0.100000\n",
            "📈 Episodio 926: Recompensa total (clipped): 15.000, Pasos: 808, Mean Reward Calculado: 0.018564 (Recompensa/Pasos)\n",
            "  652956/2000000: episode: 926, duration: 30.700s, episode steps: 808, steps per second:  26, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.014282, mae: 1.397848, mean_q: 1.700765, mean_eps: 0.100000\n",
            "📈 Episodio 927: Recompensa total (clipped): 30.000, Pasos: 1063, Mean Reward Calculado: 0.028222 (Recompensa/Pasos)\n",
            "  654019/2000000: episode: 927, duration: 40.145s, episode steps: 1063, steps per second:  26, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.915 [0.000, 5.000],  loss: 0.014406, mae: 1.393732, mean_q: 1.696591, mean_eps: 0.100000\n",
            "📈 Episodio 928: Recompensa total (clipped): 18.000, Pasos: 1058, Mean Reward Calculado: 0.017013 (Recompensa/Pasos)\n",
            "  655077/2000000: episode: 928, duration: 39.942s, episode steps: 1058, steps per second:  26, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.014924, mae: 1.386827, mean_q: 1.689626, mean_eps: 0.100000\n",
            "📈 Episodio 929: Recompensa total (clipped): 21.000, Pasos: 764, Mean Reward Calculado: 0.027487 (Recompensa/Pasos)\n",
            "  655841/2000000: episode: 929, duration: 28.951s, episode steps: 764, steps per second:  26, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014023, mae: 1.402896, mean_q: 1.708911, mean_eps: 0.100000\n",
            "📈 Episodio 930: Recompensa total (clipped): 14.000, Pasos: 1179, Mean Reward Calculado: 0.011874 (Recompensa/Pasos)\n",
            "  657020/2000000: episode: 930, duration: 44.177s, episode steps: 1179, steps per second:  27, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.014554, mae: 1.400885, mean_q: 1.708510, mean_eps: 0.100000\n",
            "📈 Episodio 931: Recompensa total (clipped): 23.000, Pasos: 1012, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            "  658032/2000000: episode: 931, duration: 38.369s, episode steps: 1012, steps per second:  26, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.013796, mae: 1.387102, mean_q: 1.689610, mean_eps: 0.100000\n",
            "📈 Episodio 932: Recompensa total (clipped): 18.000, Pasos: 881, Mean Reward Calculado: 0.020431 (Recompensa/Pasos)\n",
            "  658913/2000000: episode: 932, duration: 33.189s, episode steps: 881, steps per second:  27, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.016620, mae: 1.397771, mean_q: 1.703798, mean_eps: 0.100000\n",
            "📈 Episodio 933: Recompensa total (clipped): 7.000, Pasos: 389, Mean Reward Calculado: 0.017995 (Recompensa/Pasos)\n",
            "  659302/2000000: episode: 933, duration: 14.530s, episode steps: 389, steps per second:  27, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.014004, mae: 1.390001, mean_q: 1.694598, mean_eps: 0.100000\n",
            "📊 Paso 660,000/2,000,000 (33.0%) - 32.4 pasos/seg - ETA: 11.5h - Memoria: 10507.31 MB\n",
            "📈 Episodio 934: Recompensa total (clipped): 24.000, Pasos: 901, Mean Reward Calculado: 0.026637 (Recompensa/Pasos)\n",
            "  660203/2000000: episode: 934, duration: 33.735s, episode steps: 901, steps per second:  27, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.014512, mae: 1.404471, mean_q: 1.710035, mean_eps: 0.100000\n",
            "📈 Episodio 935: Recompensa total (clipped): 22.000, Pasos: 970, Mean Reward Calculado: 0.022680 (Recompensa/Pasos)\n",
            "  661173/2000000: episode: 935, duration: 36.935s, episode steps: 970, steps per second:  26, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.012980, mae: 1.389501, mean_q: 1.691415, mean_eps: 0.100000\n",
            "📈 Episodio 936: Recompensa total (clipped): 26.000, Pasos: 1108, Mean Reward Calculado: 0.023466 (Recompensa/Pasos)\n",
            "  662281/2000000: episode: 936, duration: 41.738s, episode steps: 1108, steps per second:  27, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014597, mae: 1.393150, mean_q: 1.696019, mean_eps: 0.100000\n",
            "📈 Episodio 937: Recompensa total (clipped): 26.000, Pasos: 1437, Mean Reward Calculado: 0.018093 (Recompensa/Pasos)\n",
            "  663718/2000000: episode: 937, duration: 54.287s, episode steps: 1437, steps per second:  26, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013291, mae: 1.398498, mean_q: 1.704067, mean_eps: 0.100000\n",
            "📈 Episodio 938: Recompensa total (clipped): 23.000, Pasos: 889, Mean Reward Calculado: 0.025872 (Recompensa/Pasos)\n",
            "  664607/2000000: episode: 938, duration: 33.825s, episode steps: 889, steps per second:  26, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.013262, mae: 1.398144, mean_q: 1.702450, mean_eps: 0.100000\n",
            "📈 Episodio 939: Recompensa total (clipped): 19.000, Pasos: 989, Mean Reward Calculado: 0.019211 (Recompensa/Pasos)\n",
            "  665596/2000000: episode: 939, duration: 37.408s, episode steps: 989, steps per second:  26, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015146, mae: 1.403567, mean_q: 1.708006, mean_eps: 0.100000\n",
            "📈 Episodio 940: Recompensa total (clipped): 28.000, Pasos: 992, Mean Reward Calculado: 0.028226 (Recompensa/Pasos)\n",
            "  666588/2000000: episode: 940, duration: 37.696s, episode steps: 992, steps per second:  26, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.013536, mae: 1.389858, mean_q: 1.691765, mean_eps: 0.100000\n",
            "📈 Episodio 941: Recompensa total (clipped): 25.000, Pasos: 897, Mean Reward Calculado: 0.027871 (Recompensa/Pasos)\n",
            "  667485/2000000: episode: 941, duration: 33.622s, episode steps: 897, steps per second:  27, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.014619, mae: 1.392377, mean_q: 1.694859, mean_eps: 0.100000\n",
            "📈 Episodio 942: Recompensa total (clipped): 12.000, Pasos: 584, Mean Reward Calculado: 0.020548 (Recompensa/Pasos)\n",
            "  668069/2000000: episode: 942, duration: 22.015s, episode steps: 584, steps per second:  27, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.014601, mae: 1.398778, mean_q: 1.703938, mean_eps: 0.100000\n",
            "📈 Episodio 943: Recompensa total (clipped): 11.000, Pasos: 630, Mean Reward Calculado: 0.017460 (Recompensa/Pasos)\n",
            "  668699/2000000: episode: 943, duration: 23.692s, episode steps: 630, steps per second:  27, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.015108, mae: 1.387165, mean_q: 1.690309, mean_eps: 0.100000\n",
            "📈 Episodio 944: Recompensa total (clipped): 24.000, Pasos: 1062, Mean Reward Calculado: 0.022599 (Recompensa/Pasos)\n",
            "  669761/2000000: episode: 944, duration: 39.976s, episode steps: 1062, steps per second:  27, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.863 [0.000, 5.000],  loss: 0.014600, mae: 1.392875, mean_q: 1.696240, mean_eps: 0.100000\n",
            "📈 Episodio 945: Recompensa total (clipped): 14.000, Pasos: 644, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            "  670405/2000000: episode: 945, duration: 24.384s, episode steps: 644, steps per second:  26, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014328, mae: 1.444523, mean_q: 1.760459, mean_eps: 0.100000\n",
            "📈 Episodio 946: Recompensa total (clipped): 4.000, Pasos: 500, Mean Reward Calculado: 0.008000 (Recompensa/Pasos)\n",
            "  670905/2000000: episode: 946, duration: 18.892s, episode steps: 500, steps per second:  26, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.014918, mae: 1.421019, mean_q: 1.732040, mean_eps: 0.100000\n",
            "📈 Episodio 947: Recompensa total (clipped): 8.000, Pasos: 462, Mean Reward Calculado: 0.017316 (Recompensa/Pasos)\n",
            "  671367/2000000: episode: 947, duration: 17.591s, episode steps: 462, steps per second:  26, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.177 [0.000, 5.000],  loss: 0.014820, mae: 1.458363, mean_q: 1.777678, mean_eps: 0.100000\n",
            "📈 Episodio 948: Recompensa total (clipped): 16.000, Pasos: 719, Mean Reward Calculado: 0.022253 (Recompensa/Pasos)\n",
            "  672086/2000000: episode: 948, duration: 27.311s, episode steps: 719, steps per second:  26, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.903 [0.000, 5.000],  loss: 0.015704, mae: 1.454347, mean_q: 1.773500, mean_eps: 0.100000\n",
            "📈 Episodio 949: Recompensa total (clipped): 14.000, Pasos: 620, Mean Reward Calculado: 0.022581 (Recompensa/Pasos)\n",
            "  672706/2000000: episode: 949, duration: 23.500s, episode steps: 620, steps per second:  26, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014407, mae: 1.446657, mean_q: 1.760329, mean_eps: 0.100000\n",
            "📈 Episodio 950: Recompensa total (clipped): 12.000, Pasos: 540, Mean Reward Calculado: 0.022222 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 950, pasos: 673246)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.20 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 950 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 12.00\n",
            "   Media últimos 100: 16.20 / 20.0\n",
            "   Mejor promedio histórico: 16.20\n",
            "   Estado: 📈 81.0% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  673246/2000000: episode: 950, duration: 45.082s, episode steps: 540, steps per second:  12, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.013781, mae: 1.419379, mean_q: 1.727212, mean_eps: 0.100000\n",
            "📈 Episodio 951: Recompensa total (clipped): 12.000, Pasos: 499, Mean Reward Calculado: 0.024048 (Recompensa/Pasos)\n",
            "  673745/2000000: episode: 951, duration: 19.097s, episode steps: 499, steps per second:  26, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.015276, mae: 1.447399, mean_q: 1.762731, mean_eps: 0.100000\n",
            "📈 Episodio 952: Recompensa total (clipped): 13.000, Pasos: 701, Mean Reward Calculado: 0.018545 (Recompensa/Pasos)\n",
            "  674446/2000000: episode: 952, duration: 26.680s, episode steps: 701, steps per second:  26, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.014621, mae: 1.421625, mean_q: 1.732891, mean_eps: 0.100000\n",
            "📈 Episodio 953: Recompensa total (clipped): 25.000, Pasos: 972, Mean Reward Calculado: 0.025720 (Recompensa/Pasos)\n",
            "  675418/2000000: episode: 953, duration: 37.126s, episode steps: 972, steps per second:  26, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.034 [0.000, 5.000],  loss: 0.015186, mae: 1.431308, mean_q: 1.742691, mean_eps: 0.100000\n",
            "📈 Episodio 954: Recompensa total (clipped): 23.000, Pasos: 977, Mean Reward Calculado: 0.023541 (Recompensa/Pasos)\n",
            "  676395/2000000: episode: 954, duration: 37.252s, episode steps: 977, steps per second:  26, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.013489, mae: 1.437207, mean_q: 1.749262, mean_eps: 0.100000\n",
            "📈 Episodio 955: Recompensa total (clipped): 15.000, Pasos: 718, Mean Reward Calculado: 0.020891 (Recompensa/Pasos)\n",
            "  677113/2000000: episode: 955, duration: 27.483s, episode steps: 718, steps per second:  26, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.014957, mae: 1.429438, mean_q: 1.741326, mean_eps: 0.100000\n",
            "📈 Episodio 956: Recompensa total (clipped): 10.000, Pasos: 671, Mean Reward Calculado: 0.014903 (Recompensa/Pasos)\n",
            "  677784/2000000: episode: 956, duration: 25.844s, episode steps: 671, steps per second:  26, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.013306, mae: 1.447649, mean_q: 1.762267, mean_eps: 0.100000\n",
            "📈 Episodio 957: Recompensa total (clipped): 15.000, Pasos: 697, Mean Reward Calculado: 0.021521 (Recompensa/Pasos)\n",
            "  678481/2000000: episode: 957, duration: 27.023s, episode steps: 697, steps per second:  26, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.015567, mae: 1.443536, mean_q: 1.758524, mean_eps: 0.100000\n",
            "📈 Episodio 958: Recompensa total (clipped): 18.000, Pasos: 921, Mean Reward Calculado: 0.019544 (Recompensa/Pasos)\n",
            "  679402/2000000: episode: 958, duration: 34.804s, episode steps: 921, steps per second:  26, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014929, mae: 1.433563, mean_q: 1.745824, mean_eps: 0.100000\n",
            "📊 Paso 680,000/2,000,000 (34.0%) - 32.1 pasos/seg - ETA: 11.4h - Memoria: 10542.79 MB\n",
            "📈 Episodio 959: Recompensa total (clipped): 23.000, Pasos: 1256, Mean Reward Calculado: 0.018312 (Recompensa/Pasos)\n",
            "  680658/2000000: episode: 959, duration: 47.674s, episode steps: 1256, steps per second:  26, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.013506, mae: 1.451082, mean_q: 1.768276, mean_eps: 0.100000\n",
            "📈 Episodio 960: Recompensa total (clipped): 19.000, Pasos: 950, Mean Reward Calculado: 0.020000 (Recompensa/Pasos)\n",
            "  681608/2000000: episode: 960, duration: 36.168s, episode steps: 950, steps per second:  26, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.013902, mae: 1.448510, mean_q: 1.764394, mean_eps: 0.100000\n",
            "📈 Episodio 961: Recompensa total (clipped): 7.000, Pasos: 456, Mean Reward Calculado: 0.015351 (Recompensa/Pasos)\n",
            "  682064/2000000: episode: 961, duration: 17.501s, episode steps: 456, steps per second:  26, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.013653, mae: 1.463726, mean_q: 1.782852, mean_eps: 0.100000\n",
            "📈 Episodio 962: Recompensa total (clipped): 18.000, Pasos: 1061, Mean Reward Calculado: 0.016965 (Recompensa/Pasos)\n",
            "  683125/2000000: episode: 962, duration: 40.433s, episode steps: 1061, steps per second:  26, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.013957, mae: 1.433888, mean_q: 1.746287, mean_eps: 0.100000\n",
            "📈 Episodio 963: Recompensa total (clipped): 8.000, Pasos: 390, Mean Reward Calculado: 0.020513 (Recompensa/Pasos)\n",
            "  683515/2000000: episode: 963, duration: 14.703s, episode steps: 390, steps per second:  27, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.015944, mae: 1.462173, mean_q: 1.780499, mean_eps: 0.100000\n",
            "📈 Episodio 964: Recompensa total (clipped): 15.000, Pasos: 783, Mean Reward Calculado: 0.019157 (Recompensa/Pasos)\n",
            "  684298/2000000: episode: 964, duration: 30.039s, episode steps: 783, steps per second:  26, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.015358, mae: 1.441539, mean_q: 1.752813, mean_eps: 0.100000\n",
            "📈 Episodio 965: Recompensa total (clipped): 27.000, Pasos: 1353, Mean Reward Calculado: 0.019956 (Recompensa/Pasos)\n",
            "  685651/2000000: episode: 965, duration: 51.358s, episode steps: 1353, steps per second:  26, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.015071, mae: 1.453403, mean_q: 1.768505, mean_eps: 0.100000\n",
            "📈 Episodio 966: Recompensa total (clipped): 18.000, Pasos: 867, Mean Reward Calculado: 0.020761 (Recompensa/Pasos)\n",
            "  686518/2000000: episode: 966, duration: 32.639s, episode steps: 867, steps per second:  27, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.014356, mae: 1.440814, mean_q: 1.755469, mean_eps: 0.100000\n",
            "📈 Episodio 967: Recompensa total (clipped): 22.000, Pasos: 791, Mean Reward Calculado: 0.027813 (Recompensa/Pasos)\n",
            "  687309/2000000: episode: 967, duration: 30.416s, episode steps: 791, steps per second:  26, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.015104, mae: 1.445737, mean_q: 1.758811, mean_eps: 0.100000\n",
            "📈 Episodio 968: Recompensa total (clipped): 16.000, Pasos: 776, Mean Reward Calculado: 0.020619 (Recompensa/Pasos)\n",
            "  688085/2000000: episode: 968, duration: 29.557s, episode steps: 776, steps per second:  26, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.014760, mae: 1.444741, mean_q: 1.756025, mean_eps: 0.100000\n",
            "📈 Episodio 969: Recompensa total (clipped): 8.000, Pasos: 502, Mean Reward Calculado: 0.015936 (Recompensa/Pasos)\n",
            "  688587/2000000: episode: 969, duration: 19.388s, episode steps: 502, steps per second:  26, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.014733, mae: 1.444723, mean_q: 1.759280, mean_eps: 0.100000\n",
            "📈 Episodio 970: Recompensa total (clipped): 19.000, Pasos: 806, Mean Reward Calculado: 0.023573 (Recompensa/Pasos)\n",
            "  689393/2000000: episode: 970, duration: 30.710s, episode steps: 806, steps per second:  26, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.943 [0.000, 5.000],  loss: 0.013942, mae: 1.446526, mean_q: 1.760469, mean_eps: 0.100000\n",
            "📈 Episodio 971: Recompensa total (clipped): 21.000, Pasos: 921, Mean Reward Calculado: 0.022801 (Recompensa/Pasos)\n",
            "  690314/2000000: episode: 971, duration: 34.896s, episode steps: 921, steps per second:  26, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.014164, mae: 1.457321, mean_q: 1.773351, mean_eps: 0.100000\n",
            "📈 Episodio 972: Recompensa total (clipped): 15.000, Pasos: 497, Mean Reward Calculado: 0.030181 (Recompensa/Pasos)\n",
            "  690811/2000000: episode: 972, duration: 18.834s, episode steps: 497, steps per second:  26, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.155 [0.000, 5.000],  loss: 0.013578, mae: 1.479733, mean_q: 1.800615, mean_eps: 0.100000\n",
            "📈 Episodio 973: Recompensa total (clipped): 16.000, Pasos: 894, Mean Reward Calculado: 0.017897 (Recompensa/Pasos)\n",
            "  691705/2000000: episode: 973, duration: 34.411s, episode steps: 894, steps per second:  26, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013355, mae: 1.483892, mean_q: 1.806502, mean_eps: 0.100000\n",
            "📈 Episodio 974: Recompensa total (clipped): 12.000, Pasos: 650, Mean Reward Calculado: 0.018462 (Recompensa/Pasos)\n",
            "  692355/2000000: episode: 974, duration: 25.196s, episode steps: 650, steps per second:  26, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.013591, mae: 1.473236, mean_q: 1.792766, mean_eps: 0.100000\n",
            "📈 Episodio 975: Recompensa total (clipped): 12.000, Pasos: 627, Mean Reward Calculado: 0.019139 (Recompensa/Pasos)\n",
            "  692982/2000000: episode: 975, duration: 24.191s, episode steps: 627, steps per second:  26, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.014551, mae: 1.495280, mean_q: 1.819991, mean_eps: 0.100000\n",
            "📈 Episodio 976: Recompensa total (clipped): 21.000, Pasos: 1007, Mean Reward Calculado: 0.020854 (Recompensa/Pasos)\n",
            "  693989/2000000: episode: 976, duration: 38.187s, episode steps: 1007, steps per second:  26, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.014609, mae: 1.490389, mean_q: 1.812432, mean_eps: 0.100000\n",
            "📈 Episodio 977: Recompensa total (clipped): 16.000, Pasos: 713, Mean Reward Calculado: 0.022440 (Recompensa/Pasos)\n",
            "  694702/2000000: episode: 977, duration: 27.604s, episode steps: 713, steps per second:  26, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.123 [0.000, 5.000],  loss: 0.013739, mae: 1.476082, mean_q: 1.794453, mean_eps: 0.100000\n",
            "📈 Episodio 978: Recompensa total (clipped): 19.000, Pasos: 843, Mean Reward Calculado: 0.022539 (Recompensa/Pasos)\n",
            "  695545/2000000: episode: 978, duration: 32.861s, episode steps: 843, steps per second:  26, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.013478, mae: 1.475373, mean_q: 1.795450, mean_eps: 0.100000\n",
            "📈 Episodio 979: Recompensa total (clipped): 10.000, Pasos: 698, Mean Reward Calculado: 0.014327 (Recompensa/Pasos)\n",
            "  696243/2000000: episode: 979, duration: 26.885s, episode steps: 698, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.013301, mae: 1.480782, mean_q: 1.800000, mean_eps: 0.100000\n",
            "📈 Episodio 980: Recompensa total (clipped): 13.000, Pasos: 955, Mean Reward Calculado: 0.013613 (Recompensa/Pasos)\n",
            "  697198/2000000: episode: 980, duration: 36.527s, episode steps: 955, steps per second:  26, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.014766, mae: 1.473888, mean_q: 1.791078, mean_eps: 0.100000\n",
            "📈 Episodio 981: Recompensa total (clipped): 15.000, Pasos: 654, Mean Reward Calculado: 0.022936 (Recompensa/Pasos)\n",
            "  697852/2000000: episode: 981, duration: 25.107s, episode steps: 654, steps per second:  26, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.914 [0.000, 5.000],  loss: 0.014416, mae: 1.476090, mean_q: 1.796165, mean_eps: 0.100000\n",
            "📈 Episodio 982: Recompensa total (clipped): 32.000, Pasos: 1574, Mean Reward Calculado: 0.020330 (Recompensa/Pasos)\n",
            "  699426/2000000: episode: 982, duration: 60.697s, episode steps: 1574, steps per second:  26, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.014741, mae: 1.476418, mean_q: 1.795570, mean_eps: 0.100000\n",
            "📊 Paso 700,000/2,000,000 (35.0%) - 31.9 pasos/seg - ETA: 11.3h - Memoria: 10531.50 MB\n",
            "📈 Episodio 983: Recompensa total (clipped): 14.000, Pasos: 727, Mean Reward Calculado: 0.019257 (Recompensa/Pasos)\n",
            "  700153/2000000: episode: 983, duration: 28.229s, episode steps: 727, steps per second:  26, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.013011, mae: 1.487117, mean_q: 1.807179, mean_eps: 0.100000\n",
            "📈 Episodio 984: Recompensa total (clipped): 19.000, Pasos: 1330, Mean Reward Calculado: 0.014286 (Recompensa/Pasos)\n",
            "  701483/2000000: episode: 984, duration: 51.628s, episode steps: 1330, steps per second:  26, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014062, mae: 1.494660, mean_q: 1.817803, mean_eps: 0.100000\n",
            "📈 Episodio 985: Recompensa total (clipped): 25.000, Pasos: 1309, Mean Reward Calculado: 0.019099 (Recompensa/Pasos)\n",
            "  702792/2000000: episode: 985, duration: 50.549s, episode steps: 1309, steps per second:  26, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.013740, mae: 1.491252, mean_q: 1.813834, mean_eps: 0.100000\n",
            "📈 Episodio 986: Recompensa total (clipped): 7.000, Pasos: 509, Mean Reward Calculado: 0.013752 (Recompensa/Pasos)\n",
            "  703301/2000000: episode: 986, duration: 19.375s, episode steps: 509, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.041 [0.000, 5.000],  loss: 0.014939, mae: 1.481510, mean_q: 1.800578, mean_eps: 0.100000\n",
            "📈 Episodio 987: Recompensa total (clipped): 28.000, Pasos: 1274, Mean Reward Calculado: 0.021978 (Recompensa/Pasos)\n",
            "  704575/2000000: episode: 987, duration: 49.014s, episode steps: 1274, steps per second:  26, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.014465, mae: 1.473461, mean_q: 1.789665, mean_eps: 0.100000\n",
            "📈 Episodio 988: Recompensa total (clipped): 8.000, Pasos: 426, Mean Reward Calculado: 0.018779 (Recompensa/Pasos)\n",
            "  705001/2000000: episode: 988, duration: 16.380s, episode steps: 426, steps per second:  26, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.915 [0.000, 5.000],  loss: 0.013799, mae: 1.490060, mean_q: 1.811897, mean_eps: 0.100000\n",
            "📈 Episodio 989: Recompensa total (clipped): 26.000, Pasos: 1176, Mean Reward Calculado: 0.022109 (Recompensa/Pasos)\n",
            "  706177/2000000: episode: 989, duration: 45.575s, episode steps: 1176, steps per second:  26, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.015258, mae: 1.475577, mean_q: 1.791986, mean_eps: 0.100000\n",
            "📈 Episodio 990: Recompensa total (clipped): 8.000, Pasos: 496, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            "  706673/2000000: episode: 990, duration: 18.971s, episode steps: 496, steps per second:  26, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.015817, mae: 1.503926, mean_q: 1.827010, mean_eps: 0.100000\n",
            "📈 Episodio 991: Recompensa total (clipped): 27.000, Pasos: 930, Mean Reward Calculado: 0.029032 (Recompensa/Pasos)\n",
            "  707603/2000000: episode: 991, duration: 36.098s, episode steps: 930, steps per second:  26, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014192, mae: 1.477557, mean_q: 1.792636, mean_eps: 0.100000\n",
            "📈 Episodio 992: Recompensa total (clipped): 13.000, Pasos: 502, Mean Reward Calculado: 0.025896 (Recompensa/Pasos)\n",
            "  708105/2000000: episode: 992, duration: 19.506s, episode steps: 502, steps per second:  26, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.015255, mae: 1.505869, mean_q: 1.828151, mean_eps: 0.100000\n",
            "📈 Episodio 993: Recompensa total (clipped): 11.000, Pasos: 742, Mean Reward Calculado: 0.014825 (Recompensa/Pasos)\n",
            "  708847/2000000: episode: 993, duration: 28.500s, episode steps: 742, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.016910, mae: 1.488499, mean_q: 1.809525, mean_eps: 0.100000\n",
            "📈 Episodio 994: Recompensa total (clipped): 12.000, Pasos: 652, Mean Reward Calculado: 0.018405 (Recompensa/Pasos)\n",
            "  709499/2000000: episode: 994, duration: 24.880s, episode steps: 652, steps per second:  26, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.013754, mae: 1.488249, mean_q: 1.808141, mean_eps: 0.100000\n",
            "📈 Episodio 995: Recompensa total (clipped): 19.000, Pasos: 846, Mean Reward Calculado: 0.022459 (Recompensa/Pasos)\n",
            "  710345/2000000: episode: 995, duration: 32.511s, episode steps: 846, steps per second:  26, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.014330, mae: 1.496911, mean_q: 1.819520, mean_eps: 0.100000\n",
            "📈 Episodio 996: Recompensa total (clipped): 19.000, Pasos: 857, Mean Reward Calculado: 0.022170 (Recompensa/Pasos)\n",
            "  711202/2000000: episode: 996, duration: 33.018s, episode steps: 857, steps per second:  26, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.013445, mae: 1.515248, mean_q: 1.841697, mean_eps: 0.100000\n",
            "📈 Episodio 997: Recompensa total (clipped): 11.000, Pasos: 520, Mean Reward Calculado: 0.021154 (Recompensa/Pasos)\n",
            "  711722/2000000: episode: 997, duration: 20.307s, episode steps: 520, steps per second:  26, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.138 [0.000, 5.000],  loss: 0.015536, mae: 1.496813, mean_q: 1.821184, mean_eps: 0.100000\n",
            "📈 Episodio 998: Recompensa total (clipped): 7.000, Pasos: 368, Mean Reward Calculado: 0.019022 (Recompensa/Pasos)\n",
            "  712090/2000000: episode: 998, duration: 14.231s, episode steps: 368, steps per second:  26, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.014053, mae: 1.512567, mean_q: 1.838089, mean_eps: 0.100000\n",
            "📈 Episodio 999: Recompensa total (clipped): 28.000, Pasos: 1232, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            "  713322/2000000: episode: 999, duration: 48.090s, episode steps: 1232, steps per second:  26, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.013835, mae: 1.512891, mean_q: 1.838707, mean_eps: 0.100000\n",
            "📈 Episodio 1000: Recompensa total (clipped): 8.000, Pasos: 486, Mean Reward Calculado: 0.016461 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1000, pasos: 713808)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.31 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1000 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 8.00\n",
            "   Media últimos 100: 17.31 / 20.0\n",
            "   Mejor promedio histórico: 17.31\n",
            "   Estado: 📈 86.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "💾 Guardado modelo principal lastest: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_model.h5\n",
            "💾 Guardado modelo target lastest: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_target.h5\n",
            "💾 Memoria lastest guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_memory.pkl\n",
            "💾 Checkpoint lastest guardado (ep: 1000, pasos: 713808)\n",
            "✅ Checkpoint guardado para episodio 1000\n",
            "  713808/2000000: episode: 1000, duration: 126.445s, episode steps: 486, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.013651, mae: 1.490467, mean_q: 1.811473, mean_eps: 0.100000\n",
            "📈 Episodio 1001: Recompensa total (clipped): 23.000, Pasos: 1002, Mean Reward Calculado: 0.022954 (Recompensa/Pasos)\n",
            "  714810/2000000: episode: 1001, duration: 40.272s, episode steps: 1002, steps per second:  25, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014083, mae: 1.510896, mean_q: 1.834702, mean_eps: 0.100000\n",
            "📈 Episodio 1002: Recompensa total (clipped): 22.000, Pasos: 979, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            "  715789/2000000: episode: 1002, duration: 38.451s, episode steps: 979, steps per second:  25, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014763, mae: 1.524396, mean_q: 1.852363, mean_eps: 0.100000\n",
            "📈 Episodio 1003: Recompensa total (clipped): 15.000, Pasos: 686, Mean Reward Calculado: 0.021866 (Recompensa/Pasos)\n",
            "  716475/2000000: episode: 1003, duration: 26.668s, episode steps: 686, steps per second:  26, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.014886, mae: 1.507309, mean_q: 1.831858, mean_eps: 0.100000\n",
            "📈 Episodio 1004: Recompensa total (clipped): 19.000, Pasos: 952, Mean Reward Calculado: 0.019958 (Recompensa/Pasos)\n",
            "  717427/2000000: episode: 1004, duration: 37.353s, episode steps: 952, steps per second:  25, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.015561, mae: 1.518377, mean_q: 1.844141, mean_eps: 0.100000\n",
            "📈 Episodio 1005: Recompensa total (clipped): 27.000, Pasos: 1021, Mean Reward Calculado: 0.026445 (Recompensa/Pasos)\n",
            "  718448/2000000: episode: 1005, duration: 39.554s, episode steps: 1021, steps per second:  26, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.015363, mae: 1.518169, mean_q: 1.844254, mean_eps: 0.100000\n",
            "📈 Episodio 1006: Recompensa total (clipped): 20.000, Pasos: 706, Mean Reward Calculado: 0.028329 (Recompensa/Pasos)\n",
            "  719154/2000000: episode: 1006, duration: 27.059s, episode steps: 706, steps per second:  26, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.014288, mae: 1.505812, mean_q: 1.828495, mean_eps: 0.100000\n",
            "📊 Paso 720,000/2,000,000 (36.0%) - 31.6 pasos/seg - ETA: 11.3h - Memoria: 10578.45 MB\n",
            "📈 Episodio 1007: Recompensa total (clipped): 21.000, Pasos: 1012, Mean Reward Calculado: 0.020751 (Recompensa/Pasos)\n",
            "  720166/2000000: episode: 1007, duration: 39.474s, episode steps: 1012, steps per second:  26, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.014833, mae: 1.512973, mean_q: 1.838049, mean_eps: 0.100000\n",
            "📈 Episodio 1008: Recompensa total (clipped): 23.000, Pasos: 1143, Mean Reward Calculado: 0.020122 (Recompensa/Pasos)\n",
            "  721309/2000000: episode: 1008, duration: 44.460s, episode steps: 1143, steps per second:  26, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.014346, mae: 1.520243, mean_q: 1.848052, mean_eps: 0.100000\n",
            "📈 Episodio 1009: Recompensa total (clipped): 15.000, Pasos: 625, Mean Reward Calculado: 0.024000 (Recompensa/Pasos)\n",
            "  721934/2000000: episode: 1009, duration: 24.239s, episode steps: 625, steps per second:  26, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.014182, mae: 1.510154, mean_q: 1.835095, mean_eps: 0.100000\n",
            "📈 Episodio 1010: Recompensa total (clipped): 18.000, Pasos: 727, Mean Reward Calculado: 0.024759 (Recompensa/Pasos)\n",
            "  722661/2000000: episode: 1010, duration: 28.298s, episode steps: 727, steps per second:  26, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.013774, mae: 1.491302, mean_q: 1.811374, mean_eps: 0.100000\n",
            "📈 Episodio 1011: Recompensa total (clipped): 30.000, Pasos: 1486, Mean Reward Calculado: 0.020188 (Recompensa/Pasos)\n",
            "  724147/2000000: episode: 1011, duration: 57.740s, episode steps: 1486, steps per second:  26, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.014605, mae: 1.504550, mean_q: 1.826364, mean_eps: 0.100000\n",
            "📈 Episodio 1012: Recompensa total (clipped): 14.000, Pasos: 691, Mean Reward Calculado: 0.020260 (Recompensa/Pasos)\n",
            "  724838/2000000: episode: 1012, duration: 26.910s, episode steps: 691, steps per second:  26, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.013710, mae: 1.496668, mean_q: 1.816761, mean_eps: 0.100000\n",
            "📈 Episodio 1013: Recompensa total (clipped): 8.000, Pasos: 636, Mean Reward Calculado: 0.012579 (Recompensa/Pasos)\n",
            "  725474/2000000: episode: 1013, duration: 24.686s, episode steps: 636, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.015106, mae: 1.517104, mean_q: 1.841379, mean_eps: 0.100000\n",
            "📈 Episodio 1014: Recompensa total (clipped): 21.000, Pasos: 1060, Mean Reward Calculado: 0.019811 (Recompensa/Pasos)\n",
            "  726534/2000000: episode: 1014, duration: 40.709s, episode steps: 1060, steps per second:  26, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.014553, mae: 1.514220, mean_q: 1.837865, mean_eps: 0.100000\n",
            "📈 Episodio 1015: Recompensa total (clipped): 23.000, Pasos: 1134, Mean Reward Calculado: 0.020282 (Recompensa/Pasos)\n",
            "  727668/2000000: episode: 1015, duration: 44.581s, episode steps: 1134, steps per second:  25, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014469, mae: 1.505059, mean_q: 1.829214, mean_eps: 0.100000\n",
            "📈 Episodio 1016: Recompensa total (clipped): 15.000, Pasos: 795, Mean Reward Calculado: 0.018868 (Recompensa/Pasos)\n",
            "  728463/2000000: episode: 1016, duration: 30.936s, episode steps: 795, steps per second:  26, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.014289, mae: 1.506013, mean_q: 1.829151, mean_eps: 0.100000\n",
            "📈 Episodio 1017: Recompensa total (clipped): 5.000, Pasos: 382, Mean Reward Calculado: 0.013089 (Recompensa/Pasos)\n",
            "  728845/2000000: episode: 1017, duration: 14.870s, episode steps: 382, steps per second:  26, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.015035, mae: 1.537983, mean_q: 1.868356, mean_eps: 0.100000\n",
            "📈 Episodio 1018: Recompensa total (clipped): 15.000, Pasos: 829, Mean Reward Calculado: 0.018094 (Recompensa/Pasos)\n",
            "  729674/2000000: episode: 1018, duration: 32.163s, episode steps: 829, steps per second:  26, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.014051, mae: 1.506846, mean_q: 1.830175, mean_eps: 0.100000\n",
            "📈 Episodio 1019: Recompensa total (clipped): 30.000, Pasos: 1129, Mean Reward Calculado: 0.026572 (Recompensa/Pasos)\n",
            "  730803/2000000: episode: 1019, duration: 43.658s, episode steps: 1129, steps per second:  26, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.013809, mae: 1.539853, mean_q: 1.870472, mean_eps: 0.100000\n",
            "📈 Episodio 1020: Recompensa total (clipped): 17.000, Pasos: 948, Mean Reward Calculado: 0.017932 (Recompensa/Pasos)\n",
            "  731751/2000000: episode: 1020, duration: 37.170s, episode steps: 948, steps per second:  26, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.013208, mae: 1.538091, mean_q: 1.871030, mean_eps: 0.100000\n",
            "📈 Episodio 1021: Recompensa total (clipped): 5.000, Pasos: 608, Mean Reward Calculado: 0.008224 (Recompensa/Pasos)\n",
            "  732359/2000000: episode: 1021, duration: 23.382s, episode steps: 608, steps per second:  26, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.015352, mae: 1.557854, mean_q: 1.895991, mean_eps: 0.100000\n",
            "📈 Episodio 1022: Recompensa total (clipped): 12.000, Pasos: 632, Mean Reward Calculado: 0.018987 (Recompensa/Pasos)\n",
            "  732991/2000000: episode: 1022, duration: 24.471s, episode steps: 632, steps per second:  26, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.016464, mae: 1.540357, mean_q: 1.870843, mean_eps: 0.100000\n",
            "📈 Episodio 1023: Recompensa total (clipped): 14.000, Pasos: 628, Mean Reward Calculado: 0.022293 (Recompensa/Pasos)\n",
            "  733619/2000000: episode: 1023, duration: 24.549s, episode steps: 628, steps per second:  26, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014825, mae: 1.531021, mean_q: 1.862064, mean_eps: 0.100000\n",
            "📈 Episodio 1024: Recompensa total (clipped): 21.000, Pasos: 768, Mean Reward Calculado: 0.027344 (Recompensa/Pasos)\n",
            "  734387/2000000: episode: 1024, duration: 29.713s, episode steps: 768, steps per second:  26, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.015418, mae: 1.537933, mean_q: 1.865967, mean_eps: 0.100000\n",
            "📈 Episodio 1025: Recompensa total (clipped): 32.000, Pasos: 1308, Mean Reward Calculado: 0.024465 (Recompensa/Pasos)\n",
            "  735695/2000000: episode: 1025, duration: 51.674s, episode steps: 1308, steps per second:  25, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.014136, mae: 1.542104, mean_q: 1.873505, mean_eps: 0.100000\n",
            "📈 Episodio 1026: Recompensa total (clipped): 18.000, Pasos: 878, Mean Reward Calculado: 0.020501 (Recompensa/Pasos)\n",
            "  736573/2000000: episode: 1026, duration: 34.357s, episode steps: 878, steps per second:  26, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.015453, mae: 1.541016, mean_q: 1.872099, mean_eps: 0.100000\n",
            "📈 Episodio 1027: Recompensa total (clipped): 11.000, Pasos: 722, Mean Reward Calculado: 0.015235 (Recompensa/Pasos)\n",
            "  737295/2000000: episode: 1027, duration: 27.799s, episode steps: 722, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013992, mae: 1.554426, mean_q: 1.889130, mean_eps: 0.100000\n",
            "📈 Episodio 1028: Recompensa total (clipped): 23.000, Pasos: 833, Mean Reward Calculado: 0.027611 (Recompensa/Pasos)\n",
            "  738128/2000000: episode: 1028, duration: 32.880s, episode steps: 833, steps per second:  25, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.015342, mae: 1.542416, mean_q: 1.871718, mean_eps: 0.100000\n",
            "📈 Episodio 1029: Recompensa total (clipped): 29.000, Pasos: 1156, Mean Reward Calculado: 0.025087 (Recompensa/Pasos)\n",
            "  739284/2000000: episode: 1029, duration: 45.818s, episode steps: 1156, steps per second:  25, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.015708, mae: 1.543816, mean_q: 1.873916, mean_eps: 0.100000\n",
            "📊 Paso 740,000/2,000,000 (37.0%) - 31.4 pasos/seg - ETA: 11.2h - Memoria: 10546.86 MB\n",
            "📈 Episodio 1030: Recompensa total (clipped): 33.000, Pasos: 1432, Mean Reward Calculado: 0.023045 (Recompensa/Pasos)\n",
            "  740716/2000000: episode: 1030, duration: 56.583s, episode steps: 1432, steps per second:  25, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.015510, mae: 1.549480, mean_q: 1.882699, mean_eps: 0.100000\n",
            "📈 Episodio 1031: Recompensa total (clipped): 11.000, Pasos: 477, Mean Reward Calculado: 0.023061 (Recompensa/Pasos)\n",
            "  741193/2000000: episode: 1031, duration: 18.869s, episode steps: 477, steps per second:  25, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.013361, mae: 1.533985, mean_q: 1.864731, mean_eps: 0.100000\n",
            "📈 Episodio 1032: Recompensa total (clipped): 16.000, Pasos: 665, Mean Reward Calculado: 0.024060 (Recompensa/Pasos)\n",
            "  741858/2000000: episode: 1032, duration: 25.978s, episode steps: 665, steps per second:  26, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.013551, mae: 1.540296, mean_q: 1.871411, mean_eps: 0.100000\n",
            "📈 Episodio 1033: Recompensa total (clipped): 13.000, Pasos: 611, Mean Reward Calculado: 0.021277 (Recompensa/Pasos)\n",
            "  742469/2000000: episode: 1033, duration: 23.801s, episode steps: 611, steps per second:  26, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015062, mae: 1.552696, mean_q: 1.885174, mean_eps: 0.100000\n",
            "📈 Episodio 1034: Recompensa total (clipped): 7.000, Pasos: 448, Mean Reward Calculado: 0.015625 (Recompensa/Pasos)\n",
            "  742917/2000000: episode: 1034, duration: 17.663s, episode steps: 448, steps per second:  25, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.516 [0.000, 5.000],  loss: 0.016466, mae: 1.566079, mean_q: 1.902779, mean_eps: 0.100000\n",
            "📈 Episodio 1035: Recompensa total (clipped): 21.000, Pasos: 918, Mean Reward Calculado: 0.022876 (Recompensa/Pasos)\n",
            "  743835/2000000: episode: 1035, duration: 36.392s, episode steps: 918, steps per second:  25, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.015138, mae: 1.547278, mean_q: 1.880600, mean_eps: 0.100000\n",
            "📈 Episodio 1036: Recompensa total (clipped): 17.000, Pasos: 816, Mean Reward Calculado: 0.020833 (Recompensa/Pasos)\n",
            "  744651/2000000: episode: 1036, duration: 32.455s, episode steps: 816, steps per second:  25, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.015760, mae: 1.543767, mean_q: 1.876265, mean_eps: 0.100000\n",
            "📈 Episodio 1037: Recompensa total (clipped): 13.000, Pasos: 525, Mean Reward Calculado: 0.024762 (Recompensa/Pasos)\n",
            "  745176/2000000: episode: 1037, duration: 20.783s, episode steps: 525, steps per second:  25, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.013869, mae: 1.530619, mean_q: 1.857954, mean_eps: 0.100000\n",
            "📈 Episodio 1038: Recompensa total (clipped): 13.000, Pasos: 792, Mean Reward Calculado: 0.016414 (Recompensa/Pasos)\n",
            "  745968/2000000: episode: 1038, duration: 31.638s, episode steps: 792, steps per second:  25, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.013313, mae: 1.548102, mean_q: 1.879871, mean_eps: 0.100000\n",
            "📈 Episodio 1039: Recompensa total (clipped): 7.000, Pasos: 525, Mean Reward Calculado: 0.013333 (Recompensa/Pasos)\n",
            "  746493/2000000: episode: 1039, duration: 21.006s, episode steps: 525, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.013669, mae: 1.548027, mean_q: 1.880370, mean_eps: 0.100000\n",
            "📈 Episodio 1040: Recompensa total (clipped): 9.000, Pasos: 486, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
            "  746979/2000000: episode: 1040, duration: 19.115s, episode steps: 486, steps per second:  25, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015061, mae: 1.538433, mean_q: 1.869939, mean_eps: 0.100000\n",
            "📈 Episodio 1041: Recompensa total (clipped): 21.000, Pasos: 835, Mean Reward Calculado: 0.025150 (Recompensa/Pasos)\n",
            "  747814/2000000: episode: 1041, duration: 32.925s, episode steps: 835, steps per second:  25, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.013963, mae: 1.553465, mean_q: 1.889046, mean_eps: 0.100000\n",
            "📈 Episodio 1042: Recompensa total (clipped): 15.000, Pasos: 688, Mean Reward Calculado: 0.021802 (Recompensa/Pasos)\n",
            "  748502/2000000: episode: 1042, duration: 26.952s, episode steps: 688, steps per second:  26, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015948, mae: 1.541419, mean_q: 1.869611, mean_eps: 0.100000\n",
            "📈 Episodio 1043: Recompensa total (clipped): 18.000, Pasos: 752, Mean Reward Calculado: 0.023936 (Recompensa/Pasos)\n",
            "  749254/2000000: episode: 1043, duration: 29.507s, episode steps: 752, steps per second:  25, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.015228, mae: 1.538956, mean_q: 1.871780, mean_eps: 0.100000\n",
            "📈 Episodio 1044: Recompensa total (clipped): 10.000, Pasos: 702, Mean Reward Calculado: 0.014245 (Recompensa/Pasos)\n",
            "  749956/2000000: episode: 1044, duration: 27.910s, episode steps: 702, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.015081, mae: 1.525670, mean_q: 1.851839, mean_eps: 0.100000\n",
            "📈 Episodio 1045: Recompensa total (clipped): 24.000, Pasos: 1106, Mean Reward Calculado: 0.021700 (Recompensa/Pasos)\n",
            "  751062/2000000: episode: 1045, duration: 44.199s, episode steps: 1106, steps per second:  25, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.013931, mae: 1.577805, mean_q: 1.917860, mean_eps: 0.100000\n",
            "📈 Episodio 1046: Recompensa total (clipped): 14.000, Pasos: 671, Mean Reward Calculado: 0.020864 (Recompensa/Pasos)\n",
            "  751733/2000000: episode: 1046, duration: 26.787s, episode steps: 671, steps per second:  25, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014117, mae: 1.590649, mean_q: 1.931997, mean_eps: 0.100000\n",
            "📈 Episodio 1047: Recompensa total (clipped): 4.000, Pasos: 359, Mean Reward Calculado: 0.011142 (Recompensa/Pasos)\n",
            "  752092/2000000: episode: 1047, duration: 14.339s, episode steps: 359, steps per second:  25, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013475, mae: 1.586213, mean_q: 1.928681, mean_eps: 0.100000\n",
            "📈 Episodio 1048: Recompensa total (clipped): 10.000, Pasos: 527, Mean Reward Calculado: 0.018975 (Recompensa/Pasos)\n",
            "  752619/2000000: episode: 1048, duration: 20.949s, episode steps: 527, steps per second:  25, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.014779, mae: 1.595593, mean_q: 1.940156, mean_eps: 0.100000\n",
            "📈 Episodio 1049: Recompensa total (clipped): 6.000, Pasos: 351, Mean Reward Calculado: 0.017094 (Recompensa/Pasos)\n",
            "  752970/2000000: episode: 1049, duration: 14.062s, episode steps: 351, steps per second:  25, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.014092, mae: 1.583024, mean_q: 1.925892, mean_eps: 0.100000\n",
            "📈 Episodio 1050: Recompensa total (clipped): 22.000, Pasos: 923, Mean Reward Calculado: 0.023835 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1050, pasos: 753893)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.73 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1050 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 22.00\n",
            "   Media últimos 100: 16.73 / 20.0\n",
            "   Mejor promedio histórico: 16.73\n",
            "   Estado: 📈 83.7% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  753893/2000000: episode: 1050, duration: 203.552s, episode steps: 923, steps per second:   5, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.014336, mae: 1.585296, mean_q: 1.925128, mean_eps: 0.100000\n",
            "📈 Episodio 1051: Recompensa total (clipped): 15.000, Pasos: 664, Mean Reward Calculado: 0.022590 (Recompensa/Pasos)\n",
            "  754557/2000000: episode: 1051, duration: 26.204s, episode steps: 664, steps per second:  25, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.013924, mae: 1.579961, mean_q: 1.918342, mean_eps: 0.100000\n",
            "📈 Episodio 1052: Recompensa total (clipped): 20.000, Pasos: 907, Mean Reward Calculado: 0.022051 (Recompensa/Pasos)\n",
            "  755464/2000000: episode: 1052, duration: 36.479s, episode steps: 907, steps per second:  25, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.015197, mae: 1.596538, mean_q: 1.938652, mean_eps: 0.100000\n",
            "📈 Episodio 1053: Recompensa total (clipped): 11.000, Pasos: 640, Mean Reward Calculado: 0.017188 (Recompensa/Pasos)\n",
            "  756104/2000000: episode: 1053, duration: 26.105s, episode steps: 640, steps per second:  25, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.015582, mae: 1.588161, mean_q: 1.927185, mean_eps: 0.100000\n",
            "📈 Episodio 1054: Recompensa total (clipped): 21.000, Pasos: 872, Mean Reward Calculado: 0.024083 (Recompensa/Pasos)\n",
            "  756976/2000000: episode: 1054, duration: 35.239s, episode steps: 872, steps per second:  25, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.015048, mae: 1.587175, mean_q: 1.927401, mean_eps: 0.100000\n",
            "📈 Episodio 1055: Recompensa total (clipped): 21.000, Pasos: 995, Mean Reward Calculado: 0.021106 (Recompensa/Pasos)\n",
            "  757971/2000000: episode: 1055, duration: 39.616s, episode steps: 995, steps per second:  25, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.014511, mae: 1.593290, mean_q: 1.935427, mean_eps: 0.100000\n",
            "📈 Episodio 1056: Recompensa total (clipped): 8.000, Pasos: 521, Mean Reward Calculado: 0.015355 (Recompensa/Pasos)\n",
            "  758492/2000000: episode: 1056, duration: 21.298s, episode steps: 521, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014435, mae: 1.602209, mean_q: 1.946429, mean_eps: 0.100000\n",
            "📈 Episodio 1057: Recompensa total (clipped): 12.000, Pasos: 728, Mean Reward Calculado: 0.016484 (Recompensa/Pasos)\n",
            "  759220/2000000: episode: 1057, duration: 28.949s, episode steps: 728, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.014561, mae: 1.594299, mean_q: 1.936305, mean_eps: 0.100000\n",
            "📊 Paso 760,000/2,000,000 (38.0%) - 31.0 pasos/seg - ETA: 11.1h - Memoria: 10623.14 MB\n",
            "📈 Episodio 1058: Recompensa total (clipped): 14.000, Pasos: 785, Mean Reward Calculado: 0.017834 (Recompensa/Pasos)\n",
            "  760005/2000000: episode: 1058, duration: 31.250s, episode steps: 785, steps per second:  25, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015440, mae: 1.599936, mean_q: 1.940243, mean_eps: 0.100000\n",
            "📈 Episodio 1059: Recompensa total (clipped): 13.000, Pasos: 625, Mean Reward Calculado: 0.020800 (Recompensa/Pasos)\n",
            "  760630/2000000: episode: 1059, duration: 24.741s, episode steps: 625, steps per second:  25, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.864 [0.000, 5.000],  loss: 0.012748, mae: 1.601009, mean_q: 1.944478, mean_eps: 0.100000\n",
            "📈 Episodio 1060: Recompensa total (clipped): 8.000, Pasos: 492, Mean Reward Calculado: 0.016260 (Recompensa/Pasos)\n",
            "  761122/2000000: episode: 1060, duration: 19.386s, episode steps: 492, steps per second:  25, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012812, mae: 1.597093, mean_q: 1.940119, mean_eps: 0.100000\n",
            "📈 Episodio 1061: Recompensa total (clipped): 10.000, Pasos: 610, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            "  761732/2000000: episode: 1061, duration: 24.188s, episode steps: 610, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.015577, mae: 1.598644, mean_q: 1.939661, mean_eps: 0.100000\n",
            "📈 Episodio 1062: Recompensa total (clipped): 20.000, Pasos: 801, Mean Reward Calculado: 0.024969 (Recompensa/Pasos)\n",
            "  762533/2000000: episode: 1062, duration: 31.925s, episode steps: 801, steps per second:  25, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.015194, mae: 1.594499, mean_q: 1.937937, mean_eps: 0.100000\n",
            "📈 Episodio 1063: Recompensa total (clipped): 24.000, Pasos: 1001, Mean Reward Calculado: 0.023976 (Recompensa/Pasos)\n",
            "  763534/2000000: episode: 1063, duration: 40.246s, episode steps: 1001, steps per second:  25, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.015133, mae: 1.595894, mean_q: 1.936170, mean_eps: 0.100000\n",
            "📈 Episodio 1064: Recompensa total (clipped): 24.000, Pasos: 1028, Mean Reward Calculado: 0.023346 (Recompensa/Pasos)\n",
            "  764562/2000000: episode: 1064, duration: 40.846s, episode steps: 1028, steps per second:  25, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.016264, mae: 1.606980, mean_q: 1.950501, mean_eps: 0.100000\n",
            "📈 Episodio 1065: Recompensa total (clipped): 18.000, Pasos: 755, Mean Reward Calculado: 0.023841 (Recompensa/Pasos)\n",
            "  765317/2000000: episode: 1065, duration: 30.195s, episode steps: 755, steps per second:  25, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.014509, mae: 1.594159, mean_q: 1.933724, mean_eps: 0.100000\n",
            "📈 Episodio 1066: Recompensa total (clipped): 30.000, Pasos: 1112, Mean Reward Calculado: 0.026978 (Recompensa/Pasos)\n",
            "  766429/2000000: episode: 1066, duration: 44.191s, episode steps: 1112, steps per second:  25, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.014160, mae: 1.590738, mean_q: 1.930339, mean_eps: 0.100000\n",
            "📈 Episodio 1067: Recompensa total (clipped): 12.000, Pasos: 699, Mean Reward Calculado: 0.017167 (Recompensa/Pasos)\n",
            "  767128/2000000: episode: 1067, duration: 27.516s, episode steps: 699, steps per second:  25, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.014328, mae: 1.592345, mean_q: 1.935394, mean_eps: 0.100000\n",
            "📈 Episodio 1068: Recompensa total (clipped): 18.000, Pasos: 794, Mean Reward Calculado: 0.022670 (Recompensa/Pasos)\n",
            "  767922/2000000: episode: 1068, duration: 31.646s, episode steps: 794, steps per second:  25, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014819, mae: 1.592220, mean_q: 1.931655, mean_eps: 0.100000\n",
            "📈 Episodio 1069: Recompensa total (clipped): 12.000, Pasos: 653, Mean Reward Calculado: 0.018377 (Recompensa/Pasos)\n",
            "  768575/2000000: episode: 1069, duration: 25.884s, episode steps: 653, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014589, mae: 1.578951, mean_q: 1.916649, mean_eps: 0.100000\n",
            "📈 Episodio 1070: Recompensa total (clipped): 16.000, Pasos: 762, Mean Reward Calculado: 0.020997 (Recompensa/Pasos)\n",
            "  769337/2000000: episode: 1070, duration: 30.253s, episode steps: 762, steps per second:  25, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.015736, mae: 1.599352, mean_q: 1.941006, mean_eps: 0.100000\n",
            "📈 Episodio 1071: Recompensa total (clipped): 19.000, Pasos: 880, Mean Reward Calculado: 0.021591 (Recompensa/Pasos)\n",
            "  770217/2000000: episode: 1071, duration: 35.051s, episode steps: 880, steps per second:  25, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.014877, mae: 1.600520, mean_q: 1.942118, mean_eps: 0.100000\n",
            "📈 Episodio 1072: Recompensa total (clipped): 14.000, Pasos: 654, Mean Reward Calculado: 0.021407 (Recompensa/Pasos)\n",
            "  770871/2000000: episode: 1072, duration: 25.888s, episode steps: 654, steps per second:  25, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.013832, mae: 1.617000, mean_q: 1.962447, mean_eps: 0.100000\n",
            "📈 Episodio 1073: Recompensa total (clipped): 23.000, Pasos: 989, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            "  771860/2000000: episode: 1073, duration: 39.600s, episode steps: 989, steps per second:  25, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.015101, mae: 1.616692, mean_q: 1.961763, mean_eps: 0.100000\n",
            "📈 Episodio 1074: Recompensa total (clipped): 9.000, Pasos: 634, Mean Reward Calculado: 0.014196 (Recompensa/Pasos)\n",
            "  772494/2000000: episode: 1074, duration: 25.241s, episode steps: 634, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.795 [0.000, 5.000],  loss: 0.015356, mae: 1.620429, mean_q: 1.965497, mean_eps: 0.100000\n",
            "📈 Episodio 1075: Recompensa total (clipped): 17.000, Pasos: 771, Mean Reward Calculado: 0.022049 (Recompensa/Pasos)\n",
            "  773265/2000000: episode: 1075, duration: 31.165s, episode steps: 771, steps per second:  25, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014795, mae: 1.624692, mean_q: 1.972390, mean_eps: 0.100000\n",
            "📈 Episodio 1076: Recompensa total (clipped): 26.000, Pasos: 1080, Mean Reward Calculado: 0.024074 (Recompensa/Pasos)\n",
            "  774345/2000000: episode: 1076, duration: 43.625s, episode steps: 1080, steps per second:  25, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015347, mae: 1.624176, mean_q: 1.970270, mean_eps: 0.100000\n",
            "📈 Episodio 1077: Recompensa total (clipped): 21.000, Pasos: 945, Mean Reward Calculado: 0.022222 (Recompensa/Pasos)\n",
            "  775290/2000000: episode: 1077, duration: 37.989s, episode steps: 945, steps per second:  25, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014887, mae: 1.628471, mean_q: 1.979403, mean_eps: 0.100000\n",
            "📈 Episodio 1078: Recompensa total (clipped): 21.000, Pasos: 866, Mean Reward Calculado: 0.024249 (Recompensa/Pasos)\n",
            "  776156/2000000: episode: 1078, duration: 34.731s, episode steps: 866, steps per second:  25, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.016355, mae: 1.631996, mean_q: 1.977603, mean_eps: 0.100000\n",
            "📈 Episodio 1079: Recompensa total (clipped): 27.000, Pasos: 1293, Mean Reward Calculado: 0.020882 (Recompensa/Pasos)\n",
            "  777449/2000000: episode: 1079, duration: 51.793s, episode steps: 1293, steps per second:  25, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.014655, mae: 1.625472, mean_q: 1.971576, mean_eps: 0.100000\n",
            "📈 Episodio 1080: Recompensa total (clipped): 18.000, Pasos: 730, Mean Reward Calculado: 0.024658 (Recompensa/Pasos)\n",
            "  778179/2000000: episode: 1080, duration: 29.171s, episode steps: 730, steps per second:  25, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.015931, mae: 1.626293, mean_q: 1.973438, mean_eps: 0.100000\n",
            "📈 Episodio 1081: Recompensa total (clipped): 28.000, Pasos: 1026, Mean Reward Calculado: 0.027290 (Recompensa/Pasos)\n",
            "  779205/2000000: episode: 1081, duration: 41.129s, episode steps: 1026, steps per second:  25, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.015012, mae: 1.614187, mean_q: 1.958121, mean_eps: 0.100000\n",
            "📊 Paso 780,000/2,000,000 (39.0%) - 30.8 pasos/seg - ETA: 11.0h - Memoria: 10574.46 MB\n",
            "📈 Episodio 1082: Recompensa total (clipped): 21.000, Pasos: 895, Mean Reward Calculado: 0.023464 (Recompensa/Pasos)\n",
            "  780100/2000000: episode: 1082, duration: 35.641s, episode steps: 895, steps per second:  25, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014466, mae: 1.631683, mean_q: 1.978865, mean_eps: 0.100000\n",
            "📈 Episodio 1083: Recompensa total (clipped): 14.000, Pasos: 646, Mean Reward Calculado: 0.021672 (Recompensa/Pasos)\n",
            "  780746/2000000: episode: 1083, duration: 25.795s, episode steps: 646, steps per second:  25, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.014045, mae: 1.632612, mean_q: 1.980634, mean_eps: 0.100000\n",
            "📈 Episodio 1084: Recompensa total (clipped): 10.000, Pasos: 621, Mean Reward Calculado: 0.016103 (Recompensa/Pasos)\n",
            "  781367/2000000: episode: 1084, duration: 24.584s, episode steps: 621, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.014963, mae: 1.614868, mean_q: 1.960247, mean_eps: 0.100000\n",
            "📈 Episodio 1085: Recompensa total (clipped): 30.000, Pasos: 1060, Mean Reward Calculado: 0.028302 (Recompensa/Pasos)\n",
            "  782427/2000000: episode: 1085, duration: 42.569s, episode steps: 1060, steps per second:  25, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.014505, mae: 1.639772, mean_q: 1.989305, mean_eps: 0.100000\n",
            "📈 Episodio 1086: Recompensa total (clipped): 10.000, Pasos: 619, Mean Reward Calculado: 0.016155 (Recompensa/Pasos)\n",
            "  783046/2000000: episode: 1086, duration: 24.702s, episode steps: 619, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.014456, mae: 1.619780, mean_q: 1.963597, mean_eps: 0.100000\n",
            "📈 Episodio 1087: Recompensa total (clipped): 23.000, Pasos: 889, Mean Reward Calculado: 0.025872 (Recompensa/Pasos)\n",
            "  783935/2000000: episode: 1087, duration: 35.980s, episode steps: 889, steps per second:  25, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.207 [0.000, 5.000],  loss: 0.014421, mae: 1.619110, mean_q: 1.964621, mean_eps: 0.100000\n",
            "📈 Episodio 1088: Recompensa total (clipped): 30.000, Pasos: 1289, Mean Reward Calculado: 0.023274 (Recompensa/Pasos)\n",
            "  785224/2000000: episode: 1088, duration: 52.011s, episode steps: 1289, steps per second:  25, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.014471, mae: 1.636031, mean_q: 1.983528, mean_eps: 0.100000\n",
            "📈 Episodio 1089: Recompensa total (clipped): 11.000, Pasos: 621, Mean Reward Calculado: 0.017713 (Recompensa/Pasos)\n",
            "  785845/2000000: episode: 1089, duration: 25.274s, episode steps: 621, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.015645, mae: 1.611701, mean_q: 1.957990, mean_eps: 0.100000\n",
            "📈 Episodio 1090: Recompensa total (clipped): 27.000, Pasos: 989, Mean Reward Calculado: 0.027300 (Recompensa/Pasos)\n",
            "  786834/2000000: episode: 1090, duration: 39.704s, episode steps: 989, steps per second:  25, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.012794, mae: 1.623096, mean_q: 1.968474, mean_eps: 0.100000\n",
            "📈 Episodio 1091: Recompensa total (clipped): 7.000, Pasos: 503, Mean Reward Calculado: 0.013917 (Recompensa/Pasos)\n",
            "  787337/2000000: episode: 1091, duration: 20.314s, episode steps: 503, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.072 [0.000, 5.000],  loss: 0.015921, mae: 1.640540, mean_q: 1.987601, mean_eps: 0.100000\n",
            "📈 Episodio 1092: Recompensa total (clipped): 29.000, Pasos: 1051, Mean Reward Calculado: 0.027593 (Recompensa/Pasos)\n",
            "  788388/2000000: episode: 1092, duration: 42.551s, episode steps: 1051, steps per second:  25, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.015281, mae: 1.633089, mean_q: 1.979568, mean_eps: 0.100000\n",
            "📈 Episodio 1093: Recompensa total (clipped): 16.000, Pasos: 738, Mean Reward Calculado: 0.021680 (Recompensa/Pasos)\n",
            "  789126/2000000: episode: 1093, duration: 29.945s, episode steps: 738, steps per second:  25, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014905, mae: 1.635916, mean_q: 1.986047, mean_eps: 0.100000\n",
            "📈 Episodio 1094: Recompensa total (clipped): 10.000, Pasos: 641, Mean Reward Calculado: 0.015601 (Recompensa/Pasos)\n",
            "  789767/2000000: episode: 1094, duration: 25.832s, episode steps: 641, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014330, mae: 1.627688, mean_q: 1.971836, mean_eps: 0.100000\n",
            "📈 Episodio 1095: Recompensa total (clipped): 14.000, Pasos: 798, Mean Reward Calculado: 0.017544 (Recompensa/Pasos)\n",
            "  790565/2000000: episode: 1095, duration: 32.291s, episode steps: 798, steps per second:  25, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.015299, mae: 1.654925, mean_q: 2.008118, mean_eps: 0.100000\n",
            "📈 Episodio 1096: Recompensa total (clipped): 19.000, Pasos: 856, Mean Reward Calculado: 0.022196 (Recompensa/Pasos)\n",
            "  791421/2000000: episode: 1096, duration: 34.421s, episode steps: 856, steps per second:  25, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.016589, mae: 1.671339, mean_q: 2.027119, mean_eps: 0.100000\n",
            "📈 Episodio 1097: Recompensa total (clipped): 13.000, Pasos: 633, Mean Reward Calculado: 0.020537 (Recompensa/Pasos)\n",
            "  792054/2000000: episode: 1097, duration: 25.293s, episode steps: 633, steps per second:  25, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014467, mae: 1.665064, mean_q: 2.020506, mean_eps: 0.100000\n",
            "📈 Episodio 1098: Recompensa total (clipped): 21.000, Pasos: 768, Mean Reward Calculado: 0.027344 (Recompensa/Pasos)\n",
            "  792822/2000000: episode: 1098, duration: 30.998s, episode steps: 768, steps per second:  25, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.013846, mae: 1.653129, mean_q: 2.005603, mean_eps: 0.100000\n",
            "📈 Episodio 1099: Recompensa total (clipped): 20.000, Pasos: 756, Mean Reward Calculado: 0.026455 (Recompensa/Pasos)\n",
            "  793578/2000000: episode: 1099, duration: 30.641s, episode steps: 756, steps per second:  25, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.014229, mae: 1.652046, mean_q: 2.005435, mean_eps: 0.100000\n",
            "📈 Episodio 1100: Recompensa total (clipped): 22.000, Pasos: 964, Mean Reward Calculado: 0.022822 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1100, pasos: 794542)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.47 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1100 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 22.00\n",
            "   Media últimos 100: 17.47 / 20.0\n",
            "   Mejor promedio histórico: 17.47\n",
            "   Estado: 📈 87.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  794542/2000000: episode: 1100, duration: 96.643s, episode steps: 964, steps per second:  10, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.015149, mae: 1.644631, mean_q: 1.994289, mean_eps: 0.100000\n",
            "📈 Episodio 1101: Recompensa total (clipped): 13.000, Pasos: 679, Mean Reward Calculado: 0.019146 (Recompensa/Pasos)\n",
            "  795221/2000000: episode: 1101, duration: 28.031s, episode steps: 679, steps per second:  24, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015483, mae: 1.651779, mean_q: 2.002391, mean_eps: 0.100000\n",
            "📈 Episodio 1102: Recompensa total (clipped): 24.000, Pasos: 956, Mean Reward Calculado: 0.025105 (Recompensa/Pasos)\n",
            "  796177/2000000: episode: 1102, duration: 38.386s, episode steps: 956, steps per second:  25, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.015223, mae: 1.655904, mean_q: 2.009400, mean_eps: 0.100000\n",
            "📈 Episodio 1103: Recompensa total (clipped): 21.000, Pasos: 1023, Mean Reward Calculado: 0.020528 (Recompensa/Pasos)\n",
            "  797200/2000000: episode: 1103, duration: 41.435s, episode steps: 1023, steps per second:  25, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.014974, mae: 1.642171, mean_q: 1.992737, mean_eps: 0.100000\n",
            "📈 Episodio 1104: Recompensa total (clipped): 15.000, Pasos: 679, Mean Reward Calculado: 0.022091 (Recompensa/Pasos)\n",
            "  797879/2000000: episode: 1104, duration: 27.740s, episode steps: 679, steps per second:  24, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.013774, mae: 1.657490, mean_q: 2.009676, mean_eps: 0.100000\n",
            "📈 Episodio 1105: Recompensa total (clipped): 14.000, Pasos: 611, Mean Reward Calculado: 0.022913 (Recompensa/Pasos)\n",
            "  798490/2000000: episode: 1105, duration: 25.076s, episode steps: 611, steps per second:  24, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016653, mae: 1.661027, mean_q: 2.012504, mean_eps: 0.100000\n",
            "📈 Episodio 1106: Recompensa total (clipped): 14.000, Pasos: 657, Mean Reward Calculado: 0.021309 (Recompensa/Pasos)\n",
            "  799147/2000000: episode: 1106, duration: 26.556s, episode steps: 657, steps per second:  25, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014539, mae: 1.642224, mean_q: 1.994201, mean_eps: 0.100000\n",
            "📊 Paso 800,000/2,000,000 (40.0%) - 30.5 pasos/seg - ETA: 10.9h - Memoria: 10626.46 MB\n",
            "📈 Episodio 1107: Recompensa total (clipped): 26.000, Pasos: 908, Mean Reward Calculado: 0.028634 (Recompensa/Pasos)\n",
            "  800055/2000000: episode: 1107, duration: 36.670s, episode steps: 908, steps per second:  25, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.015481, mae: 1.650407, mean_q: 2.001673, mean_eps: 0.100000\n",
            "📈 Episodio 1108: Recompensa total (clipped): 21.000, Pasos: 1088, Mean Reward Calculado: 0.019301 (Recompensa/Pasos)\n",
            "  801143/2000000: episode: 1108, duration: 43.868s, episode steps: 1088, steps per second:  25, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.014115, mae: 1.653908, mean_q: 2.007713, mean_eps: 0.100000\n",
            "📈 Episodio 1109: Recompensa total (clipped): 24.000, Pasos: 999, Mean Reward Calculado: 0.024024 (Recompensa/Pasos)\n",
            "  802142/2000000: episode: 1109, duration: 41.573s, episode steps: 999, steps per second:  24, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014626, mae: 1.638202, mean_q: 1.987189, mean_eps: 0.100000\n",
            "📈 Episodio 1110: Recompensa total (clipped): 17.000, Pasos: 742, Mean Reward Calculado: 0.022911 (Recompensa/Pasos)\n",
            "  802884/2000000: episode: 1110, duration: 30.346s, episode steps: 742, steps per second:  24, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014747, mae: 1.648069, mean_q: 1.999016, mean_eps: 0.100000\n",
            "📈 Episodio 1111: Recompensa total (clipped): 13.000, Pasos: 552, Mean Reward Calculado: 0.023551 (Recompensa/Pasos)\n",
            "  803436/2000000: episode: 1111, duration: 22.599s, episode steps: 552, steps per second:  24, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.016130, mae: 1.618322, mean_q: 1.963210, mean_eps: 0.100000\n",
            "📈 Episodio 1112: Recompensa total (clipped): 22.000, Pasos: 893, Mean Reward Calculado: 0.024636 (Recompensa/Pasos)\n",
            "  804329/2000000: episode: 1112, duration: 36.465s, episode steps: 893, steps per second:  24, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.155 [0.000, 5.000],  loss: 0.013720, mae: 1.642169, mean_q: 1.994079, mean_eps: 0.100000\n",
            "📈 Episodio 1113: Recompensa total (clipped): 24.000, Pasos: 913, Mean Reward Calculado: 0.026287 (Recompensa/Pasos)\n",
            "  805242/2000000: episode: 1113, duration: 37.973s, episode steps: 913, steps per second:  24, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015110, mae: 1.638398, mean_q: 1.986052, mean_eps: 0.100000\n",
            "📈 Episodio 1114: Recompensa total (clipped): 26.000, Pasos: 985, Mean Reward Calculado: 0.026396 (Recompensa/Pasos)\n",
            "  806227/2000000: episode: 1114, duration: 40.405s, episode steps: 985, steps per second:  24, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.013819, mae: 1.637188, mean_q: 1.986377, mean_eps: 0.100000\n",
            "📈 Episodio 1115: Recompensa total (clipped): 9.000, Pasos: 637, Mean Reward Calculado: 0.014129 (Recompensa/Pasos)\n",
            "  806864/2000000: episode: 1115, duration: 26.292s, episode steps: 637, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.013398, mae: 1.633741, mean_q: 1.980647, mean_eps: 0.100000\n",
            "📈 Episodio 1116: Recompensa total (clipped): 16.000, Pasos: 833, Mean Reward Calculado: 0.019208 (Recompensa/Pasos)\n",
            "  807697/2000000: episode: 1116, duration: 34.186s, episode steps: 833, steps per second:  24, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.014814, mae: 1.657161, mean_q: 2.010859, mean_eps: 0.100000\n",
            "📈 Episodio 1117: Recompensa total (clipped): 8.000, Pasos: 476, Mean Reward Calculado: 0.016807 (Recompensa/Pasos)\n",
            "  808173/2000000: episode: 1117, duration: 19.282s, episode steps: 476, steps per second:  25, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.016062, mae: 1.641891, mean_q: 1.994955, mean_eps: 0.100000\n",
            "📈 Episodio 1118: Recompensa total (clipped): 26.000, Pasos: 1137, Mean Reward Calculado: 0.022867 (Recompensa/Pasos)\n",
            "  809310/2000000: episode: 1118, duration: 46.069s, episode steps: 1137, steps per second:  25, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.014749, mae: 1.647030, mean_q: 2.001459, mean_eps: 0.100000\n",
            "📈 Episodio 1119: Recompensa total (clipped): 14.000, Pasos: 518, Mean Reward Calculado: 0.027027 (Recompensa/Pasos)\n",
            "  809828/2000000: episode: 1119, duration: 21.123s, episode steps: 518, steps per second:  25, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.131 [0.000, 5.000],  loss: 0.016365, mae: 1.638028, mean_q: 1.990222, mean_eps: 0.100000\n",
            "📈 Episodio 1120: Recompensa total (clipped): 15.000, Pasos: 775, Mean Reward Calculado: 0.019355 (Recompensa/Pasos)\n",
            "  810603/2000000: episode: 1120, duration: 31.330s, episode steps: 775, steps per second:  25, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.013708, mae: 1.656958, mean_q: 2.012149, mean_eps: 0.100000\n",
            "📈 Episodio 1121: Recompensa total (clipped): 20.000, Pasos: 830, Mean Reward Calculado: 0.024096 (Recompensa/Pasos)\n",
            "  811433/2000000: episode: 1121, duration: 33.877s, episode steps: 830, steps per second:  25, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.014706, mae: 1.670635, mean_q: 2.027047, mean_eps: 0.100000\n",
            "📈 Episodio 1122: Recompensa total (clipped): 28.000, Pasos: 1106, Mean Reward Calculado: 0.025316 (Recompensa/Pasos)\n",
            "  812539/2000000: episode: 1122, duration: 44.922s, episode steps: 1106, steps per second:  25, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.015735, mae: 1.664658, mean_q: 2.020532, mean_eps: 0.100000\n",
            "📈 Episodio 1123: Recompensa total (clipped): 19.000, Pasos: 740, Mean Reward Calculado: 0.025676 (Recompensa/Pasos)\n",
            "  813279/2000000: episode: 1123, duration: 30.036s, episode steps: 740, steps per second:  25, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.014758, mae: 1.655415, mean_q: 2.009429, mean_eps: 0.100000\n",
            "📈 Episodio 1124: Recompensa total (clipped): 24.000, Pasos: 880, Mean Reward Calculado: 0.027273 (Recompensa/Pasos)\n",
            "  814159/2000000: episode: 1124, duration: 35.599s, episode steps: 880, steps per second:  25, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.015032, mae: 1.654063, mean_q: 2.008756, mean_eps: 0.100000\n",
            "📈 Episodio 1125: Recompensa total (clipped): 20.000, Pasos: 850, Mean Reward Calculado: 0.023529 (Recompensa/Pasos)\n",
            "  815009/2000000: episode: 1125, duration: 34.703s, episode steps: 850, steps per second:  24, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.015145, mae: 1.654742, mean_q: 2.007748, mean_eps: 0.100000\n",
            "📈 Episodio 1126: Recompensa total (clipped): 15.000, Pasos: 788, Mean Reward Calculado: 0.019036 (Recompensa/Pasos)\n",
            "  815797/2000000: episode: 1126, duration: 32.309s, episode steps: 788, steps per second:  24, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.014833, mae: 1.639260, mean_q: 1.989768, mean_eps: 0.100000\n",
            "📈 Episodio 1127: Recompensa total (clipped): 25.000, Pasos: 1019, Mean Reward Calculado: 0.024534 (Recompensa/Pasos)\n",
            "  816816/2000000: episode: 1127, duration: 42.036s, episode steps: 1019, steps per second:  24, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.015165, mae: 1.651249, mean_q: 2.004249, mean_eps: 0.100000\n",
            "📈 Episodio 1128: Recompensa total (clipped): 18.000, Pasos: 1128, Mean Reward Calculado: 0.015957 (Recompensa/Pasos)\n",
            "  817944/2000000: episode: 1128, duration: 46.518s, episode steps: 1128, steps per second:  24, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014451, mae: 1.649715, mean_q: 2.003463, mean_eps: 0.100000\n",
            "📈 Episodio 1129: Recompensa total (clipped): 15.000, Pasos: 683, Mean Reward Calculado: 0.021962 (Recompensa/Pasos)\n",
            "  818627/2000000: episode: 1129, duration: 28.346s, episode steps: 683, steps per second:  24, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.063 [0.000, 5.000],  loss: 0.015664, mae: 1.639767, mean_q: 1.990363, mean_eps: 0.100000\n",
            "📈 Episodio 1130: Recompensa total (clipped): 10.000, Pasos: 464, Mean Reward Calculado: 0.021552 (Recompensa/Pasos)\n",
            "  819091/2000000: episode: 1130, duration: 19.099s, episode steps: 464, steps per second:  24, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.015702, mae: 1.641557, mean_q: 1.990652, mean_eps: 0.100000\n",
            "📈 Episodio 1131: Recompensa total (clipped): 13.000, Pasos: 625, Mean Reward Calculado: 0.020800 (Recompensa/Pasos)\n",
            "  819716/2000000: episode: 1131, duration: 25.629s, episode steps: 625, steps per second:  24, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.016366, mae: 1.648605, mean_q: 1.999466, mean_eps: 0.100000\n",
            "📊 Paso 820,000/2,000,000 (41.0%) - 30.3 pasos/seg - ETA: 10.8h - Memoria: 10621.93 MB\n",
            "📈 Episodio 1132: Recompensa total (clipped): 15.000, Pasos: 816, Mean Reward Calculado: 0.018382 (Recompensa/Pasos)\n",
            "  820532/2000000: episode: 1132, duration: 33.687s, episode steps: 816, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.015389, mae: 1.680045, mean_q: 2.039835, mean_eps: 0.100000\n",
            "📈 Episodio 1133: Recompensa total (clipped): 13.000, Pasos: 612, Mean Reward Calculado: 0.021242 (Recompensa/Pasos)\n",
            "  821144/2000000: episode: 1133, duration: 24.935s, episode steps: 612, steps per second:  25, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.015384, mae: 1.683942, mean_q: 2.044083, mean_eps: 0.100000\n",
            "📈 Episodio 1134: Recompensa total (clipped): 24.000, Pasos: 1006, Mean Reward Calculado: 0.023857 (Recompensa/Pasos)\n",
            "  822150/2000000: episode: 1134, duration: 41.242s, episode steps: 1006, steps per second:  24, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.016048, mae: 1.693095, mean_q: 2.054871, mean_eps: 0.100000\n",
            "📈 Episodio 1135: Recompensa total (clipped): 15.000, Pasos: 661, Mean Reward Calculado: 0.022693 (Recompensa/Pasos)\n",
            "  822811/2000000: episode: 1135, duration: 26.848s, episode steps: 661, steps per second:  25, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.015356, mae: 1.701847, mean_q: 2.066457, mean_eps: 0.100000\n",
            "📈 Episodio 1136: Recompensa total (clipped): 14.000, Pasos: 678, Mean Reward Calculado: 0.020649 (Recompensa/Pasos)\n",
            "  823489/2000000: episode: 1136, duration: 27.923s, episode steps: 678, steps per second:  24, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.016697, mae: 1.695757, mean_q: 2.059190, mean_eps: 0.100000\n",
            "📈 Episodio 1137: Recompensa total (clipped): 18.000, Pasos: 774, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            "  824263/2000000: episode: 1137, duration: 32.017s, episode steps: 774, steps per second:  24, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.016102, mae: 1.699052, mean_q: 2.060709, mean_eps: 0.100000\n",
            "📈 Episodio 1138: Recompensa total (clipped): 29.000, Pasos: 1408, Mean Reward Calculado: 0.020597 (Recompensa/Pasos)\n",
            "  825671/2000000: episode: 1138, duration: 58.528s, episode steps: 1408, steps per second:  24, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.016249, mae: 1.680585, mean_q: 2.037704, mean_eps: 0.100000\n",
            "📈 Episodio 1139: Recompensa total (clipped): 28.000, Pasos: 944, Mean Reward Calculado: 0.029661 (Recompensa/Pasos)\n",
            "  826615/2000000: episode: 1139, duration: 39.360s, episode steps: 944, steps per second:  24, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.017328, mae: 1.685098, mean_q: 2.045098, mean_eps: 0.100000\n",
            "📈 Episodio 1140: Recompensa total (clipped): 14.000, Pasos: 775, Mean Reward Calculado: 0.018065 (Recompensa/Pasos)\n",
            "  827390/2000000: episode: 1140, duration: 32.044s, episode steps: 775, steps per second:  24, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.016121, mae: 1.702363, mean_q: 2.063720, mean_eps: 0.100000\n",
            "📈 Episodio 1141: Recompensa total (clipped): 16.000, Pasos: 791, Mean Reward Calculado: 0.020228 (Recompensa/Pasos)\n",
            "  828181/2000000: episode: 1141, duration: 32.950s, episode steps: 791, steps per second:  24, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015342, mae: 1.691197, mean_q: 2.051888, mean_eps: 0.100000\n",
            "📈 Episodio 1142: Recompensa total (clipped): 24.000, Pasos: 1094, Mean Reward Calculado: 0.021938 (Recompensa/Pasos)\n",
            "  829275/2000000: episode: 1142, duration: 44.928s, episode steps: 1094, steps per second:  24, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.016573, mae: 1.693026, mean_q: 2.057041, mean_eps: 0.100000\n",
            "📈 Episodio 1143: Recompensa total (clipped): 13.000, Pasos: 648, Mean Reward Calculado: 0.020062 (Recompensa/Pasos)\n",
            "  829923/2000000: episode: 1143, duration: 26.679s, episode steps: 648, steps per second:  24, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.015664, mae: 1.676355, mean_q: 2.033778, mean_eps: 0.100000\n",
            "📈 Episodio 1144: Recompensa total (clipped): 21.000, Pasos: 892, Mean Reward Calculado: 0.023543 (Recompensa/Pasos)\n",
            "  830815/2000000: episode: 1144, duration: 36.631s, episode steps: 892, steps per second:  24, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.014556, mae: 1.718139, mean_q: 2.083765, mean_eps: 0.100000\n",
            "📈 Episodio 1145: Recompensa total (clipped): 19.000, Pasos: 1064, Mean Reward Calculado: 0.017857 (Recompensa/Pasos)\n",
            "  831879/2000000: episode: 1145, duration: 43.952s, episode steps: 1064, steps per second:  24, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.015519, mae: 1.720195, mean_q: 2.087745, mean_eps: 0.100000\n",
            "📈 Episodio 1146: Recompensa total (clipped): 16.000, Pasos: 650, Mean Reward Calculado: 0.024615 (Recompensa/Pasos)\n",
            "  832529/2000000: episode: 1146, duration: 26.890s, episode steps: 650, steps per second:  24, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.014900, mae: 1.721011, mean_q: 2.091077, mean_eps: 0.100000\n",
            "📈 Episodio 1147: Recompensa total (clipped): 21.000, Pasos: 865, Mean Reward Calculado: 0.024277 (Recompensa/Pasos)\n",
            "  833394/2000000: episode: 1147, duration: 35.276s, episode steps: 865, steps per second:  25, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.015725, mae: 1.720906, mean_q: 2.087464, mean_eps: 0.100000\n",
            "📈 Episodio 1148: Recompensa total (clipped): 19.000, Pasos: 838, Mean Reward Calculado: 0.022673 (Recompensa/Pasos)\n",
            "  834232/2000000: episode: 1148, duration: 34.677s, episode steps: 838, steps per second:  24, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.016885, mae: 1.723884, mean_q: 2.091122, mean_eps: 0.100000\n",
            "📈 Episodio 1149: Recompensa total (clipped): 19.000, Pasos: 861, Mean Reward Calculado: 0.022067 (Recompensa/Pasos)\n",
            "  835093/2000000: episode: 1149, duration: 35.980s, episode steps: 861, steps per second:  24, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.016112, mae: 1.713269, mean_q: 2.077308, mean_eps: 0.100000\n",
            "📈 Episodio 1150: Recompensa total (clipped): 9.000, Pasos: 441, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1150, pasos: 835534)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.13 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1150 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 9.00\n",
            "   Media últimos 100: 18.13 / 20.0\n",
            "   Mejor promedio histórico: 18.13\n",
            "   Estado: 📈 90.6% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  835534/2000000: episode: 1150, duration: 89.874s, episode steps: 441, steps per second:   5, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.015387, mae: 1.711568, mean_q: 2.075809, mean_eps: 0.100000\n",
            "📈 Episodio 1151: Recompensa total (clipped): 19.000, Pasos: 834, Mean Reward Calculado: 0.022782 (Recompensa/Pasos)\n",
            "  836368/2000000: episode: 1151, duration: 34.922s, episode steps: 834, steps per second:  24, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.015551, mae: 1.728259, mean_q: 2.097557, mean_eps: 0.100000\n",
            "📈 Episodio 1152: Recompensa total (clipped): 18.000, Pasos: 814, Mean Reward Calculado: 0.022113 (Recompensa/Pasos)\n",
            "  837182/2000000: episode: 1152, duration: 33.865s, episode steps: 814, steps per second:  24, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.016408, mae: 1.717133, mean_q: 2.081715, mean_eps: 0.100000\n",
            "📈 Episodio 1153: Recompensa total (clipped): 12.000, Pasos: 635, Mean Reward Calculado: 0.018898 (Recompensa/Pasos)\n",
            "  837817/2000000: episode: 1153, duration: 26.292s, episode steps: 635, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.015516, mae: 1.705164, mean_q: 2.066424, mean_eps: 0.100000\n",
            "📈 Episodio 1154: Recompensa total (clipped): 13.000, Pasos: 793, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            "  838610/2000000: episode: 1154, duration: 32.832s, episode steps: 793, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.014708, mae: 1.702541, mean_q: 2.061768, mean_eps: 0.100000\n",
            "📈 Episodio 1155: Recompensa total (clipped): 5.000, Pasos: 495, Mean Reward Calculado: 0.010101 (Recompensa/Pasos)\n",
            "  839105/2000000: episode: 1155, duration: 20.663s, episode steps: 495, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.015978, mae: 1.705992, mean_q: 2.068135, mean_eps: 0.100000\n",
            "📊 Paso 840,000/2,000,000 (42.0%) - 30.1 pasos/seg - ETA: 10.7h - Memoria: 10656.66 MB\n",
            "📈 Episodio 1156: Recompensa total (clipped): 32.000, Pasos: 1294, Mean Reward Calculado: 0.024730 (Recompensa/Pasos)\n",
            "  840399/2000000: episode: 1156, duration: 54.285s, episode steps: 1294, steps per second:  24, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.014434, mae: 1.717128, mean_q: 2.084964, mean_eps: 0.100000\n",
            "📈 Episodio 1157: Recompensa total (clipped): 24.000, Pasos: 1058, Mean Reward Calculado: 0.022684 (Recompensa/Pasos)\n",
            "  841457/2000000: episode: 1157, duration: 44.549s, episode steps: 1058, steps per second:  24, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015657, mae: 1.728037, mean_q: 2.099941, mean_eps: 0.100000\n",
            "📈 Episodio 1158: Recompensa total (clipped): 18.000, Pasos: 800, Mean Reward Calculado: 0.022500 (Recompensa/Pasos)\n",
            "  842257/2000000: episode: 1158, duration: 33.680s, episode steps: 800, steps per second:  24, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.016228, mae: 1.753734, mean_q: 2.130443, mean_eps: 0.100000\n",
            "📈 Episodio 1159: Recompensa total (clipped): 17.000, Pasos: 1044, Mean Reward Calculado: 0.016284 (Recompensa/Pasos)\n",
            "  843301/2000000: episode: 1159, duration: 43.541s, episode steps: 1044, steps per second:  24, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.015864, mae: 1.749974, mean_q: 2.124581, mean_eps: 0.100000\n",
            "📈 Episodio 1160: Recompensa total (clipped): 22.000, Pasos: 1001, Mean Reward Calculado: 0.021978 (Recompensa/Pasos)\n",
            "  844302/2000000: episode: 1160, duration: 42.041s, episode steps: 1001, steps per second:  24, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.968 [0.000, 5.000],  loss: 0.016021, mae: 1.754087, mean_q: 2.127644, mean_eps: 0.100000\n",
            "📈 Episodio 1161: Recompensa total (clipped): 20.000, Pasos: 877, Mean Reward Calculado: 0.022805 (Recompensa/Pasos)\n",
            "  845179/2000000: episode: 1161, duration: 36.830s, episode steps: 877, steps per second:  24, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.016436, mae: 1.747098, mean_q: 2.121083, mean_eps: 0.100000\n",
            "📈 Episodio 1162: Recompensa total (clipped): 24.000, Pasos: 1249, Mean Reward Calculado: 0.019215 (Recompensa/Pasos)\n",
            "  846428/2000000: episode: 1162, duration: 52.655s, episode steps: 1249, steps per second:  24, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.015410, mae: 1.746343, mean_q: 2.118419, mean_eps: 0.100000\n",
            "📈 Episodio 1163: Recompensa total (clipped): 21.000, Pasos: 934, Mean Reward Calculado: 0.022484 (Recompensa/Pasos)\n",
            "  847362/2000000: episode: 1163, duration: 39.089s, episode steps: 934, steps per second:  24, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.015689, mae: 1.749107, mean_q: 2.123634, mean_eps: 0.100000\n",
            "📈 Episodio 1164: Recompensa total (clipped): 12.000, Pasos: 702, Mean Reward Calculado: 0.017094 (Recompensa/Pasos)\n",
            "  848064/2000000: episode: 1164, duration: 29.562s, episode steps: 702, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.014330, mae: 1.727798, mean_q: 2.095019, mean_eps: 0.100000\n",
            "📈 Episodio 1165: Recompensa total (clipped): 17.000, Pasos: 790, Mean Reward Calculado: 0.021519 (Recompensa/Pasos)\n",
            "  848854/2000000: episode: 1165, duration: 32.641s, episode steps: 790, steps per second:  24, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.016541, mae: 1.733023, mean_q: 2.102376, mean_eps: 0.100000\n",
            "📈 Episodio 1166: Recompensa total (clipped): 23.000, Pasos: 660, Mean Reward Calculado: 0.034848 (Recompensa/Pasos)\n",
            "  849514/2000000: episode: 1166, duration: 27.580s, episode steps: 660, steps per second:  24, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.016418, mae: 1.731377, mean_q: 2.098496, mean_eps: 0.100000\n",
            "📈 Episodio 1167: Recompensa total (clipped): 22.000, Pasos: 1002, Mean Reward Calculado: 0.021956 (Recompensa/Pasos)\n",
            "  850516/2000000: episode: 1167, duration: 41.865s, episode steps: 1002, steps per second:  24, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.014609, mae: 1.737903, mean_q: 2.108811, mean_eps: 0.100000\n",
            "📈 Episodio 1168: Recompensa total (clipped): 27.000, Pasos: 1444, Mean Reward Calculado: 0.018698 (Recompensa/Pasos)\n",
            "  851960/2000000: episode: 1168, duration: 59.918s, episode steps: 1444, steps per second:  24, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.015234, mae: 1.743483, mean_q: 2.114894, mean_eps: 0.100000\n",
            "📈 Episodio 1169: Recompensa total (clipped): 14.000, Pasos: 660, Mean Reward Calculado: 0.021212 (Recompensa/Pasos)\n",
            "  852620/2000000: episode: 1169, duration: 27.524s, episode steps: 660, steps per second:  24, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016173, mae: 1.723568, mean_q: 2.087464, mean_eps: 0.100000\n",
            "📈 Episodio 1170: Recompensa total (clipped): 15.000, Pasos: 673, Mean Reward Calculado: 0.022288 (Recompensa/Pasos)\n",
            "  853293/2000000: episode: 1170, duration: 28.281s, episode steps: 673, steps per second:  24, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.014546, mae: 1.726709, mean_q: 2.095352, mean_eps: 0.100000\n",
            "📈 Episodio 1171: Recompensa total (clipped): 16.000, Pasos: 818, Mean Reward Calculado: 0.019560 (Recompensa/Pasos)\n",
            "  854111/2000000: episode: 1171, duration: 33.624s, episode steps: 818, steps per second:  24, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.014875, mae: 1.743014, mean_q: 2.111482, mean_eps: 0.100000\n",
            "📈 Episodio 1172: Recompensa total (clipped): 15.000, Pasos: 879, Mean Reward Calculado: 0.017065 (Recompensa/Pasos)\n",
            "  854990/2000000: episode: 1172, duration: 36.566s, episode steps: 879, steps per second:  24, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.016216, mae: 1.753100, mean_q: 2.125456, mean_eps: 0.100000\n",
            "📈 Episodio 1173: Recompensa total (clipped): 23.000, Pasos: 1164, Mean Reward Calculado: 0.019759 (Recompensa/Pasos)\n",
            "  856154/2000000: episode: 1173, duration: 48.507s, episode steps: 1164, steps per second:  24, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.016342, mae: 1.745294, mean_q: 2.115490, mean_eps: 0.100000\n",
            "📈 Episodio 1174: Recompensa total (clipped): 22.000, Pasos: 963, Mean Reward Calculado: 0.022845 (Recompensa/Pasos)\n",
            "  857117/2000000: episode: 1174, duration: 39.776s, episode steps: 963, steps per second:  24, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.015910, mae: 1.763037, mean_q: 2.137027, mean_eps: 0.100000\n",
            "📈 Episodio 1175: Recompensa total (clipped): 23.000, Pasos: 994, Mean Reward Calculado: 0.023139 (Recompensa/Pasos)\n",
            "  858111/2000000: episode: 1175, duration: 41.641s, episode steps: 994, steps per second:  24, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.015898, mae: 1.744366, mean_q: 2.113536, mean_eps: 0.100000\n",
            "📈 Episodio 1176: Recompensa total (clipped): 19.000, Pasos: 899, Mean Reward Calculado: 0.021135 (Recompensa/Pasos)\n",
            "  859010/2000000: episode: 1176, duration: 38.218s, episode steps: 899, steps per second:  24, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015384, mae: 1.753747, mean_q: 2.126883, mean_eps: 0.100000\n",
            "📈 Episodio 1177: Recompensa total (clipped): 16.000, Pasos: 954, Mean Reward Calculado: 0.016771 (Recompensa/Pasos)\n",
            "  859964/2000000: episode: 1177, duration: 40.360s, episode steps: 954, steps per second:  24, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.015801, mae: 1.746617, mean_q: 2.118107, mean_eps: 0.100000\n",
            "📊 Paso 860,000/2,000,000 (43.0%) - 29.9 pasos/seg - ETA: 10.6h - Memoria: 10599.52 MB\n",
            "📈 Episodio 1178: Recompensa total (clipped): 25.000, Pasos: 974, Mean Reward Calculado: 0.025667 (Recompensa/Pasos)\n",
            "  860938/2000000: episode: 1178, duration: 40.832s, episode steps: 974, steps per second:  24, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.015412, mae: 1.729325, mean_q: 2.098435, mean_eps: 0.100000\n",
            "📈 Episodio 1179: Recompensa total (clipped): 24.000, Pasos: 1259, Mean Reward Calculado: 0.019063 (Recompensa/Pasos)\n",
            "  862197/2000000: episode: 1179, duration: 53.651s, episode steps: 1259, steps per second:  23, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015759, mae: 1.747819, mean_q: 2.122294, mean_eps: 0.100000\n",
            "📈 Episodio 1180: Recompensa total (clipped): 8.000, Pasos: 563, Mean Reward Calculado: 0.014210 (Recompensa/Pasos)\n",
            "  862760/2000000: episode: 1180, duration: 23.763s, episode steps: 563, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.016204, mae: 1.732168, mean_q: 2.104313, mean_eps: 0.100000\n",
            "📈 Episodio 1181: Recompensa total (clipped): 14.000, Pasos: 951, Mean Reward Calculado: 0.014721 (Recompensa/Pasos)\n",
            "  863711/2000000: episode: 1181, duration: 39.812s, episode steps: 951, steps per second:  24, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.015899, mae: 1.736201, mean_q: 2.106721, mean_eps: 0.100000\n",
            "📈 Episodio 1182: Recompensa total (clipped): 14.000, Pasos: 654, Mean Reward Calculado: 0.021407 (Recompensa/Pasos)\n",
            "  864365/2000000: episode: 1182, duration: 27.292s, episode steps: 654, steps per second:  24, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014762, mae: 1.734770, mean_q: 2.103234, mean_eps: 0.100000\n",
            "📈 Episodio 1183: Recompensa total (clipped): 18.000, Pasos: 797, Mean Reward Calculado: 0.022585 (Recompensa/Pasos)\n",
            "  865162/2000000: episode: 1183, duration: 33.259s, episode steps: 797, steps per second:  24, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.017372, mae: 1.757619, mean_q: 2.131918, mean_eps: 0.100000\n",
            "📈 Episodio 1184: Recompensa total (clipped): 23.000, Pasos: 1138, Mean Reward Calculado: 0.020211 (Recompensa/Pasos)\n",
            "  866300/2000000: episode: 1184, duration: 48.368s, episode steps: 1138, steps per second:  24, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.015987, mae: 1.735578, mean_q: 2.105525, mean_eps: 0.100000\n",
            "📈 Episodio 1185: Recompensa total (clipped): 23.000, Pasos: 940, Mean Reward Calculado: 0.024468 (Recompensa/Pasos)\n",
            "  867240/2000000: episode: 1185, duration: 39.726s, episode steps: 940, steps per second:  24, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.017796, mae: 1.755538, mean_q: 2.130123, mean_eps: 0.100000\n",
            "📈 Episodio 1186: Recompensa total (clipped): 28.000, Pasos: 998, Mean Reward Calculado: 0.028056 (Recompensa/Pasos)\n",
            "  868238/2000000: episode: 1186, duration: 42.600s, episode steps: 998, steps per second:  23, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.015527, mae: 1.750958, mean_q: 2.121275, mean_eps: 0.100000\n",
            "📈 Episodio 1187: Recompensa total (clipped): 7.000, Pasos: 447, Mean Reward Calculado: 0.015660 (Recompensa/Pasos)\n",
            "  868685/2000000: episode: 1187, duration: 18.944s, episode steps: 447, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.017193, mae: 1.761244, mean_q: 2.133718, mean_eps: 0.100000\n",
            "📈 Episodio 1188: Recompensa total (clipped): 14.000, Pasos: 651, Mean Reward Calculado: 0.021505 (Recompensa/Pasos)\n",
            "  869336/2000000: episode: 1188, duration: 27.860s, episode steps: 651, steps per second:  23, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.015861, mae: 1.737938, mean_q: 2.106544, mean_eps: 0.100000\n",
            "📈 Episodio 1189: Recompensa total (clipped): 21.000, Pasos: 1082, Mean Reward Calculado: 0.019409 (Recompensa/Pasos)\n",
            "  870418/2000000: episode: 1189, duration: 46.126s, episode steps: 1082, steps per second:  23, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.014310, mae: 1.726592, mean_q: 2.093362, mean_eps: 0.100000\n",
            "📈 Episodio 1190: Recompensa total (clipped): 25.000, Pasos: 978, Mean Reward Calculado: 0.025562 (Recompensa/Pasos)\n",
            "  871396/2000000: episode: 1190, duration: 40.860s, episode steps: 978, steps per second:  24, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.016169, mae: 1.758576, mean_q: 2.135492, mean_eps: 0.100000\n",
            "📈 Episodio 1191: Recompensa total (clipped): 11.000, Pasos: 822, Mean Reward Calculado: 0.013382 (Recompensa/Pasos)\n",
            "  872218/2000000: episode: 1191, duration: 34.491s, episode steps: 822, steps per second:  24, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.015723, mae: 1.760285, mean_q: 2.135454, mean_eps: 0.100000\n",
            "📈 Episodio 1192: Recompensa total (clipped): 22.000, Pasos: 849, Mean Reward Calculado: 0.025913 (Recompensa/Pasos)\n",
            "  873067/2000000: episode: 1192, duration: 35.989s, episode steps: 849, steps per second:  24, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.016794, mae: 1.767628, mean_q: 2.142085, mean_eps: 0.100000\n",
            "📈 Episodio 1193: Recompensa total (clipped): 29.000, Pasos: 1123, Mean Reward Calculado: 0.025824 (Recompensa/Pasos)\n",
            "  874190/2000000: episode: 1193, duration: 48.198s, episode steps: 1123, steps per second:  23, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.016471, mae: 1.750635, mean_q: 2.123723, mean_eps: 0.100000\n",
            "📈 Episodio 1194: Recompensa total (clipped): 16.000, Pasos: 749, Mean Reward Calculado: 0.021362 (Recompensa/Pasos)\n",
            "  874939/2000000: episode: 1194, duration: 31.964s, episode steps: 749, steps per second:  23, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.017558, mae: 1.741717, mean_q: 2.113343, mean_eps: 0.100000\n",
            "📈 Episodio 1195: Recompensa total (clipped): 21.000, Pasos: 997, Mean Reward Calculado: 0.021063 (Recompensa/Pasos)\n",
            "  875936/2000000: episode: 1195, duration: 42.587s, episode steps: 997, steps per second:  23, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.015357, mae: 1.766674, mean_q: 2.144147, mean_eps: 0.100000\n",
            "📈 Episodio 1196: Recompensa total (clipped): 17.000, Pasos: 717, Mean Reward Calculado: 0.023710 (Recompensa/Pasos)\n",
            "  876653/2000000: episode: 1196, duration: 30.427s, episode steps: 717, steps per second:  24, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015448, mae: 1.752396, mean_q: 2.126815, mean_eps: 0.100000\n",
            "📈 Episodio 1197: Recompensa total (clipped): 24.000, Pasos: 863, Mean Reward Calculado: 0.027810 (Recompensa/Pasos)\n",
            "  877516/2000000: episode: 1197, duration: 36.493s, episode steps: 863, steps per second:  24, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.015890, mae: 1.743286, mean_q: 2.113033, mean_eps: 0.100000\n",
            "📈 Episodio 1198: Recompensa total (clipped): 9.000, Pasos: 506, Mean Reward Calculado: 0.017787 (Recompensa/Pasos)\n",
            "  878022/2000000: episode: 1198, duration: 21.739s, episode steps: 506, steps per second:  23, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.761 [0.000, 5.000],  loss: 0.018381, mae: 1.755681, mean_q: 2.127714, mean_eps: 0.100000\n",
            "📈 Episodio 1199: Recompensa total (clipped): 6.000, Pasos: 552, Mean Reward Calculado: 0.010870 (Recompensa/Pasos)\n",
            "  878574/2000000: episode: 1199, duration: 23.291s, episode steps: 552, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.017950, mae: 1.743480, mean_q: 2.116330, mean_eps: 0.100000\n",
            "📈 Episodio 1200: Recompensa total (clipped): 5.000, Pasos: 522, Mean Reward Calculado: 0.009579 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1200, pasos: 879096)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.29 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1200 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 5.00\n",
            "   Media últimos 100: 18.29 / 20.0\n",
            "   Mejor promedio histórico: 18.29\n",
            "   Estado: 📈 91.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  879096/2000000: episode: 1200, duration: 81.623s, episode steps: 522, steps per second:   6, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.015217, mae: 1.746879, mean_q: 2.120426, mean_eps: 0.100000\n",
            "📈 Episodio 1201: Recompensa total (clipped): 12.000, Pasos: 787, Mean Reward Calculado: 0.015248 (Recompensa/Pasos)\n",
            "  879883/2000000: episode: 1201, duration: 33.900s, episode steps: 787, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.016367, mae: 1.744189, mean_q: 2.118556, mean_eps: 0.100000\n",
            "📊 Paso 880,000/2,000,000 (44.0%) - 29.7 pasos/seg - ETA: 10.5h - Memoria: 10637.95 MB\n",
            "📈 Episodio 1202: Recompensa total (clipped): 22.000, Pasos: 1046, Mean Reward Calculado: 0.021033 (Recompensa/Pasos)\n",
            "  880929/2000000: episode: 1202, duration: 44.422s, episode steps: 1046, steps per second:  24, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.014547, mae: 1.757375, mean_q: 2.133098, mean_eps: 0.100000\n",
            "📈 Episodio 1203: Recompensa total (clipped): 13.000, Pasos: 841, Mean Reward Calculado: 0.015458 (Recompensa/Pasos)\n",
            "  881770/2000000: episode: 1203, duration: 35.214s, episode steps: 841, steps per second:  24, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.016075, mae: 1.759120, mean_q: 2.135122, mean_eps: 0.100000\n",
            "📈 Episodio 1204: Recompensa total (clipped): 15.000, Pasos: 557, Mean Reward Calculado: 0.026930 (Recompensa/Pasos)\n",
            "  882327/2000000: episode: 1204, duration: 23.285s, episode steps: 557, steps per second:  24, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.016980, mae: 1.763810, mean_q: 2.141487, mean_eps: 0.100000\n",
            "📈 Episodio 1205: Recompensa total (clipped): 25.000, Pasos: 821, Mean Reward Calculado: 0.030451 (Recompensa/Pasos)\n",
            "  883148/2000000: episode: 1205, duration: 35.121s, episode steps: 821, steps per second:  23, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.015869, mae: 1.758830, mean_q: 2.134524, mean_eps: 0.100000\n",
            "📈 Episodio 1206: Recompensa total (clipped): 19.000, Pasos: 819, Mean Reward Calculado: 0.023199 (Recompensa/Pasos)\n",
            "  883967/2000000: episode: 1206, duration: 34.841s, episode steps: 819, steps per second:  24, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.014498, mae: 1.768419, mean_q: 2.146869, mean_eps: 0.100000\n",
            "📈 Episodio 1207: Recompensa total (clipped): 15.000, Pasos: 789, Mean Reward Calculado: 0.019011 (Recompensa/Pasos)\n",
            "  884756/2000000: episode: 1207, duration: 33.828s, episode steps: 789, steps per second:  23, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.017924, mae: 1.780663, mean_q: 2.160914, mean_eps: 0.100000\n",
            "📈 Episodio 1208: Recompensa total (clipped): 44.000, Pasos: 1882, Mean Reward Calculado: 0.023379 (Recompensa/Pasos)\n",
            "  886638/2000000: episode: 1208, duration: 80.820s, episode steps: 1882, steps per second:  23, episode reward: 44.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.015919, mae: 1.761428, mean_q: 2.137262, mean_eps: 0.100000\n",
            "📈 Episodio 1209: Recompensa total (clipped): 22.000, Pasos: 979, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            "  887617/2000000: episode: 1209, duration: 42.580s, episode steps: 979, steps per second:  23, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.016757, mae: 1.771938, mean_q: 2.149157, mean_eps: 0.100000\n",
            "📈 Episodio 1210: Recompensa total (clipped): 18.000, Pasos: 806, Mean Reward Calculado: 0.022333 (Recompensa/Pasos)\n",
            "  888423/2000000: episode: 1210, duration: 34.434s, episode steps: 806, steps per second:  23, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.016899, mae: 1.770811, mean_q: 2.147452, mean_eps: 0.100000\n",
            "📈 Episodio 1211: Recompensa total (clipped): 25.000, Pasos: 1133, Mean Reward Calculado: 0.022065 (Recompensa/Pasos)\n",
            "  889556/2000000: episode: 1211, duration: 48.164s, episode steps: 1133, steps per second:  24, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.016743, mae: 1.768352, mean_q: 2.145808, mean_eps: 0.100000\n",
            "📈 Episodio 1212: Recompensa total (clipped): 4.000, Pasos: 373, Mean Reward Calculado: 0.010724 (Recompensa/Pasos)\n",
            "  889929/2000000: episode: 1212, duration: 16.116s, episode steps: 373, steps per second:  23, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 0.871 [0.000, 5.000],  loss: 0.015967, mae: 1.756255, mean_q: 2.130191, mean_eps: 0.100000\n",
            "📈 Episodio 1213: Recompensa total (clipped): 14.000, Pasos: 579, Mean Reward Calculado: 0.024180 (Recompensa/Pasos)\n",
            "  890508/2000000: episode: 1213, duration: 24.979s, episode steps: 579, steps per second:  23, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.014021, mae: 1.781337, mean_q: 2.165059, mean_eps: 0.100000\n",
            "📈 Episodio 1214: Recompensa total (clipped): 25.000, Pasos: 1122, Mean Reward Calculado: 0.022282 (Recompensa/Pasos)\n",
            "  891630/2000000: episode: 1214, duration: 48.117s, episode steps: 1122, steps per second:  23, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015798, mae: 1.788546, mean_q: 2.168620, mean_eps: 0.100000\n",
            "📈 Episodio 1215: Recompensa total (clipped): 11.000, Pasos: 508, Mean Reward Calculado: 0.021654 (Recompensa/Pasos)\n",
            "  892138/2000000: episode: 1215, duration: 21.429s, episode steps: 508, steps per second:  24, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.016096, mae: 1.779500, mean_q: 2.159748, mean_eps: 0.100000\n",
            "📈 Episodio 1216: Recompensa total (clipped): 23.000, Pasos: 1037, Mean Reward Calculado: 0.022179 (Recompensa/Pasos)\n",
            "  893175/2000000: episode: 1216, duration: 44.375s, episode steps: 1037, steps per second:  23, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015601, mae: 1.774542, mean_q: 2.153079, mean_eps: 0.100000\n",
            "📈 Episodio 1217: Recompensa total (clipped): 9.000, Pasos: 578, Mean Reward Calculado: 0.015571 (Recompensa/Pasos)\n",
            "  893753/2000000: episode: 1217, duration: 24.662s, episode steps: 578, steps per second:  23, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015988, mae: 1.786234, mean_q: 2.164955, mean_eps: 0.100000\n",
            "📈 Episodio 1218: Recompensa total (clipped): 16.000, Pasos: 877, Mean Reward Calculado: 0.018244 (Recompensa/Pasos)\n",
            "  894630/2000000: episode: 1218, duration: 37.393s, episode steps: 877, steps per second:  23, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.017831, mae: 1.781591, mean_q: 2.156871, mean_eps: 0.100000\n",
            "📈 Episodio 1219: Recompensa total (clipped): 13.000, Pasos: 515, Mean Reward Calculado: 0.025243 (Recompensa/Pasos)\n",
            "  895145/2000000: episode: 1219, duration: 22.248s, episode steps: 515, steps per second:  23, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.015182, mae: 1.778373, mean_q: 2.154940, mean_eps: 0.100000\n",
            "📈 Episodio 1220: Recompensa total (clipped): 10.000, Pasos: 488, Mean Reward Calculado: 0.020492 (Recompensa/Pasos)\n",
            "  895633/2000000: episode: 1220, duration: 21.368s, episode steps: 488, steps per second:  23, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.018229, mae: 1.781967, mean_q: 2.158991, mean_eps: 0.100000\n",
            "📈 Episodio 1221: Recompensa total (clipped): 9.000, Pasos: 509, Mean Reward Calculado: 0.017682 (Recompensa/Pasos)\n",
            "  896142/2000000: episode: 1221, duration: 21.752s, episode steps: 509, steps per second:  23, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.016102, mae: 1.796923, mean_q: 2.183166, mean_eps: 0.100000\n",
            "📈 Episodio 1222: Recompensa total (clipped): 18.000, Pasos: 612, Mean Reward Calculado: 0.029412 (Recompensa/Pasos)\n",
            "  896754/2000000: episode: 1222, duration: 25.808s, episode steps: 612, steps per second:  24, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016619, mae: 1.756851, mean_q: 2.127092, mean_eps: 0.100000\n",
            "📈 Episodio 1223: Recompensa total (clipped): 18.000, Pasos: 927, Mean Reward Calculado: 0.019417 (Recompensa/Pasos)\n",
            "  897681/2000000: episode: 1223, duration: 39.471s, episode steps: 927, steps per second:  23, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014801, mae: 1.768907, mean_q: 2.144865, mean_eps: 0.100000\n",
            "📈 Episodio 1224: Recompensa total (clipped): 9.000, Pasos: 435, Mean Reward Calculado: 0.020690 (Recompensa/Pasos)\n",
            "  898116/2000000: episode: 1224, duration: 18.724s, episode steps: 435, steps per second:  23, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.014139, mae: 1.775235, mean_q: 2.152736, mean_eps: 0.100000\n",
            "📈 Episodio 1225: Recompensa total (clipped): 19.000, Pasos: 962, Mean Reward Calculado: 0.019751 (Recompensa/Pasos)\n",
            "  899078/2000000: episode: 1225, duration: 41.187s, episode steps: 962, steps per second:  23, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.016838, mae: 1.768382, mean_q: 2.142858, mean_eps: 0.100000\n",
            "📊 Paso 900,000/2,000,000 (45.0%) - 29.5 pasos/seg - ETA: 10.4h - Memoria: 10651.66 MB\n",
            "📈 Episodio 1226: Recompensa total (clipped): 25.000, Pasos: 1012, Mean Reward Calculado: 0.024704 (Recompensa/Pasos)\n",
            "  900090/2000000: episode: 1226, duration: 43.163s, episode steps: 1012, steps per second:  23, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.015099, mae: 1.762738, mean_q: 2.139112, mean_eps: 0.100000\n",
            "📈 Episodio 1227: Recompensa total (clipped): 24.000, Pasos: 950, Mean Reward Calculado: 0.025263 (Recompensa/Pasos)\n",
            "  901040/2000000: episode: 1227, duration: 40.930s, episode steps: 950, steps per second:  23, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.015985, mae: 1.785191, mean_q: 2.166073, mean_eps: 0.100000\n",
            "📈 Episodio 1228: Recompensa total (clipped): 26.000, Pasos: 1187, Mean Reward Calculado: 0.021904 (Recompensa/Pasos)\n",
            "  902227/2000000: episode: 1228, duration: 51.185s, episode steps: 1187, steps per second:  23, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016483, mae: 1.785281, mean_q: 2.163953, mean_eps: 0.100000\n",
            "📈 Episodio 1229: Recompensa total (clipped): 21.000, Pasos: 1032, Mean Reward Calculado: 0.020349 (Recompensa/Pasos)\n",
            "  903259/2000000: episode: 1229, duration: 44.910s, episode steps: 1032, steps per second:  23, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.015841, mae: 1.770419, mean_q: 2.147422, mean_eps: 0.100000\n",
            "📈 Episodio 1230: Recompensa total (clipped): 18.000, Pasos: 799, Mean Reward Calculado: 0.022528 (Recompensa/Pasos)\n",
            "  904058/2000000: episode: 1230, duration: 34.409s, episode steps: 799, steps per second:  23, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.015663, mae: 1.779482, mean_q: 2.157313, mean_eps: 0.100000\n",
            "📈 Episodio 1231: Recompensa total (clipped): 22.000, Pasos: 883, Mean Reward Calculado: 0.024915 (Recompensa/Pasos)\n",
            "  904941/2000000: episode: 1231, duration: 38.549s, episode steps: 883, steps per second:  23, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.658 [0.000, 5.000],  loss: 0.015906, mae: 1.767798, mean_q: 2.142750, mean_eps: 0.100000\n",
            "📈 Episodio 1232: Recompensa total (clipped): 18.000, Pasos: 832, Mean Reward Calculado: 0.021635 (Recompensa/Pasos)\n",
            "  905773/2000000: episode: 1232, duration: 35.792s, episode steps: 832, steps per second:  23, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.614 [0.000, 5.000],  loss: 0.017158, mae: 1.772711, mean_q: 2.147590, mean_eps: 0.100000\n",
            "📈 Episodio 1233: Recompensa total (clipped): 19.000, Pasos: 840, Mean Reward Calculado: 0.022619 (Recompensa/Pasos)\n",
            "  906613/2000000: episode: 1233, duration: 36.284s, episode steps: 840, steps per second:  23, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.016913, mae: 1.780096, mean_q: 2.157321, mean_eps: 0.100000\n",
            "📈 Episodio 1234: Recompensa total (clipped): 23.000, Pasos: 1247, Mean Reward Calculado: 0.018444 (Recompensa/Pasos)\n",
            "  907860/2000000: episode: 1234, duration: 53.177s, episode steps: 1247, steps per second:  23, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.015866, mae: 1.778308, mean_q: 2.155696, mean_eps: 0.100000\n",
            "📈 Episodio 1235: Recompensa total (clipped): 12.000, Pasos: 608, Mean Reward Calculado: 0.019737 (Recompensa/Pasos)\n",
            "  908468/2000000: episode: 1235, duration: 26.346s, episode steps: 608, steps per second:  23, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.015738, mae: 1.777145, mean_q: 2.153853, mean_eps: 0.100000\n",
            "📈 Episodio 1236: Recompensa total (clipped): 24.000, Pasos: 1221, Mean Reward Calculado: 0.019656 (Recompensa/Pasos)\n",
            "  909689/2000000: episode: 1236, duration: 53.131s, episode steps: 1221, steps per second:  23, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015191, mae: 1.769644, mean_q: 2.145143, mean_eps: 0.100000\n",
            "📈 Episodio 1237: Recompensa total (clipped): 25.000, Pasos: 804, Mean Reward Calculado: 0.031095 (Recompensa/Pasos)\n",
            "  910493/2000000: episode: 1237, duration: 34.787s, episode steps: 804, steps per second:  23, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.015248, mae: 1.770672, mean_q: 2.146222, mean_eps: 0.100000\n",
            "📈 Episodio 1238: Recompensa total (clipped): 15.000, Pasos: 613, Mean Reward Calculado: 0.024470 (Recompensa/Pasos)\n",
            "  911106/2000000: episode: 1238, duration: 26.522s, episode steps: 613, steps per second:  23, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.016708, mae: 1.774879, mean_q: 2.153545, mean_eps: 0.100000\n",
            "📈 Episodio 1239: Recompensa total (clipped): 19.000, Pasos: 830, Mean Reward Calculado: 0.022892 (Recompensa/Pasos)\n",
            "  911936/2000000: episode: 1239, duration: 36.028s, episode steps: 830, steps per second:  23, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014540, mae: 1.768501, mean_q: 2.145353, mean_eps: 0.100000\n",
            "📈 Episodio 1240: Recompensa total (clipped): 9.000, Pasos: 761, Mean Reward Calculado: 0.011827 (Recompensa/Pasos)\n",
            "  912697/2000000: episode: 1240, duration: 33.379s, episode steps: 761, steps per second:  23, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.539 [0.000, 5.000],  loss: 0.014216, mae: 1.772636, mean_q: 2.147573, mean_eps: 0.100000\n",
            "📈 Episodio 1241: Recompensa total (clipped): 11.000, Pasos: 486, Mean Reward Calculado: 0.022634 (Recompensa/Pasos)\n",
            "  913183/2000000: episode: 1241, duration: 20.867s, episode steps: 486, steps per second:  23, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.015638, mae: 1.768834, mean_q: 2.139956, mean_eps: 0.100000\n",
            "📈 Episodio 1242: Recompensa total (clipped): 25.000, Pasos: 992, Mean Reward Calculado: 0.025202 (Recompensa/Pasos)\n",
            "  914175/2000000: episode: 1242, duration: 41.985s, episode steps: 992, steps per second:  24, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.016027, mae: 1.774532, mean_q: 2.148530, mean_eps: 0.100000\n",
            "📈 Episodio 1243: Recompensa total (clipped): 21.000, Pasos: 1042, Mean Reward Calculado: 0.020154 (Recompensa/Pasos)\n",
            "  915217/2000000: episode: 1243, duration: 44.595s, episode steps: 1042, steps per second:  23, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.016402, mae: 1.770653, mean_q: 2.144586, mean_eps: 0.100000\n",
            "📈 Episodio 1244: Recompensa total (clipped): 6.000, Pasos: 502, Mean Reward Calculado: 0.011952 (Recompensa/Pasos)\n",
            "  915719/2000000: episode: 1244, duration: 21.649s, episode steps: 502, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.017355, mae: 1.779771, mean_q: 2.156319, mean_eps: 0.100000\n",
            "📈 Episodio 1245: Recompensa total (clipped): 20.000, Pasos: 1059, Mean Reward Calculado: 0.018886 (Recompensa/Pasos)\n",
            "  916778/2000000: episode: 1245, duration: 45.568s, episode steps: 1059, steps per second:  23, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.015777, mae: 1.766410, mean_q: 2.141418, mean_eps: 0.100000\n",
            "📈 Episodio 1246: Recompensa total (clipped): 11.000, Pasos: 637, Mean Reward Calculado: 0.017268 (Recompensa/Pasos)\n",
            "  917415/2000000: episode: 1246, duration: 27.923s, episode steps: 637, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.016427, mae: 1.771469, mean_q: 2.145657, mean_eps: 0.100000\n",
            "📈 Episodio 1247: Recompensa total (clipped): 15.000, Pasos: 610, Mean Reward Calculado: 0.024590 (Recompensa/Pasos)\n",
            "  918025/2000000: episode: 1247, duration: 26.167s, episode steps: 610, steps per second:  23, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.014841, mae: 1.770002, mean_q: 2.144242, mean_eps: 0.100000\n",
            "📈 Episodio 1248: Recompensa total (clipped): 16.000, Pasos: 586, Mean Reward Calculado: 0.027304 (Recompensa/Pasos)\n",
            "  918611/2000000: episode: 1248, duration: 25.238s, episode steps: 586, steps per second:  23, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.763 [0.000, 5.000],  loss: 0.016793, mae: 1.759929, mean_q: 2.131983, mean_eps: 0.100000\n",
            "📈 Episodio 1249: Recompensa total (clipped): 15.000, Pasos: 740, Mean Reward Calculado: 0.020270 (Recompensa/Pasos)\n",
            "  919351/2000000: episode: 1249, duration: 31.998s, episode steps: 740, steps per second:  23, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.016133, mae: 1.766274, mean_q: 2.138989, mean_eps: 0.100000\n",
            "📊 Paso 920,000/2,000,000 (46.0%) - 29.3 pasos/seg - ETA: 10.2h - Memoria: 10654.44 MB\n",
            "📈 Episodio 1250: Recompensa total (clipped): 22.000, Pasos: 827, Mean Reward Calculado: 0.026602 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1250, pasos: 920178)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.03 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1250 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 22.00\n",
            "   Media últimos 100: 18.03 / 20.0\n",
            "   Mejor promedio histórico: 18.03\n",
            "   Estado: 📈 90.2% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  920178/2000000: episode: 1250, duration: 117.502s, episode steps: 827, steps per second:   7, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.015234, mae: 1.792819, mean_q: 2.172598, mean_eps: 0.100000\n",
            "📈 Episodio 1251: Recompensa total (clipped): 25.000, Pasos: 1195, Mean Reward Calculado: 0.020921 (Recompensa/Pasos)\n",
            "  921373/2000000: episode: 1251, duration: 52.248s, episode steps: 1195, steps per second:  23, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014424, mae: 1.802132, mean_q: 2.186832, mean_eps: 0.100000\n",
            "📈 Episodio 1252: Recompensa total (clipped): 26.000, Pasos: 1130, Mean Reward Calculado: 0.023009 (Recompensa/Pasos)\n",
            "  922503/2000000: episode: 1252, duration: 49.281s, episode steps: 1130, steps per second:  23, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015128, mae: 1.813756, mean_q: 2.198836, mean_eps: 0.100000\n",
            "📈 Episodio 1253: Recompensa total (clipped): 28.000, Pasos: 1153, Mean Reward Calculado: 0.024284 (Recompensa/Pasos)\n",
            "  923656/2000000: episode: 1253, duration: 49.521s, episode steps: 1153, steps per second:  23, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014893, mae: 1.811550, mean_q: 2.196034, mean_eps: 0.100000\n",
            "📈 Episodio 1254: Recompensa total (clipped): 13.000, Pasos: 624, Mean Reward Calculado: 0.020833 (Recompensa/Pasos)\n",
            "  924280/2000000: episode: 1254, duration: 27.489s, episode steps: 624, steps per second:  23, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.015828, mae: 1.812223, mean_q: 2.195990, mean_eps: 0.100000\n",
            "📈 Episodio 1255: Recompensa total (clipped): 26.000, Pasos: 947, Mean Reward Calculado: 0.027455 (Recompensa/Pasos)\n",
            "  925227/2000000: episode: 1255, duration: 42.162s, episode steps: 947, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017030, mae: 1.814396, mean_q: 2.201400, mean_eps: 0.100000\n",
            "📈 Episodio 1256: Recompensa total (clipped): 15.000, Pasos: 708, Mean Reward Calculado: 0.021186 (Recompensa/Pasos)\n",
            "  925935/2000000: episode: 1256, duration: 31.158s, episode steps: 708, steps per second:  23, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.016227, mae: 1.813859, mean_q: 2.199355, mean_eps: 0.100000\n",
            "📈 Episodio 1257: Recompensa total (clipped): 12.000, Pasos: 563, Mean Reward Calculado: 0.021314 (Recompensa/Pasos)\n",
            "  926498/2000000: episode: 1257, duration: 24.782s, episode steps: 563, steps per second:  23, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.015893, mae: 1.783700, mean_q: 2.161675, mean_eps: 0.100000\n",
            "📈 Episodio 1258: Recompensa total (clipped): 11.000, Pasos: 527, Mean Reward Calculado: 0.020873 (Recompensa/Pasos)\n",
            "  927025/2000000: episode: 1258, duration: 23.067s, episode steps: 527, steps per second:  23, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.017264, mae: 1.807362, mean_q: 2.189024, mean_eps: 0.100000\n",
            "📈 Episodio 1259: Recompensa total (clipped): 17.000, Pasos: 966, Mean Reward Calculado: 0.017598 (Recompensa/Pasos)\n",
            "  927991/2000000: episode: 1259, duration: 42.644s, episode steps: 966, steps per second:  23, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.016253, mae: 1.813187, mean_q: 2.197588, mean_eps: 0.100000\n",
            "📈 Episodio 1260: Recompensa total (clipped): 21.000, Pasos: 680, Mean Reward Calculado: 0.030882 (Recompensa/Pasos)\n",
            "  928671/2000000: episode: 1260, duration: 29.633s, episode steps: 680, steps per second:  23, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.016573, mae: 1.793445, mean_q: 2.171925, mean_eps: 0.100000\n",
            "📈 Episodio 1261: Recompensa total (clipped): 26.000, Pasos: 989, Mean Reward Calculado: 0.026289 (Recompensa/Pasos)\n",
            "  929660/2000000: episode: 1261, duration: 43.750s, episode steps: 989, steps per second:  23, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.017849, mae: 1.803971, mean_q: 2.185814, mean_eps: 0.100000\n",
            "📈 Episodio 1262: Recompensa total (clipped): 19.000, Pasos: 883, Mean Reward Calculado: 0.021518 (Recompensa/Pasos)\n",
            "  930543/2000000: episode: 1262, duration: 38.025s, episode steps: 883, steps per second:  23, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.015947, mae: 1.808907, mean_q: 2.193275, mean_eps: 0.100000\n",
            "📈 Episodio 1263: Recompensa total (clipped): 18.000, Pasos: 601, Mean Reward Calculado: 0.029950 (Recompensa/Pasos)\n",
            "  931144/2000000: episode: 1263, duration: 26.194s, episode steps: 601, steps per second:  23, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.015387, mae: 1.788056, mean_q: 2.168491, mean_eps: 0.100000\n",
            "📈 Episodio 1264: Recompensa total (clipped): 22.000, Pasos: 1002, Mean Reward Calculado: 0.021956 (Recompensa/Pasos)\n",
            "  932146/2000000: episode: 1264, duration: 43.201s, episode steps: 1002, steps per second:  23, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.015964, mae: 1.810456, mean_q: 2.193198, mean_eps: 0.100000\n",
            "📈 Episodio 1265: Recompensa total (clipped): 15.000, Pasos: 549, Mean Reward Calculado: 0.027322 (Recompensa/Pasos)\n",
            "  932695/2000000: episode: 1265, duration: 24.238s, episode steps: 549, steps per second:  23, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.015131, mae: 1.821088, mean_q: 2.206668, mean_eps: 0.100000\n",
            "📈 Episodio 1266: Recompensa total (clipped): 20.000, Pasos: 726, Mean Reward Calculado: 0.027548 (Recompensa/Pasos)\n",
            "  933421/2000000: episode: 1266, duration: 31.777s, episode steps: 726, steps per second:  23, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.017700, mae: 1.824187, mean_q: 2.208902, mean_eps: 0.100000\n",
            "📈 Episodio 1267: Recompensa total (clipped): 15.000, Pasos: 716, Mean Reward Calculado: 0.020950 (Recompensa/Pasos)\n",
            "  934137/2000000: episode: 1267, duration: 31.867s, episode steps: 716, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015744, mae: 1.801572, mean_q: 2.183174, mean_eps: 0.100000\n",
            "📈 Episodio 1268: Recompensa total (clipped): 10.000, Pasos: 642, Mean Reward Calculado: 0.015576 (Recompensa/Pasos)\n",
            "  934779/2000000: episode: 1268, duration: 27.985s, episode steps: 642, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015985, mae: 1.818058, mean_q: 2.204533, mean_eps: 0.100000\n",
            "📈 Episodio 1269: Recompensa total (clipped): 19.000, Pasos: 990, Mean Reward Calculado: 0.019192 (Recompensa/Pasos)\n",
            "  935769/2000000: episode: 1269, duration: 43.113s, episode steps: 990, steps per second:  23, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.015746, mae: 1.796771, mean_q: 2.178202, mean_eps: 0.100000\n",
            "📈 Episodio 1270: Recompensa total (clipped): 20.000, Pasos: 915, Mean Reward Calculado: 0.021858 (Recompensa/Pasos)\n",
            "  936684/2000000: episode: 1270, duration: 40.187s, episode steps: 915, steps per second:  23, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.015222, mae: 1.805959, mean_q: 2.190052, mean_eps: 0.100000\n",
            "📈 Episodio 1271: Recompensa total (clipped): 15.000, Pasos: 906, Mean Reward Calculado: 0.016556 (Recompensa/Pasos)\n",
            "  937590/2000000: episode: 1271, duration: 39.751s, episode steps: 906, steps per second:  23, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015454, mae: 1.804672, mean_q: 2.186415, mean_eps: 0.100000\n",
            "📈 Episodio 1272: Recompensa total (clipped): 15.000, Pasos: 940, Mean Reward Calculado: 0.015957 (Recompensa/Pasos)\n",
            "  938530/2000000: episode: 1272, duration: 41.312s, episode steps: 940, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.016338, mae: 1.804216, mean_q: 2.189503, mean_eps: 0.100000\n",
            "📈 Episodio 1273: Recompensa total (clipped): 24.000, Pasos: 1064, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
            "  939594/2000000: episode: 1273, duration: 46.981s, episode steps: 1064, steps per second:  23, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.016928, mae: 1.805569, mean_q: 2.187021, mean_eps: 0.100000\n",
            "📊 Paso 940,000/2,000,000 (47.0%) - 29.1 pasos/seg - ETA: 10.1h - Memoria: 15225.79 MB\n",
            "📈 Episodio 1274: Recompensa total (clipped): 15.000, Pasos: 557, Mean Reward Calculado: 0.026930 (Recompensa/Pasos)\n",
            "  940151/2000000: episode: 1274, duration: 24.519s, episode steps: 557, steps per second:  23, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.014834, mae: 1.780708, mean_q: 2.155799, mean_eps: 0.100000\n",
            "📈 Episodio 1275: Recompensa total (clipped): 29.000, Pasos: 1193, Mean Reward Calculado: 0.024308 (Recompensa/Pasos)\n",
            "  941344/2000000: episode: 1275, duration: 52.638s, episode steps: 1193, steps per second:  23, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014547, mae: 1.813874, mean_q: 2.198697, mean_eps: 0.100000\n",
            "📈 Episodio 1276: Recompensa total (clipped): 23.000, Pasos: 897, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            "  942241/2000000: episode: 1276, duration: 40.034s, episode steps: 897, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.014163, mae: 1.799597, mean_q: 2.180596, mean_eps: 0.100000\n",
            "📈 Episodio 1277: Recompensa total (clipped): 16.000, Pasos: 612, Mean Reward Calculado: 0.026144 (Recompensa/Pasos)\n",
            "  942853/2000000: episode: 1277, duration: 27.233s, episode steps: 612, steps per second:  22, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.015161, mae: 1.832236, mean_q: 2.222114, mean_eps: 0.100000\n",
            "📈 Episodio 1278: Recompensa total (clipped): 22.000, Pasos: 1004, Mean Reward Calculado: 0.021912 (Recompensa/Pasos)\n",
            "  943857/2000000: episode: 1278, duration: 44.545s, episode steps: 1004, steps per second:  23, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.016035, mae: 1.817055, mean_q: 2.200191, mean_eps: 0.100000\n",
            "📈 Episodio 1279: Recompensa total (clipped): 27.000, Pasos: 875, Mean Reward Calculado: 0.030857 (Recompensa/Pasos)\n",
            "  944732/2000000: episode: 1279, duration: 38.767s, episode steps: 875, steps per second:  23, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.016543, mae: 1.823401, mean_q: 2.209654, mean_eps: 0.100000\n",
            "📈 Episodio 1280: Recompensa total (clipped): 32.000, Pasos: 1450, Mean Reward Calculado: 0.022069 (Recompensa/Pasos)\n",
            "  946182/2000000: episode: 1280, duration: 64.350s, episode steps: 1450, steps per second:  23, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.016032, mae: 1.820601, mean_q: 2.206584, mean_eps: 0.100000\n",
            "📈 Episodio 1281: Recompensa total (clipped): 18.000, Pasos: 733, Mean Reward Calculado: 0.024557 (Recompensa/Pasos)\n",
            "  946915/2000000: episode: 1281, duration: 32.445s, episode steps: 733, steps per second:  23, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.014331, mae: 1.793903, mean_q: 2.175205, mean_eps: 0.100000\n",
            "📈 Episodio 1282: Recompensa total (clipped): 31.000, Pasos: 1586, Mean Reward Calculado: 0.019546 (Recompensa/Pasos)\n",
            "  948501/2000000: episode: 1282, duration: 69.494s, episode steps: 1586, steps per second:  23, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.015427, mae: 1.828383, mean_q: 2.215680, mean_eps: 0.100000\n",
            "📈 Episodio 1283: Recompensa total (clipped): 18.000, Pasos: 703, Mean Reward Calculado: 0.025605 (Recompensa/Pasos)\n",
            "  949204/2000000: episode: 1283, duration: 30.967s, episode steps: 703, steps per second:  23, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.016263, mae: 1.817700, mean_q: 2.201393, mean_eps: 0.100000\n",
            "📈 Episodio 1284: Recompensa total (clipped): 28.000, Pasos: 1209, Mean Reward Calculado: 0.023160 (Recompensa/Pasos)\n",
            "  950413/2000000: episode: 1284, duration: 53.363s, episode steps: 1209, steps per second:  23, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.014747, mae: 1.827363, mean_q: 2.215327, mean_eps: 0.100000\n",
            "📈 Episodio 1285: Recompensa total (clipped): 25.000, Pasos: 982, Mean Reward Calculado: 0.025458 (Recompensa/Pasos)\n",
            "  951395/2000000: episode: 1285, duration: 43.595s, episode steps: 982, steps per second:  23, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.015374, mae: 1.844695, mean_q: 2.236200, mean_eps: 0.100000\n",
            "📈 Episodio 1286: Recompensa total (clipped): 32.000, Pasos: 1346, Mean Reward Calculado: 0.023774 (Recompensa/Pasos)\n",
            "  952741/2000000: episode: 1286, duration: 59.287s, episode steps: 1346, steps per second:  23, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.014996, mae: 1.844656, mean_q: 2.236380, mean_eps: 0.100000\n",
            "📈 Episodio 1287: Recompensa total (clipped): 12.000, Pasos: 684, Mean Reward Calculado: 0.017544 (Recompensa/Pasos)\n",
            "  953425/2000000: episode: 1287, duration: 30.176s, episode steps: 684, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.017505, mae: 1.837404, mean_q: 2.227361, mean_eps: 0.100000\n",
            "📈 Episodio 1288: Recompensa total (clipped): 18.000, Pasos: 863, Mean Reward Calculado: 0.020857 (Recompensa/Pasos)\n",
            "  954288/2000000: episode: 1288, duration: 38.419s, episode steps: 863, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.015860, mae: 1.829836, mean_q: 2.216070, mean_eps: 0.100000\n",
            "📈 Episodio 1289: Recompensa total (clipped): 20.000, Pasos: 947, Mean Reward Calculado: 0.021119 (Recompensa/Pasos)\n",
            "  955235/2000000: episode: 1289, duration: 41.765s, episode steps: 947, steps per second:  23, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.014838, mae: 1.838541, mean_q: 2.229594, mean_eps: 0.100000\n",
            "📈 Episodio 1290: Recompensa total (clipped): 18.000, Pasos: 960, Mean Reward Calculado: 0.018750 (Recompensa/Pasos)\n",
            "  956195/2000000: episode: 1290, duration: 42.121s, episode steps: 960, steps per second:  23, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.016427, mae: 1.845343, mean_q: 2.236228, mean_eps: 0.100000\n",
            "📈 Episodio 1291: Recompensa total (clipped): 32.000, Pasos: 1081, Mean Reward Calculado: 0.029602 (Recompensa/Pasos)\n",
            "  957276/2000000: episode: 1291, duration: 47.225s, episode steps: 1081, steps per second:  23, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015617, mae: 1.829452, mean_q: 2.216603, mean_eps: 0.100000\n",
            "📈 Episodio 1292: Recompensa total (clipped): 31.000, Pasos: 1288, Mean Reward Calculado: 0.024068 (Recompensa/Pasos)\n",
            "  958564/2000000: episode: 1292, duration: 56.844s, episode steps: 1288, steps per second:  23, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.015511, mae: 1.822820, mean_q: 2.208502, mean_eps: 0.100000\n",
            "📈 Episodio 1293: Recompensa total (clipped): 21.000, Pasos: 909, Mean Reward Calculado: 0.023102 (Recompensa/Pasos)\n",
            "  959473/2000000: episode: 1293, duration: 39.824s, episode steps: 909, steps per second:  23, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.015917, mae: 1.844266, mean_q: 2.233515, mean_eps: 0.100000\n",
            "📊 Paso 960,000/2,000,000 (48.0%) - 28.9 pasos/seg - ETA: 10.0h - Memoria: 15183.82 MB\n",
            "📈 Episodio 1294: Recompensa total (clipped): 18.000, Pasos: 813, Mean Reward Calculado: 0.022140 (Recompensa/Pasos)\n",
            "  960286/2000000: episode: 1294, duration: 35.273s, episode steps: 813, steps per second:  23, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.015304, mae: 1.820592, mean_q: 2.211952, mean_eps: 0.100000\n",
            "📈 Episodio 1295: Recompensa total (clipped): 15.000, Pasos: 671, Mean Reward Calculado: 0.022355 (Recompensa/Pasos)\n",
            "  960957/2000000: episode: 1295, duration: 29.465s, episode steps: 671, steps per second:  23, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.054 [0.000, 5.000],  loss: 0.015823, mae: 1.831041, mean_q: 2.220695, mean_eps: 0.100000\n",
            "📈 Episodio 1296: Recompensa total (clipped): 14.000, Pasos: 642, Mean Reward Calculado: 0.021807 (Recompensa/Pasos)\n",
            "  961599/2000000: episode: 1296, duration: 27.749s, episode steps: 642, steps per second:  23, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.016345, mae: 1.860603, mean_q: 2.256852, mean_eps: 0.100000\n",
            "📈 Episodio 1297: Recompensa total (clipped): 14.000, Pasos: 557, Mean Reward Calculado: 0.025135 (Recompensa/Pasos)\n",
            "  962156/2000000: episode: 1297, duration: 24.706s, episode steps: 557, steps per second:  23, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.194 [0.000, 5.000],  loss: 0.015667, mae: 1.856290, mean_q: 2.251690, mean_eps: 0.100000\n",
            "📈 Episodio 1298: Recompensa total (clipped): 23.000, Pasos: 839, Mean Reward Calculado: 0.027414 (Recompensa/Pasos)\n",
            "  962995/2000000: episode: 1298, duration: 37.074s, episode steps: 839, steps per second:  23, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.016844, mae: 1.838860, mean_q: 2.228387, mean_eps: 0.100000\n",
            "📈 Episodio 1299: Recompensa total (clipped): 21.000, Pasos: 838, Mean Reward Calculado: 0.025060 (Recompensa/Pasos)\n",
            "  963833/2000000: episode: 1299, duration: 37.115s, episode steps: 838, steps per second:  23, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.015554, mae: 1.840546, mean_q: 2.230381, mean_eps: 0.100000\n",
            "📈 Episodio 1300: Recompensa total (clipped): 26.000, Pasos: 1081, Mean Reward Calculado: 0.024052 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1300, pasos: 964914)\n",
            "💾 NUEVO MEJOR PROMEDIO: 19.21 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1300 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 26.00\n",
            "   Media últimos 100: 19.21 / 20.0\n",
            "   Mejor promedio histórico: 19.21\n",
            "   Estado: 📈 96.0% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "  964914/2000000: episode: 1300, duration: 81.889s, episode steps: 1081, steps per second:  13, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.016523, mae: 1.836850, mean_q: 2.225711, mean_eps: 0.100000\n",
            "📈 Episodio 1301: Recompensa total (clipped): 6.000, Pasos: 477, Mean Reward Calculado: 0.012579 (Recompensa/Pasos)\n",
            "  965391/2000000: episode: 1301, duration: 21.306s, episode steps: 477, steps per second:  22, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.016079, mae: 1.836551, mean_q: 2.227703, mean_eps: 0.100000\n",
            "📈 Episodio 1302: Recompensa total (clipped): 13.000, Pasos: 698, Mean Reward Calculado: 0.018625 (Recompensa/Pasos)\n",
            "  966089/2000000: episode: 1302, duration: 31.184s, episode steps: 698, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.017468, mae: 1.852950, mean_q: 2.248631, mean_eps: 0.100000\n",
            "📈 Episodio 1303: Recompensa total (clipped): 34.000, Pasos: 1384, Mean Reward Calculado: 0.024566 (Recompensa/Pasos)\n",
            "  967473/2000000: episode: 1303, duration: 61.294s, episode steps: 1384, steps per second:  23, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.016849, mae: 1.835624, mean_q: 2.225048, mean_eps: 0.100000\n",
            "📈 Episodio 1304: Recompensa total (clipped): 20.000, Pasos: 1080, Mean Reward Calculado: 0.018519 (Recompensa/Pasos)\n",
            "  968553/2000000: episode: 1304, duration: 47.704s, episode steps: 1080, steps per second:  23, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.015758, mae: 1.842272, mean_q: 2.233184, mean_eps: 0.100000\n",
            "📈 Episodio 1305: Recompensa total (clipped): 15.000, Pasos: 793, Mean Reward Calculado: 0.018916 (Recompensa/Pasos)\n",
            "  969346/2000000: episode: 1305, duration: 35.093s, episode steps: 793, steps per second:  23, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.017080, mae: 1.842375, mean_q: 2.230970, mean_eps: 0.100000\n",
            "📈 Episodio 1306: Recompensa total (clipped): 24.000, Pasos: 991, Mean Reward Calculado: 0.024218 (Recompensa/Pasos)\n",
            "  970337/2000000: episode: 1306, duration: 44.243s, episode steps: 991, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.016462, mae: 1.852108, mean_q: 2.244817, mean_eps: 0.100000\n",
            "📈 Episodio 1307: Recompensa total (clipped): 24.000, Pasos: 927, Mean Reward Calculado: 0.025890 (Recompensa/Pasos)\n",
            "  971264/2000000: episode: 1307, duration: 41.284s, episode steps: 927, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.016393, mae: 1.863218, mean_q: 2.259266, mean_eps: 0.100000\n",
            "📈 Episodio 1308: Recompensa total (clipped): 24.000, Pasos: 879, Mean Reward Calculado: 0.027304 (Recompensa/Pasos)\n",
            "  972143/2000000: episode: 1308, duration: 39.236s, episode steps: 879, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.017116, mae: 1.868151, mean_q: 2.264228, mean_eps: 0.100000\n",
            "📈 Episodio 1309: Recompensa total (clipped): 27.000, Pasos: 1056, Mean Reward Calculado: 0.025568 (Recompensa/Pasos)\n",
            "  973199/2000000: episode: 1309, duration: 47.342s, episode steps: 1056, steps per second:  22, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.016301, mae: 1.862577, mean_q: 2.258293, mean_eps: 0.100000\n",
            "📈 Episodio 1310: Recompensa total (clipped): 18.000, Pasos: 1339, Mean Reward Calculado: 0.013443 (Recompensa/Pasos)\n",
            "  974538/2000000: episode: 1310, duration: 59.800s, episode steps: 1339, steps per second:  22, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.016478, mae: 1.858854, mean_q: 2.255353, mean_eps: 0.100000\n",
            "📈 Episodio 1311: Recompensa total (clipped): 20.000, Pasos: 855, Mean Reward Calculado: 0.023392 (Recompensa/Pasos)\n",
            "  975393/2000000: episode: 1311, duration: 38.496s, episode steps: 855, steps per second:  22, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.017626, mae: 1.872797, mean_q: 2.269888, mean_eps: 0.100000\n",
            "📈 Episodio 1312: Recompensa total (clipped): 24.000, Pasos: 904, Mean Reward Calculado: 0.026549 (Recompensa/Pasos)\n",
            "  976297/2000000: episode: 1312, duration: 40.747s, episode steps: 904, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.016073, mae: 1.860319, mean_q: 2.254678, mean_eps: 0.100000\n",
            "📈 Episodio 1313: Recompensa total (clipped): 8.000, Pasos: 466, Mean Reward Calculado: 0.017167 (Recompensa/Pasos)\n",
            "  976763/2000000: episode: 1313, duration: 21.224s, episode steps: 466, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.017104, mae: 1.856448, mean_q: 2.249100, mean_eps: 0.100000\n",
            "📈 Episodio 1314: Recompensa total (clipped): 23.000, Pasos: 837, Mean Reward Calculado: 0.027479 (Recompensa/Pasos)\n",
            "  977600/2000000: episode: 1314, duration: 37.927s, episode steps: 837, steps per second:  22, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.016939, mae: 1.846548, mean_q: 2.235416, mean_eps: 0.100000\n",
            "📈 Episodio 1315: Recompensa total (clipped): 16.000, Pasos: 731, Mean Reward Calculado: 0.021888 (Recompensa/Pasos)\n",
            "  978331/2000000: episode: 1315, duration: 32.970s, episode steps: 731, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015369, mae: 1.840577, mean_q: 2.228289, mean_eps: 0.100000\n",
            "📈 Episodio 1316: Recompensa total (clipped): 12.000, Pasos: 727, Mean Reward Calculado: 0.016506 (Recompensa/Pasos)\n",
            "  979058/2000000: episode: 1316, duration: 33.179s, episode steps: 727, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.016960, mae: 1.847439, mean_q: 2.237500, mean_eps: 0.100000\n",
            "📈 Episodio 1317: Recompensa total (clipped): 8.000, Pasos: 638, Mean Reward Calculado: 0.012539 (Recompensa/Pasos)\n",
            "  979696/2000000: episode: 1317, duration: 28.832s, episode steps: 638, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.016365, mae: 1.850128, mean_q: 2.238798, mean_eps: 0.100000\n",
            "📊 Paso 980,000/2,000,000 (49.0%) - 28.7 pasos/seg - ETA: 9.9h - Memoria: 15235.53 MB\n",
            "📈 Episodio 1318: Recompensa total (clipped): 16.000, Pasos: 522, Mean Reward Calculado: 0.030651 (Recompensa/Pasos)\n",
            "  980218/2000000: episode: 1318, duration: 23.721s, episode steps: 522, steps per second:  22, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.016851, mae: 1.871387, mean_q: 2.270112, mean_eps: 0.100000\n",
            "📈 Episodio 1319: Recompensa total (clipped): 8.000, Pasos: 581, Mean Reward Calculado: 0.013769 (Recompensa/Pasos)\n",
            "  980799/2000000: episode: 1319, duration: 26.056s, episode steps: 581, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 0.015066, mae: 1.891145, mean_q: 2.294452, mean_eps: 0.100000\n",
            "📈 Episodio 1320: Recompensa total (clipped): 10.000, Pasos: 577, Mean Reward Calculado: 0.017331 (Recompensa/Pasos)\n",
            "  981376/2000000: episode: 1320, duration: 26.143s, episode steps: 577, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.015669, mae: 1.905542, mean_q: 2.309013, mean_eps: 0.100000\n",
            "📈 Episodio 1321: Recompensa total (clipped): 15.000, Pasos: 653, Mean Reward Calculado: 0.022971 (Recompensa/Pasos)\n",
            "  982029/2000000: episode: 1321, duration: 29.419s, episode steps: 653, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.016269, mae: 1.896081, mean_q: 2.299498, mean_eps: 0.100000\n",
            "📈 Episodio 1322: Recompensa total (clipped): 15.000, Pasos: 771, Mean Reward Calculado: 0.019455 (Recompensa/Pasos)\n",
            "  982800/2000000: episode: 1322, duration: 34.504s, episode steps: 771, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.027 [0.000, 5.000],  loss: 0.017825, mae: 1.907216, mean_q: 2.313123, mean_eps: 0.100000\n",
            "📈 Episodio 1323: Recompensa total (clipped): 21.000, Pasos: 1043, Mean Reward Calculado: 0.020134 (Recompensa/Pasos)\n",
            "  983843/2000000: episode: 1323, duration: 46.876s, episode steps: 1043, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.017076, mae: 1.899660, mean_q: 2.300642, mean_eps: 0.100000\n",
            "📈 Episodio 1324: Recompensa total (clipped): 9.000, Pasos: 662, Mean Reward Calculado: 0.013595 (Recompensa/Pasos)\n",
            "  984505/2000000: episode: 1324, duration: 30.000s, episode steps: 662, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.016800, mae: 1.892909, mean_q: 2.291010, mean_eps: 0.100000\n",
            "📈 Episodio 1325: Recompensa total (clipped): 30.000, Pasos: 1179, Mean Reward Calculado: 0.025445 (Recompensa/Pasos)\n",
            "  985684/2000000: episode: 1325, duration: 53.006s, episode steps: 1179, steps per second:  22, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.018304, mae: 1.920057, mean_q: 2.326131, mean_eps: 0.100000\n",
            "📈 Episodio 1326: Recompensa total (clipped): 21.000, Pasos: 1056, Mean Reward Calculado: 0.019886 (Recompensa/Pasos)\n",
            "  986740/2000000: episode: 1326, duration: 48.073s, episode steps: 1056, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016141, mae: 1.888744, mean_q: 2.288873, mean_eps: 0.100000\n",
            "📈 Episodio 1327: Recompensa total (clipped): 21.000, Pasos: 1006, Mean Reward Calculado: 0.020875 (Recompensa/Pasos)\n",
            "  987746/2000000: episode: 1327, duration: 45.406s, episode steps: 1006, steps per second:  22, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.016782, mae: 1.904526, mean_q: 2.305597, mean_eps: 0.100000\n",
            "📈 Episodio 1328: Recompensa total (clipped): 9.000, Pasos: 540, Mean Reward Calculado: 0.016667 (Recompensa/Pasos)\n",
            "  988286/2000000: episode: 1328, duration: 23.953s, episode steps: 540, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.015595, mae: 1.888640, mean_q: 2.285191, mean_eps: 0.100000\n",
            "📈 Episodio 1329: Recompensa total (clipped): 13.000, Pasos: 518, Mean Reward Calculado: 0.025097 (Recompensa/Pasos)\n",
            "  988804/2000000: episode: 1329, duration: 23.415s, episode steps: 518, steps per second:  22, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.016991, mae: 1.896827, mean_q: 2.295706, mean_eps: 0.100000\n",
            "📈 Episodio 1330: Recompensa total (clipped): 23.000, Pasos: 1062, Mean Reward Calculado: 0.021657 (Recompensa/Pasos)\n",
            "  989866/2000000: episode: 1330, duration: 47.763s, episode steps: 1062, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.014975, mae: 1.882389, mean_q: 2.279830, mean_eps: 0.100000\n",
            "📈 Episodio 1331: Recompensa total (clipped): 19.000, Pasos: 1014, Mean Reward Calculado: 0.018738 (Recompensa/Pasos)\n",
            "  990880/2000000: episode: 1331, duration: 46.381s, episode steps: 1014, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.016597, mae: 1.899573, mean_q: 2.301924, mean_eps: 0.100000\n",
            "📈 Episodio 1332: Recompensa total (clipped): 12.000, Pasos: 508, Mean Reward Calculado: 0.023622 (Recompensa/Pasos)\n",
            "  991388/2000000: episode: 1332, duration: 23.142s, episode steps: 508, steps per second:  22, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.016195, mae: 1.881446, mean_q: 2.279226, mean_eps: 0.100000\n",
            "📈 Episodio 1333: Recompensa total (clipped): 19.000, Pasos: 1012, Mean Reward Calculado: 0.018775 (Recompensa/Pasos)\n",
            "  992400/2000000: episode: 1333, duration: 46.300s, episode steps: 1012, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015752, mae: 1.892292, mean_q: 2.293518, mean_eps: 0.100000\n",
            "📈 Episodio 1334: Recompensa total (clipped): 14.000, Pasos: 650, Mean Reward Calculado: 0.021538 (Recompensa/Pasos)\n",
            "  993050/2000000: episode: 1334, duration: 29.658s, episode steps: 650, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.015445, mae: 1.912232, mean_q: 2.316360, mean_eps: 0.100000\n",
            "📈 Episodio 1335: Recompensa total (clipped): 25.000, Pasos: 1086, Mean Reward Calculado: 0.023020 (Recompensa/Pasos)\n",
            "  994136/2000000: episode: 1335, duration: 49.422s, episode steps: 1086, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.016266, mae: 1.886970, mean_q: 2.284950, mean_eps: 0.100000\n",
            "📈 Episodio 1336: Recompensa total (clipped): 17.000, Pasos: 844, Mean Reward Calculado: 0.020142 (Recompensa/Pasos)\n",
            "  994980/2000000: episode: 1336, duration: 38.450s, episode steps: 844, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.018176, mae: 1.895776, mean_q: 2.298311, mean_eps: 0.100000\n",
            "📈 Episodio 1337: Recompensa total (clipped): 29.000, Pasos: 1122, Mean Reward Calculado: 0.025847 (Recompensa/Pasos)\n",
            "  996102/2000000: episode: 1337, duration: 49.978s, episode steps: 1122, steps per second:  22, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.017418, mae: 1.908079, mean_q: 2.310283, mean_eps: 0.100000\n",
            "📈 Episodio 1338: Recompensa total (clipped): 18.000, Pasos: 756, Mean Reward Calculado: 0.023810 (Recompensa/Pasos)\n",
            "  996858/2000000: episode: 1338, duration: 34.417s, episode steps: 756, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.016602, mae: 1.902344, mean_q: 2.306956, mean_eps: 0.100000\n",
            "📈 Episodio 1339: Recompensa total (clipped): 28.000, Pasos: 956, Mean Reward Calculado: 0.029289 (Recompensa/Pasos)\n",
            "  997814/2000000: episode: 1339, duration: 43.456s, episode steps: 956, steps per second:  22, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.017184, mae: 1.885195, mean_q: 2.283181, mean_eps: 0.100000\n",
            "📈 Episodio 1340: Recompensa total (clipped): 16.000, Pasos: 758, Mean Reward Calculado: 0.021108 (Recompensa/Pasos)\n",
            "  998572/2000000: episode: 1340, duration: 34.338s, episode steps: 758, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.015601, mae: 1.885030, mean_q: 2.283744, mean_eps: 0.100000\n",
            "📈 Episodio 1341: Recompensa total (clipped): 18.000, Pasos: 974, Mean Reward Calculado: 0.018480 (Recompensa/Pasos)\n",
            "  999546/2000000: episode: 1341, duration: 44.130s, episode steps: 974, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.017453, mae: 1.887961, mean_q: 2.284644, mean_eps: 0.100000\n",
            "📊 Paso 1,000,000/2,000,000 (50.0%) - 28.5 pasos/seg - ETA: 9.7h - Memoria: 15246.21 MB\n",
            "📈 Episodio 1342: Recompensa total (clipped): 15.000, Pasos: 705, Mean Reward Calculado: 0.021277 (Recompensa/Pasos)\n",
            " 1000251/2000000: episode: 1342, duration: 31.687s, episode steps: 705, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.017680, mae: 1.895654, mean_q: 2.295507, mean_eps: 0.100000\n",
            "📈 Episodio 1343: Recompensa total (clipped): 13.000, Pasos: 618, Mean Reward Calculado: 0.021036 (Recompensa/Pasos)\n",
            " 1000869/2000000: episode: 1343, duration: 28.029s, episode steps: 618, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.016016, mae: 1.909032, mean_q: 2.312941, mean_eps: 0.100000\n",
            "📈 Episodio 1344: Recompensa total (clipped): 9.000, Pasos: 603, Mean Reward Calculado: 0.014925 (Recompensa/Pasos)\n",
            " 1001472/2000000: episode: 1344, duration: 27.239s, episode steps: 603, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.017327, mae: 1.874516, mean_q: 2.271585, mean_eps: 0.100000\n",
            "📈 Episodio 1345: Recompensa total (clipped): 31.000, Pasos: 1378, Mean Reward Calculado: 0.022496 (Recompensa/Pasos)\n",
            " 1002850/2000000: episode: 1345, duration: 61.474s, episode steps: 1378, steps per second:  22, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.016738, mae: 1.902299, mean_q: 2.305220, mean_eps: 0.100000\n",
            "📈 Episodio 1346: Recompensa total (clipped): 24.000, Pasos: 956, Mean Reward Calculado: 0.025105 (Recompensa/Pasos)\n",
            " 1003806/2000000: episode: 1346, duration: 43.249s, episode steps: 956, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.018060, mae: 1.914102, mean_q: 2.319095, mean_eps: 0.100000\n",
            "📈 Episodio 1347: Recompensa total (clipped): 21.000, Pasos: 940, Mean Reward Calculado: 0.022340 (Recompensa/Pasos)\n",
            " 1004746/2000000: episode: 1347, duration: 42.859s, episode steps: 940, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016673, mae: 1.882165, mean_q: 2.280359, mean_eps: 0.100000\n",
            "📈 Episodio 1348: Recompensa total (clipped): 19.000, Pasos: 1109, Mean Reward Calculado: 0.017133 (Recompensa/Pasos)\n",
            " 1005855/2000000: episode: 1348, duration: 50.379s, episode steps: 1109, steps per second:  22, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.018442, mae: 1.895141, mean_q: 2.293280, mean_eps: 0.100000\n",
            "📈 Episodio 1349: Recompensa total (clipped): 22.000, Pasos: 1022, Mean Reward Calculado: 0.021526 (Recompensa/Pasos)\n",
            " 1006877/2000000: episode: 1349, duration: 46.692s, episode steps: 1022, steps per second:  22, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.018198, mae: 1.916033, mean_q: 2.322122, mean_eps: 0.100000\n",
            "📈 Episodio 1350: Recompensa total (clipped): 16.000, Pasos: 707, Mean Reward Calculado: 0.022631 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1350, pasos: 1007584)\n",
            "💾 NUEVO MEJOR PROMEDIO: 19.43 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1350 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 16.00\n",
            "   Media últimos 100: 19.43 / 20.0\n",
            "   Mejor promedio histórico: 19.43\n",
            "   Estado: 📈 97.2% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1007584/2000000: episode: 1350, duration: 113.695s, episode steps: 707, steps per second:   6, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.487 [0.000, 5.000],  loss: 0.016609, mae: 1.886106, mean_q: 2.287597, mean_eps: 0.100000\n",
            "📈 Episodio 1351: Recompensa total (clipped): 17.000, Pasos: 683, Mean Reward Calculado: 0.024890 (Recompensa/Pasos)\n",
            " 1008267/2000000: episode: 1351, duration: 30.445s, episode steps: 683, steps per second:  22, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.017961, mae: 1.888742, mean_q: 2.285902, mean_eps: 0.100000\n",
            "📈 Episodio 1352: Recompensa total (clipped): 10.000, Pasos: 666, Mean Reward Calculado: 0.015015 (Recompensa/Pasos)\n",
            " 1008933/2000000: episode: 1352, duration: 30.001s, episode steps: 666, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.019289, mae: 1.906491, mean_q: 2.307416, mean_eps: 0.100000\n",
            "📈 Episodio 1353: Recompensa total (clipped): 19.000, Pasos: 1037, Mean Reward Calculado: 0.018322 (Recompensa/Pasos)\n",
            " 1009970/2000000: episode: 1353, duration: 46.762s, episode steps: 1037, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.017959, mae: 1.899553, mean_q: 2.299144, mean_eps: 0.100000\n",
            "📈 Episodio 1354: Recompensa total (clipped): 26.000, Pasos: 1031, Mean Reward Calculado: 0.025218 (Recompensa/Pasos)\n",
            " 1011001/2000000: episode: 1354, duration: 47.370s, episode steps: 1031, steps per second:  22, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.017914, mae: 1.961496, mean_q: 2.379072, mean_eps: 0.100000\n",
            "📈 Episodio 1355: Recompensa total (clipped): 14.000, Pasos: 617, Mean Reward Calculado: 0.022690 (Recompensa/Pasos)\n",
            " 1011618/2000000: episode: 1355, duration: 27.919s, episode steps: 617, steps per second:  22, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.020808, mae: 1.946916, mean_q: 2.360413, mean_eps: 0.100000\n",
            "📈 Episodio 1356: Recompensa total (clipped): 21.000, Pasos: 764, Mean Reward Calculado: 0.027487 (Recompensa/Pasos)\n",
            " 1012382/2000000: episode: 1356, duration: 34.721s, episode steps: 764, steps per second:  22, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.018322, mae: 1.943155, mean_q: 2.355702, mean_eps: 0.100000\n",
            "📈 Episodio 1357: Recompensa total (clipped): 9.000, Pasos: 654, Mean Reward Calculado: 0.013761 (Recompensa/Pasos)\n",
            " 1013036/2000000: episode: 1357, duration: 29.334s, episode steps: 654, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.018443, mae: 1.946925, mean_q: 2.358420, mean_eps: 0.100000\n",
            "📈 Episodio 1358: Recompensa total (clipped): 13.000, Pasos: 659, Mean Reward Calculado: 0.019727 (Recompensa/Pasos)\n",
            " 1013695/2000000: episode: 1358, duration: 30.042s, episode steps: 659, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.017569, mae: 1.926297, mean_q: 2.337050, mean_eps: 0.100000\n",
            "📈 Episodio 1359: Recompensa total (clipped): 10.000, Pasos: 512, Mean Reward Calculado: 0.019531 (Recompensa/Pasos)\n",
            " 1014207/2000000: episode: 1359, duration: 23.005s, episode steps: 512, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.019596, mae: 1.942409, mean_q: 2.353990, mean_eps: 0.100000\n",
            "📈 Episodio 1360: Recompensa total (clipped): 28.000, Pasos: 962, Mean Reward Calculado: 0.029106 (Recompensa/Pasos)\n",
            " 1015169/2000000: episode: 1360, duration: 43.527s, episode steps: 962, steps per second:  22, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.157 [0.000, 5.000],  loss: 0.017568, mae: 1.936643, mean_q: 2.345178, mean_eps: 0.100000\n",
            "📈 Episodio 1361: Recompensa total (clipped): 22.000, Pasos: 927, Mean Reward Calculado: 0.023732 (Recompensa/Pasos)\n",
            " 1016096/2000000: episode: 1361, duration: 41.995s, episode steps: 927, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016120, mae: 1.936431, mean_q: 2.344740, mean_eps: 0.100000\n",
            "📈 Episodio 1362: Recompensa total (clipped): 13.000, Pasos: 628, Mean Reward Calculado: 0.020701 (Recompensa/Pasos)\n",
            " 1016724/2000000: episode: 1362, duration: 28.344s, episode steps: 628, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.019036, mae: 1.954566, mean_q: 2.368437, mean_eps: 0.100000\n",
            "📈 Episodio 1363: Recompensa total (clipped): 8.000, Pasos: 535, Mean Reward Calculado: 0.014953 (Recompensa/Pasos)\n",
            " 1017259/2000000: episode: 1363, duration: 24.205s, episode steps: 535, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.019635, mae: 1.960963, mean_q: 2.377269, mean_eps: 0.100000\n",
            "📈 Episodio 1364: Recompensa total (clipped): 13.000, Pasos: 718, Mean Reward Calculado: 0.018106 (Recompensa/Pasos)\n",
            " 1017977/2000000: episode: 1364, duration: 32.201s, episode steps: 718, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.018480, mae: 1.945326, mean_q: 2.357354, mean_eps: 0.100000\n",
            "📈 Episodio 1365: Recompensa total (clipped): 19.000, Pasos: 1020, Mean Reward Calculado: 0.018627 (Recompensa/Pasos)\n",
            " 1018997/2000000: episode: 1365, duration: 45.304s, episode steps: 1020, steps per second:  23, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.683 [0.000, 5.000],  loss: 0.017342, mae: 1.926536, mean_q: 2.333477, mean_eps: 0.100000\n",
            "📈 Episodio 1366: Recompensa total (clipped): 19.000, Pasos: 804, Mean Reward Calculado: 0.023632 (Recompensa/Pasos)\n",
            " 1019801/2000000: episode: 1366, duration: 36.170s, episode steps: 804, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.904 [0.000, 5.000],  loss: 0.018144, mae: 1.932443, mean_q: 2.339822, mean_eps: 0.100000\n",
            "📊 Paso 1,020,000/2,000,000 (51.0%) - 28.3 pasos/seg - ETA: 9.6h - Memoria: 15195.37 MB\n",
            "📈 Episodio 1367: Recompensa total (clipped): 11.000, Pasos: 939, Mean Reward Calculado: 0.011715 (Recompensa/Pasos)\n",
            " 1020740/2000000: episode: 1367, duration: 43.036s, episode steps: 939, steps per second:  22, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.018388, mae: 1.952502, mean_q: 2.366458, mean_eps: 0.100000\n",
            "📈 Episodio 1368: Recompensa total (clipped): 24.000, Pasos: 1266, Mean Reward Calculado: 0.018957 (Recompensa/Pasos)\n",
            " 1022006/2000000: episode: 1368, duration: 57.169s, episode steps: 1266, steps per second:  22, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.016992, mae: 1.981114, mean_q: 2.401554, mean_eps: 0.100000\n",
            "📈 Episodio 1369: Recompensa total (clipped): 25.000, Pasos: 1166, Mean Reward Calculado: 0.021441 (Recompensa/Pasos)\n",
            " 1023172/2000000: episode: 1369, duration: 52.549s, episode steps: 1166, steps per second:  22, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.018642, mae: 1.954222, mean_q: 2.366467, mean_eps: 0.100000\n",
            "📈 Episodio 1370: Recompensa total (clipped): 10.000, Pasos: 615, Mean Reward Calculado: 0.016260 (Recompensa/Pasos)\n",
            " 1023787/2000000: episode: 1370, duration: 27.737s, episode steps: 615, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.943 [0.000, 5.000],  loss: 0.018162, mae: 1.954655, mean_q: 2.365401, mean_eps: 0.100000\n",
            "📈 Episodio 1371: Recompensa total (clipped): 11.000, Pasos: 622, Mean Reward Calculado: 0.017685 (Recompensa/Pasos)\n",
            " 1024409/2000000: episode: 1371, duration: 28.479s, episode steps: 622, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.510 [0.000, 5.000],  loss: 0.020788, mae: 1.953756, mean_q: 2.366082, mean_eps: 0.100000\n",
            "📈 Episodio 1372: Recompensa total (clipped): 8.000, Pasos: 484, Mean Reward Calculado: 0.016529 (Recompensa/Pasos)\n",
            " 1024893/2000000: episode: 1372, duration: 21.440s, episode steps: 484, steps per second:  23, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016253, mae: 1.948097, mean_q: 2.359989, mean_eps: 0.100000\n",
            "📈 Episodio 1373: Recompensa total (clipped): 18.000, Pasos: 730, Mean Reward Calculado: 0.024658 (Recompensa/Pasos)\n",
            " 1025623/2000000: episode: 1373, duration: 32.621s, episode steps: 730, steps per second:  22, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.017722, mae: 1.950281, mean_q: 2.361595, mean_eps: 0.100000\n",
            "📈 Episodio 1374: Recompensa total (clipped): 12.000, Pasos: 642, Mean Reward Calculado: 0.018692 (Recompensa/Pasos)\n",
            " 1026265/2000000: episode: 1374, duration: 29.171s, episode steps: 642, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.232 [0.000, 5.000],  loss: 0.018237, mae: 1.968992, mean_q: 2.385647, mean_eps: 0.100000\n",
            "📈 Episodio 1375: Recompensa total (clipped): 17.000, Pasos: 923, Mean Reward Calculado: 0.018418 (Recompensa/Pasos)\n",
            " 1027188/2000000: episode: 1375, duration: 41.702s, episode steps: 923, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.019767, mae: 1.958765, mean_q: 2.371732, mean_eps: 0.100000\n",
            "📈 Episodio 1376: Recompensa total (clipped): 11.000, Pasos: 848, Mean Reward Calculado: 0.012972 (Recompensa/Pasos)\n",
            " 1028036/2000000: episode: 1376, duration: 38.497s, episode steps: 848, steps per second:  22, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.019118, mae: 1.951632, mean_q: 2.363121, mean_eps: 0.100000\n",
            "📈 Episodio 1377: Recompensa total (clipped): 18.000, Pasos: 809, Mean Reward Calculado: 0.022250 (Recompensa/Pasos)\n",
            " 1028845/2000000: episode: 1377, duration: 36.603s, episode steps: 809, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.017995, mae: 1.937007, mean_q: 2.345121, mean_eps: 0.100000\n",
            "📈 Episodio 1378: Recompensa total (clipped): 16.000, Pasos: 914, Mean Reward Calculado: 0.017505 (Recompensa/Pasos)\n",
            " 1029759/2000000: episode: 1378, duration: 41.166s, episode steps: 914, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.018583, mae: 1.937828, mean_q: 2.346607, mean_eps: 0.100000\n",
            "📈 Episodio 1379: Recompensa total (clipped): 7.000, Pasos: 561, Mean Reward Calculado: 0.012478 (Recompensa/Pasos)\n",
            " 1030320/2000000: episode: 1379, duration: 25.088s, episode steps: 561, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.018135, mae: 1.957124, mean_q: 2.372027, mean_eps: 0.100000\n",
            "📈 Episodio 1380: Recompensa total (clipped): 24.000, Pasos: 875, Mean Reward Calculado: 0.027429 (Recompensa/Pasos)\n",
            " 1031195/2000000: episode: 1380, duration: 39.638s, episode steps: 875, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.017708, mae: 1.982254, mean_q: 2.400532, mean_eps: 0.100000\n",
            "📈 Episodio 1381: Recompensa total (clipped): 20.000, Pasos: 856, Mean Reward Calculado: 0.023364 (Recompensa/Pasos)\n",
            " 1032051/2000000: episode: 1381, duration: 38.655s, episode steps: 856, steps per second:  22, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.741 [0.000, 5.000],  loss: 0.016912, mae: 1.976046, mean_q: 2.393936, mean_eps: 0.100000\n",
            "📈 Episodio 1382: Recompensa total (clipped): 22.000, Pasos: 964, Mean Reward Calculado: 0.022822 (Recompensa/Pasos)\n",
            " 1033015/2000000: episode: 1382, duration: 43.524s, episode steps: 964, steps per second:  22, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.601 [0.000, 5.000],  loss: 0.018481, mae: 1.980745, mean_q: 2.399650, mean_eps: 0.100000\n",
            "📈 Episodio 1383: Recompensa total (clipped): 6.000, Pasos: 525, Mean Reward Calculado: 0.011429 (Recompensa/Pasos)\n",
            " 1033540/2000000: episode: 1383, duration: 23.719s, episode steps: 525, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016712, mae: 1.981742, mean_q: 2.399519, mean_eps: 0.100000\n",
            "📈 Episodio 1384: Recompensa total (clipped): 16.000, Pasos: 813, Mean Reward Calculado: 0.019680 (Recompensa/Pasos)\n",
            " 1034353/2000000: episode: 1384, duration: 36.395s, episode steps: 813, steps per second:  22, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.018387, mae: 1.981558, mean_q: 2.397038, mean_eps: 0.100000\n",
            "📈 Episodio 1385: Recompensa total (clipped): 8.000, Pasos: 501, Mean Reward Calculado: 0.015968 (Recompensa/Pasos)\n",
            " 1034854/2000000: episode: 1385, duration: 22.592s, episode steps: 501, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.018222, mae: 1.954852, mean_q: 2.367376, mean_eps: 0.100000\n",
            "📈 Episodio 1386: Recompensa total (clipped): 23.000, Pasos: 948, Mean Reward Calculado: 0.024262 (Recompensa/Pasos)\n",
            " 1035802/2000000: episode: 1386, duration: 42.872s, episode steps: 948, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.017744, mae: 1.969667, mean_q: 2.385574, mean_eps: 0.100000\n",
            "📈 Episodio 1387: Recompensa total (clipped): 24.000, Pasos: 922, Mean Reward Calculado: 0.026030 (Recompensa/Pasos)\n",
            " 1036724/2000000: episode: 1387, duration: 41.842s, episode steps: 922, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.015866, mae: 1.955511, mean_q: 2.368088, mean_eps: 0.100000\n",
            "📈 Episodio 1388: Recompensa total (clipped): 23.000, Pasos: 900, Mean Reward Calculado: 0.025556 (Recompensa/Pasos)\n",
            " 1037624/2000000: episode: 1388, duration: 40.340s, episode steps: 900, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.018826, mae: 1.982819, mean_q: 2.399761, mean_eps: 0.100000\n",
            "📈 Episodio 1389: Recompensa total (clipped): 23.000, Pasos: 861, Mean Reward Calculado: 0.026713 (Recompensa/Pasos)\n",
            " 1038485/2000000: episode: 1389, duration: 38.599s, episode steps: 861, steps per second:  22, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.018707, mae: 1.982753, mean_q: 2.399889, mean_eps: 0.100000\n",
            "📈 Episodio 1390: Recompensa total (clipped): 13.000, Pasos: 719, Mean Reward Calculado: 0.018081 (Recompensa/Pasos)\n",
            " 1039204/2000000: episode: 1390, duration: 32.337s, episode steps: 719, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.020559, mae: 1.967996, mean_q: 2.384383, mean_eps: 0.100000\n",
            "📈 Episodio 1391: Recompensa total (clipped): 11.000, Pasos: 543, Mean Reward Calculado: 0.020258 (Recompensa/Pasos)\n",
            " 1039747/2000000: episode: 1391, duration: 24.600s, episode steps: 543, steps per second:  22, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.016534, mae: 1.961469, mean_q: 2.372229, mean_eps: 0.100000\n",
            "📊 Paso 1,040,000/2,000,000 (52.0%) - 28.1 pasos/seg - ETA: 9.5h - Memoria: 15192.25 MB\n",
            "📈 Episodio 1392: Recompensa total (clipped): 21.000, Pasos: 923, Mean Reward Calculado: 0.022752 (Recompensa/Pasos)\n",
            " 1040670/2000000: episode: 1392, duration: 41.627s, episode steps: 923, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.016841, mae: 1.977633, mean_q: 2.393285, mean_eps: 0.100000\n",
            "📈 Episodio 1393: Recompensa total (clipped): 18.000, Pasos: 768, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            " 1041438/2000000: episode: 1393, duration: 34.257s, episode steps: 768, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.018239, mae: 1.988363, mean_q: 2.410216, mean_eps: 0.100000\n",
            "📈 Episodio 1394: Recompensa total (clipped): 17.000, Pasos: 822, Mean Reward Calculado: 0.020681 (Recompensa/Pasos)\n",
            " 1042260/2000000: episode: 1394, duration: 37.302s, episode steps: 822, steps per second:  22, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.017874, mae: 1.968187, mean_q: 2.381382, mean_eps: 0.100000\n",
            "📈 Episodio 1395: Recompensa total (clipped): 25.000, Pasos: 1290, Mean Reward Calculado: 0.019380 (Recompensa/Pasos)\n",
            " 1043550/2000000: episode: 1395, duration: 57.669s, episode steps: 1290, steps per second:  22, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.018345, mae: 1.967384, mean_q: 2.380924, mean_eps: 0.100000\n",
            "📈 Episodio 1396: Recompensa total (clipped): 17.000, Pasos: 896, Mean Reward Calculado: 0.018973 (Recompensa/Pasos)\n",
            " 1044446/2000000: episode: 1396, duration: 40.416s, episode steps: 896, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.017892, mae: 1.978223, mean_q: 2.393283, mean_eps: 0.100000\n",
            "📈 Episodio 1397: Recompensa total (clipped): 30.000, Pasos: 1140, Mean Reward Calculado: 0.026316 (Recompensa/Pasos)\n",
            " 1045586/2000000: episode: 1397, duration: 51.378s, episode steps: 1140, steps per second:  22, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.017509, mae: 1.976477, mean_q: 2.390325, mean_eps: 0.100000\n",
            "📈 Episodio 1398: Recompensa total (clipped): 20.000, Pasos: 1088, Mean Reward Calculado: 0.018382 (Recompensa/Pasos)\n",
            " 1046674/2000000: episode: 1398, duration: 48.577s, episode steps: 1088, steps per second:  22, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.017760, mae: 1.975955, mean_q: 2.392218, mean_eps: 0.100000\n",
            "📈 Episodio 1399: Recompensa total (clipped): 20.000, Pasos: 802, Mean Reward Calculado: 0.024938 (Recompensa/Pasos)\n",
            " 1047476/2000000: episode: 1399, duration: 35.966s, episode steps: 802, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.017273, mae: 1.967022, mean_q: 2.381205, mean_eps: 0.100000\n",
            "📈 Episodio 1400: Recompensa total (clipped): 27.000, Pasos: 1197, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1400, pasos: 1048673)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.69 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1400 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 27.00\n",
            "   Media últimos 100: 17.69 / 20.0\n",
            "   Mejor promedio histórico: 17.69\n",
            "   Estado: 📈 88.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1048673/2000000: episode: 1400, duration: 124.533s, episode steps: 1197, steps per second:  10, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.017965, mae: 1.960659, mean_q: 2.371245, mean_eps: 0.100000\n",
            "📈 Episodio 1401: Recompensa total (clipped): 29.000, Pasos: 1118, Mean Reward Calculado: 0.025939 (Recompensa/Pasos)\n",
            " 1049791/2000000: episode: 1401, duration: 49.858s, episode steps: 1118, steps per second:  22, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.018150, mae: 1.964093, mean_q: 2.375642, mean_eps: 0.100000\n",
            "📈 Episodio 1402: Recompensa total (clipped): 31.000, Pasos: 1390, Mean Reward Calculado: 0.022302 (Recompensa/Pasos)\n",
            " 1051181/2000000: episode: 1402, duration: 63.635s, episode steps: 1390, steps per second:  22, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.017705, mae: 1.987748, mean_q: 2.406180, mean_eps: 0.100000\n",
            "📈 Episodio 1403: Recompensa total (clipped): 20.000, Pasos: 1195, Mean Reward Calculado: 0.016736 (Recompensa/Pasos)\n",
            " 1052376/2000000: episode: 1403, duration: 54.027s, episode steps: 1195, steps per second:  22, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.018266, mae: 2.002262, mean_q: 2.423475, mean_eps: 0.100000\n",
            "📈 Episodio 1404: Recompensa total (clipped): 18.000, Pasos: 902, Mean Reward Calculado: 0.019956 (Recompensa/Pasos)\n",
            " 1053278/2000000: episode: 1404, duration: 40.948s, episode steps: 902, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.018364, mae: 1.980600, mean_q: 2.394970, mean_eps: 0.100000\n",
            "📈 Episodio 1405: Recompensa total (clipped): 10.000, Pasos: 760, Mean Reward Calculado: 0.013158 (Recompensa/Pasos)\n",
            " 1054038/2000000: episode: 1405, duration: 34.803s, episode steps: 760, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.017062, mae: 1.976126, mean_q: 2.390148, mean_eps: 0.100000\n",
            "📈 Episodio 1406: Recompensa total (clipped): 23.000, Pasos: 1161, Mean Reward Calculado: 0.019811 (Recompensa/Pasos)\n",
            " 1055199/2000000: episode: 1406, duration: 52.177s, episode steps: 1161, steps per second:  22, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.018297, mae: 1.980079, mean_q: 2.392951, mean_eps: 0.100000\n",
            "📈 Episodio 1407: Recompensa total (clipped): 23.000, Pasos: 883, Mean Reward Calculado: 0.026048 (Recompensa/Pasos)\n",
            " 1056082/2000000: episode: 1407, duration: 39.717s, episode steps: 883, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.017306, mae: 1.990220, mean_q: 2.406640, mean_eps: 0.100000\n",
            "📈 Episodio 1408: Recompensa total (clipped): 9.000, Pasos: 508, Mean Reward Calculado: 0.017717 (Recompensa/Pasos)\n",
            " 1056590/2000000: episode: 1408, duration: 22.833s, episode steps: 508, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.017228, mae: 1.990033, mean_q: 2.406050, mean_eps: 0.100000\n",
            "📈 Episodio 1409: Recompensa total (clipped): 12.000, Pasos: 544, Mean Reward Calculado: 0.022059 (Recompensa/Pasos)\n",
            " 1057134/2000000: episode: 1409, duration: 24.752s, episode steps: 544, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.018778, mae: 1.986400, mean_q: 2.399487, mean_eps: 0.100000\n",
            "📈 Episodio 1410: Recompensa total (clipped): 27.000, Pasos: 1000, Mean Reward Calculado: 0.027000 (Recompensa/Pasos)\n",
            " 1058134/2000000: episode: 1410, duration: 45.103s, episode steps: 1000, steps per second:  22, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.017891, mae: 2.003066, mean_q: 2.419553, mean_eps: 0.100000\n",
            "📈 Episodio 1411: Recompensa total (clipped): 14.000, Pasos: 971, Mean Reward Calculado: 0.014418 (Recompensa/Pasos)\n",
            " 1059105/2000000: episode: 1411, duration: 43.866s, episode steps: 971, steps per second:  22, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.018616, mae: 1.988822, mean_q: 2.404573, mean_eps: 0.100000\n",
            "📈 Episodio 1412: Recompensa total (clipped): 21.000, Pasos: 868, Mean Reward Calculado: 0.024194 (Recompensa/Pasos)\n",
            " 1059973/2000000: episode: 1412, duration: 39.087s, episode steps: 868, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.020330, mae: 1.979138, mean_q: 2.394676, mean_eps: 0.100000\n",
            "📊 Paso 1,060,000/2,000,000 (53.0%) - 28.0 pasos/seg - ETA: 9.3h - Memoria: 15230.68 MB\n",
            "📈 Episodio 1413: Recompensa total (clipped): 21.000, Pasos: 682, Mean Reward Calculado: 0.030792 (Recompensa/Pasos)\n",
            " 1060655/2000000: episode: 1413, duration: 30.441s, episode steps: 682, steps per second:  22, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.015811, mae: 2.007109, mean_q: 2.429399, mean_eps: 0.100000\n",
            "📈 Episodio 1414: Recompensa total (clipped): 22.000, Pasos: 1055, Mean Reward Calculado: 0.020853 (Recompensa/Pasos)\n",
            " 1061710/2000000: episode: 1414, duration: 47.244s, episode steps: 1055, steps per second:  22, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.017792, mae: 1.983833, mean_q: 2.400443, mean_eps: 0.100000\n",
            "📈 Episodio 1415: Recompensa total (clipped): 12.000, Pasos: 554, Mean Reward Calculado: 0.021661 (Recompensa/Pasos)\n",
            " 1062264/2000000: episode: 1415, duration: 25.199s, episode steps: 554, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.017797, mae: 1.987929, mean_q: 2.404605, mean_eps: 0.100000\n",
            "📈 Episodio 1416: Recompensa total (clipped): 8.000, Pasos: 562, Mean Reward Calculado: 0.014235 (Recompensa/Pasos)\n",
            " 1062826/2000000: episode: 1416, duration: 25.667s, episode steps: 562, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.017757, mae: 1.998742, mean_q: 2.418423, mean_eps: 0.100000\n",
            "📈 Episodio 1417: Recompensa total (clipped): 7.000, Pasos: 534, Mean Reward Calculado: 0.013109 (Recompensa/Pasos)\n",
            " 1063360/2000000: episode: 1417, duration: 24.222s, episode steps: 534, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.016770, mae: 1.977132, mean_q: 2.390834, mean_eps: 0.100000\n",
            "📈 Episodio 1418: Recompensa total (clipped): 25.000, Pasos: 838, Mean Reward Calculado: 0.029833 (Recompensa/Pasos)\n",
            " 1064198/2000000: episode: 1418, duration: 37.798s, episode steps: 838, steps per second:  22, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.016666, mae: 1.992875, mean_q: 2.410811, mean_eps: 0.100000\n",
            "📈 Episodio 1419: Recompensa total (clipped): 14.000, Pasos: 889, Mean Reward Calculado: 0.015748 (Recompensa/Pasos)\n",
            " 1065087/2000000: episode: 1419, duration: 39.889s, episode steps: 889, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.016006, mae: 1.992246, mean_q: 2.410113, mean_eps: 0.100000\n",
            "📈 Episodio 1420: Recompensa total (clipped): 19.000, Pasos: 1021, Mean Reward Calculado: 0.018609 (Recompensa/Pasos)\n",
            " 1066108/2000000: episode: 1420, duration: 45.703s, episode steps: 1021, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.019530, mae: 1.992057, mean_q: 2.409408, mean_eps: 0.100000\n",
            "📈 Episodio 1421: Recompensa total (clipped): 21.000, Pasos: 1115, Mean Reward Calculado: 0.018834 (Recompensa/Pasos)\n",
            " 1067223/2000000: episode: 1421, duration: 50.198s, episode steps: 1115, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015989, mae: 1.982803, mean_q: 2.397870, mean_eps: 0.100000\n",
            "📈 Episodio 1422: Recompensa total (clipped): 16.000, Pasos: 865, Mean Reward Calculado: 0.018497 (Recompensa/Pasos)\n",
            " 1068088/2000000: episode: 1422, duration: 38.397s, episode steps: 865, steps per second:  23, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.018458, mae: 2.005747, mean_q: 2.429144, mean_eps: 0.100000\n",
            "📈 Episodio 1423: Recompensa total (clipped): 23.000, Pasos: 908, Mean Reward Calculado: 0.025330 (Recompensa/Pasos)\n",
            " 1068996/2000000: episode: 1423, duration: 41.306s, episode steps: 908, steps per second:  22, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.096 [0.000, 5.000],  loss: 0.017949, mae: 1.988297, mean_q: 2.404724, mean_eps: 0.100000\n",
            "📈 Episodio 1424: Recompensa total (clipped): 10.000, Pasos: 611, Mean Reward Calculado: 0.016367 (Recompensa/Pasos)\n",
            " 1069607/2000000: episode: 1424, duration: 27.515s, episode steps: 611, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.019244, mae: 1.984948, mean_q: 2.398929, mean_eps: 0.100000\n",
            "📈 Episodio 1425: Recompensa total (clipped): 17.000, Pasos: 720, Mean Reward Calculado: 0.023611 (Recompensa/Pasos)\n",
            " 1070327/2000000: episode: 1425, duration: 32.385s, episode steps: 720, steps per second:  22, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.017102, mae: 1.987666, mean_q: 2.401721, mean_eps: 0.100000\n",
            "📈 Episodio 1426: Recompensa total (clipped): 9.000, Pasos: 734, Mean Reward Calculado: 0.012262 (Recompensa/Pasos)\n",
            " 1071061/2000000: episode: 1426, duration: 33.022s, episode steps: 734, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.018575, mae: 1.993806, mean_q: 2.410655, mean_eps: 0.100000\n",
            "📈 Episodio 1427: Recompensa total (clipped): 23.000, Pasos: 1058, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            " 1072119/2000000: episode: 1427, duration: 47.247s, episode steps: 1058, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.017304, mae: 1.989562, mean_q: 2.406303, mean_eps: 0.100000\n",
            "📈 Episodio 1428: Recompensa total (clipped): 7.000, Pasos: 381, Mean Reward Calculado: 0.018373 (Recompensa/Pasos)\n",
            " 1072500/2000000: episode: 1428, duration: 17.350s, episode steps: 381, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015274, mae: 1.965028, mean_q: 2.377370, mean_eps: 0.100000\n",
            "📈 Episodio 1429: Recompensa total (clipped): 16.000, Pasos: 659, Mean Reward Calculado: 0.024279 (Recompensa/Pasos)\n",
            " 1073159/2000000: episode: 1429, duration: 29.760s, episode steps: 659, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.015923, mae: 1.994408, mean_q: 2.411698, mean_eps: 0.100000\n",
            "📈 Episodio 1430: Recompensa total (clipped): 9.000, Pasos: 614, Mean Reward Calculado: 0.014658 (Recompensa/Pasos)\n",
            " 1073773/2000000: episode: 1430, duration: 27.770s, episode steps: 614, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.018077, mae: 1.977792, mean_q: 2.389849, mean_eps: 0.100000\n",
            "📈 Episodio 1431: Recompensa total (clipped): 20.000, Pasos: 812, Mean Reward Calculado: 0.024631 (Recompensa/Pasos)\n",
            " 1074585/2000000: episode: 1431, duration: 36.507s, episode steps: 812, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.017898, mae: 1.978548, mean_q: 2.391831, mean_eps: 0.100000\n",
            "📈 Episodio 1432: Recompensa total (clipped): 12.000, Pasos: 1051, Mean Reward Calculado: 0.011418 (Recompensa/Pasos)\n",
            " 1075636/2000000: episode: 1432, duration: 46.864s, episode steps: 1051, steps per second:  22, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.018183, mae: 1.976254, mean_q: 2.385819, mean_eps: 0.100000\n",
            "📈 Episodio 1433: Recompensa total (clipped): 10.000, Pasos: 621, Mean Reward Calculado: 0.016103 (Recompensa/Pasos)\n",
            " 1076257/2000000: episode: 1433, duration: 27.694s, episode steps: 621, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.017508, mae: 1.974415, mean_q: 2.386303, mean_eps: 0.100000\n",
            "📈 Episodio 1434: Recompensa total (clipped): 11.000, Pasos: 652, Mean Reward Calculado: 0.016871 (Recompensa/Pasos)\n",
            " 1076909/2000000: episode: 1434, duration: 28.992s, episode steps: 652, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.018428, mae: 1.978691, mean_q: 2.392295, mean_eps: 0.100000\n",
            "📈 Episodio 1435: Recompensa total (clipped): 12.000, Pasos: 813, Mean Reward Calculado: 0.014760 (Recompensa/Pasos)\n",
            " 1077722/2000000: episode: 1435, duration: 36.007s, episode steps: 813, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.016996, mae: 1.975105, mean_q: 2.386057, mean_eps: 0.100000\n",
            "📈 Episodio 1436: Recompensa total (clipped): 29.000, Pasos: 1485, Mean Reward Calculado: 0.019529 (Recompensa/Pasos)\n",
            " 1079207/2000000: episode: 1436, duration: 66.447s, episode steps: 1485, steps per second:  22, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.017019, mae: 1.981161, mean_q: 2.394064, mean_eps: 0.100000\n",
            "📊 Paso 1,080,000/2,000,000 (54.0%) - 27.8 pasos/seg - ETA: 9.2h - Memoria: 15252.83 MB\n",
            "📈 Episodio 1437: Recompensa total (clipped): 31.000, Pasos: 1271, Mean Reward Calculado: 0.024390 (Recompensa/Pasos)\n",
            " 1080478/2000000: episode: 1437, duration: 56.586s, episode steps: 1271, steps per second:  22, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.017338, mae: 1.983394, mean_q: 2.398930, mean_eps: 0.100000\n",
            "📈 Episodio 1438: Recompensa total (clipped): 24.000, Pasos: 846, Mean Reward Calculado: 0.028369 (Recompensa/Pasos)\n",
            " 1081324/2000000: episode: 1438, duration: 38.250s, episode steps: 846, steps per second:  22, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.401 [0.000, 5.000],  loss: 0.018063, mae: 1.981392, mean_q: 2.395427, mean_eps: 0.100000\n",
            "📈 Episodio 1439: Recompensa total (clipped): 8.000, Pasos: 689, Mean Reward Calculado: 0.011611 (Recompensa/Pasos)\n",
            " 1082013/2000000: episode: 1439, duration: 30.787s, episode steps: 689, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.018322, mae: 2.003049, mean_q: 2.419799, mean_eps: 0.100000\n",
            "📈 Episodio 1440: Recompensa total (clipped): 8.000, Pasos: 477, Mean Reward Calculado: 0.016771 (Recompensa/Pasos)\n",
            " 1082490/2000000: episode: 1440, duration: 21.392s, episode steps: 477, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.018418, mae: 1.993672, mean_q: 2.410344, mean_eps: 0.100000\n",
            "📈 Episodio 1441: Recompensa total (clipped): 15.000, Pasos: 935, Mean Reward Calculado: 0.016043 (Recompensa/Pasos)\n",
            " 1083425/2000000: episode: 1441, duration: 42.623s, episode steps: 935, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.018547, mae: 1.983400, mean_q: 2.398197, mean_eps: 0.100000\n",
            "📈 Episodio 1442: Recompensa total (clipped): 15.000, Pasos: 636, Mean Reward Calculado: 0.023585 (Recompensa/Pasos)\n",
            " 1084061/2000000: episode: 1442, duration: 28.909s, episode steps: 636, steps per second:  22, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.016987, mae: 1.961306, mean_q: 2.368768, mean_eps: 0.100000\n",
            "📈 Episodio 1443: Recompensa total (clipped): 15.000, Pasos: 607, Mean Reward Calculado: 0.024712 (Recompensa/Pasos)\n",
            " 1084668/2000000: episode: 1443, duration: 27.768s, episode steps: 607, steps per second:  22, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.018301, mae: 1.972666, mean_q: 2.380906, mean_eps: 0.100000\n",
            "📈 Episodio 1444: Recompensa total (clipped): 18.000, Pasos: 1023, Mean Reward Calculado: 0.017595 (Recompensa/Pasos)\n",
            " 1085691/2000000: episode: 1444, duration: 46.519s, episode steps: 1023, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.017429, mae: 1.979626, mean_q: 2.390968, mean_eps: 0.100000\n",
            "📈 Episodio 1445: Recompensa total (clipped): 13.000, Pasos: 827, Mean Reward Calculado: 0.015719 (Recompensa/Pasos)\n",
            " 1086518/2000000: episode: 1445, duration: 37.434s, episode steps: 827, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.017560, mae: 1.997526, mean_q: 2.412506, mean_eps: 0.100000\n",
            "📈 Episodio 1446: Recompensa total (clipped): 21.000, Pasos: 997, Mean Reward Calculado: 0.021063 (Recompensa/Pasos)\n",
            " 1087515/2000000: episode: 1446, duration: 44.893s, episode steps: 997, steps per second:  22, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.018259, mae: 1.985413, mean_q: 2.397887, mean_eps: 0.100000\n",
            "📈 Episodio 1447: Recompensa total (clipped): 21.000, Pasos: 888, Mean Reward Calculado: 0.023649 (Recompensa/Pasos)\n",
            " 1088403/2000000: episode: 1447, duration: 40.012s, episode steps: 888, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.017869, mae: 1.986431, mean_q: 2.400671, mean_eps: 0.100000\n",
            "📈 Episodio 1448: Recompensa total (clipped): 11.000, Pasos: 581, Mean Reward Calculado: 0.018933 (Recompensa/Pasos)\n",
            " 1088984/2000000: episode: 1448, duration: 26.549s, episode steps: 581, steps per second:  22, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.016908, mae: 1.999706, mean_q: 2.416055, mean_eps: 0.100000\n",
            "📈 Episodio 1449: Recompensa total (clipped): 16.000, Pasos: 894, Mean Reward Calculado: 0.017897 (Recompensa/Pasos)\n",
            " 1089878/2000000: episode: 1449, duration: 40.718s, episode steps: 894, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.016250, mae: 1.976701, mean_q: 2.389134, mean_eps: 0.100000\n",
            "📈 Episodio 1450: Recompensa total (clipped): 9.000, Pasos: 476, Mean Reward Calculado: 0.018908 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1450, pasos: 1090354)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.92 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1450 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 9.00\n",
            "   Media últimos 100: 16.92 / 20.0\n",
            "   Mejor promedio histórico: 16.92\n",
            "   Estado: 📈 84.6% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1090354/2000000: episode: 1450, duration: 251.029s, episode steps: 476, steps per second:   2, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.016619, mae: 1.985587, mean_q: 2.397975, mean_eps: 0.100000\n",
            "📈 Episodio 1451: Recompensa total (clipped): 6.000, Pasos: 474, Mean Reward Calculado: 0.012658 (Recompensa/Pasos)\n",
            " 1090828/2000000: episode: 1451, duration: 21.583s, episode steps: 474, steps per second:  22, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.016098, mae: 1.987694, mean_q: 2.404122, mean_eps: 0.100000\n",
            "📈 Episodio 1452: Recompensa total (clipped): 12.000, Pasos: 633, Mean Reward Calculado: 0.018957 (Recompensa/Pasos)\n",
            " 1091461/2000000: episode: 1452, duration: 28.645s, episode steps: 633, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.904 [0.000, 5.000],  loss: 0.018031, mae: 1.997735, mean_q: 2.413962, mean_eps: 0.100000\n",
            "📈 Episodio 1453: Recompensa total (clipped): 22.000, Pasos: 1032, Mean Reward Calculado: 0.021318 (Recompensa/Pasos)\n",
            " 1092493/2000000: episode: 1453, duration: 46.922s, episode steps: 1032, steps per second:  22, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.017356, mae: 1.981837, mean_q: 2.395568, mean_eps: 0.100000\n",
            "📈 Episodio 1454: Recompensa total (clipped): 16.000, Pasos: 703, Mean Reward Calculado: 0.022760 (Recompensa/Pasos)\n",
            " 1093196/2000000: episode: 1454, duration: 31.914s, episode steps: 703, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.583 [0.000, 5.000],  loss: 0.016180, mae: 1.983341, mean_q: 2.398899, mean_eps: 0.100000\n",
            "📈 Episodio 1455: Recompensa total (clipped): 19.000, Pasos: 787, Mean Reward Calculado: 0.024142 (Recompensa/Pasos)\n",
            " 1093983/2000000: episode: 1455, duration: 35.494s, episode steps: 787, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.018953, mae: 2.003850, mean_q: 2.419221, mean_eps: 0.100000\n",
            "📈 Episodio 1456: Recompensa total (clipped): 8.000, Pasos: 587, Mean Reward Calculado: 0.013629 (Recompensa/Pasos)\n",
            " 1094570/2000000: episode: 1456, duration: 26.501s, episode steps: 587, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.016408, mae: 1.996341, mean_q: 2.411411, mean_eps: 0.100000\n",
            "📈 Episodio 1457: Recompensa total (clipped): 20.000, Pasos: 884, Mean Reward Calculado: 0.022624 (Recompensa/Pasos)\n",
            " 1095454/2000000: episode: 1457, duration: 40.192s, episode steps: 884, steps per second:  22, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.018821, mae: 1.976852, mean_q: 2.389171, mean_eps: 0.100000\n",
            "📈 Episodio 1458: Recompensa total (clipped): 13.000, Pasos: 536, Mean Reward Calculado: 0.024254 (Recompensa/Pasos)\n",
            " 1095990/2000000: episode: 1458, duration: 24.321s, episode steps: 536, steps per second:  22, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.018111, mae: 2.014540, mean_q: 2.434485, mean_eps: 0.100000\n",
            "📈 Episodio 1459: Recompensa total (clipped): 5.000, Pasos: 670, Mean Reward Calculado: 0.007463 (Recompensa/Pasos)\n",
            " 1096660/2000000: episode: 1459, duration: 30.355s, episode steps: 670, steps per second:  22, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.015207, mae: 1.970829, mean_q: 2.380275, mean_eps: 0.100000\n",
            "📈 Episodio 1460: Recompensa total (clipped): 12.000, Pasos: 724, Mean Reward Calculado: 0.016575 (Recompensa/Pasos)\n",
            " 1097384/2000000: episode: 1460, duration: 33.178s, episode steps: 724, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.017663, mae: 2.002500, mean_q: 2.419606, mean_eps: 0.100000\n",
            "📈 Episodio 1461: Recompensa total (clipped): 15.000, Pasos: 865, Mean Reward Calculado: 0.017341 (Recompensa/Pasos)\n",
            " 1098249/2000000: episode: 1461, duration: 39.505s, episode steps: 865, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.017497, mae: 1.993018, mean_q: 2.407200, mean_eps: 0.100000\n",
            "📈 Episodio 1462: Recompensa total (clipped): 19.000, Pasos: 768, Mean Reward Calculado: 0.024740 (Recompensa/Pasos)\n",
            " 1099017/2000000: episode: 1462, duration: 34.814s, episode steps: 768, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.016909, mae: 1.975180, mean_q: 2.385775, mean_eps: 0.100000\n",
            "📊 Paso 1,100,000/2,000,000 (55.0%) - 27.5 pasos/seg - ETA: 9.1h - Memoria: 15151.25 MB\n",
            "📈 Episodio 1463: Recompensa total (clipped): 28.000, Pasos: 1253, Mean Reward Calculado: 0.022346 (Recompensa/Pasos)\n",
            " 1100270/2000000: episode: 1463, duration: 57.026s, episode steps: 1253, steps per second:  22, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.017187, mae: 1.996500, mean_q: 2.412193, mean_eps: 0.100000\n",
            "📈 Episodio 1464: Recompensa total (clipped): 8.000, Pasos: 483, Mean Reward Calculado: 0.016563 (Recompensa/Pasos)\n",
            " 1100753/2000000: episode: 1464, duration: 22.046s, episode steps: 483, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.018181, mae: 2.008067, mean_q: 2.423754, mean_eps: 0.100000\n",
            "📈 Episodio 1465: Recompensa total (clipped): 15.000, Pasos: 805, Mean Reward Calculado: 0.018634 (Recompensa/Pasos)\n",
            " 1101558/2000000: episode: 1465, duration: 35.806s, episode steps: 805, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.989 [0.000, 5.000],  loss: 0.017188, mae: 2.013190, mean_q: 2.431473, mean_eps: 0.100000\n",
            "📈 Episodio 1466: Recompensa total (clipped): 20.000, Pasos: 1149, Mean Reward Calculado: 0.017406 (Recompensa/Pasos)\n",
            " 1102707/2000000: episode: 1466, duration: 51.218s, episode steps: 1149, steps per second:  22, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.016506, mae: 2.011605, mean_q: 2.428166, mean_eps: 0.100000\n",
            "📈 Episodio 1467: Recompensa total (clipped): 12.000, Pasos: 666, Mean Reward Calculado: 0.018018 (Recompensa/Pasos)\n",
            " 1103373/2000000: episode: 1467, duration: 29.998s, episode steps: 666, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.068 [0.000, 5.000],  loss: 0.016501, mae: 2.005527, mean_q: 2.422625, mean_eps: 0.100000\n",
            "📈 Episodio 1468: Recompensa total (clipped): 20.000, Pasos: 822, Mean Reward Calculado: 0.024331 (Recompensa/Pasos)\n",
            " 1104195/2000000: episode: 1468, duration: 37.487s, episode steps: 822, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.017526, mae: 2.005947, mean_q: 2.420886, mean_eps: 0.100000\n",
            "📈 Episodio 1469: Recompensa total (clipped): 9.000, Pasos: 518, Mean Reward Calculado: 0.017375 (Recompensa/Pasos)\n",
            " 1104713/2000000: episode: 1469, duration: 23.712s, episode steps: 518, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.018023, mae: 2.009176, mean_q: 2.424403, mean_eps: 0.100000\n",
            "📈 Episodio 1470: Recompensa total (clipped): 12.000, Pasos: 784, Mean Reward Calculado: 0.015306 (Recompensa/Pasos)\n",
            " 1105497/2000000: episode: 1470, duration: 35.195s, episode steps: 784, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.017761, mae: 2.021974, mean_q: 2.439336, mean_eps: 0.100000\n",
            "📈 Episodio 1471: Recompensa total (clipped): 20.000, Pasos: 1142, Mean Reward Calculado: 0.017513 (Recompensa/Pasos)\n",
            " 1106639/2000000: episode: 1471, duration: 51.716s, episode steps: 1142, steps per second:  22, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.017938, mae: 2.019404, mean_q: 2.439455, mean_eps: 0.100000\n",
            "📈 Episodio 1472: Recompensa total (clipped): 13.000, Pasos: 708, Mean Reward Calculado: 0.018362 (Recompensa/Pasos)\n",
            " 1107347/2000000: episode: 1472, duration: 32.043s, episode steps: 708, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014604, mae: 2.003926, mean_q: 2.422699, mean_eps: 0.100000\n",
            "📈 Episodio 1473: Recompensa total (clipped): 19.000, Pasos: 937, Mean Reward Calculado: 0.020277 (Recompensa/Pasos)\n",
            " 1108284/2000000: episode: 1473, duration: 42.515s, episode steps: 937, steps per second:  22, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.017598, mae: 2.014337, mean_q: 2.435723, mean_eps: 0.100000\n",
            "📈 Episodio 1474: Recompensa total (clipped): 15.000, Pasos: 634, Mean Reward Calculado: 0.023659 (Recompensa/Pasos)\n",
            " 1108918/2000000: episode: 1474, duration: 28.498s, episode steps: 634, steps per second:  22, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.018771, mae: 2.017059, mean_q: 2.436706, mean_eps: 0.100000\n",
            "📈 Episodio 1475: Recompensa total (clipped): 24.000, Pasos: 895, Mean Reward Calculado: 0.026816 (Recompensa/Pasos)\n",
            " 1109813/2000000: episode: 1475, duration: 40.096s, episode steps: 895, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.017885, mae: 1.994976, mean_q: 2.409796, mean_eps: 0.100000\n",
            "📈 Episodio 1476: Recompensa total (clipped): 20.000, Pasos: 789, Mean Reward Calculado: 0.025349 (Recompensa/Pasos)\n",
            " 1110602/2000000: episode: 1476, duration: 35.819s, episode steps: 789, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.017298, mae: 2.045377, mean_q: 2.472374, mean_eps: 0.100000\n",
            "📈 Episodio 1477: Recompensa total (clipped): 14.000, Pasos: 616, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            " 1111218/2000000: episode: 1477, duration: 28.018s, episode steps: 616, steps per second:  22, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.017051, mae: 2.037684, mean_q: 2.461391, mean_eps: 0.100000\n",
            "📈 Episodio 1478: Recompensa total (clipped): 18.000, Pasos: 752, Mean Reward Calculado: 0.023936 (Recompensa/Pasos)\n",
            " 1111970/2000000: episode: 1478, duration: 34.436s, episode steps: 752, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.016939, mae: 2.038376, mean_q: 2.461700, mean_eps: 0.100000\n",
            "📈 Episodio 1479: Recompensa total (clipped): 20.000, Pasos: 1055, Mean Reward Calculado: 0.018957 (Recompensa/Pasos)\n",
            " 1113025/2000000: episode: 1479, duration: 47.379s, episode steps: 1055, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.018039, mae: 2.034565, mean_q: 2.457562, mean_eps: 0.100000\n",
            "📈 Episodio 1480: Recompensa total (clipped): 7.000, Pasos: 496, Mean Reward Calculado: 0.014113 (Recompensa/Pasos)\n",
            " 1113521/2000000: episode: 1480, duration: 22.317s, episode steps: 496, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.700 [0.000, 5.000],  loss: 0.017585, mae: 2.013873, mean_q: 2.432861, mean_eps: 0.100000\n",
            "📈 Episodio 1481: Recompensa total (clipped): 12.000, Pasos: 751, Mean Reward Calculado: 0.015979 (Recompensa/Pasos)\n",
            " 1114272/2000000: episode: 1481, duration: 33.949s, episode steps: 751, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.018008, mae: 2.037107, mean_q: 2.458840, mean_eps: 0.100000\n",
            "📈 Episodio 1482: Recompensa total (clipped): 18.000, Pasos: 705, Mean Reward Calculado: 0.025532 (Recompensa/Pasos)\n",
            " 1114977/2000000: episode: 1482, duration: 31.620s, episode steps: 705, steps per second:  22, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.098 [0.000, 5.000],  loss: 0.018360, mae: 2.034699, mean_q: 2.456194, mean_eps: 0.100000\n",
            "📈 Episodio 1483: Recompensa total (clipped): 21.000, Pasos: 1173, Mean Reward Calculado: 0.017903 (Recompensa/Pasos)\n",
            " 1116150/2000000: episode: 1483, duration: 52.708s, episode steps: 1173, steps per second:  22, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.018039, mae: 2.029678, mean_q: 2.452625, mean_eps: 0.100000\n",
            "📈 Episodio 1484: Recompensa total (clipped): 7.000, Pasos: 631, Mean Reward Calculado: 0.011094 (Recompensa/Pasos)\n",
            " 1116781/2000000: episode: 1484, duration: 28.542s, episode steps: 631, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016845, mae: 2.049769, mean_q: 2.476872, mean_eps: 0.100000\n",
            "📈 Episodio 1485: Recompensa total (clipped): 17.000, Pasos: 684, Mean Reward Calculado: 0.024854 (Recompensa/Pasos)\n",
            " 1117465/2000000: episode: 1485, duration: 30.780s, episode steps: 684, steps per second:  22, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.018203, mae: 2.038937, mean_q: 2.462367, mean_eps: 0.100000\n",
            "📈 Episodio 1486: Recompensa total (clipped): 20.000, Pasos: 1009, Mean Reward Calculado: 0.019822 (Recompensa/Pasos)\n",
            " 1118474/2000000: episode: 1486, duration: 45.134s, episode steps: 1009, steps per second:  22, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.019592, mae: 2.027442, mean_q: 2.447553, mean_eps: 0.100000\n",
            "📈 Episodio 1487: Recompensa total (clipped): 20.000, Pasos: 847, Mean Reward Calculado: 0.023613 (Recompensa/Pasos)\n",
            " 1119321/2000000: episode: 1487, duration: 37.725s, episode steps: 847, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.019849, mae: 2.040961, mean_q: 2.462764, mean_eps: 0.100000\n",
            "📊 Paso 1,120,000/2,000,000 (56.0%) - 27.4 pasos/seg - ETA: 8.9h - Memoria: 15163.36 MB\n",
            "📈 Episodio 1488: Recompensa total (clipped): 24.000, Pasos: 937, Mean Reward Calculado: 0.025614 (Recompensa/Pasos)\n",
            " 1120258/2000000: episode: 1488, duration: 41.896s, episode steps: 937, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.018299, mae: 2.018094, mean_q: 2.436888, mean_eps: 0.100000\n",
            "📈 Episodio 1489: Recompensa total (clipped): 13.000, Pasos: 683, Mean Reward Calculado: 0.019034 (Recompensa/Pasos)\n",
            " 1120941/2000000: episode: 1489, duration: 31.192s, episode steps: 683, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.017156, mae: 2.024641, mean_q: 2.445424, mean_eps: 0.100000\n",
            "📈 Episodio 1490: Recompensa total (clipped): 30.000, Pasos: 1313, Mean Reward Calculado: 0.022848 (Recompensa/Pasos)\n",
            " 1122254/2000000: episode: 1490, duration: 59.309s, episode steps: 1313, steps per second:  22, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.017308, mae: 2.041126, mean_q: 2.464039, mean_eps: 0.100000\n",
            "📈 Episodio 1491: Recompensa total (clipped): 6.000, Pasos: 617, Mean Reward Calculado: 0.009724 (Recompensa/Pasos)\n",
            " 1122871/2000000: episode: 1491, duration: 27.758s, episode steps: 617, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.017206, mae: 2.048166, mean_q: 2.473647, mean_eps: 0.100000\n",
            "📈 Episodio 1492: Recompensa total (clipped): 29.000, Pasos: 1223, Mean Reward Calculado: 0.023712 (Recompensa/Pasos)\n",
            " 1124094/2000000: episode: 1492, duration: 55.221s, episode steps: 1223, steps per second:  22, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.019171, mae: 2.053490, mean_q: 2.478300, mean_eps: 0.100000\n",
            "📈 Episodio 1493: Recompensa total (clipped): 21.000, Pasos: 898, Mean Reward Calculado: 0.023385 (Recompensa/Pasos)\n",
            " 1124992/2000000: episode: 1493, duration: 40.984s, episode steps: 898, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.017039, mae: 2.042900, mean_q: 2.465727, mean_eps: 0.100000\n",
            "📈 Episodio 1494: Recompensa total (clipped): 23.000, Pasos: 929, Mean Reward Calculado: 0.024758 (Recompensa/Pasos)\n",
            " 1125921/2000000: episode: 1494, duration: 42.438s, episode steps: 929, steps per second:  22, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016310, mae: 2.018481, mean_q: 2.438015, mean_eps: 0.100000\n",
            "📈 Episodio 1495: Recompensa total (clipped): 11.000, Pasos: 650, Mean Reward Calculado: 0.016923 (Recompensa/Pasos)\n",
            " 1126571/2000000: episode: 1495, duration: 29.431s, episode steps: 650, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017259, mae: 2.035664, mean_q: 2.459614, mean_eps: 0.100000\n",
            "📈 Episodio 1496: Recompensa total (clipped): 23.000, Pasos: 996, Mean Reward Calculado: 0.023092 (Recompensa/Pasos)\n",
            " 1127567/2000000: episode: 1496, duration: 45.595s, episode steps: 996, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.018514, mae: 2.024650, mean_q: 2.445463, mean_eps: 0.100000\n",
            "📈 Episodio 1497: Recompensa total (clipped): 5.000, Pasos: 382, Mean Reward Calculado: 0.013089 (Recompensa/Pasos)\n",
            " 1127949/2000000: episode: 1497, duration: 17.518s, episode steps: 382, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.017584, mae: 2.031878, mean_q: 2.454224, mean_eps: 0.100000\n",
            "📈 Episodio 1498: Recompensa total (clipped): 8.000, Pasos: 367, Mean Reward Calculado: 0.021798 (Recompensa/Pasos)\n",
            " 1128316/2000000: episode: 1498, duration: 16.598s, episode steps: 367, steps per second:  22, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.018402, mae: 2.028656, mean_q: 2.450052, mean_eps: 0.100000\n",
            "📈 Episodio 1499: Recompensa total (clipped): 13.000, Pasos: 689, Mean Reward Calculado: 0.018868 (Recompensa/Pasos)\n",
            " 1129005/2000000: episode: 1499, duration: 30.675s, episode steps: 689, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.189 [0.000, 5.000],  loss: 0.017125, mae: 2.017878, mean_q: 2.439153, mean_eps: 0.100000\n",
            "📈 Episodio 1500: Recompensa total (clipped): 11.000, Pasos: 640, Mean Reward Calculado: 0.017188 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1500, pasos: 1129645)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.28 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1500 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 11.00\n",
            "   Media últimos 100: 16.28 / 20.0\n",
            "   Mejor promedio histórico: 16.28\n",
            "   Estado: 📈 81.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1129645/2000000: episode: 1500, duration: 91.370s, episode steps: 640, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.017175, mae: 2.031148, mean_q: 2.452459, mean_eps: 0.100000\n",
            "📈 Episodio 1501: Recompensa total (clipped): 10.000, Pasos: 426, Mean Reward Calculado: 0.023474 (Recompensa/Pasos)\n",
            " 1130071/2000000: episode: 1501, duration: 19.153s, episode steps: 426, steps per second:  22, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.017280, mae: 2.031783, mean_q: 2.452895, mean_eps: 0.100000\n",
            "📈 Episodio 1502: Recompensa total (clipped): 11.000, Pasos: 457, Mean Reward Calculado: 0.024070 (Recompensa/Pasos)\n",
            " 1130528/2000000: episode: 1502, duration: 20.871s, episode steps: 457, steps per second:  22, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.016000, mae: 2.045023, mean_q: 2.468275, mean_eps: 0.100000\n",
            "📈 Episodio 1503: Recompensa total (clipped): 12.000, Pasos: 579, Mean Reward Calculado: 0.020725 (Recompensa/Pasos)\n",
            " 1131107/2000000: episode: 1503, duration: 26.140s, episode steps: 579, steps per second:  22, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.016216, mae: 2.005471, mean_q: 2.422785, mean_eps: 0.100000\n",
            "📈 Episodio 1504: Recompensa total (clipped): 23.000, Pasos: 1067, Mean Reward Calculado: 0.021556 (Recompensa/Pasos)\n",
            " 1132174/2000000: episode: 1504, duration: 48.326s, episode steps: 1067, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.017139, mae: 2.019453, mean_q: 2.440505, mean_eps: 0.100000\n",
            "📈 Episodio 1505: Recompensa total (clipped): 16.000, Pasos: 674, Mean Reward Calculado: 0.023739 (Recompensa/Pasos)\n",
            " 1132848/2000000: episode: 1505, duration: 30.765s, episode steps: 674, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.016134, mae: 2.029270, mean_q: 2.450336, mean_eps: 0.100000\n",
            "📈 Episodio 1506: Recompensa total (clipped): 8.000, Pasos: 488, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            " 1133336/2000000: episode: 1506, duration: 22.559s, episode steps: 488, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.016343, mae: 2.035549, mean_q: 2.458233, mean_eps: 0.100000\n",
            "📈 Episodio 1507: Recompensa total (clipped): 30.000, Pasos: 1173, Mean Reward Calculado: 0.025575 (Recompensa/Pasos)\n",
            " 1134509/2000000: episode: 1507, duration: 53.059s, episode steps: 1173, steps per second:  22, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.017231, mae: 2.024987, mean_q: 2.446539, mean_eps: 0.100000\n",
            "📈 Episodio 1508: Recompensa total (clipped): 17.000, Pasos: 896, Mean Reward Calculado: 0.018973 (Recompensa/Pasos)\n",
            " 1135405/2000000: episode: 1508, duration: 40.044s, episode steps: 896, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.016757, mae: 2.028307, mean_q: 2.449936, mean_eps: 0.100000\n",
            "📈 Episodio 1509: Recompensa total (clipped): 24.000, Pasos: 1161, Mean Reward Calculado: 0.020672 (Recompensa/Pasos)\n",
            " 1136566/2000000: episode: 1509, duration: 52.427s, episode steps: 1161, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.017358, mae: 2.031199, mean_q: 2.452348, mean_eps: 0.100000\n",
            "📈 Episodio 1510: Recompensa total (clipped): 24.000, Pasos: 938, Mean Reward Calculado: 0.025586 (Recompensa/Pasos)\n",
            " 1137504/2000000: episode: 1510, duration: 41.813s, episode steps: 938, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.975 [0.000, 5.000],  loss: 0.017518, mae: 2.034672, mean_q: 2.456741, mean_eps: 0.100000\n",
            "📈 Episodio 1511: Recompensa total (clipped): 10.000, Pasos: 700, Mean Reward Calculado: 0.014286 (Recompensa/Pasos)\n",
            " 1138204/2000000: episode: 1511, duration: 31.654s, episode steps: 700, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.018039, mae: 2.027007, mean_q: 2.447454, mean_eps: 0.100000\n",
            "📈 Episodio 1512: Recompensa total (clipped): 23.000, Pasos: 857, Mean Reward Calculado: 0.026838 (Recompensa/Pasos)\n",
            " 1139061/2000000: episode: 1512, duration: 38.476s, episode steps: 857, steps per second:  22, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.015017, mae: 2.018641, mean_q: 2.437005, mean_eps: 0.100000\n",
            "📈 Episodio 1513: Recompensa total (clipped): 15.000, Pasos: 818, Mean Reward Calculado: 0.018337 (Recompensa/Pasos)\n",
            " 1139879/2000000: episode: 1513, duration: 36.654s, episode steps: 818, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.016855, mae: 2.004220, mean_q: 2.418925, mean_eps: 0.100000\n",
            "📊 Paso 1,140,000/2,000,000 (57.0%) - 27.3 pasos/seg - ETA: 8.8h - Memoria: 15203.80 MB\n",
            "📈 Episodio 1514: Recompensa total (clipped): 16.000, Pasos: 778, Mean Reward Calculado: 0.020566 (Recompensa/Pasos)\n",
            " 1140657/2000000: episode: 1514, duration: 35.479s, episode steps: 778, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.014696, mae: 2.019903, mean_q: 2.441010, mean_eps: 0.100000\n",
            "📈 Episodio 1515: Recompensa total (clipped): 25.000, Pasos: 941, Mean Reward Calculado: 0.026567 (Recompensa/Pasos)\n",
            " 1141598/2000000: episode: 1515, duration: 41.695s, episode steps: 941, steps per second:  23, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.017864, mae: 2.008662, mean_q: 2.427313, mean_eps: 0.100000\n",
            "📈 Episodio 1516: Recompensa total (clipped): 24.000, Pasos: 1141, Mean Reward Calculado: 0.021034 (Recompensa/Pasos)\n",
            " 1142739/2000000: episode: 1516, duration: 50.892s, episode steps: 1141, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.016114, mae: 2.011593, mean_q: 2.429403, mean_eps: 0.100000\n",
            "📈 Episodio 1517: Recompensa total (clipped): 21.000, Pasos: 878, Mean Reward Calculado: 0.023918 (Recompensa/Pasos)\n",
            " 1143617/2000000: episode: 1517, duration: 39.413s, episode steps: 878, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.017890, mae: 2.010150, mean_q: 2.429905, mean_eps: 0.100000\n",
            "📈 Episodio 1518: Recompensa total (clipped): 29.000, Pasos: 1194, Mean Reward Calculado: 0.024288 (Recompensa/Pasos)\n",
            " 1144811/2000000: episode: 1518, duration: 53.291s, episode steps: 1194, steps per second:  22, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.017061, mae: 2.030745, mean_q: 2.453147, mean_eps: 0.100000\n",
            "📈 Episodio 1519: Recompensa total (clipped): 19.000, Pasos: 827, Mean Reward Calculado: 0.022975 (Recompensa/Pasos)\n",
            " 1145638/2000000: episode: 1519, duration: 37.013s, episode steps: 827, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.017413, mae: 2.004309, mean_q: 2.422067, mean_eps: 0.100000\n",
            "📈 Episodio 1520: Recompensa total (clipped): 13.000, Pasos: 934, Mean Reward Calculado: 0.013919 (Recompensa/Pasos)\n",
            " 1146572/2000000: episode: 1520, duration: 42.036s, episode steps: 934, steps per second:  22, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.017970, mae: 2.014410, mean_q: 2.435491, mean_eps: 0.100000\n",
            "📈 Episodio 1521: Recompensa total (clipped): 13.000, Pasos: 946, Mean Reward Calculado: 0.013742 (Recompensa/Pasos)\n",
            " 1147518/2000000: episode: 1521, duration: 42.505s, episode steps: 946, steps per second:  22, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.017124, mae: 2.001572, mean_q: 2.415963, mean_eps: 0.100000\n",
            "📈 Episodio 1522: Recompensa total (clipped): 6.000, Pasos: 540, Mean Reward Calculado: 0.011111 (Recompensa/Pasos)\n",
            " 1148058/2000000: episode: 1522, duration: 24.107s, episode steps: 540, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.016989, mae: 2.011711, mean_q: 2.431571, mean_eps: 0.100000\n",
            "📈 Episodio 1523: Recompensa total (clipped): 26.000, Pasos: 1146, Mean Reward Calculado: 0.022688 (Recompensa/Pasos)\n",
            " 1149204/2000000: episode: 1523, duration: 51.584s, episode steps: 1146, steps per second:  22, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.017908, mae: 2.013824, mean_q: 2.434209, mean_eps: 0.100000\n",
            "📈 Episodio 1524: Recompensa total (clipped): 12.000, Pasos: 657, Mean Reward Calculado: 0.018265 (Recompensa/Pasos)\n",
            " 1149861/2000000: episode: 1524, duration: 29.823s, episode steps: 657, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.018574, mae: 1.999905, mean_q: 2.416132, mean_eps: 0.100000\n",
            "📈 Episodio 1525: Recompensa total (clipped): 19.000, Pasos: 948, Mean Reward Calculado: 0.020042 (Recompensa/Pasos)\n",
            " 1150809/2000000: episode: 1525, duration: 42.819s, episode steps: 948, steps per second:  22, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.016562, mae: 2.029521, mean_q: 2.452533, mean_eps: 0.100000\n",
            "📈 Episodio 1526: Recompensa total (clipped): 18.000, Pasos: 701, Mean Reward Calculado: 0.025678 (Recompensa/Pasos)\n",
            " 1151510/2000000: episode: 1526, duration: 31.359s, episode steps: 701, steps per second:  22, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.015730, mae: 2.027897, mean_q: 2.449939, mean_eps: 0.100000\n",
            "📈 Episodio 1527: Recompensa total (clipped): 11.000, Pasos: 620, Mean Reward Calculado: 0.017742 (Recompensa/Pasos)\n",
            " 1152130/2000000: episode: 1527, duration: 27.715s, episode steps: 620, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.017542, mae: 2.031800, mean_q: 2.454016, mean_eps: 0.100000\n",
            "📈 Episodio 1528: Recompensa total (clipped): 27.000, Pasos: 1050, Mean Reward Calculado: 0.025714 (Recompensa/Pasos)\n",
            " 1153180/2000000: episode: 1528, duration: 47.603s, episode steps: 1050, steps per second:  22, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.016456, mae: 2.028127, mean_q: 2.450712, mean_eps: 0.100000\n",
            "📈 Episodio 1529: Recompensa total (clipped): 10.000, Pasos: 649, Mean Reward Calculado: 0.015408 (Recompensa/Pasos)\n",
            " 1153829/2000000: episode: 1529, duration: 29.071s, episode steps: 649, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.016570, mae: 2.051466, mean_q: 2.478789, mean_eps: 0.100000\n",
            "📈 Episodio 1530: Recompensa total (clipped): 19.000, Pasos: 1135, Mean Reward Calculado: 0.016740 (Recompensa/Pasos)\n",
            " 1154964/2000000: episode: 1530, duration: 50.796s, episode steps: 1135, steps per second:  22, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.018411, mae: 2.018045, mean_q: 2.438754, mean_eps: 0.100000\n",
            "📈 Episodio 1531: Recompensa total (clipped): 18.000, Pasos: 1006, Mean Reward Calculado: 0.017893 (Recompensa/Pasos)\n",
            " 1155970/2000000: episode: 1531, duration: 45.398s, episode steps: 1006, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.017569, mae: 2.018026, mean_q: 2.440710, mean_eps: 0.100000\n",
            "📈 Episodio 1532: Recompensa total (clipped): 13.000, Pasos: 686, Mean Reward Calculado: 0.018950 (Recompensa/Pasos)\n",
            " 1156656/2000000: episode: 1532, duration: 31.180s, episode steps: 686, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.017127, mae: 2.016669, mean_q: 2.435989, mean_eps: 0.100000\n",
            "📈 Episodio 1533: Recompensa total (clipped): 24.000, Pasos: 972, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
            " 1157628/2000000: episode: 1533, duration: 43.532s, episode steps: 972, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.017691, mae: 2.016669, mean_q: 2.438615, mean_eps: 0.100000\n",
            "📈 Episodio 1534: Recompensa total (clipped): 13.000, Pasos: 816, Mean Reward Calculado: 0.015931 (Recompensa/Pasos)\n",
            " 1158444/2000000: episode: 1534, duration: 36.954s, episode steps: 816, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.018748, mae: 2.012299, mean_q: 2.433450, mean_eps: 0.100000\n",
            "📈 Episodio 1535: Recompensa total (clipped): 26.000, Pasos: 956, Mean Reward Calculado: 0.027197 (Recompensa/Pasos)\n",
            " 1159400/2000000: episode: 1535, duration: 43.665s, episode steps: 956, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.124 [0.000, 5.000],  loss: 0.016778, mae: 2.023353, mean_q: 2.448194, mean_eps: 0.100000\n",
            "📊 Paso 1,160,000/2,000,000 (58.0%) - 27.2 pasos/seg - ETA: 8.6h - Memoria: 15206.10 MB\n",
            "📈 Episodio 1536: Recompensa total (clipped): 23.000, Pasos: 998, Mean Reward Calculado: 0.023046 (Recompensa/Pasos)\n",
            " 1160398/2000000: episode: 1536, duration: 44.798s, episode steps: 998, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.017647, mae: 2.020386, mean_q: 2.442957, mean_eps: 0.100000\n",
            "📈 Episodio 1537: Recompensa total (clipped): 18.000, Pasos: 819, Mean Reward Calculado: 0.021978 (Recompensa/Pasos)\n",
            " 1161217/2000000: episode: 1537, duration: 37.071s, episode steps: 819, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.017111, mae: 2.015437, mean_q: 2.434879, mean_eps: 0.100000\n",
            "📈 Episodio 1538: Recompensa total (clipped): 12.000, Pasos: 578, Mean Reward Calculado: 0.020761 (Recompensa/Pasos)\n",
            " 1161795/2000000: episode: 1538, duration: 25.846s, episode steps: 578, steps per second:  22, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016525, mae: 2.024404, mean_q: 2.446110, mean_eps: 0.100000\n",
            "📈 Episodio 1539: Recompensa total (clipped): 25.000, Pasos: 1167, Mean Reward Calculado: 0.021422 (Recompensa/Pasos)\n",
            " 1162962/2000000: episode: 1539, duration: 52.580s, episode steps: 1167, steps per second:  22, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.016733, mae: 2.025486, mean_q: 2.450919, mean_eps: 0.100000\n",
            "📈 Episodio 1540: Recompensa total (clipped): 15.000, Pasos: 663, Mean Reward Calculado: 0.022624 (Recompensa/Pasos)\n",
            " 1163625/2000000: episode: 1540, duration: 29.779s, episode steps: 663, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.017951, mae: 2.034163, mean_q: 2.457930, mean_eps: 0.100000\n",
            "📈 Episodio 1541: Recompensa total (clipped): 9.000, Pasos: 542, Mean Reward Calculado: 0.016605 (Recompensa/Pasos)\n",
            " 1164167/2000000: episode: 1541, duration: 24.414s, episode steps: 542, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.019116, mae: 2.027099, mean_q: 2.448665, mean_eps: 0.100000\n",
            "📈 Episodio 1542: Recompensa total (clipped): 22.000, Pasos: 1087, Mean Reward Calculado: 0.020239 (Recompensa/Pasos)\n",
            " 1165254/2000000: episode: 1542, duration: 48.948s, episode steps: 1087, steps per second:  22, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.016758, mae: 2.010541, mean_q: 2.428242, mean_eps: 0.100000\n",
            "📈 Episodio 1543: Recompensa total (clipped): 16.000, Pasos: 652, Mean Reward Calculado: 0.024540 (Recompensa/Pasos)\n",
            " 1165906/2000000: episode: 1543, duration: 28.977s, episode steps: 652, steps per second:  23, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.018072, mae: 2.023636, mean_q: 2.444929, mean_eps: 0.100000\n",
            "📈 Episodio 1544: Recompensa total (clipped): 13.000, Pasos: 691, Mean Reward Calculado: 0.018813 (Recompensa/Pasos)\n",
            " 1166597/2000000: episode: 1544, duration: 31.051s, episode steps: 691, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.016568, mae: 2.019307, mean_q: 2.438586, mean_eps: 0.100000\n",
            "📈 Episodio 1545: Recompensa total (clipped): 17.000, Pasos: 734, Mean Reward Calculado: 0.023161 (Recompensa/Pasos)\n",
            " 1167331/2000000: episode: 1545, duration: 32.793s, episode steps: 734, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.016604, mae: 2.021246, mean_q: 2.440905, mean_eps: 0.100000\n",
            "📈 Episodio 1546: Recompensa total (clipped): 12.000, Pasos: 623, Mean Reward Calculado: 0.019262 (Recompensa/Pasos)\n",
            " 1167954/2000000: episode: 1546, duration: 28.101s, episode steps: 623, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.015593, mae: 2.017791, mean_q: 2.436177, mean_eps: 0.100000\n",
            "📈 Episodio 1547: Recompensa total (clipped): 8.000, Pasos: 642, Mean Reward Calculado: 0.012461 (Recompensa/Pasos)\n",
            " 1168596/2000000: episode: 1547, duration: 29.069s, episode steps: 642, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.015698, mae: 2.011424, mean_q: 2.429591, mean_eps: 0.100000\n",
            "📈 Episodio 1548: Recompensa total (clipped): 28.000, Pasos: 1174, Mean Reward Calculado: 0.023850 (Recompensa/Pasos)\n",
            " 1169770/2000000: episode: 1548, duration: 52.892s, episode steps: 1174, steps per second:  22, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.016936, mae: 2.017203, mean_q: 2.435435, mean_eps: 0.100000\n",
            "📈 Episodio 1549: Recompensa total (clipped): 25.000, Pasos: 836, Mean Reward Calculado: 0.029904 (Recompensa/Pasos)\n",
            " 1170606/2000000: episode: 1549, duration: 37.681s, episode steps: 836, steps per second:  22, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.015904, mae: 2.037434, mean_q: 2.461043, mean_eps: 0.100000\n",
            "📈 Episodio 1550: Recompensa total (clipped): 27.000, Pasos: 1194, Mean Reward Calculado: 0.022613 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1550, pasos: 1171800)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.88 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1550 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 27.00\n",
            "   Media últimos 100: 16.88 / 20.0\n",
            "   Mejor promedio histórico: 16.88\n",
            "   Estado: 📈 84.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1171800/2000000: episode: 1550, duration: 134.383s, episode steps: 1194, steps per second:   9, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.015839, mae: 2.024606, mean_q: 2.445365, mean_eps: 0.100000\n",
            "📈 Episodio 1551: Recompensa total (clipped): 14.000, Pasos: 989, Mean Reward Calculado: 0.014156 (Recompensa/Pasos)\n",
            " 1172789/2000000: episode: 1551, duration: 45.265s, episode steps: 989, steps per second:  22, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.015695, mae: 2.023640, mean_q: 2.443915, mean_eps: 0.100000\n",
            "📈 Episodio 1552: Recompensa total (clipped): 15.000, Pasos: 639, Mean Reward Calculado: 0.023474 (Recompensa/Pasos)\n",
            " 1173428/2000000: episode: 1552, duration: 29.190s, episode steps: 639, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.017262, mae: 2.020739, mean_q: 2.441757, mean_eps: 0.100000\n",
            "📈 Episodio 1553: Recompensa total (clipped): 12.000, Pasos: 702, Mean Reward Calculado: 0.017094 (Recompensa/Pasos)\n",
            " 1174130/2000000: episode: 1553, duration: 31.873s, episode steps: 702, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.016850, mae: 2.028932, mean_q: 2.449349, mean_eps: 0.100000\n",
            "📈 Episodio 1554: Recompensa total (clipped): 12.000, Pasos: 507, Mean Reward Calculado: 0.023669 (Recompensa/Pasos)\n",
            " 1174637/2000000: episode: 1554, duration: 22.935s, episode steps: 507, steps per second:  22, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.015536, mae: 2.019992, mean_q: 2.440672, mean_eps: 0.100000\n",
            "📈 Episodio 1555: Recompensa total (clipped): 11.000, Pasos: 544, Mean Reward Calculado: 0.020221 (Recompensa/Pasos)\n",
            " 1175181/2000000: episode: 1555, duration: 24.979s, episode steps: 544, steps per second:  22, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.017189, mae: 2.025091, mean_q: 2.444221, mean_eps: 0.100000\n",
            "📈 Episodio 1556: Recompensa total (clipped): 26.000, Pasos: 1114, Mean Reward Calculado: 0.023339 (Recompensa/Pasos)\n",
            " 1176295/2000000: episode: 1556, duration: 50.611s, episode steps: 1114, steps per second:  22, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.016811, mae: 2.025133, mean_q: 2.444337, mean_eps: 0.100000\n",
            "📈 Episodio 1557: Recompensa total (clipped): 28.000, Pasos: 1022, Mean Reward Calculado: 0.027397 (Recompensa/Pasos)\n",
            " 1177317/2000000: episode: 1557, duration: 46.929s, episode steps: 1022, steps per second:  22, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.016598, mae: 2.034778, mean_q: 2.458281, mean_eps: 0.100000\n",
            "📈 Episodio 1558: Recompensa total (clipped): 12.000, Pasos: 711, Mean Reward Calculado: 0.016878 (Recompensa/Pasos)\n",
            " 1178028/2000000: episode: 1558, duration: 32.341s, episode steps: 711, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.018366, mae: 2.040364, mean_q: 2.462719, mean_eps: 0.100000\n",
            "📈 Episodio 1559: Recompensa total (clipped): 12.000, Pasos: 657, Mean Reward Calculado: 0.018265 (Recompensa/Pasos)\n",
            " 1178685/2000000: episode: 1559, duration: 29.944s, episode steps: 657, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.017019, mae: 2.032537, mean_q: 2.457209, mean_eps: 0.100000\n",
            "📈 Episodio 1560: Recompensa total (clipped): 17.000, Pasos: 770, Mean Reward Calculado: 0.022078 (Recompensa/Pasos)\n",
            " 1179455/2000000: episode: 1560, duration: 34.941s, episode steps: 770, steps per second:  22, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.017750, mae: 2.031107, mean_q: 2.454664, mean_eps: 0.100000\n",
            "📊 Paso 1,180,000/2,000,000 (59.0%) - 27.0 pasos/seg - ETA: 8.4h - Memoria: 15231.07 MB\n",
            "📈 Episodio 1561: Recompensa total (clipped): 11.000, Pasos: 674, Mean Reward Calculado: 0.016320 (Recompensa/Pasos)\n",
            " 1180129/2000000: episode: 1561, duration: 30.894s, episode steps: 674, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.017341, mae: 2.028389, mean_q: 2.450944, mean_eps: 0.100000\n",
            "📈 Episodio 1562: Recompensa total (clipped): 15.000, Pasos: 661, Mean Reward Calculado: 0.022693 (Recompensa/Pasos)\n",
            " 1180790/2000000: episode: 1562, duration: 29.889s, episode steps: 661, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.016329, mae: 2.027538, mean_q: 2.451236, mean_eps: 0.100000\n",
            "📈 Episodio 1563: Recompensa total (clipped): 13.000, Pasos: 836, Mean Reward Calculado: 0.015550 (Recompensa/Pasos)\n",
            " 1181626/2000000: episode: 1563, duration: 37.524s, episode steps: 836, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.015477, mae: 2.024429, mean_q: 2.445537, mean_eps: 0.100000\n",
            "📈 Episodio 1564: Recompensa total (clipped): 19.000, Pasos: 992, Mean Reward Calculado: 0.019153 (Recompensa/Pasos)\n",
            " 1182618/2000000: episode: 1564, duration: 44.461s, episode steps: 992, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.017026, mae: 2.014948, mean_q: 2.432317, mean_eps: 0.100000\n",
            "📈 Episodio 1565: Recompensa total (clipped): 7.000, Pasos: 676, Mean Reward Calculado: 0.010355 (Recompensa/Pasos)\n",
            " 1183294/2000000: episode: 1565, duration: 30.253s, episode steps: 676, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.015168, mae: 2.020965, mean_q: 2.440910, mean_eps: 0.100000\n",
            "📈 Episodio 1566: Recompensa total (clipped): 18.000, Pasos: 1252, Mean Reward Calculado: 0.014377 (Recompensa/Pasos)\n",
            " 1184546/2000000: episode: 1566, duration: 56.850s, episode steps: 1252, steps per second:  22, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.017733, mae: 2.024825, mean_q: 2.446453, mean_eps: 0.100000\n",
            "📈 Episodio 1567: Recompensa total (clipped): 17.000, Pasos: 789, Mean Reward Calculado: 0.021546 (Recompensa/Pasos)\n",
            " 1185335/2000000: episode: 1567, duration: 35.923s, episode steps: 789, steps per second:  22, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.018853, mae: 2.028782, mean_q: 2.454014, mean_eps: 0.100000\n",
            "📈 Episodio 1568: Recompensa total (clipped): 16.000, Pasos: 636, Mean Reward Calculado: 0.025157 (Recompensa/Pasos)\n",
            " 1185971/2000000: episode: 1568, duration: 28.929s, episode steps: 636, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.018473, mae: 2.031219, mean_q: 2.455283, mean_eps: 0.100000\n",
            "📈 Episodio 1569: Recompensa total (clipped): 12.000, Pasos: 645, Mean Reward Calculado: 0.018605 (Recompensa/Pasos)\n",
            " 1186616/2000000: episode: 1569, duration: 29.272s, episode steps: 645, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.017464, mae: 2.008198, mean_q: 2.425875, mean_eps: 0.100000\n",
            "📈 Episodio 1570: Recompensa total (clipped): 11.000, Pasos: 796, Mean Reward Calculado: 0.013819 (Recompensa/Pasos)\n",
            " 1187412/2000000: episode: 1570, duration: 36.196s, episode steps: 796, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.017297, mae: 2.010032, mean_q: 2.428804, mean_eps: 0.100000\n",
            "📈 Episodio 1571: Recompensa total (clipped): 16.000, Pasos: 922, Mean Reward Calculado: 0.017354 (Recompensa/Pasos)\n",
            " 1188334/2000000: episode: 1571, duration: 41.646s, episode steps: 922, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.017314, mae: 2.024053, mean_q: 2.444769, mean_eps: 0.100000\n",
            "📈 Episodio 1572: Recompensa total (clipped): 21.000, Pasos: 861, Mean Reward Calculado: 0.024390 (Recompensa/Pasos)\n",
            " 1189195/2000000: episode: 1572, duration: 38.635s, episode steps: 861, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.017884, mae: 2.018429, mean_q: 2.436975, mean_eps: 0.100000\n",
            "📈 Episodio 1573: Recompensa total (clipped): 10.000, Pasos: 620, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            " 1189815/2000000: episode: 1573, duration: 27.822s, episode steps: 620, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.017427, mae: 2.022742, mean_q: 2.441476, mean_eps: 0.100000\n",
            "📈 Episodio 1574: Recompensa total (clipped): 5.000, Pasos: 386, Mean Reward Calculado: 0.012953 (Recompensa/Pasos)\n",
            " 1190201/2000000: episode: 1574, duration: 17.445s, episode steps: 386, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.013632, mae: 2.030998, mean_q: 2.454988, mean_eps: 0.100000\n",
            "📈 Episodio 1575: Recompensa total (clipped): 12.000, Pasos: 835, Mean Reward Calculado: 0.014371 (Recompensa/Pasos)\n",
            " 1191036/2000000: episode: 1575, duration: 37.648s, episode steps: 835, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.016294, mae: 2.031199, mean_q: 2.452949, mean_eps: 0.100000\n",
            "📈 Episodio 1576: Recompensa total (clipped): 24.000, Pasos: 1068, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            " 1192104/2000000: episode: 1576, duration: 48.118s, episode steps: 1068, steps per second:  22, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.017048, mae: 2.031957, mean_q: 2.454037, mean_eps: 0.100000\n",
            "📈 Episodio 1577: Recompensa total (clipped): 15.000, Pasos: 896, Mean Reward Calculado: 0.016741 (Recompensa/Pasos)\n",
            " 1193000/2000000: episode: 1577, duration: 41.242s, episode steps: 896, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.015727, mae: 2.040720, mean_q: 2.464978, mean_eps: 0.100000\n",
            "📈 Episodio 1578: Recompensa total (clipped): 12.000, Pasos: 615, Mean Reward Calculado: 0.019512 (Recompensa/Pasos)\n",
            " 1193615/2000000: episode: 1578, duration: 28.126s, episode steps: 615, steps per second:  22, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.015954, mae: 2.044758, mean_q: 2.474714, mean_eps: 0.100000\n",
            "📈 Episodio 1579: Recompensa total (clipped): 6.000, Pasos: 422, Mean Reward Calculado: 0.014218 (Recompensa/Pasos)\n",
            " 1194037/2000000: episode: 1579, duration: 19.120s, episode steps: 422, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.017509, mae: 2.029383, mean_q: 2.451223, mean_eps: 0.100000\n",
            "📈 Episodio 1580: Recompensa total (clipped): 25.000, Pasos: 1075, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            " 1195112/2000000: episode: 1580, duration: 47.927s, episode steps: 1075, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015337, mae: 2.027630, mean_q: 2.449702, mean_eps: 0.100000\n",
            "📈 Episodio 1581: Recompensa total (clipped): 18.000, Pasos: 827, Mean Reward Calculado: 0.021765 (Recompensa/Pasos)\n",
            " 1195939/2000000: episode: 1581, duration: 37.584s, episode steps: 827, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016272, mae: 2.045631, mean_q: 2.470855, mean_eps: 0.100000\n",
            "📈 Episodio 1582: Recompensa total (clipped): 22.000, Pasos: 1036, Mean Reward Calculado: 0.021236 (Recompensa/Pasos)\n",
            " 1196975/2000000: episode: 1582, duration: 47.254s, episode steps: 1036, steps per second:  22, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.017459, mae: 2.028848, mean_q: 2.450334, mean_eps: 0.100000\n",
            "📈 Episodio 1583: Recompensa total (clipped): 20.000, Pasos: 925, Mean Reward Calculado: 0.021622 (Recompensa/Pasos)\n",
            " 1197900/2000000: episode: 1583, duration: 41.919s, episode steps: 925, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.016651, mae: 2.049407, mean_q: 2.475774, mean_eps: 0.100000\n",
            "📈 Episodio 1584: Recompensa total (clipped): 8.000, Pasos: 539, Mean Reward Calculado: 0.014842 (Recompensa/Pasos)\n",
            " 1198439/2000000: episode: 1584, duration: 24.387s, episode steps: 539, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.018577, mae: 2.052230, mean_q: 2.479114, mean_eps: 0.100000\n",
            "📈 Episodio 1585: Recompensa total (clipped): 20.000, Pasos: 774, Mean Reward Calculado: 0.025840 (Recompensa/Pasos)\n",
            " 1199213/2000000: episode: 1585, duration: 34.825s, episode steps: 774, steps per second:  22, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.016709, mae: 2.026278, mean_q: 2.447428, mean_eps: 0.100000\n",
            "📈 Episodio 1586: Recompensa total (clipped): 13.000, Pasos: 620, Mean Reward Calculado: 0.020968 (Recompensa/Pasos)\n",
            " 1199833/2000000: episode: 1586, duration: 27.808s, episode steps: 620, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.016410, mae: 2.032380, mean_q: 2.453913, mean_eps: 0.100000\n",
            "📊 Paso 1,200,000/2,000,000 (60.0%) - 26.9 pasos/seg - ETA: 8.3h - Memoria: 15205.34 MB\n",
            "📈 Episodio 1587: Recompensa total (clipped): 15.000, Pasos: 671, Mean Reward Calculado: 0.022355 (Recompensa/Pasos)\n",
            " 1200504/2000000: episode: 1587, duration: 30.447s, episode steps: 671, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.037 [0.000, 5.000],  loss: 0.016397, mae: 2.033407, mean_q: 2.456729, mean_eps: 0.100000\n",
            "📈 Episodio 1588: Recompensa total (clipped): 12.000, Pasos: 627, Mean Reward Calculado: 0.019139 (Recompensa/Pasos)\n",
            " 1201131/2000000: episode: 1588, duration: 28.456s, episode steps: 627, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015531, mae: 2.025641, mean_q: 2.445889, mean_eps: 0.100000\n",
            "📈 Episodio 1589: Recompensa total (clipped): 24.000, Pasos: 906, Mean Reward Calculado: 0.026490 (Recompensa/Pasos)\n",
            " 1202037/2000000: episode: 1589, duration: 40.771s, episode steps: 906, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.015857, mae: 2.032868, mean_q: 2.454355, mean_eps: 0.100000\n",
            "📈 Episodio 1590: Recompensa total (clipped): 9.000, Pasos: 429, Mean Reward Calculado: 0.020979 (Recompensa/Pasos)\n",
            " 1202466/2000000: episode: 1590, duration: 19.163s, episode steps: 429, steps per second:  22, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.112 [0.000, 5.000],  loss: 0.016730, mae: 2.021462, mean_q: 2.441052, mean_eps: 0.100000\n",
            "📈 Episodio 1591: Recompensa total (clipped): 21.000, Pasos: 937, Mean Reward Calculado: 0.022412 (Recompensa/Pasos)\n",
            " 1203403/2000000: episode: 1591, duration: 42.390s, episode steps: 937, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.016909, mae: 2.053820, mean_q: 2.478777, mean_eps: 0.100000\n",
            "📈 Episodio 1592: Recompensa total (clipped): 12.000, Pasos: 677, Mean Reward Calculado: 0.017725 (Recompensa/Pasos)\n",
            " 1204080/2000000: episode: 1592, duration: 30.645s, episode steps: 677, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.015451, mae: 2.028928, mean_q: 2.449132, mean_eps: 0.100000\n",
            "📈 Episodio 1593: Recompensa total (clipped): 23.000, Pasos: 978, Mean Reward Calculado: 0.023517 (Recompensa/Pasos)\n",
            " 1205058/2000000: episode: 1593, duration: 44.286s, episode steps: 978, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015661, mae: 2.022611, mean_q: 2.440604, mean_eps: 0.100000\n",
            "📈 Episodio 1594: Recompensa total (clipped): 13.000, Pasos: 573, Mean Reward Calculado: 0.022688 (Recompensa/Pasos)\n",
            " 1205631/2000000: episode: 1594, duration: 25.823s, episode steps: 573, steps per second:  22, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.015511, mae: 2.038117, mean_q: 2.460666, mean_eps: 0.100000\n",
            "📈 Episodio 1595: Recompensa total (clipped): 20.000, Pasos: 739, Mean Reward Calculado: 0.027064 (Recompensa/Pasos)\n",
            " 1206370/2000000: episode: 1595, duration: 33.147s, episode steps: 739, steps per second:  22, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.015262, mae: 2.024013, mean_q: 2.444147, mean_eps: 0.100000\n",
            "📈 Episodio 1596: Recompensa total (clipped): 12.000, Pasos: 714, Mean Reward Calculado: 0.016807 (Recompensa/Pasos)\n",
            " 1207084/2000000: episode: 1596, duration: 32.237s, episode steps: 714, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.017206, mae: 2.032277, mean_q: 2.452870, mean_eps: 0.100000\n",
            "📈 Episodio 1597: Recompensa total (clipped): 17.000, Pasos: 753, Mean Reward Calculado: 0.022576 (Recompensa/Pasos)\n",
            " 1207837/2000000: episode: 1597, duration: 33.884s, episode steps: 753, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.016140, mae: 2.032661, mean_q: 2.450698, mean_eps: 0.100000\n",
            "📈 Episodio 1598: Recompensa total (clipped): 18.000, Pasos: 887, Mean Reward Calculado: 0.020293 (Recompensa/Pasos)\n",
            " 1208724/2000000: episode: 1598, duration: 39.800s, episode steps: 887, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.017451, mae: 2.051331, mean_q: 2.476107, mean_eps: 0.100000\n",
            "📈 Episodio 1599: Recompensa total (clipped): 10.000, Pasos: 785, Mean Reward Calculado: 0.012739 (Recompensa/Pasos)\n",
            " 1209509/2000000: episode: 1599, duration: 35.728s, episode steps: 785, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.016537, mae: 2.036928, mean_q: 2.459389, mean_eps: 0.100000\n",
            "📈 Episodio 1600: Recompensa total (clipped): 12.000, Pasos: 718, Mean Reward Calculado: 0.016713 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1600, pasos: 1210227)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.58 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1600 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 12.00\n",
            "   Media últimos 100: 16.58 / 20.0\n",
            "   Mejor promedio histórico: 16.58\n",
            "   Estado: 📈 82.9% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1210227/2000000: episode: 1600, duration: 112.052s, episode steps: 718, steps per second:   6, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.015031, mae: 2.015352, mean_q: 2.433255, mean_eps: 0.100000\n",
            "📈 Episodio 1601: Recompensa total (clipped): 26.000, Pasos: 919, Mean Reward Calculado: 0.028292 (Recompensa/Pasos)\n",
            " 1211146/2000000: episode: 1601, duration: 42.504s, episode steps: 919, steps per second:  22, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.111 [0.000, 5.000],  loss: 0.015898, mae: 2.019589, mean_q: 2.437791, mean_eps: 0.100000\n",
            "📈 Episodio 1602: Recompensa total (clipped): 19.000, Pasos: 833, Mean Reward Calculado: 0.022809 (Recompensa/Pasos)\n",
            " 1211979/2000000: episode: 1602, duration: 37.570s, episode steps: 833, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.017384, mae: 2.021811, mean_q: 2.439433, mean_eps: 0.100000\n",
            "📈 Episodio 1603: Recompensa total (clipped): 20.000, Pasos: 1113, Mean Reward Calculado: 0.017969 (Recompensa/Pasos)\n",
            " 1213092/2000000: episode: 1603, duration: 50.258s, episode steps: 1113, steps per second:  22, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.016707, mae: 2.020612, mean_q: 2.438069, mean_eps: 0.100000\n",
            "📈 Episodio 1604: Recompensa total (clipped): 12.000, Pasos: 536, Mean Reward Calculado: 0.022388 (Recompensa/Pasos)\n",
            " 1213628/2000000: episode: 1604, duration: 24.667s, episode steps: 536, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.015774, mae: 2.028923, mean_q: 2.448068, mean_eps: 0.100000\n",
            "📈 Episodio 1605: Recompensa total (clipped): 14.000, Pasos: 857, Mean Reward Calculado: 0.016336 (Recompensa/Pasos)\n",
            " 1214485/2000000: episode: 1605, duration: 39.226s, episode steps: 857, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.018156, mae: 2.026888, mean_q: 2.446016, mean_eps: 0.100000\n",
            "📈 Episodio 1606: Recompensa total (clipped): 17.000, Pasos: 862, Mean Reward Calculado: 0.019722 (Recompensa/Pasos)\n",
            " 1215347/2000000: episode: 1606, duration: 39.525s, episode steps: 862, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.015269, mae: 2.012750, mean_q: 2.429242, mean_eps: 0.100000\n",
            "📈 Episodio 1607: Recompensa total (clipped): 21.000, Pasos: 1446, Mean Reward Calculado: 0.014523 (Recompensa/Pasos)\n",
            " 1216793/2000000: episode: 1607, duration: 66.104s, episode steps: 1446, steps per second:  22, episode reward: 21.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.015782, mae: 2.018716, mean_q: 2.436106, mean_eps: 0.100000\n",
            "📈 Episodio 1608: Recompensa total (clipped): 13.000, Pasos: 608, Mean Reward Calculado: 0.021382 (Recompensa/Pasos)\n",
            " 1217401/2000000: episode: 1608, duration: 27.991s, episode steps: 608, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.018210, mae: 2.021270, mean_q: 2.436908, mean_eps: 0.100000\n",
            "📈 Episodio 1609: Recompensa total (clipped): 11.000, Pasos: 671, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            " 1218072/2000000: episode: 1609, duration: 30.942s, episode steps: 671, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.017149, mae: 2.016933, mean_q: 2.438143, mean_eps: 0.100000\n",
            "📈 Episodio 1610: Recompensa total (clipped): 18.000, Pasos: 1128, Mean Reward Calculado: 0.015957 (Recompensa/Pasos)\n",
            " 1219200/2000000: episode: 1610, duration: 51.905s, episode steps: 1128, steps per second:  22, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.016907, mae: 2.029146, mean_q: 2.448327, mean_eps: 0.100000\n",
            "📈 Episodio 1611: Recompensa total (clipped): 17.000, Pasos: 713, Mean Reward Calculado: 0.023843 (Recompensa/Pasos)\n",
            " 1219913/2000000: episode: 1611, duration: 32.490s, episode steps: 713, steps per second:  22, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.015519, mae: 2.041043, mean_q: 2.463318, mean_eps: 0.100000\n",
            "📊 Paso 1,220,000/2,000,000 (61.0%) - 26.8 pasos/seg - ETA: 8.1h - Memoria: 15174.25 MB\n",
            "📈 Episodio 1612: Recompensa total (clipped): 20.000, Pasos: 729, Mean Reward Calculado: 0.027435 (Recompensa/Pasos)\n",
            " 1220642/2000000: episode: 1612, duration: 33.149s, episode steps: 729, steps per second:  22, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.014536, mae: 2.020215, mean_q: 2.437493, mean_eps: 0.100000\n",
            "📈 Episodio 1613: Recompensa total (clipped): 25.000, Pasos: 940, Mean Reward Calculado: 0.026596 (Recompensa/Pasos)\n",
            " 1221582/2000000: episode: 1613, duration: 42.535s, episode steps: 940, steps per second:  22, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.015287, mae: 2.010533, mean_q: 2.427224, mean_eps: 0.100000\n",
            "📈 Episodio 1614: Recompensa total (clipped): 24.000, Pasos: 733, Mean Reward Calculado: 0.032742 (Recompensa/Pasos)\n",
            " 1222315/2000000: episode: 1614, duration: 33.061s, episode steps: 733, steps per second:  22, episode reward: 24.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.015075, mae: 2.015752, mean_q: 2.434205, mean_eps: 0.100000\n",
            "📈 Episodio 1615: Recompensa total (clipped): 29.000, Pasos: 1312, Mean Reward Calculado: 0.022104 (Recompensa/Pasos)\n",
            " 1223627/2000000: episode: 1615, duration: 59.584s, episode steps: 1312, steps per second:  22, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.016246, mae: 2.035078, mean_q: 2.455720, mean_eps: 0.100000\n",
            "📈 Episodio 1616: Recompensa total (clipped): 22.000, Pasos: 1290, Mean Reward Calculado: 0.017054 (Recompensa/Pasos)\n",
            " 1224917/2000000: episode: 1616, duration: 58.994s, episode steps: 1290, steps per second:  22, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.015503, mae: 2.027344, mean_q: 2.446645, mean_eps: 0.100000\n",
            "📈 Episodio 1617: Recompensa total (clipped): 11.000, Pasos: 753, Mean Reward Calculado: 0.014608 (Recompensa/Pasos)\n",
            " 1225670/2000000: episode: 1617, duration: 34.612s, episode steps: 753, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015394, mae: 2.025545, mean_q: 2.444885, mean_eps: 0.100000\n",
            "📈 Episodio 1618: Recompensa total (clipped): 9.000, Pasos: 519, Mean Reward Calculado: 0.017341 (Recompensa/Pasos)\n",
            " 1226189/2000000: episode: 1618, duration: 23.411s, episode steps: 519, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014878, mae: 2.018125, mean_q: 2.435977, mean_eps: 0.100000\n",
            "📈 Episodio 1619: Recompensa total (clipped): 21.000, Pasos: 907, Mean Reward Calculado: 0.023153 (Recompensa/Pasos)\n",
            " 1227096/2000000: episode: 1619, duration: 41.088s, episode steps: 907, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.016686, mae: 2.027488, mean_q: 2.444914, mean_eps: 0.100000\n",
            "📈 Episodio 1620: Recompensa total (clipped): 11.000, Pasos: 516, Mean Reward Calculado: 0.021318 (Recompensa/Pasos)\n",
            " 1227612/2000000: episode: 1620, duration: 23.639s, episode steps: 516, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015918, mae: 2.038843, mean_q: 2.461224, mean_eps: 0.100000\n",
            "📈 Episodio 1621: Recompensa total (clipped): 12.000, Pasos: 809, Mean Reward Calculado: 0.014833 (Recompensa/Pasos)\n",
            " 1228421/2000000: episode: 1621, duration: 36.521s, episode steps: 809, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015384, mae: 2.031768, mean_q: 2.451063, mean_eps: 0.100000\n",
            "📈 Episodio 1622: Recompensa total (clipped): 15.000, Pasos: 697, Mean Reward Calculado: 0.021521 (Recompensa/Pasos)\n",
            " 1229118/2000000: episode: 1622, duration: 31.908s, episode steps: 697, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.016112, mae: 2.027038, mean_q: 2.442691, mean_eps: 0.100000\n",
            "📈 Episodio 1623: Recompensa total (clipped): 17.000, Pasos: 720, Mean Reward Calculado: 0.023611 (Recompensa/Pasos)\n",
            " 1229838/2000000: episode: 1623, duration: 31.998s, episode steps: 720, steps per second:  23, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.016794, mae: 2.026760, mean_q: 2.444505, mean_eps: 0.100000\n",
            "📈 Episodio 1624: Recompensa total (clipped): 6.000, Pasos: 471, Mean Reward Calculado: 0.012739 (Recompensa/Pasos)\n",
            " 1230309/2000000: episode: 1624, duration: 21.431s, episode steps: 471, steps per second:  22, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.014935, mae: 2.034575, mean_q: 2.456362, mean_eps: 0.100000\n",
            "📈 Episodio 1625: Recompensa total (clipped): 22.000, Pasos: 791, Mean Reward Calculado: 0.027813 (Recompensa/Pasos)\n",
            " 1231100/2000000: episode: 1625, duration: 35.578s, episode steps: 791, steps per second:  22, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.015639, mae: 2.027815, mean_q: 2.449547, mean_eps: 0.100000\n",
            "📈 Episodio 1626: Recompensa total (clipped): 29.000, Pasos: 1343, Mean Reward Calculado: 0.021593 (Recompensa/Pasos)\n",
            " 1232443/2000000: episode: 1626, duration: 61.362s, episode steps: 1343, steps per second:  22, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.015703, mae: 2.032768, mean_q: 2.455345, mean_eps: 0.100000\n",
            "📈 Episodio 1627: Recompensa total (clipped): 18.000, Pasos: 784, Mean Reward Calculado: 0.022959 (Recompensa/Pasos)\n",
            " 1233227/2000000: episode: 1627, duration: 35.823s, episode steps: 784, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.704 [0.000, 5.000],  loss: 0.016632, mae: 2.038947, mean_q: 2.461021, mean_eps: 0.100000\n",
            "📈 Episodio 1628: Recompensa total (clipped): 15.000, Pasos: 644, Mean Reward Calculado: 0.023292 (Recompensa/Pasos)\n",
            " 1233871/2000000: episode: 1628, duration: 28.889s, episode steps: 644, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.017073, mae: 2.038967, mean_q: 2.462582, mean_eps: 0.100000\n",
            "📈 Episodio 1629: Recompensa total (clipped): 14.000, Pasos: 795, Mean Reward Calculado: 0.017610 (Recompensa/Pasos)\n",
            " 1234666/2000000: episode: 1629, duration: 35.558s, episode steps: 795, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.015792, mae: 2.027028, mean_q: 2.448589, mean_eps: 0.100000\n",
            "📈 Episodio 1630: Recompensa total (clipped): 33.000, Pasos: 1350, Mean Reward Calculado: 0.024444 (Recompensa/Pasos)\n",
            " 1236016/2000000: episode: 1630, duration: 61.586s, episode steps: 1350, steps per second:  22, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.016362, mae: 2.031335, mean_q: 2.454426, mean_eps: 0.100000\n",
            "📈 Episodio 1631: Recompensa total (clipped): 19.000, Pasos: 839, Mean Reward Calculado: 0.022646 (Recompensa/Pasos)\n",
            " 1236855/2000000: episode: 1631, duration: 37.862s, episode steps: 839, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.016407, mae: 2.020336, mean_q: 2.438583, mean_eps: 0.100000\n",
            "📈 Episodio 1632: Recompensa total (clipped): 10.000, Pasos: 588, Mean Reward Calculado: 0.017007 (Recompensa/Pasos)\n",
            " 1237443/2000000: episode: 1632, duration: 26.938s, episode steps: 588, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.016950, mae: 2.026473, mean_q: 2.450063, mean_eps: 0.100000\n",
            "📈 Episodio 1633: Recompensa total (clipped): 22.000, Pasos: 906, Mean Reward Calculado: 0.024283 (Recompensa/Pasos)\n",
            " 1238349/2000000: episode: 1633, duration: 41.211s, episode steps: 906, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.016281, mae: 2.038191, mean_q: 2.458816, mean_eps: 0.100000\n",
            "📈 Episodio 1634: Recompensa total (clipped): 19.000, Pasos: 797, Mean Reward Calculado: 0.023839 (Recompensa/Pasos)\n",
            " 1239146/2000000: episode: 1634, duration: 36.696s, episode steps: 797, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.016043, mae: 2.027236, mean_q: 2.447007, mean_eps: 0.100000\n",
            "📊 Paso 1,240,000/2,000,000 (62.0%) - 26.7 pasos/seg - ETA: 7.9h - Memoria: 15256.09 MB\n",
            "📈 Episodio 1635: Recompensa total (clipped): 24.000, Pasos: 999, Mean Reward Calculado: 0.024024 (Recompensa/Pasos)\n",
            " 1240145/2000000: episode: 1635, duration: 45.741s, episode steps: 999, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.765 [0.000, 5.000],  loss: 0.017212, mae: 2.037609, mean_q: 2.460203, mean_eps: 0.100000\n",
            "📈 Episodio 1636: Recompensa total (clipped): 12.000, Pasos: 525, Mean Reward Calculado: 0.022857 (Recompensa/Pasos)\n",
            " 1240670/2000000: episode: 1636, duration: 24.105s, episode steps: 525, steps per second:  22, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.015009, mae: 2.018972, mean_q: 2.435044, mean_eps: 0.100000\n",
            "📈 Episodio 1637: Recompensa total (clipped): 24.000, Pasos: 1118, Mean Reward Calculado: 0.021467 (Recompensa/Pasos)\n",
            " 1241788/2000000: episode: 1637, duration: 51.081s, episode steps: 1118, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.016643, mae: 2.026261, mean_q: 2.444138, mean_eps: 0.100000\n",
            "📈 Episodio 1638: Recompensa total (clipped): 26.000, Pasos: 952, Mean Reward Calculado: 0.027311 (Recompensa/Pasos)\n",
            " 1242740/2000000: episode: 1638, duration: 43.245s, episode steps: 952, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.016481, mae: 2.036975, mean_q: 2.458767, mean_eps: 0.100000\n",
            "📈 Episodio 1639: Recompensa total (clipped): 12.000, Pasos: 626, Mean Reward Calculado: 0.019169 (Recompensa/Pasos)\n",
            " 1243366/2000000: episode: 1639, duration: 28.642s, episode steps: 626, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.016592, mae: 2.005876, mean_q: 2.422609, mean_eps: 0.100000\n",
            "📈 Episodio 1640: Recompensa total (clipped): 12.000, Pasos: 691, Mean Reward Calculado: 0.017366 (Recompensa/Pasos)\n",
            " 1244057/2000000: episode: 1640, duration: 31.378s, episode steps: 691, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.016588, mae: 2.010499, mean_q: 2.427409, mean_eps: 0.100000\n",
            "📈 Episodio 1641: Recompensa total (clipped): 29.000, Pasos: 1162, Mean Reward Calculado: 0.024957 (Recompensa/Pasos)\n",
            " 1245219/2000000: episode: 1641, duration: 52.720s, episode steps: 1162, steps per second:  22, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.016378, mae: 2.013368, mean_q: 2.429934, mean_eps: 0.100000\n",
            "📈 Episodio 1642: Recompensa total (clipped): 15.000, Pasos: 495, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
            " 1245714/2000000: episode: 1642, duration: 22.738s, episode steps: 495, steps per second:  22, episode reward: 15.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.426 [0.000, 5.000],  loss: 0.018159, mae: 2.020489, mean_q: 2.435864, mean_eps: 0.100000\n",
            "📈 Episodio 1643: Recompensa total (clipped): 8.000, Pasos: 817, Mean Reward Calculado: 0.009792 (Recompensa/Pasos)\n",
            " 1246531/2000000: episode: 1643, duration: 37.564s, episode steps: 817, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.017241, mae: 2.026477, mean_q: 2.445328, mean_eps: 0.100000\n",
            "📈 Episodio 1644: Recompensa total (clipped): 25.000, Pasos: 1125, Mean Reward Calculado: 0.022222 (Recompensa/Pasos)\n",
            " 1247656/2000000: episode: 1644, duration: 51.499s, episode steps: 1125, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014986, mae: 2.002999, mean_q: 2.417209, mean_eps: 0.100000\n",
            "📈 Episodio 1645: Recompensa total (clipped): 23.000, Pasos: 929, Mean Reward Calculado: 0.024758 (Recompensa/Pasos)\n",
            " 1248585/2000000: episode: 1645, duration: 42.139s, episode steps: 929, steps per second:  22, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.017363, mae: 2.036198, mean_q: 2.459916, mean_eps: 0.100000\n",
            "📈 Episodio 1646: Recompensa total (clipped): 24.000, Pasos: 1135, Mean Reward Calculado: 0.021145 (Recompensa/Pasos)\n",
            " 1249720/2000000: episode: 1646, duration: 50.942s, episode steps: 1135, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.016534, mae: 1.999620, mean_q: 2.413289, mean_eps: 0.100000\n",
            "📈 Episodio 1647: Recompensa total (clipped): 23.000, Pasos: 1123, Mean Reward Calculado: 0.020481 (Recompensa/Pasos)\n",
            " 1250843/2000000: episode: 1647, duration: 50.688s, episode steps: 1123, steps per second:  22, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.014981, mae: 2.016923, mean_q: 2.435523, mean_eps: 0.100000\n",
            "📈 Episodio 1648: Recompensa total (clipped): 9.000, Pasos: 535, Mean Reward Calculado: 0.016822 (Recompensa/Pasos)\n",
            " 1251378/2000000: episode: 1648, duration: 24.313s, episode steps: 535, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.016318, mae: 2.017657, mean_q: 2.434665, mean_eps: 0.100000\n",
            "📈 Episodio 1649: Recompensa total (clipped): 12.000, Pasos: 534, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            " 1251912/2000000: episode: 1649, duration: 24.379s, episode steps: 534, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.016488, mae: 2.035067, mean_q: 2.453514, mean_eps: 0.100000\n",
            "📈 Episodio 1650: Recompensa total (clipped): 30.000, Pasos: 1141, Mean Reward Calculado: 0.026293 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1650, pasos: 1253053)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.82 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1650 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 30.00\n",
            "   Media últimos 100: 16.82 / 20.0\n",
            "   Mejor promedio histórico: 16.82\n",
            "   Estado: 📈 84.1% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1253053/2000000: episode: 1650, duration: 107.692s, episode steps: 1141, steps per second:  11, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.015434, mae: 2.001170, mean_q: 2.414871, mean_eps: 0.100000\n",
            "📈 Episodio 1651: Recompensa total (clipped): 18.000, Pasos: 973, Mean Reward Calculado: 0.018499 (Recompensa/Pasos)\n",
            " 1254026/2000000: episode: 1651, duration: 44.378s, episode steps: 973, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.015659, mae: 2.014765, mean_q: 2.434086, mean_eps: 0.100000\n",
            "📈 Episodio 1652: Recompensa total (clipped): 16.000, Pasos: 697, Mean Reward Calculado: 0.022956 (Recompensa/Pasos)\n",
            " 1254723/2000000: episode: 1652, duration: 31.582s, episode steps: 697, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.162 [0.000, 5.000],  loss: 0.016230, mae: 2.017359, mean_q: 2.434269, mean_eps: 0.100000\n",
            "📈 Episodio 1653: Recompensa total (clipped): 10.000, Pasos: 631, Mean Reward Calculado: 0.015848 (Recompensa/Pasos)\n",
            " 1255354/2000000: episode: 1653, duration: 28.896s, episode steps: 631, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.016159, mae: 2.009401, mean_q: 2.423967, mean_eps: 0.100000\n",
            "📈 Episodio 1654: Recompensa total (clipped): 21.000, Pasos: 816, Mean Reward Calculado: 0.025735 (Recompensa/Pasos)\n",
            " 1256170/2000000: episode: 1654, duration: 37.943s, episode steps: 816, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.017139, mae: 2.022010, mean_q: 2.438492, mean_eps: 0.100000\n",
            "📈 Episodio 1655: Recompensa total (clipped): 14.000, Pasos: 787, Mean Reward Calculado: 0.017789 (Recompensa/Pasos)\n",
            " 1256957/2000000: episode: 1655, duration: 35.900s, episode steps: 787, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.016873, mae: 2.014567, mean_q: 2.428717, mean_eps: 0.100000\n",
            "📈 Episodio 1656: Recompensa total (clipped): 27.000, Pasos: 1242, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            " 1258199/2000000: episode: 1656, duration: 55.824s, episode steps: 1242, steps per second:  22, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016592, mae: 2.008200, mean_q: 2.421536, mean_eps: 0.100000\n",
            "📈 Episodio 1657: Recompensa total (clipped): 17.000, Pasos: 751, Mean Reward Calculado: 0.022636 (Recompensa/Pasos)\n",
            " 1258950/2000000: episode: 1657, duration: 33.933s, episode steps: 751, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.041 [0.000, 5.000],  loss: 0.016493, mae: 2.010743, mean_q: 2.426415, mean_eps: 0.100000\n",
            "📈 Episodio 1658: Recompensa total (clipped): 6.000, Pasos: 493, Mean Reward Calculado: 0.012170 (Recompensa/Pasos)\n",
            " 1259443/2000000: episode: 1658, duration: 22.365s, episode steps: 493, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.961 [0.000, 5.000],  loss: 0.016970, mae: 1.997840, mean_q: 2.411582, mean_eps: 0.100000\n",
            "📈 Episodio 1659: Recompensa total (clipped): 6.000, Pasos: 527, Mean Reward Calculado: 0.011385 (Recompensa/Pasos)\n",
            " 1259970/2000000: episode: 1659, duration: 24.135s, episode steps: 527, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.082 [0.000, 5.000],  loss: 0.017258, mae: 2.020876, mean_q: 2.437309, mean_eps: 0.100000\n",
            "📊 Paso 1,260,000/2,000,000 (63.0%) - 26.5 pasos/seg - ETA: 7.7h - Memoria: 15255.01 MB\n",
            "📈 Episodio 1660: Recompensa total (clipped): 17.000, Pasos: 907, Mean Reward Calculado: 0.018743 (Recompensa/Pasos)\n",
            " 1260877/2000000: episode: 1660, duration: 41.469s, episode steps: 907, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.015267, mae: 2.023570, mean_q: 2.441343, mean_eps: 0.100000\n",
            "📈 Episodio 1661: Recompensa total (clipped): 29.000, Pasos: 1337, Mean Reward Calculado: 0.021690 (Recompensa/Pasos)\n",
            " 1262214/2000000: episode: 1661, duration: 60.986s, episode steps: 1337, steps per second:  22, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.015116, mae: 2.027939, mean_q: 2.445753, mean_eps: 0.100000\n",
            "📈 Episodio 1662: Recompensa total (clipped): 27.000, Pasos: 1324, Mean Reward Calculado: 0.020393 (Recompensa/Pasos)\n",
            " 1263538/2000000: episode: 1662, duration: 59.217s, episode steps: 1324, steps per second:  22, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.015809, mae: 2.016394, mean_q: 2.431723, mean_eps: 0.100000\n",
            "📈 Episodio 1663: Recompensa total (clipped): 20.000, Pasos: 1051, Mean Reward Calculado: 0.019029 (Recompensa/Pasos)\n",
            " 1264589/2000000: episode: 1663, duration: 47.222s, episode steps: 1051, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.015220, mae: 2.021651, mean_q: 2.438435, mean_eps: 0.100000\n",
            "📈 Episodio 1664: Recompensa total (clipped): 22.000, Pasos: 754, Mean Reward Calculado: 0.029178 (Recompensa/Pasos)\n",
            " 1265343/2000000: episode: 1664, duration: 33.532s, episode steps: 754, steps per second:  22, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.016905, mae: 2.016099, mean_q: 2.429262, mean_eps: 0.100000\n",
            "📈 Episodio 1665: Recompensa total (clipped): 15.000, Pasos: 925, Mean Reward Calculado: 0.016216 (Recompensa/Pasos)\n",
            " 1266268/2000000: episode: 1665, duration: 41.505s, episode steps: 925, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.016119, mae: 2.014990, mean_q: 2.429507, mean_eps: 0.100000\n",
            "📈 Episodio 1666: Recompensa total (clipped): 14.000, Pasos: 634, Mean Reward Calculado: 0.022082 (Recompensa/Pasos)\n",
            " 1266902/2000000: episode: 1666, duration: 28.504s, episode steps: 634, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.013908, mae: 2.009848, mean_q: 2.425061, mean_eps: 0.100000\n",
            "📈 Episodio 1667: Recompensa total (clipped): 11.000, Pasos: 444, Mean Reward Calculado: 0.024775 (Recompensa/Pasos)\n",
            " 1267346/2000000: episode: 1667, duration: 20.187s, episode steps: 444, steps per second:  22, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.669 [0.000, 5.000],  loss: 0.015224, mae: 2.018072, mean_q: 2.434985, mean_eps: 0.100000\n",
            "📈 Episodio 1668: Recompensa total (clipped): 27.000, Pasos: 1324, Mean Reward Calculado: 0.020393 (Recompensa/Pasos)\n",
            " 1268670/2000000: episode: 1668, duration: 59.202s, episode steps: 1324, steps per second:  22, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.016733, mae: 2.016759, mean_q: 2.434313, mean_eps: 0.100000\n",
            "📈 Episodio 1669: Recompensa total (clipped): 13.000, Pasos: 853, Mean Reward Calculado: 0.015240 (Recompensa/Pasos)\n",
            " 1269523/2000000: episode: 1669, duration: 38.430s, episode steps: 853, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.016943, mae: 2.018937, mean_q: 2.433535, mean_eps: 0.100000\n",
            "📈 Episodio 1670: Recompensa total (clipped): 7.000, Pasos: 531, Mean Reward Calculado: 0.013183 (Recompensa/Pasos)\n",
            " 1270054/2000000: episode: 1670, duration: 24.860s, episode steps: 531, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.015743, mae: 1.992247, mean_q: 2.404894, mean_eps: 0.100000\n",
            "📈 Episodio 1671: Recompensa total (clipped): 5.000, Pasos: 353, Mean Reward Calculado: 0.014164 (Recompensa/Pasos)\n",
            " 1270407/2000000: episode: 1671, duration: 16.286s, episode steps: 353, steps per second:  22, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.015887, mae: 2.039923, mean_q: 2.463528, mean_eps: 0.100000\n",
            "📈 Episodio 1672: Recompensa total (clipped): 13.000, Pasos: 523, Mean Reward Calculado: 0.024857 (Recompensa/Pasos)\n",
            " 1270930/2000000: episode: 1672, duration: 23.916s, episode steps: 523, steps per second:  22, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.016300, mae: 2.014533, mean_q: 2.432288, mean_eps: 0.100000\n",
            "📈 Episodio 1673: Recompensa total (clipped): 14.000, Pasos: 668, Mean Reward Calculado: 0.020958 (Recompensa/Pasos)\n",
            " 1271598/2000000: episode: 1673, duration: 29.912s, episode steps: 668, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.015856, mae: 2.015048, mean_q: 2.431580, mean_eps: 0.100000\n",
            "📈 Episodio 1674: Recompensa total (clipped): 30.000, Pasos: 1321, Mean Reward Calculado: 0.022710 (Recompensa/Pasos)\n",
            " 1272919/2000000: episode: 1674, duration: 59.788s, episode steps: 1321, steps per second:  22, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.016311, mae: 2.016820, mean_q: 2.433215, mean_eps: 0.100000\n",
            "📈 Episodio 1675: Recompensa total (clipped): 8.000, Pasos: 815, Mean Reward Calculado: 0.009816 (Recompensa/Pasos)\n",
            " 1273734/2000000: episode: 1675, duration: 36.711s, episode steps: 815, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.016621, mae: 2.026982, mean_q: 2.445677, mean_eps: 0.100000\n",
            "📈 Episodio 1676: Recompensa total (clipped): 21.000, Pasos: 1180, Mean Reward Calculado: 0.017797 (Recompensa/Pasos)\n",
            " 1274914/2000000: episode: 1676, duration: 53.958s, episode steps: 1180, steps per second:  22, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.015368, mae: 2.012594, mean_q: 2.429749, mean_eps: 0.100000\n",
            "📈 Episodio 1677: Recompensa total (clipped): 11.000, Pasos: 607, Mean Reward Calculado: 0.018122 (Recompensa/Pasos)\n",
            " 1275521/2000000: episode: 1677, duration: 27.635s, episode steps: 607, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015900, mae: 2.033953, mean_q: 2.454538, mean_eps: 0.100000\n",
            "📈 Episodio 1678: Recompensa total (clipped): 8.000, Pasos: 506, Mean Reward Calculado: 0.015810 (Recompensa/Pasos)\n",
            " 1276027/2000000: episode: 1678, duration: 23.200s, episode steps: 506, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.017019, mae: 2.003324, mean_q: 2.415843, mean_eps: 0.100000\n",
            "📈 Episodio 1679: Recompensa total (clipped): 19.000, Pasos: 917, Mean Reward Calculado: 0.020720 (Recompensa/Pasos)\n",
            " 1276944/2000000: episode: 1679, duration: 41.519s, episode steps: 917, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.017200, mae: 2.008966, mean_q: 2.423397, mean_eps: 0.100000\n",
            "📈 Episodio 1680: Recompensa total (clipped): 11.000, Pasos: 667, Mean Reward Calculado: 0.016492 (Recompensa/Pasos)\n",
            " 1277611/2000000: episode: 1680, duration: 29.878s, episode steps: 667, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.015326, mae: 2.026069, mean_q: 2.445805, mean_eps: 0.100000\n",
            "📈 Episodio 1681: Recompensa total (clipped): 15.000, Pasos: 934, Mean Reward Calculado: 0.016060 (Recompensa/Pasos)\n",
            " 1278545/2000000: episode: 1681, duration: 41.894s, episode steps: 934, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.016476, mae: 2.015272, mean_q: 2.431726, mean_eps: 0.100000\n",
            "📈 Episodio 1682: Recompensa total (clipped): 7.000, Pasos: 629, Mean Reward Calculado: 0.011129 (Recompensa/Pasos)\n",
            " 1279174/2000000: episode: 1682, duration: 28.087s, episode steps: 629, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.016006, mae: 2.009765, mean_q: 2.425606, mean_eps: 0.100000\n",
            "📈 Episodio 1683: Recompensa total (clipped): 5.000, Pasos: 700, Mean Reward Calculado: 0.007143 (Recompensa/Pasos)\n",
            " 1279874/2000000: episode: 1683, duration: 31.543s, episode steps: 700, steps per second:  22, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.711 [0.000, 5.000],  loss: 0.017026, mae: 2.013394, mean_q: 2.427849, mean_eps: 0.100000\n",
            "📊 Paso 1,280,000/2,000,000 (64.0%) - 26.5 pasos/seg - ETA: 7.6h - Memoria: 15247.47 MB\n",
            "📈 Episodio 1684: Recompensa total (clipped): 10.000, Pasos: 561, Mean Reward Calculado: 0.017825 (Recompensa/Pasos)\n",
            " 1280435/2000000: episode: 1684, duration: 25.265s, episode steps: 561, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014816, mae: 2.008728, mean_q: 2.423888, mean_eps: 0.100000\n",
            "📈 Episodio 1685: Recompensa total (clipped): 8.000, Pasos: 609, Mean Reward Calculado: 0.013136 (Recompensa/Pasos)\n",
            " 1281044/2000000: episode: 1685, duration: 27.898s, episode steps: 609, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.067 [0.000, 5.000],  loss: 0.016124, mae: 2.006829, mean_q: 2.420884, mean_eps: 0.100000\n",
            "📈 Episodio 1686: Recompensa total (clipped): 5.000, Pasos: 660, Mean Reward Calculado: 0.007576 (Recompensa/Pasos)\n",
            " 1281704/2000000: episode: 1686, duration: 29.835s, episode steps: 660, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.014499, mae: 2.022983, mean_q: 2.440188, mean_eps: 0.100000\n",
            "📈 Episodio 1687: Recompensa total (clipped): 19.000, Pasos: 1119, Mean Reward Calculado: 0.016979 (Recompensa/Pasos)\n",
            " 1282823/2000000: episode: 1687, duration: 51.072s, episode steps: 1119, steps per second:  22, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.015515, mae: 2.017893, mean_q: 2.435507, mean_eps: 0.100000\n",
            "📈 Episodio 1688: Recompensa total (clipped): 22.000, Pasos: 921, Mean Reward Calculado: 0.023887 (Recompensa/Pasos)\n",
            " 1283744/2000000: episode: 1688, duration: 42.235s, episode steps: 921, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.016435, mae: 2.029097, mean_q: 2.447994, mean_eps: 0.100000\n",
            "📈 Episodio 1689: Recompensa total (clipped): 3.000, Pasos: 635, Mean Reward Calculado: 0.004724 (Recompensa/Pasos)\n",
            " 1284379/2000000: episode: 1689, duration: 29.021s, episode steps: 635, steps per second:  22, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.016196, mae: 2.027152, mean_q: 2.445836, mean_eps: 0.100000\n",
            "📈 Episodio 1690: Recompensa total (clipped): 7.000, Pasos: 464, Mean Reward Calculado: 0.015086 (Recompensa/Pasos)\n",
            " 1284843/2000000: episode: 1690, duration: 21.276s, episode steps: 464, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.016537, mae: 2.018276, mean_q: 2.433407, mean_eps: 0.100000\n",
            "📈 Episodio 1691: Recompensa total (clipped): 16.000, Pasos: 840, Mean Reward Calculado: 0.019048 (Recompensa/Pasos)\n",
            " 1285683/2000000: episode: 1691, duration: 38.358s, episode steps: 840, steps per second:  22, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.015442, mae: 2.013731, mean_q: 2.428771, mean_eps: 0.100000\n",
            "📈 Episodio 1692: Recompensa total (clipped): 8.000, Pasos: 562, Mean Reward Calculado: 0.014235 (Recompensa/Pasos)\n",
            " 1286245/2000000: episode: 1692, duration: 25.657s, episode steps: 562, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.015577, mae: 2.017043, mean_q: 2.434716, mean_eps: 0.100000\n",
            "📈 Episodio 1693: Recompensa total (clipped): 13.000, Pasos: 656, Mean Reward Calculado: 0.019817 (Recompensa/Pasos)\n",
            " 1286901/2000000: episode: 1693, duration: 29.431s, episode steps: 656, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.016080, mae: 2.010703, mean_q: 2.424094, mean_eps: 0.100000\n",
            "📈 Episodio 1694: Recompensa total (clipped): 9.000, Pasos: 531, Mean Reward Calculado: 0.016949 (Recompensa/Pasos)\n",
            " 1287432/2000000: episode: 1694, duration: 23.920s, episode steps: 531, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.019120, mae: 2.031894, mean_q: 2.450627, mean_eps: 0.100000\n",
            "📈 Episodio 1695: Recompensa total (clipped): 9.000, Pasos: 721, Mean Reward Calculado: 0.012483 (Recompensa/Pasos)\n",
            " 1288153/2000000: episode: 1695, duration: 32.720s, episode steps: 721, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.016908, mae: 2.008480, mean_q: 2.421355, mean_eps: 0.100000\n",
            "📈 Episodio 1696: Recompensa total (clipped): 22.000, Pasos: 895, Mean Reward Calculado: 0.024581 (Recompensa/Pasos)\n",
            " 1289048/2000000: episode: 1696, duration: 40.995s, episode steps: 895, steps per second:  22, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.015994, mae: 2.015011, mean_q: 2.431173, mean_eps: 0.100000\n",
            "📈 Episodio 1697: Recompensa total (clipped): 17.000, Pasos: 865, Mean Reward Calculado: 0.019653 (Recompensa/Pasos)\n",
            " 1289913/2000000: episode: 1697, duration: 39.699s, episode steps: 865, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.015960, mae: 2.032815, mean_q: 2.451864, mean_eps: 0.100000\n",
            "📈 Episodio 1698: Recompensa total (clipped): 9.000, Pasos: 448, Mean Reward Calculado: 0.020089 (Recompensa/Pasos)\n",
            " 1290361/2000000: episode: 1698, duration: 20.454s, episode steps: 448, steps per second:  22, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.223 [0.000, 5.000],  loss: 0.015209, mae: 2.038194, mean_q: 2.460894, mean_eps: 0.100000\n",
            "📈 Episodio 1699: Recompensa total (clipped): 23.000, Pasos: 1511, Mean Reward Calculado: 0.015222 (Recompensa/Pasos)\n",
            " 1291872/2000000: episode: 1699, duration: 68.494s, episode steps: 1511, steps per second:  22, episode reward: 23.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.016048, mae: 2.020812, mean_q: 2.439835, mean_eps: 0.100000\n",
            "📈 Episodio 1700: Recompensa total (clipped): 6.000, Pasos: 445, Mean Reward Calculado: 0.013483 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1700, pasos: 1292317)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.29 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1700 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 6.00\n",
            "   Media últimos 100: 16.29 / 20.0\n",
            "   Mejor promedio histórico: 16.29\n",
            "   Estado: 📈 81.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1292317/2000000: episode: 1700, duration: 81.866s, episode steps: 445, steps per second:   5, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.169 [0.000, 5.000],  loss: 0.014694, mae: 2.021776, mean_q: 2.438451, mean_eps: 0.100000\n",
            "📈 Episodio 1701: Recompensa total (clipped): 12.000, Pasos: 655, Mean Reward Calculado: 0.018321 (Recompensa/Pasos)\n",
            " 1292972/2000000: episode: 1701, duration: 29.556s, episode steps: 655, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.186 [0.000, 5.000],  loss: 0.017423, mae: 2.024170, mean_q: 2.444145, mean_eps: 0.100000\n",
            "📈 Episodio 1702: Recompensa total (clipped): 15.000, Pasos: 890, Mean Reward Calculado: 0.016854 (Recompensa/Pasos)\n",
            " 1293862/2000000: episode: 1702, duration: 40.359s, episode steps: 890, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.016081, mae: 2.015577, mean_q: 2.432198, mean_eps: 0.100000\n",
            "📈 Episodio 1703: Recompensa total (clipped): 9.000, Pasos: 646, Mean Reward Calculado: 0.013932 (Recompensa/Pasos)\n",
            " 1294508/2000000: episode: 1703, duration: 29.420s, episode steps: 646, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.016255, mae: 2.017593, mean_q: 2.435004, mean_eps: 0.100000\n",
            "📈 Episodio 1704: Recompensa total (clipped): 14.000, Pasos: 847, Mean Reward Calculado: 0.016529 (Recompensa/Pasos)\n",
            " 1295355/2000000: episode: 1704, duration: 38.688s, episode steps: 847, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.017100, mae: 2.022223, mean_q: 2.439886, mean_eps: 0.100000\n",
            "📈 Episodio 1705: Recompensa total (clipped): 7.000, Pasos: 686, Mean Reward Calculado: 0.010204 (Recompensa/Pasos)\n",
            " 1296041/2000000: episode: 1705, duration: 31.373s, episode steps: 686, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.016183, mae: 2.013300, mean_q: 2.429327, mean_eps: 0.100000\n",
            "📈 Episodio 1706: Recompensa total (clipped): 29.000, Pasos: 1063, Mean Reward Calculado: 0.027281 (Recompensa/Pasos)\n",
            " 1297104/2000000: episode: 1706, duration: 48.792s, episode steps: 1063, steps per second:  22, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.971 [0.000, 5.000],  loss: 0.015397, mae: 2.028178, mean_q: 2.447712, mean_eps: 0.100000\n",
            "📈 Episodio 1707: Recompensa total (clipped): 20.000, Pasos: 915, Mean Reward Calculado: 0.021858 (Recompensa/Pasos)\n",
            " 1298019/2000000: episode: 1707, duration: 41.822s, episode steps: 915, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.489 [0.000, 5.000],  loss: 0.017072, mae: 2.032202, mean_q: 2.449397, mean_eps: 0.100000\n",
            "📈 Episodio 1708: Recompensa total (clipped): 20.000, Pasos: 1034, Mean Reward Calculado: 0.019342 (Recompensa/Pasos)\n",
            " 1299053/2000000: episode: 1708, duration: 46.988s, episode steps: 1034, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.017084, mae: 2.026014, mean_q: 2.441647, mean_eps: 0.100000\n",
            "📈 Episodio 1709: Recompensa total (clipped): 12.000, Pasos: 655, Mean Reward Calculado: 0.018321 (Recompensa/Pasos)\n",
            " 1299708/2000000: episode: 1709, duration: 30.176s, episode steps: 655, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.018091, mae: 2.027070, mean_q: 2.444422, mean_eps: 0.100000\n",
            "📊 Paso 1,300,000/2,000,000 (65.0%) - 26.3 pasos/seg - ETA: 7.4h - Memoria: 15225.70 MB\n",
            "📈 Episodio 1710: Recompensa total (clipped): 3.000, Pasos: 348, Mean Reward Calculado: 0.008621 (Recompensa/Pasos)\n",
            " 1300056/2000000: episode: 1710, duration: 16.141s, episode steps: 348, steps per second:  22, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.013746, mae: 2.028674, mean_q: 2.449901, mean_eps: 0.100000\n",
            "📈 Episodio 1711: Recompensa total (clipped): 17.000, Pasos: 692, Mean Reward Calculado: 0.024566 (Recompensa/Pasos)\n",
            " 1300748/2000000: episode: 1711, duration: 31.749s, episode steps: 692, steps per second:  22, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.014744, mae: 2.060496, mean_q: 2.487550, mean_eps: 0.100000\n",
            "📈 Episodio 1712: Recompensa total (clipped): 12.000, Pasos: 658, Mean Reward Calculado: 0.018237 (Recompensa/Pasos)\n",
            " 1301406/2000000: episode: 1712, duration: 30.493s, episode steps: 658, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.016048, mae: 2.047647, mean_q: 2.471208, mean_eps: 0.100000\n",
            "📈 Episodio 1713: Recompensa total (clipped): 20.000, Pasos: 838, Mean Reward Calculado: 0.023866 (Recompensa/Pasos)\n",
            " 1302244/2000000: episode: 1713, duration: 37.702s, episode steps: 838, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.015583, mae: 2.038384, mean_q: 2.460153, mean_eps: 0.100000\n",
            "📈 Episodio 1714: Recompensa total (clipped): 7.000, Pasos: 643, Mean Reward Calculado: 0.010886 (Recompensa/Pasos)\n",
            " 1302887/2000000: episode: 1714, duration: 28.925s, episode steps: 643, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.015198, mae: 2.025789, mean_q: 2.444173, mean_eps: 0.100000\n",
            "📈 Episodio 1715: Recompensa total (clipped): 17.000, Pasos: 617, Mean Reward Calculado: 0.027553 (Recompensa/Pasos)\n",
            " 1303504/2000000: episode: 1715, duration: 28.398s, episode steps: 617, steps per second:  22, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.013375, mae: 2.042237, mean_q: 2.464881, mean_eps: 0.100000\n",
            "📈 Episodio 1716: Recompensa total (clipped): 22.000, Pasos: 1110, Mean Reward Calculado: 0.019820 (Recompensa/Pasos)\n",
            " 1304614/2000000: episode: 1716, duration: 50.628s, episode steps: 1110, steps per second:  22, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.015887, mae: 2.028943, mean_q: 2.448002, mean_eps: 0.100000\n",
            "📈 Episodio 1717: Recompensa total (clipped): 10.000, Pasos: 610, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            " 1305224/2000000: episode: 1717, duration: 27.694s, episode steps: 610, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015574, mae: 2.023911, mean_q: 2.442653, mean_eps: 0.100000\n",
            "📈 Episodio 1718: Recompensa total (clipped): 13.000, Pasos: 908, Mean Reward Calculado: 0.014317 (Recompensa/Pasos)\n",
            " 1306132/2000000: episode: 1718, duration: 41.060s, episode steps: 908, steps per second:  22, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017343, mae: 2.043962, mean_q: 2.467252, mean_eps: 0.100000\n",
            "📈 Episodio 1719: Recompensa total (clipped): 17.000, Pasos: 768, Mean Reward Calculado: 0.022135 (Recompensa/Pasos)\n",
            " 1306900/2000000: episode: 1719, duration: 34.843s, episode steps: 768, steps per second:  22, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.017525, mae: 2.035498, mean_q: 2.455384, mean_eps: 0.100000\n",
            "📈 Episodio 1720: Recompensa total (clipped): 12.000, Pasos: 639, Mean Reward Calculado: 0.018779 (Recompensa/Pasos)\n",
            " 1307539/2000000: episode: 1720, duration: 28.810s, episode steps: 639, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.016057, mae: 2.039829, mean_q: 2.461606, mean_eps: 0.100000\n",
            "📈 Episodio 1721: Recompensa total (clipped): 25.000, Pasos: 1236, Mean Reward Calculado: 0.020227 (Recompensa/Pasos)\n",
            " 1308775/2000000: episode: 1721, duration: 55.407s, episode steps: 1236, steps per second:  22, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.016521, mae: 2.024761, mean_q: 2.443240, mean_eps: 0.100000\n",
            "📈 Episodio 1722: Recompensa total (clipped): 26.000, Pasos: 1002, Mean Reward Calculado: 0.025948 (Recompensa/Pasos)\n",
            " 1309777/2000000: episode: 1722, duration: 45.396s, episode steps: 1002, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.018148, mae: 2.027645, mean_q: 2.447089, mean_eps: 0.100000\n",
            "📈 Episodio 1723: Recompensa total (clipped): 15.000, Pasos: 545, Mean Reward Calculado: 0.027523 (Recompensa/Pasos)\n",
            " 1310322/2000000: episode: 1723, duration: 24.254s, episode steps: 545, steps per second:  22, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.015366, mae: 2.047280, mean_q: 2.473431, mean_eps: 0.100000\n",
            "📈 Episodio 1724: Recompensa total (clipped): 22.000, Pasos: 896, Mean Reward Calculado: 0.024554 (Recompensa/Pasos)\n",
            " 1311218/2000000: episode: 1724, duration: 40.616s, episode steps: 896, steps per second:  22, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.015228, mae: 2.037661, mean_q: 2.460372, mean_eps: 0.100000\n",
            "📈 Episodio 1725: Recompensa total (clipped): 13.000, Pasos: 641, Mean Reward Calculado: 0.020281 (Recompensa/Pasos)\n",
            " 1311859/2000000: episode: 1725, duration: 29.229s, episode steps: 641, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.015388, mae: 2.023043, mean_q: 2.440017, mean_eps: 0.100000\n",
            "📈 Episodio 1726: Recompensa total (clipped): 3.000, Pasos: 381, Mean Reward Calculado: 0.007874 (Recompensa/Pasos)\n",
            " 1312240/2000000: episode: 1726, duration: 17.577s, episode steps: 381, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.055 [0.000, 5.000],  loss: 0.017556, mae: 2.053079, mean_q: 2.478008, mean_eps: 0.100000\n",
            "📈 Episodio 1727: Recompensa total (clipped): 20.000, Pasos: 1148, Mean Reward Calculado: 0.017422 (Recompensa/Pasos)\n",
            " 1313388/2000000: episode: 1727, duration: 52.517s, episode steps: 1148, steps per second:  22, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.015811, mae: 2.035120, mean_q: 2.455465, mean_eps: 0.100000\n",
            "📈 Episodio 1728: Recompensa total (clipped): 5.000, Pasos: 341, Mean Reward Calculado: 0.014663 (Recompensa/Pasos)\n",
            " 1313729/2000000: episode: 1728, duration: 15.367s, episode steps: 341, steps per second:  22, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.466 [0.000, 5.000],  loss: 0.016803, mae: 2.036518, mean_q: 2.457256, mean_eps: 0.100000\n",
            "📈 Episodio 1729: Recompensa total (clipped): 31.000, Pasos: 1326, Mean Reward Calculado: 0.023379 (Recompensa/Pasos)\n",
            " 1315055/2000000: episode: 1729, duration: 60.289s, episode steps: 1326, steps per second:  22, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.016302, mae: 2.041309, mean_q: 2.462907, mean_eps: 0.100000\n",
            "📈 Episodio 1730: Recompensa total (clipped): 20.000, Pasos: 904, Mean Reward Calculado: 0.022124 (Recompensa/Pasos)\n",
            " 1315959/2000000: episode: 1730, duration: 40.853s, episode steps: 904, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.016694, mae: 2.040914, mean_q: 2.463020, mean_eps: 0.100000\n",
            "📈 Episodio 1731: Recompensa total (clipped): 18.000, Pasos: 827, Mean Reward Calculado: 0.021765 (Recompensa/Pasos)\n",
            " 1316786/2000000: episode: 1731, duration: 37.248s, episode steps: 827, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.016056, mae: 2.027564, mean_q: 2.445349, mean_eps: 0.100000\n",
            "📈 Episodio 1732: Recompensa total (clipped): 16.000, Pasos: 659, Mean Reward Calculado: 0.024279 (Recompensa/Pasos)\n",
            " 1317445/2000000: episode: 1732, duration: 30.333s, episode steps: 659, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.015292, mae: 2.033488, mean_q: 2.454441, mean_eps: 0.100000\n",
            "📈 Episodio 1733: Recompensa total (clipped): 16.000, Pasos: 1022, Mean Reward Calculado: 0.015656 (Recompensa/Pasos)\n",
            " 1318467/2000000: episode: 1733, duration: 46.528s, episode steps: 1022, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.016382, mae: 2.045601, mean_q: 2.466965, mean_eps: 0.100000\n",
            "📈 Episodio 1734: Recompensa total (clipped): 23.000, Pasos: 981, Mean Reward Calculado: 0.023445 (Recompensa/Pasos)\n",
            " 1319448/2000000: episode: 1734, duration: 44.382s, episode steps: 981, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.016490, mae: 2.053843, mean_q: 2.476781, mean_eps: 0.100000\n",
            "📊 Paso 1,320,000/2,000,000 (66.0%) - 26.3 pasos/seg - ETA: 7.2h - Memoria: 15249.88 MB\n",
            "📈 Episodio 1735: Recompensa total (clipped): 23.000, Pasos: 727, Mean Reward Calculado: 0.031637 (Recompensa/Pasos)\n",
            " 1320175/2000000: episode: 1735, duration: 32.768s, episode steps: 727, steps per second:  22, episode reward: 23.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014981, mae: 2.048465, mean_q: 2.471299, mean_eps: 0.100000\n",
            "📈 Episodio 1736: Recompensa total (clipped): 18.000, Pasos: 850, Mean Reward Calculado: 0.021176 (Recompensa/Pasos)\n",
            " 1321025/2000000: episode: 1736, duration: 38.336s, episode steps: 850, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013945, mae: 2.060053, mean_q: 2.486701, mean_eps: 0.100000\n",
            "📈 Episodio 1737: Recompensa total (clipped): 12.000, Pasos: 810, Mean Reward Calculado: 0.014815 (Recompensa/Pasos)\n",
            " 1321835/2000000: episode: 1737, duration: 37.234s, episode steps: 810, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.014061, mae: 2.039460, mean_q: 2.462091, mean_eps: 0.100000\n",
            "📈 Episodio 1738: Recompensa total (clipped): 8.000, Pasos: 389, Mean Reward Calculado: 0.020566 (Recompensa/Pasos)\n",
            " 1322224/2000000: episode: 1738, duration: 17.786s, episode steps: 389, steps per second:  22, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.082 [0.000, 5.000],  loss: 0.015624, mae: 2.045610, mean_q: 2.466998, mean_eps: 0.100000\n",
            "📈 Episodio 1739: Recompensa total (clipped): 20.000, Pasos: 906, Mean Reward Calculado: 0.022075 (Recompensa/Pasos)\n",
            " 1323130/2000000: episode: 1739, duration: 41.505s, episode steps: 906, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.016351, mae: 2.056386, mean_q: 2.481609, mean_eps: 0.100000\n",
            "📈 Episodio 1740: Recompensa total (clipped): 16.000, Pasos: 527, Mean Reward Calculado: 0.030361 (Recompensa/Pasos)\n",
            " 1323657/2000000: episode: 1740, duration: 23.897s, episode steps: 527, steps per second:  22, episode reward: 16.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.017026, mae: 2.030634, mean_q: 2.450296, mean_eps: 0.100000\n",
            "📈 Episodio 1741: Recompensa total (clipped): 22.000, Pasos: 1014, Mean Reward Calculado: 0.021696 (Recompensa/Pasos)\n",
            " 1324671/2000000: episode: 1741, duration: 45.889s, episode steps: 1014, steps per second:  22, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.015809, mae: 2.049799, mean_q: 2.472304, mean_eps: 0.100000\n",
            "📈 Episodio 1742: Recompensa total (clipped): 15.000, Pasos: 737, Mean Reward Calculado: 0.020353 (Recompensa/Pasos)\n",
            " 1325408/2000000: episode: 1742, duration: 33.576s, episode steps: 737, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.015876, mae: 2.051843, mean_q: 2.475442, mean_eps: 0.100000\n",
            "📈 Episodio 1743: Recompensa total (clipped): 13.000, Pasos: 632, Mean Reward Calculado: 0.020570 (Recompensa/Pasos)\n",
            " 1326040/2000000: episode: 1743, duration: 28.632s, episode steps: 632, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.016205, mae: 2.039675, mean_q: 2.462207, mean_eps: 0.100000\n",
            "📈 Episodio 1744: Recompensa total (clipped): 8.000, Pasos: 380, Mean Reward Calculado: 0.021053 (Recompensa/Pasos)\n",
            " 1326420/2000000: episode: 1744, duration: 17.244s, episode steps: 380, steps per second:  22, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.013500, mae: 2.044872, mean_q: 2.470570, mean_eps: 0.100000\n",
            "📈 Episodio 1745: Recompensa total (clipped): 31.000, Pasos: 1231, Mean Reward Calculado: 0.025183 (Recompensa/Pasos)\n",
            " 1327651/2000000: episode: 1745, duration: 55.566s, episode steps: 1231, steps per second:  22, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.015152, mae: 2.045169, mean_q: 2.468250, mean_eps: 0.100000\n",
            "📈 Episodio 1746: Recompensa total (clipped): 21.000, Pasos: 886, Mean Reward Calculado: 0.023702 (Recompensa/Pasos)\n",
            " 1328537/2000000: episode: 1746, duration: 39.808s, episode steps: 886, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016017, mae: 2.058041, mean_q: 2.484602, mean_eps: 0.100000\n",
            "📈 Episodio 1747: Recompensa total (clipped): 11.000, Pasos: 664, Mean Reward Calculado: 0.016566 (Recompensa/Pasos)\n",
            " 1329201/2000000: episode: 1747, duration: 29.951s, episode steps: 664, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.014549, mae: 2.049979, mean_q: 2.473070, mean_eps: 0.100000\n",
            "📈 Episodio 1748: Recompensa total (clipped): 16.000, Pasos: 943, Mean Reward Calculado: 0.016967 (Recompensa/Pasos)\n",
            " 1330144/2000000: episode: 1748, duration: 43.480s, episode steps: 943, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.015592, mae: 2.055361, mean_q: 2.480919, mean_eps: 0.100000\n",
            "📈 Episodio 1749: Recompensa total (clipped): 23.000, Pasos: 909, Mean Reward Calculado: 0.025303 (Recompensa/Pasos)\n",
            " 1331053/2000000: episode: 1749, duration: 41.123s, episode steps: 909, steps per second:  22, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.016020, mae: 2.070538, mean_q: 2.499077, mean_eps: 0.100000\n",
            "📈 Episodio 1750: Recompensa total (clipped): 12.000, Pasos: 646, Mean Reward Calculado: 0.018576 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1750, pasos: 1331699)\n",
            "💾 NUEVO MEJOR PROMEDIO: 15.20 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1750 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 12.00\n",
            "   Media últimos 100: 15.20 / 20.0\n",
            "   Mejor promedio histórico: 15.20\n",
            "   Estado: 📈 76.0% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1331699/2000000: episode: 1750, duration: 85.168s, episode steps: 646, steps per second:   8, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.014903, mae: 2.067912, mean_q: 2.499312, mean_eps: 0.100000\n",
            "📈 Episodio 1751: Recompensa total (clipped): 25.000, Pasos: 923, Mean Reward Calculado: 0.027086 (Recompensa/Pasos)\n",
            " 1332622/2000000: episode: 1751, duration: 42.104s, episode steps: 923, steps per second:  22, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.017413, mae: 2.056647, mean_q: 2.481828, mean_eps: 0.100000\n",
            "📈 Episodio 1752: Recompensa total (clipped): 18.000, Pasos: 729, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
            " 1333351/2000000: episode: 1752, duration: 33.091s, episode steps: 729, steps per second:  22, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.016462, mae: 2.081091, mean_q: 2.510875, mean_eps: 0.100000\n",
            "📈 Episodio 1753: Recompensa total (clipped): 13.000, Pasos: 707, Mean Reward Calculado: 0.018388 (Recompensa/Pasos)\n",
            " 1334058/2000000: episode: 1753, duration: 32.283s, episode steps: 707, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014874, mae: 2.077998, mean_q: 2.509122, mean_eps: 0.100000\n",
            "📈 Episodio 1754: Recompensa total (clipped): 5.000, Pasos: 515, Mean Reward Calculado: 0.009709 (Recompensa/Pasos)\n",
            " 1334573/2000000: episode: 1754, duration: 23.615s, episode steps: 515, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.159 [0.000, 5.000],  loss: 0.015050, mae: 2.058690, mean_q: 2.484168, mean_eps: 0.100000\n",
            "📈 Episodio 1755: Recompensa total (clipped): 5.000, Pasos: 490, Mean Reward Calculado: 0.010204 (Recompensa/Pasos)\n",
            " 1335063/2000000: episode: 1755, duration: 22.408s, episode steps: 490, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016632, mae: 2.087189, mean_q: 2.518893, mean_eps: 0.100000\n",
            "📈 Episodio 1756: Recompensa total (clipped): 7.000, Pasos: 386, Mean Reward Calculado: 0.018135 (Recompensa/Pasos)\n",
            " 1335449/2000000: episode: 1756, duration: 17.996s, episode steps: 386, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.251 [0.000, 5.000],  loss: 0.017627, mae: 2.060407, mean_q: 2.484392, mean_eps: 0.100000\n",
            "📈 Episodio 1757: Recompensa total (clipped): 25.000, Pasos: 1056, Mean Reward Calculado: 0.023674 (Recompensa/Pasos)\n",
            " 1336505/2000000: episode: 1757, duration: 48.038s, episode steps: 1056, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.015865, mae: 2.066959, mean_q: 2.491294, mean_eps: 0.100000\n",
            "📈 Episodio 1758: Recompensa total (clipped): 10.000, Pasos: 686, Mean Reward Calculado: 0.014577 (Recompensa/Pasos)\n",
            " 1337191/2000000: episode: 1758, duration: 31.530s, episode steps: 686, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.018378, mae: 2.087693, mean_q: 2.514798, mean_eps: 0.100000\n",
            "📈 Episodio 1759: Recompensa total (clipped): 38.000, Pasos: 1601, Mean Reward Calculado: 0.023735 (Recompensa/Pasos)\n",
            " 1338792/2000000: episode: 1759, duration: 74.186s, episode steps: 1601, steps per second:  22, episode reward: 38.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.015814, mae: 2.069481, mean_q: 2.493896, mean_eps: 0.100000\n",
            "📈 Episodio 1760: Recompensa total (clipped): 17.000, Pasos: 872, Mean Reward Calculado: 0.019495 (Recompensa/Pasos)\n",
            " 1339664/2000000: episode: 1760, duration: 40.002s, episode steps: 872, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.015409, mae: 2.064927, mean_q: 2.490749, mean_eps: 0.100000\n",
            "📊 Paso 1,340,000/2,000,000 (67.0%) - 26.2 pasos/seg - ETA: 7.0h - Memoria: 15251.19 MB\n",
            "📈 Episodio 1761: Recompensa total (clipped): 13.000, Pasos: 657, Mean Reward Calculado: 0.019787 (Recompensa/Pasos)\n",
            " 1340321/2000000: episode: 1761, duration: 30.321s, episode steps: 657, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.013611, mae: 2.070639, mean_q: 2.499463, mean_eps: 0.100000\n",
            "📈 Episodio 1762: Recompensa total (clipped): 19.000, Pasos: 800, Mean Reward Calculado: 0.023750 (Recompensa/Pasos)\n",
            " 1341121/2000000: episode: 1762, duration: 37.306s, episode steps: 800, steps per second:  21, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.014856, mae: 2.066344, mean_q: 2.493186, mean_eps: 0.100000\n",
            "📈 Episodio 1763: Recompensa total (clipped): 21.000, Pasos: 892, Mean Reward Calculado: 0.023543 (Recompensa/Pasos)\n",
            " 1342013/2000000: episode: 1763, duration: 40.820s, episode steps: 892, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.015931, mae: 2.058436, mean_q: 2.486100, mean_eps: 0.100000\n",
            "📈 Episodio 1764: Recompensa total (clipped): 18.000, Pasos: 872, Mean Reward Calculado: 0.020642 (Recompensa/Pasos)\n",
            " 1342885/2000000: episode: 1764, duration: 39.933s, episode steps: 872, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.015418, mae: 2.070853, mean_q: 2.498547, mean_eps: 0.100000\n",
            "📈 Episodio 1765: Recompensa total (clipped): 18.000, Pasos: 694, Mean Reward Calculado: 0.025937 (Recompensa/Pasos)\n",
            " 1343579/2000000: episode: 1765, duration: 31.784s, episode steps: 694, steps per second:  22, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.017366, mae: 2.063607, mean_q: 2.491061, mean_eps: 0.100000\n",
            "📈 Episodio 1766: Recompensa total (clipped): 22.000, Pasos: 970, Mean Reward Calculado: 0.022680 (Recompensa/Pasos)\n",
            " 1344549/2000000: episode: 1766, duration: 44.360s, episode steps: 970, steps per second:  22, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.015429, mae: 2.061911, mean_q: 2.487363, mean_eps: 0.100000\n",
            "📈 Episodio 1767: Recompensa total (clipped): 16.000, Pasos: 844, Mean Reward Calculado: 0.018957 (Recompensa/Pasos)\n",
            " 1345393/2000000: episode: 1767, duration: 38.969s, episode steps: 844, steps per second:  22, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.014181, mae: 2.063347, mean_q: 2.488975, mean_eps: 0.100000\n",
            "📈 Episodio 1768: Recompensa total (clipped): 12.000, Pasos: 627, Mean Reward Calculado: 0.019139 (Recompensa/Pasos)\n",
            " 1346020/2000000: episode: 1768, duration: 28.966s, episode steps: 627, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.589 [0.000, 5.000],  loss: 0.014858, mae: 2.058056, mean_q: 2.485025, mean_eps: 0.100000\n",
            "📈 Episodio 1769: Recompensa total (clipped): 14.000, Pasos: 627, Mean Reward Calculado: 0.022329 (Recompensa/Pasos)\n",
            " 1346647/2000000: episode: 1769, duration: 28.690s, episode steps: 627, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.016671, mae: 2.066353, mean_q: 2.492641, mean_eps: 0.100000\n",
            "📈 Episodio 1770: Recompensa total (clipped): 30.000, Pasos: 1678, Mean Reward Calculado: 0.017878 (Recompensa/Pasos)\n",
            " 1348325/2000000: episode: 1770, duration: 76.501s, episode steps: 1678, steps per second:  22, episode reward: 30.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.015553, mae: 2.055161, mean_q: 2.480933, mean_eps: 0.100000\n",
            "📈 Episodio 1771: Recompensa total (clipped): 24.000, Pasos: 897, Mean Reward Calculado: 0.026756 (Recompensa/Pasos)\n",
            " 1349222/2000000: episode: 1771, duration: 40.869s, episode steps: 897, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.014791, mae: 2.070923, mean_q: 2.497650, mean_eps: 0.100000\n",
            "📈 Episodio 1772: Recompensa total (clipped): 23.000, Pasos: 858, Mean Reward Calculado: 0.026807 (Recompensa/Pasos)\n",
            " 1350080/2000000: episode: 1772, duration: 39.107s, episode steps: 858, steps per second:  22, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.014549, mae: 2.059903, mean_q: 2.486687, mean_eps: 0.100000\n",
            "📈 Episodio 1773: Recompensa total (clipped): 17.000, Pasos: 630, Mean Reward Calculado: 0.026984 (Recompensa/Pasos)\n",
            " 1350710/2000000: episode: 1773, duration: 28.920s, episode steps: 630, steps per second:  22, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.014879, mae: 2.059909, mean_q: 2.485622, mean_eps: 0.100000\n",
            "📈 Episodio 1774: Recompensa total (clipped): 17.000, Pasos: 820, Mean Reward Calculado: 0.020732 (Recompensa/Pasos)\n",
            " 1351530/2000000: episode: 1774, duration: 37.451s, episode steps: 820, steps per second:  22, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.013735, mae: 2.067275, mean_q: 2.494560, mean_eps: 0.100000\n",
            "📈 Episodio 1775: Recompensa total (clipped): 20.000, Pasos: 1036, Mean Reward Calculado: 0.019305 (Recompensa/Pasos)\n",
            " 1352566/2000000: episode: 1775, duration: 46.930s, episode steps: 1036, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.015798, mae: 2.060538, mean_q: 2.488792, mean_eps: 0.100000\n",
            "📈 Episodio 1776: Recompensa total (clipped): 20.000, Pasos: 762, Mean Reward Calculado: 0.026247 (Recompensa/Pasos)\n",
            " 1353328/2000000: episode: 1776, duration: 34.335s, episode steps: 762, steps per second:  22, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.015738, mae: 2.066465, mean_q: 2.495846, mean_eps: 0.100000\n",
            "📈 Episodio 1777: Recompensa total (clipped): 17.000, Pasos: 791, Mean Reward Calculado: 0.021492 (Recompensa/Pasos)\n",
            " 1354119/2000000: episode: 1777, duration: 35.943s, episode steps: 791, steps per second:  22, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.016565, mae: 2.056893, mean_q: 2.480515, mean_eps: 0.100000\n",
            "📈 Episodio 1778: Recompensa total (clipped): 5.000, Pasos: 372, Mean Reward Calculado: 0.013441 (Recompensa/Pasos)\n",
            " 1354491/2000000: episode: 1778, duration: 17.126s, episode steps: 372, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.015343, mae: 2.070316, mean_q: 2.495679, mean_eps: 0.100000\n",
            "📈 Episodio 1779: Recompensa total (clipped): 19.000, Pasos: 713, Mean Reward Calculado: 0.026648 (Recompensa/Pasos)\n",
            " 1355204/2000000: episode: 1779, duration: 32.622s, episode steps: 713, steps per second:  22, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.014787, mae: 2.064268, mean_q: 2.488620, mean_eps: 0.100000\n",
            "📈 Episodio 1780: Recompensa total (clipped): 28.000, Pasos: 1345, Mean Reward Calculado: 0.020818 (Recompensa/Pasos)\n",
            " 1356549/2000000: episode: 1780, duration: 61.605s, episode steps: 1345, steps per second:  22, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016086, mae: 2.062206, mean_q: 2.488597, mean_eps: 0.100000\n",
            "📈 Episodio 1781: Recompensa total (clipped): 31.000, Pasos: 1341, Mean Reward Calculado: 0.023117 (Recompensa/Pasos)\n",
            " 1357890/2000000: episode: 1781, duration: 60.390s, episode steps: 1341, steps per second:  22, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.016236, mae: 2.062092, mean_q: 2.487653, mean_eps: 0.100000\n",
            "📈 Episodio 1782: Recompensa total (clipped): 24.000, Pasos: 1096, Mean Reward Calculado: 0.021898 (Recompensa/Pasos)\n",
            " 1358986/2000000: episode: 1782, duration: 49.472s, episode steps: 1096, steps per second:  22, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.015517, mae: 2.056173, mean_q: 2.481293, mean_eps: 0.100000\n",
            "📈 Episodio 1783: Recompensa total (clipped): 24.000, Pasos: 898, Mean Reward Calculado: 0.026726 (Recompensa/Pasos)\n",
            " 1359884/2000000: episode: 1783, duration: 40.387s, episode steps: 898, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.016390, mae: 2.057128, mean_q: 2.480196, mean_eps: 0.100000\n",
            "📊 Paso 1,360,000/2,000,000 (68.0%) - 26.1 pasos/seg - ETA: 6.8h - Memoria: 15262.68 MB\n",
            "📈 Episodio 1784: Recompensa total (clipped): 10.000, Pasos: 466, Mean Reward Calculado: 0.021459 (Recompensa/Pasos)\n",
            " 1360350/2000000: episode: 1784, duration: 21.299s, episode steps: 466, steps per second:  22, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.016529, mae: 2.077968, mean_q: 2.505548, mean_eps: 0.100000\n",
            "📈 Episodio 1785: Recompensa total (clipped): 26.000, Pasos: 1042, Mean Reward Calculado: 0.024952 (Recompensa/Pasos)\n",
            " 1361392/2000000: episode: 1785, duration: 47.508s, episode steps: 1042, steps per second:  22, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.014407, mae: 2.068929, mean_q: 2.498604, mean_eps: 0.100000\n",
            "📈 Episodio 1786: Recompensa total (clipped): 22.000, Pasos: 979, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            " 1362371/2000000: episode: 1786, duration: 44.844s, episode steps: 979, steps per second:  22, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.016407, mae: 2.096753, mean_q: 2.528656, mean_eps: 0.100000\n",
            "📈 Episodio 1787: Recompensa total (clipped): 9.000, Pasos: 522, Mean Reward Calculado: 0.017241 (Recompensa/Pasos)\n",
            " 1362893/2000000: episode: 1787, duration: 23.850s, episode steps: 522, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.238 [0.000, 5.000],  loss: 0.013993, mae: 2.051957, mean_q: 2.477865, mean_eps: 0.100000\n",
            "📈 Episodio 1788: Recompensa total (clipped): 20.000, Pasos: 774, Mean Reward Calculado: 0.025840 (Recompensa/Pasos)\n",
            " 1363667/2000000: episode: 1788, duration: 35.057s, episode steps: 774, steps per second:  22, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.015722, mae: 2.075106, mean_q: 2.502189, mean_eps: 0.100000\n",
            "📈 Episodio 1789: Recompensa total (clipped): 15.000, Pasos: 694, Mean Reward Calculado: 0.021614 (Recompensa/Pasos)\n",
            " 1364361/2000000: episode: 1789, duration: 31.264s, episode steps: 694, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.016510, mae: 2.088348, mean_q: 2.516659, mean_eps: 0.100000\n",
            "📈 Episodio 1790: Recompensa total (clipped): 19.000, Pasos: 835, Mean Reward Calculado: 0.022754 (Recompensa/Pasos)\n",
            " 1365196/2000000: episode: 1790, duration: 37.775s, episode steps: 835, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.016550, mae: 2.085644, mean_q: 2.515009, mean_eps: 0.100000\n",
            "📈 Episodio 1791: Recompensa total (clipped): 34.000, Pasos: 1400, Mean Reward Calculado: 0.024286 (Recompensa/Pasos)\n",
            " 1366596/2000000: episode: 1791, duration: 63.335s, episode steps: 1400, steps per second:  22, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.017221, mae: 2.074220, mean_q: 2.500507, mean_eps: 0.100000\n",
            "📈 Episodio 1792: Recompensa total (clipped): 19.000, Pasos: 766, Mean Reward Calculado: 0.024804 (Recompensa/Pasos)\n",
            " 1367362/2000000: episode: 1792, duration: 35.012s, episode steps: 766, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.015901, mae: 2.072069, mean_q: 2.500758, mean_eps: 0.100000\n",
            "📈 Episodio 1793: Recompensa total (clipped): 24.000, Pasos: 952, Mean Reward Calculado: 0.025210 (Recompensa/Pasos)\n",
            " 1368314/2000000: episode: 1793, duration: 43.059s, episode steps: 952, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.016788, mae: 2.081080, mean_q: 2.508607, mean_eps: 0.100000\n",
            "📈 Episodio 1794: Recompensa total (clipped): 16.000, Pasos: 670, Mean Reward Calculado: 0.023881 (Recompensa/Pasos)\n",
            " 1368984/2000000: episode: 1794, duration: 30.066s, episode steps: 670, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.016787, mae: 2.079196, mean_q: 2.507447, mean_eps: 0.100000\n",
            "📈 Episodio 1795: Recompensa total (clipped): 7.000, Pasos: 524, Mean Reward Calculado: 0.013359 (Recompensa/Pasos)\n",
            " 1369508/2000000: episode: 1795, duration: 23.653s, episode steps: 524, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.018800, mae: 2.058847, mean_q: 2.483402, mean_eps: 0.100000\n",
            "📈 Episodio 1796: Recompensa total (clipped): 15.000, Pasos: 701, Mean Reward Calculado: 0.021398 (Recompensa/Pasos)\n",
            " 1370209/2000000: episode: 1796, duration: 31.924s, episode steps: 701, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014200, mae: 2.069401, mean_q: 2.495376, mean_eps: 0.100000\n",
            "📈 Episodio 1797: Recompensa total (clipped): 4.000, Pasos: 494, Mean Reward Calculado: 0.008097 (Recompensa/Pasos)\n",
            " 1370703/2000000: episode: 1797, duration: 22.355s, episode steps: 494, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016427, mae: 2.082883, mean_q: 2.512738, mean_eps: 0.100000\n",
            "📈 Episodio 1798: Recompensa total (clipped): 25.000, Pasos: 1024, Mean Reward Calculado: 0.024414 (Recompensa/Pasos)\n",
            " 1371727/2000000: episode: 1798, duration: 46.327s, episode steps: 1024, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015666, mae: 2.084682, mean_q: 2.515254, mean_eps: 0.100000\n",
            "📈 Episodio 1799: Recompensa total (clipped): 18.000, Pasos: 801, Mean Reward Calculado: 0.022472 (Recompensa/Pasos)\n",
            " 1372528/2000000: episode: 1799, duration: 36.314s, episode steps: 801, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.016807, mae: 2.076431, mean_q: 2.503170, mean_eps: 0.100000\n",
            "📈 Episodio 1800: Recompensa total (clipped): 9.000, Pasos: 365, Mean Reward Calculado: 0.024658 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1800, pasos: 1372893)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.17 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1800 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 9.00\n",
            "   Media últimos 100: 17.17 / 20.0\n",
            "   Mejor promedio histórico: 17.17\n",
            "   Estado: 📈 85.9% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1372893/2000000: episode: 1800, duration: 87.384s, episode steps: 365, steps per second:   4, episode reward:  9.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.214 [0.000, 5.000],  loss: 0.018447, mae: 2.077545, mean_q: 2.505542, mean_eps: 0.100000\n",
            "📈 Episodio 1801: Recompensa total (clipped): 21.000, Pasos: 989, Mean Reward Calculado: 0.021234 (Recompensa/Pasos)\n",
            " 1373882/2000000: episode: 1801, duration: 45.284s, episode steps: 989, steps per second:  22, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.017519, mae: 2.074148, mean_q: 2.500785, mean_eps: 0.100000\n",
            "📈 Episodio 1802: Recompensa total (clipped): 20.000, Pasos: 1349, Mean Reward Calculado: 0.014826 (Recompensa/Pasos)\n",
            " 1375231/2000000: episode: 1802, duration: 62.062s, episode steps: 1349, steps per second:  22, episode reward: 20.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.017060, mae: 2.073452, mean_q: 2.500987, mean_eps: 0.100000\n",
            "📈 Episodio 1803: Recompensa total (clipped): 13.000, Pasos: 692, Mean Reward Calculado: 0.018786 (Recompensa/Pasos)\n",
            " 1375923/2000000: episode: 1803, duration: 31.824s, episode steps: 692, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.182 [0.000, 5.000],  loss: 0.017125, mae: 2.073355, mean_q: 2.502157, mean_eps: 0.100000\n",
            "📈 Episodio 1804: Recompensa total (clipped): 18.000, Pasos: 873, Mean Reward Calculado: 0.020619 (Recompensa/Pasos)\n",
            " 1376796/2000000: episode: 1804, duration: 39.933s, episode steps: 873, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.016993, mae: 2.067043, mean_q: 2.493320, mean_eps: 0.100000\n",
            "📈 Episodio 1805: Recompensa total (clipped): 30.000, Pasos: 1102, Mean Reward Calculado: 0.027223 (Recompensa/Pasos)\n",
            " 1377898/2000000: episode: 1805, duration: 50.489s, episode steps: 1102, steps per second:  22, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.015326, mae: 2.070998, mean_q: 2.496609, mean_eps: 0.100000\n",
            "📈 Episodio 1806: Recompensa total (clipped): 12.000, Pasos: 649, Mean Reward Calculado: 0.018490 (Recompensa/Pasos)\n",
            " 1378547/2000000: episode: 1806, duration: 29.091s, episode steps: 649, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.017563, mae: 2.070856, mean_q: 2.500761, mean_eps: 0.100000\n",
            "📈 Episodio 1807: Recompensa total (clipped): 13.000, Pasos: 491, Mean Reward Calculado: 0.026477 (Recompensa/Pasos)\n",
            " 1379038/2000000: episode: 1807, duration: 22.368s, episode steps: 491, steps per second:  22, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.017721, mae: 2.063573, mean_q: 2.487102, mean_eps: 0.100000\n",
            "📈 Episodio 1808: Recompensa total (clipped): 13.000, Pasos: 555, Mean Reward Calculado: 0.023423 (Recompensa/Pasos)\n",
            " 1379593/2000000: episode: 1808, duration: 25.630s, episode steps: 555, steps per second:  22, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.189 [0.000, 5.000],  loss: 0.016426, mae: 2.064764, mean_q: 2.493041, mean_eps: 0.100000\n",
            "📊 Paso 1,380,000/2,000,000 (69.0%) - 26.0 pasos/seg - ETA: 6.6h - Memoria: 15257.42 MB\n",
            "📈 Episodio 1809: Recompensa total (clipped): 18.000, Pasos: 842, Mean Reward Calculado: 0.021378 (Recompensa/Pasos)\n",
            " 1380435/2000000: episode: 1809, duration: 38.591s, episode steps: 842, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.018002, mae: 2.067945, mean_q: 2.492133, mean_eps: 0.100000\n",
            "📈 Episodio 1810: Recompensa total (clipped): 14.000, Pasos: 645, Mean Reward Calculado: 0.021705 (Recompensa/Pasos)\n",
            " 1381080/2000000: episode: 1810, duration: 29.618s, episode steps: 645, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.014610, mae: 2.058802, mean_q: 2.485274, mean_eps: 0.100000\n",
            "📈 Episodio 1811: Recompensa total (clipped): 12.000, Pasos: 630, Mean Reward Calculado: 0.019048 (Recompensa/Pasos)\n",
            " 1381710/2000000: episode: 1811, duration: 29.092s, episode steps: 630, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.015743, mae: 2.064794, mean_q: 2.490636, mean_eps: 0.100000\n",
            "📈 Episodio 1812: Recompensa total (clipped): 21.000, Pasos: 1125, Mean Reward Calculado: 0.018667 (Recompensa/Pasos)\n",
            " 1382835/2000000: episode: 1812, duration: 51.137s, episode steps: 1125, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016346, mae: 2.062837, mean_q: 2.487188, mean_eps: 0.100000\n",
            "📈 Episodio 1813: Recompensa total (clipped): 13.000, Pasos: 502, Mean Reward Calculado: 0.025896 (Recompensa/Pasos)\n",
            " 1383337/2000000: episode: 1813, duration: 22.845s, episode steps: 502, steps per second:  22, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.017566, mae: 2.077712, mean_q: 2.505556, mean_eps: 0.100000\n",
            "📈 Episodio 1814: Recompensa total (clipped): 14.000, Pasos: 706, Mean Reward Calculado: 0.019830 (Recompensa/Pasos)\n",
            " 1384043/2000000: episode: 1814, duration: 32.213s, episode steps: 706, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.016772, mae: 2.076940, mean_q: 2.503906, mean_eps: 0.100000\n",
            "📈 Episodio 1815: Recompensa total (clipped): 6.000, Pasos: 497, Mean Reward Calculado: 0.012072 (Recompensa/Pasos)\n",
            " 1384540/2000000: episode: 1815, duration: 22.713s, episode steps: 497, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.014004, mae: 2.057513, mean_q: 2.482651, mean_eps: 0.100000\n",
            "📈 Episodio 1816: Recompensa total (clipped): 12.000, Pasos: 542, Mean Reward Calculado: 0.022140 (Recompensa/Pasos)\n",
            " 1385082/2000000: episode: 1816, duration: 24.612s, episode steps: 542, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.015241, mae: 2.068885, mean_q: 2.496468, mean_eps: 0.100000\n",
            "📈 Episodio 1817: Recompensa total (clipped): 17.000, Pasos: 983, Mean Reward Calculado: 0.017294 (Recompensa/Pasos)\n",
            " 1386065/2000000: episode: 1817, duration: 44.349s, episode steps: 983, steps per second:  22, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.015239, mae: 2.048819, mean_q: 2.472556, mean_eps: 0.100000\n",
            "📈 Episodio 1818: Recompensa total (clipped): 30.000, Pasos: 1136, Mean Reward Calculado: 0.026408 (Recompensa/Pasos)\n",
            " 1387201/2000000: episode: 1818, duration: 51.283s, episode steps: 1136, steps per second:  22, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.015595, mae: 2.061991, mean_q: 2.486703, mean_eps: 0.100000\n",
            "📈 Episodio 1819: Recompensa total (clipped): 28.000, Pasos: 819, Mean Reward Calculado: 0.034188 (Recompensa/Pasos)\n",
            " 1388020/2000000: episode: 1819, duration: 37.099s, episode steps: 819, steps per second:  22, episode reward: 28.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016455, mae: 2.051712, mean_q: 2.475378, mean_eps: 0.100000\n",
            "📈 Episodio 1820: Recompensa total (clipped): 27.000, Pasos: 1044, Mean Reward Calculado: 0.025862 (Recompensa/Pasos)\n",
            " 1389064/2000000: episode: 1820, duration: 47.525s, episode steps: 1044, steps per second:  22, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.015419, mae: 2.065572, mean_q: 2.491930, mean_eps: 0.100000\n",
            "📈 Episodio 1821: Recompensa total (clipped): 6.000, Pasos: 538, Mean Reward Calculado: 0.011152 (Recompensa/Pasos)\n",
            " 1389602/2000000: episode: 1821, duration: 24.351s, episode steps: 538, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.016656, mae: 2.059520, mean_q: 2.484568, mean_eps: 0.100000\n",
            "📈 Episodio 1822: Recompensa total (clipped): 7.000, Pasos: 495, Mean Reward Calculado: 0.014141 (Recompensa/Pasos)\n",
            " 1390097/2000000: episode: 1822, duration: 22.833s, episode steps: 495, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.015931, mae: 2.066198, mean_q: 2.496416, mean_eps: 0.100000\n",
            "📈 Episodio 1823: Recompensa total (clipped): 24.000, Pasos: 763, Mean Reward Calculado: 0.031455 (Recompensa/Pasos)\n",
            " 1390860/2000000: episode: 1823, duration: 34.877s, episode steps: 763, steps per second:  22, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.016654, mae: 2.070745, mean_q: 2.497609, mean_eps: 0.100000\n",
            "📈 Episodio 1824: Recompensa total (clipped): 31.000, Pasos: 1240, Mean Reward Calculado: 0.025000 (Recompensa/Pasos)\n",
            " 1392100/2000000: episode: 1824, duration: 57.158s, episode steps: 1240, steps per second:  22, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014544, mae: 2.053072, mean_q: 2.478264, mean_eps: 0.100000\n",
            "📈 Episodio 1825: Recompensa total (clipped): 13.000, Pasos: 701, Mean Reward Calculado: 0.018545 (Recompensa/Pasos)\n",
            " 1392801/2000000: episode: 1825, duration: 32.513s, episode steps: 701, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.018641, mae: 2.046394, mean_q: 2.468966, mean_eps: 0.100000\n",
            "📈 Episodio 1826: Recompensa total (clipped): 28.000, Pasos: 1211, Mean Reward Calculado: 0.023121 (Recompensa/Pasos)\n",
            " 1394012/2000000: episode: 1826, duration: 55.377s, episode steps: 1211, steps per second:  22, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.017265, mae: 2.051530, mean_q: 2.477289, mean_eps: 0.100000\n",
            "📈 Episodio 1827: Recompensa total (clipped): 14.000, Pasos: 618, Mean Reward Calculado: 0.022654 (Recompensa/Pasos)\n",
            " 1394630/2000000: episode: 1827, duration: 28.767s, episode steps: 618, steps per second:  21, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.939 [0.000, 5.000],  loss: 0.017986, mae: 2.084523, mean_q: 2.515336, mean_eps: 0.100000\n",
            "📈 Episodio 1828: Recompensa total (clipped): 22.000, Pasos: 917, Mean Reward Calculado: 0.023991 (Recompensa/Pasos)\n",
            " 1395547/2000000: episode: 1828, duration: 42.368s, episode steps: 917, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.015356, mae: 2.061007, mean_q: 2.487975, mean_eps: 0.100000\n",
            "📈 Episodio 1829: Recompensa total (clipped): 13.000, Pasos: 683, Mean Reward Calculado: 0.019034 (Recompensa/Pasos)\n",
            " 1396230/2000000: episode: 1829, duration: 31.462s, episode steps: 683, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.019438, mae: 2.067490, mean_q: 2.493495, mean_eps: 0.100000\n",
            "📈 Episodio 1830: Recompensa total (clipped): 12.000, Pasos: 480, Mean Reward Calculado: 0.025000 (Recompensa/Pasos)\n",
            " 1396710/2000000: episode: 1830, duration: 21.924s, episode steps: 480, steps per second:  22, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.017199, mae: 2.074503, mean_q: 2.501070, mean_eps: 0.100000\n",
            "📈 Episodio 1831: Recompensa total (clipped): 16.000, Pasos: 731, Mean Reward Calculado: 0.021888 (Recompensa/Pasos)\n",
            " 1397441/2000000: episode: 1831, duration: 33.941s, episode steps: 731, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.017716, mae: 2.061927, mean_q: 2.485676, mean_eps: 0.100000\n",
            "📈 Episodio 1832: Recompensa total (clipped): 21.000, Pasos: 778, Mean Reward Calculado: 0.026992 (Recompensa/Pasos)\n",
            " 1398219/2000000: episode: 1832, duration: 35.270s, episode steps: 778, steps per second:  22, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.017369, mae: 2.072753, mean_q: 2.500403, mean_eps: 0.100000\n",
            "📈 Episodio 1833: Recompensa total (clipped): 16.000, Pasos: 686, Mean Reward Calculado: 0.023324 (Recompensa/Pasos)\n",
            " 1398905/2000000: episode: 1833, duration: 30.954s, episode steps: 686, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.015464, mae: 2.072475, mean_q: 2.501381, mean_eps: 0.100000\n",
            "📈 Episodio 1834: Recompensa total (clipped): 25.000, Pasos: 909, Mean Reward Calculado: 0.027503 (Recompensa/Pasos)\n",
            " 1399814/2000000: episode: 1834, duration: 41.211s, episode steps: 909, steps per second:  22, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.016430, mae: 2.059468, mean_q: 2.485113, mean_eps: 0.100000\n",
            "📊 Paso 1,400,000/2,000,000 (70.0%) - 25.9 pasos/seg - ETA: 6.4h - Memoria: 15261.35 MB\n",
            "📈 Episodio 1835: Recompensa total (clipped): 8.000, Pasos: 496, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            " 1400310/2000000: episode: 1835, duration: 22.783s, episode steps: 496, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.014822, mae: 2.069612, mean_q: 2.503927, mean_eps: 0.100000\n",
            "📈 Episodio 1836: Recompensa total (clipped): 11.000, Pasos: 592, Mean Reward Calculado: 0.018581 (Recompensa/Pasos)\n",
            " 1400902/2000000: episode: 1836, duration: 27.251s, episode steps: 592, steps per second:  22, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.016491, mae: 2.069922, mean_q: 2.500604, mean_eps: 0.100000\n",
            "📈 Episodio 1837: Recompensa total (clipped): 17.000, Pasos: 1012, Mean Reward Calculado: 0.016798 (Recompensa/Pasos)\n",
            " 1401914/2000000: episode: 1837, duration: 46.513s, episode steps: 1012, steps per second:  22, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.015808, mae: 2.075175, mean_q: 2.503624, mean_eps: 0.100000\n",
            "📈 Episodio 1838: Recompensa total (clipped): 27.000, Pasos: 1118, Mean Reward Calculado: 0.024150 (Recompensa/Pasos)\n",
            " 1403032/2000000: episode: 1838, duration: 51.754s, episode steps: 1118, steps per second:  22, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.016289, mae: 2.070201, mean_q: 2.498154, mean_eps: 0.100000\n",
            "📈 Episodio 1839: Recompensa total (clipped): 24.000, Pasos: 1019, Mean Reward Calculado: 0.023553 (Recompensa/Pasos)\n",
            " 1404051/2000000: episode: 1839, duration: 46.171s, episode steps: 1019, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016030, mae: 2.057139, mean_q: 2.481529, mean_eps: 0.100000\n",
            "📈 Episodio 1840: Recompensa total (clipped): 14.000, Pasos: 597, Mean Reward Calculado: 0.023451 (Recompensa/Pasos)\n",
            " 1404648/2000000: episode: 1840, duration: 27.429s, episode steps: 597, steps per second:  22, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.015758, mae: 2.046303, mean_q: 2.469812, mean_eps: 0.100000\n",
            "📈 Episodio 1841: Recompensa total (clipped): 22.000, Pasos: 1064, Mean Reward Calculado: 0.020677 (Recompensa/Pasos)\n",
            " 1405712/2000000: episode: 1841, duration: 48.423s, episode steps: 1064, steps per second:  22, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.670 [0.000, 5.000],  loss: 0.014835, mae: 2.063661, mean_q: 2.489134, mean_eps: 0.100000\n",
            "📈 Episodio 1842: Recompensa total (clipped): 9.000, Pasos: 399, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
            " 1406111/2000000: episode: 1842, duration: 18.483s, episode steps: 399, steps per second:  22, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.093 [0.000, 5.000],  loss: 0.016207, mae: 2.038355, mean_q: 2.459205, mean_eps: 0.100000\n",
            "📈 Episodio 1843: Recompensa total (clipped): 10.000, Pasos: 641, Mean Reward Calculado: 0.015601 (Recompensa/Pasos)\n",
            " 1406752/2000000: episode: 1843, duration: 29.937s, episode steps: 641, steps per second:  21, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.115 [0.000, 5.000],  loss: 0.016233, mae: 2.052342, mean_q: 2.477184, mean_eps: 0.100000\n",
            "📈 Episodio 1844: Recompensa total (clipped): 24.000, Pasos: 983, Mean Reward Calculado: 0.024415 (Recompensa/Pasos)\n",
            " 1407735/2000000: episode: 1844, duration: 45.435s, episode steps: 983, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.016525, mae: 2.076177, mean_q: 2.502698, mean_eps: 0.100000\n",
            "📈 Episodio 1845: Recompensa total (clipped): 11.000, Pasos: 502, Mean Reward Calculado: 0.021912 (Recompensa/Pasos)\n",
            " 1408237/2000000: episode: 1845, duration: 23.196s, episode steps: 502, steps per second:  22, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.440 [0.000, 5.000],  loss: 0.017780, mae: 2.065176, mean_q: 2.489585, mean_eps: 0.100000\n",
            "📈 Episodio 1846: Recompensa total (clipped): 5.000, Pasos: 364, Mean Reward Calculado: 0.013736 (Recompensa/Pasos)\n",
            " 1408601/2000000: episode: 1846, duration: 16.711s, episode steps: 364, steps per second:  22, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.016318, mae: 2.091227, mean_q: 2.521859, mean_eps: 0.100000\n",
            "📈 Episodio 1847: Recompensa total (clipped): 24.000, Pasos: 752, Mean Reward Calculado: 0.031915 (Recompensa/Pasos)\n",
            " 1409353/2000000: episode: 1847, duration: 34.595s, episode steps: 752, steps per second:  22, episode reward: 24.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015755, mae: 2.064389, mean_q: 2.490078, mean_eps: 0.100000\n",
            "📈 Episodio 1848: Recompensa total (clipped): 17.000, Pasos: 1049, Mean Reward Calculado: 0.016206 (Recompensa/Pasos)\n",
            " 1410402/2000000: episode: 1848, duration: 48.430s, episode steps: 1049, steps per second:  22, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.016963, mae: 2.076030, mean_q: 2.504271, mean_eps: 0.100000\n",
            "📈 Episodio 1849: Recompensa total (clipped): 24.000, Pasos: 1028, Mean Reward Calculado: 0.023346 (Recompensa/Pasos)\n",
            " 1411430/2000000: episode: 1849, duration: 47.380s, episode steps: 1028, steps per second:  22, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016490, mae: 2.074606, mean_q: 2.502086, mean_eps: 0.100000\n",
            "📈 Episodio 1850: Recompensa total (clipped): 12.000, Pasos: 908, Mean Reward Calculado: 0.013216 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1850, pasos: 1412338)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.66 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1850 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 12.00\n",
            "   Media últimos 100: 17.66 / 20.0\n",
            "   Mejor promedio histórico: 17.66\n",
            "   Estado: 📈 88.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1412338/2000000: episode: 1850, duration: 101.416s, episode steps: 908, steps per second:   9, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.016352, mae: 2.089683, mean_q: 2.521128, mean_eps: 0.100000\n",
            "📈 Episodio 1851: Recompensa total (clipped): 14.000, Pasos: 543, Mean Reward Calculado: 0.025783 (Recompensa/Pasos)\n",
            " 1412881/2000000: episode: 1851, duration: 24.604s, episode steps: 543, steps per second:  22, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.015951, mae: 2.085693, mean_q: 2.520443, mean_eps: 0.100000\n",
            "📈 Episodio 1852: Recompensa total (clipped): 36.000, Pasos: 1622, Mean Reward Calculado: 0.022195 (Recompensa/Pasos)\n",
            " 1414503/2000000: episode: 1852, duration: 74.826s, episode steps: 1622, steps per second:  22, episode reward: 36.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.016175, mae: 2.085272, mean_q: 2.515168, mean_eps: 0.100000\n",
            "📈 Episodio 1853: Recompensa total (clipped): 25.000, Pasos: 993, Mean Reward Calculado: 0.025176 (Recompensa/Pasos)\n",
            " 1415496/2000000: episode: 1853, duration: 46.174s, episode steps: 993, steps per second:  22, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.017658, mae: 2.086285, mean_q: 2.514720, mean_eps: 0.100000\n",
            "📈 Episodio 1854: Recompensa total (clipped): 8.000, Pasos: 533, Mean Reward Calculado: 0.015009 (Recompensa/Pasos)\n",
            " 1416029/2000000: episode: 1854, duration: 24.944s, episode steps: 533, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.015431, mae: 2.099531, mean_q: 2.531088, mean_eps: 0.100000\n",
            "📈 Episodio 1855: Recompensa total (clipped): 7.000, Pasos: 335, Mean Reward Calculado: 0.020896 (Recompensa/Pasos)\n",
            " 1416364/2000000: episode: 1855, duration: 15.558s, episode steps: 335, steps per second:  22, episode reward:  7.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.017608, mae: 2.098146, mean_q: 2.533683, mean_eps: 0.100000\n",
            "📈 Episodio 1856: Recompensa total (clipped): 23.000, Pasos: 915, Mean Reward Calculado: 0.025137 (Recompensa/Pasos)\n",
            " 1417279/2000000: episode: 1856, duration: 42.245s, episode steps: 915, steps per second:  22, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.784 [0.000, 5.000],  loss: 0.017354, mae: 2.083090, mean_q: 2.511409, mean_eps: 0.100000\n",
            "📈 Episodio 1857: Recompensa total (clipped): 25.000, Pasos: 1047, Mean Reward Calculado: 0.023878 (Recompensa/Pasos)\n",
            " 1418326/2000000: episode: 1857, duration: 48.623s, episode steps: 1047, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.016319, mae: 2.078446, mean_q: 2.505511, mean_eps: 0.100000\n",
            "📈 Episodio 1858: Recompensa total (clipped): 9.000, Pasos: 620, Mean Reward Calculado: 0.014516 (Recompensa/Pasos)\n",
            " 1418946/2000000: episode: 1858, duration: 28.493s, episode steps: 620, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.017052, mae: 2.082860, mean_q: 2.513347, mean_eps: 0.100000\n",
            "📈 Episodio 1859: Recompensa total (clipped): 19.000, Pasos: 726, Mean Reward Calculado: 0.026171 (Recompensa/Pasos)\n",
            " 1419672/2000000: episode: 1859, duration: 33.666s, episode steps: 726, steps per second:  22, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.300 [0.000, 5.000],  loss: 0.016291, mae: 2.076825, mean_q: 2.505258, mean_eps: 0.100000\n",
            "📊 Paso 1,420,000/2,000,000 (71.0%) - 25.8 pasos/seg - ETA: 6.2h - Memoria: 15256.91 MB\n",
            "📈 Episodio 1860: Recompensa total (clipped): 16.000, Pasos: 521, Mean Reward Calculado: 0.030710 (Recompensa/Pasos)\n",
            " 1420193/2000000: episode: 1860, duration: 24.243s, episode steps: 521, steps per second:  21, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.016837, mae: 2.108482, mean_q: 2.543082, mean_eps: 0.100000\n",
            "📈 Episodio 1861: Recompensa total (clipped): 6.000, Pasos: 435, Mean Reward Calculado: 0.013793 (Recompensa/Pasos)\n",
            " 1420628/2000000: episode: 1861, duration: 20.124s, episode steps: 435, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.497 [0.000, 5.000],  loss: 0.016541, mae: 2.092844, mean_q: 2.525721, mean_eps: 0.100000\n",
            "📈 Episodio 1862: Recompensa total (clipped): 14.000, Pasos: 792, Mean Reward Calculado: 0.017677 (Recompensa/Pasos)\n",
            " 1421420/2000000: episode: 1862, duration: 36.894s, episode steps: 792, steps per second:  21, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.016065, mae: 2.081228, mean_q: 2.513124, mean_eps: 0.100000\n",
            "📈 Episodio 1863: Recompensa total (clipped): 10.000, Pasos: 561, Mean Reward Calculado: 0.017825 (Recompensa/Pasos)\n",
            " 1421981/2000000: episode: 1863, duration: 25.797s, episode steps: 561, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.017570, mae: 2.098431, mean_q: 2.534445, mean_eps: 0.100000\n",
            "📈 Episodio 1864: Recompensa total (clipped): 30.000, Pasos: 1119, Mean Reward Calculado: 0.026810 (Recompensa/Pasos)\n",
            " 1423100/2000000: episode: 1864, duration: 51.088s, episode steps: 1119, steps per second:  22, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.015790, mae: 2.088728, mean_q: 2.520477, mean_eps: 0.100000\n",
            "📈 Episodio 1865: Recompensa total (clipped): 12.000, Pasos: 477, Mean Reward Calculado: 0.025157 (Recompensa/Pasos)\n",
            " 1423577/2000000: episode: 1865, duration: 21.791s, episode steps: 477, steps per second:  22, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.018673, mae: 2.073000, mean_q: 2.500702, mean_eps: 0.100000\n",
            "📈 Episodio 1866: Recompensa total (clipped): 7.000, Pasos: 475, Mean Reward Calculado: 0.014737 (Recompensa/Pasos)\n",
            " 1424052/2000000: episode: 1866, duration: 21.793s, episode steps: 475, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.019715, mae: 2.071993, mean_q: 2.499913, mean_eps: 0.100000\n",
            "📈 Episodio 1867: Recompensa total (clipped): 7.000, Pasos: 508, Mean Reward Calculado: 0.013780 (Recompensa/Pasos)\n",
            " 1424560/2000000: episode: 1867, duration: 22.927s, episode steps: 508, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015633, mae: 2.081542, mean_q: 2.510321, mean_eps: 0.100000\n",
            "📈 Episodio 1868: Recompensa total (clipped): 10.000, Pasos: 541, Mean Reward Calculado: 0.018484 (Recompensa/Pasos)\n",
            " 1425101/2000000: episode: 1868, duration: 24.556s, episode steps: 541, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.017669, mae: 2.098177, mean_q: 2.533462, mean_eps: 0.100000\n",
            "📈 Episodio 1869: Recompensa total (clipped): 17.000, Pasos: 754, Mean Reward Calculado: 0.022546 (Recompensa/Pasos)\n",
            " 1425855/2000000: episode: 1869, duration: 34.537s, episode steps: 754, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.017761, mae: 2.085524, mean_q: 2.515932, mean_eps: 0.100000\n",
            "📈 Episodio 1870: Recompensa total (clipped): 18.000, Pasos: 922, Mean Reward Calculado: 0.019523 (Recompensa/Pasos)\n",
            " 1426777/2000000: episode: 1870, duration: 42.273s, episode steps: 922, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.016328, mae: 2.089340, mean_q: 2.521370, mean_eps: 0.100000\n",
            "📈 Episodio 1871: Recompensa total (clipped): 25.000, Pasos: 1380, Mean Reward Calculado: 0.018116 (Recompensa/Pasos)\n",
            " 1428157/2000000: episode: 1871, duration: 62.901s, episode steps: 1380, steps per second:  22, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.016909, mae: 2.077703, mean_q: 2.506198, mean_eps: 0.100000\n",
            "📈 Episodio 1872: Recompensa total (clipped): 11.000, Pasos: 611, Mean Reward Calculado: 0.018003 (Recompensa/Pasos)\n",
            " 1428768/2000000: episode: 1872, duration: 27.849s, episode steps: 611, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.016743, mae: 2.083333, mean_q: 2.510138, mean_eps: 0.100000\n",
            "📈 Episodio 1873: Recompensa total (clipped): 16.000, Pasos: 788, Mean Reward Calculado: 0.020305 (Recompensa/Pasos)\n",
            " 1429556/2000000: episode: 1873, duration: 36.411s, episode steps: 788, steps per second:  22, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.015962, mae: 2.080296, mean_q: 2.509142, mean_eps: 0.100000\n",
            "📈 Episodio 1874: Recompensa total (clipped): 8.000, Pasos: 519, Mean Reward Calculado: 0.015414 (Recompensa/Pasos)\n",
            " 1430075/2000000: episode: 1874, duration: 23.592s, episode steps: 519, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.015393, mae: 2.092772, mean_q: 2.529025, mean_eps: 0.100000\n",
            "📈 Episodio 1875: Recompensa total (clipped): 13.000, Pasos: 629, Mean Reward Calculado: 0.020668 (Recompensa/Pasos)\n",
            " 1430704/2000000: episode: 1875, duration: 28.654s, episode steps: 629, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.015522, mae: 2.074005, mean_q: 2.503543, mean_eps: 0.100000\n",
            "📈 Episodio 1876: Recompensa total (clipped): 11.000, Pasos: 634, Mean Reward Calculado: 0.017350 (Recompensa/Pasos)\n",
            " 1431338/2000000: episode: 1876, duration: 28.678s, episode steps: 634, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.016676, mae: 2.060054, mean_q: 2.487523, mean_eps: 0.100000\n",
            "📈 Episodio 1877: Recompensa total (clipped): 19.000, Pasos: 759, Mean Reward Calculado: 0.025033 (Recompensa/Pasos)\n",
            " 1432097/2000000: episode: 1877, duration: 34.321s, episode steps: 759, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.015367, mae: 2.091001, mean_q: 2.523080, mean_eps: 0.100000\n",
            "📈 Episodio 1878: Recompensa total (clipped): 14.000, Pasos: 626, Mean Reward Calculado: 0.022364 (Recompensa/Pasos)\n",
            " 1432723/2000000: episode: 1878, duration: 28.157s, episode steps: 626, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.016405, mae: 2.061161, mean_q: 2.490297, mean_eps: 0.100000\n",
            "📈 Episodio 1879: Recompensa total (clipped): 15.000, Pasos: 579, Mean Reward Calculado: 0.025907 (Recompensa/Pasos)\n",
            " 1433302/2000000: episode: 1879, duration: 26.212s, episode steps: 579, steps per second:  22, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.016934, mae: 2.066104, mean_q: 2.493304, mean_eps: 0.100000\n",
            "📈 Episodio 1880: Recompensa total (clipped): 10.000, Pasos: 433, Mean Reward Calculado: 0.023095 (Recompensa/Pasos)\n",
            " 1433735/2000000: episode: 1880, duration: 19.353s, episode steps: 433, steps per second:  22, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.492 [0.000, 5.000],  loss: 0.017751, mae: 2.055774, mean_q: 2.481107, mean_eps: 0.100000\n",
            "📈 Episodio 1881: Recompensa total (clipped): 25.000, Pasos: 975, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            " 1434710/2000000: episode: 1881, duration: 44.462s, episode steps: 975, steps per second:  22, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.016192, mae: 2.062489, mean_q: 2.489358, mean_eps: 0.100000\n",
            "📈 Episodio 1882: Recompensa total (clipped): 16.000, Pasos: 647, Mean Reward Calculado: 0.024730 (Recompensa/Pasos)\n",
            " 1435357/2000000: episode: 1882, duration: 29.450s, episode steps: 647, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015624, mae: 2.066158, mean_q: 2.493895, mean_eps: 0.100000\n",
            "📈 Episodio 1883: Recompensa total (clipped): 5.000, Pasos: 474, Mean Reward Calculado: 0.010549 (Recompensa/Pasos)\n",
            " 1435831/2000000: episode: 1883, duration: 21.242s, episode steps: 474, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.243 [0.000, 5.000],  loss: 0.018375, mae: 2.078673, mean_q: 2.506392, mean_eps: 0.100000\n",
            "📈 Episodio 1884: Recompensa total (clipped): 13.000, Pasos: 646, Mean Reward Calculado: 0.020124 (Recompensa/Pasos)\n",
            " 1436477/2000000: episode: 1884, duration: 29.366s, episode steps: 646, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.016229, mae: 2.078591, mean_q: 2.509095, mean_eps: 0.100000\n",
            "📈 Episodio 1885: Recompensa total (clipped): 20.000, Pasos: 1040, Mean Reward Calculado: 0.019231 (Recompensa/Pasos)\n",
            " 1437517/2000000: episode: 1885, duration: 46.945s, episode steps: 1040, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.015678, mae: 2.071831, mean_q: 2.500257, mean_eps: 0.100000\n",
            "📈 Episodio 1886: Recompensa total (clipped): 29.000, Pasos: 1042, Mean Reward Calculado: 0.027831 (Recompensa/Pasos)\n",
            " 1438559/2000000: episode: 1886, duration: 47.371s, episode steps: 1042, steps per second:  22, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.016198, mae: 2.080582, mean_q: 2.509661, mean_eps: 0.100000\n",
            "📈 Episodio 1887: Recompensa total (clipped): 21.000, Pasos: 821, Mean Reward Calculado: 0.025579 (Recompensa/Pasos)\n",
            " 1439380/2000000: episode: 1887, duration: 37.533s, episode steps: 821, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.015698, mae: 2.074440, mean_q: 2.504235, mean_eps: 0.100000\n",
            "📊 Paso 1,440,000/2,000,000 (72.0%) - 25.7 pasos/seg - ETA: 6.0h - Memoria: 15260.73 MB\n",
            "📈 Episodio 1888: Recompensa total (clipped): 23.000, Pasos: 999, Mean Reward Calculado: 0.023023 (Recompensa/Pasos)\n",
            " 1440379/2000000: episode: 1888, duration: 45.225s, episode steps: 999, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.015940, mae: 2.089438, mean_q: 2.522572, mean_eps: 0.100000\n",
            "📈 Episodio 1889: Recompensa total (clipped): 9.000, Pasos: 768, Mean Reward Calculado: 0.011719 (Recompensa/Pasos)\n",
            " 1441147/2000000: episode: 1889, duration: 34.713s, episode steps: 768, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.010 [0.000, 5.000],  loss: 0.015809, mae: 2.092775, mean_q: 2.525602, mean_eps: 0.100000\n",
            "📈 Episodio 1890: Recompensa total (clipped): 18.000, Pasos: 785, Mean Reward Calculado: 0.022930 (Recompensa/Pasos)\n",
            " 1441932/2000000: episode: 1890, duration: 35.779s, episode steps: 785, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.015644, mae: 2.091495, mean_q: 2.523905, mean_eps: 0.100000\n",
            "📈 Episodio 1891: Recompensa total (clipped): 27.000, Pasos: 975, Mean Reward Calculado: 0.027692 (Recompensa/Pasos)\n",
            " 1442907/2000000: episode: 1891, duration: 44.435s, episode steps: 975, steps per second:  22, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.017441, mae: 2.096157, mean_q: 2.530392, mean_eps: 0.100000\n",
            "📈 Episodio 1892: Recompensa total (clipped): 24.000, Pasos: 946, Mean Reward Calculado: 0.025370 (Recompensa/Pasos)\n",
            " 1443853/2000000: episode: 1892, duration: 43.358s, episode steps: 946, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015021, mae: 2.093438, mean_q: 2.526536, mean_eps: 0.100000\n",
            "📈 Episodio 1893: Recompensa total (clipped): 15.000, Pasos: 641, Mean Reward Calculado: 0.023401 (Recompensa/Pasos)\n",
            " 1444494/2000000: episode: 1893, duration: 29.397s, episode steps: 641, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.016198, mae: 2.072089, mean_q: 2.501427, mean_eps: 0.100000\n",
            "📈 Episodio 1894: Recompensa total (clipped): 10.000, Pasos: 615, Mean Reward Calculado: 0.016260 (Recompensa/Pasos)\n",
            " 1445109/2000000: episode: 1894, duration: 28.042s, episode steps: 615, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.015459, mae: 2.091445, mean_q: 2.523813, mean_eps: 0.100000\n",
            "📈 Episodio 1895: Recompensa total (clipped): 10.000, Pasos: 587, Mean Reward Calculado: 0.017036 (Recompensa/Pasos)\n",
            " 1445696/2000000: episode: 1895, duration: 27.008s, episode steps: 587, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.015330, mae: 2.086094, mean_q: 2.515789, mean_eps: 0.100000\n",
            "📈 Episodio 1896: Recompensa total (clipped): 24.000, Pasos: 1014, Mean Reward Calculado: 0.023669 (Recompensa/Pasos)\n",
            " 1446710/2000000: episode: 1896, duration: 46.254s, episode steps: 1014, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.016440, mae: 2.090469, mean_q: 2.523064, mean_eps: 0.100000\n",
            "📈 Episodio 1897: Recompensa total (clipped): 14.000, Pasos: 652, Mean Reward Calculado: 0.021472 (Recompensa/Pasos)\n",
            " 1447362/2000000: episode: 1897, duration: 29.808s, episode steps: 652, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.016504, mae: 2.100501, mean_q: 2.536426, mean_eps: 0.100000\n",
            "📈 Episodio 1898: Recompensa total (clipped): 7.000, Pasos: 380, Mean Reward Calculado: 0.018421 (Recompensa/Pasos)\n",
            " 1447742/2000000: episode: 1898, duration: 17.203s, episode steps: 380, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.476 [0.000, 5.000],  loss: 0.016460, mae: 2.090648, mean_q: 2.527590, mean_eps: 0.100000\n",
            "📈 Episodio 1899: Recompensa total (clipped): 15.000, Pasos: 525, Mean Reward Calculado: 0.028571 (Recompensa/Pasos)\n",
            " 1448267/2000000: episode: 1899, duration: 23.935s, episode steps: 525, steps per second:  22, episode reward: 15.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.017520, mae: 2.089221, mean_q: 2.521638, mean_eps: 0.100000\n",
            "📈 Episodio 1900: Recompensa total (clipped): 10.000, Pasos: 662, Mean Reward Calculado: 0.015106 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1900, pasos: 1448929)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.49 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1900 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 10.00\n",
            "   Media últimos 100: 16.49 / 20.0\n",
            "   Mejor promedio histórico: 16.49\n",
            "   Estado: 📈 82.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1448929/2000000: episode: 1900, duration: 94.490s, episode steps: 662, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.016784, mae: 2.079273, mean_q: 2.510784, mean_eps: 0.100000\n",
            "📈 Episodio 1901: Recompensa total (clipped): 14.000, Pasos: 691, Mean Reward Calculado: 0.020260 (Recompensa/Pasos)\n",
            " 1449620/2000000: episode: 1901, duration: 31.769s, episode steps: 691, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.015257, mae: 2.086770, mean_q: 2.520038, mean_eps: 0.100000\n",
            "📈 Episodio 1902: Recompensa total (clipped): 29.000, Pasos: 1389, Mean Reward Calculado: 0.020878 (Recompensa/Pasos)\n",
            " 1451009/2000000: episode: 1902, duration: 63.490s, episode steps: 1389, steps per second:  22, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.015511, mae: 2.112586, mean_q: 2.551367, mean_eps: 0.100000\n",
            "📈 Episodio 1903: Recompensa total (clipped): 16.000, Pasos: 802, Mean Reward Calculado: 0.019950 (Recompensa/Pasos)\n",
            " 1451811/2000000: episode: 1903, duration: 36.559s, episode steps: 802, steps per second:  22, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.016385, mae: 2.083219, mean_q: 2.516442, mean_eps: 0.100000\n",
            "📈 Episodio 1904: Recompensa total (clipped): 10.000, Pasos: 531, Mean Reward Calculado: 0.018832 (Recompensa/Pasos)\n",
            " 1452342/2000000: episode: 1904, duration: 24.527s, episode steps: 531, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.016445, mae: 2.111926, mean_q: 2.548836, mean_eps: 0.100000\n",
            "📈 Episodio 1905: Recompensa total (clipped): 25.000, Pasos: 1134, Mean Reward Calculado: 0.022046 (Recompensa/Pasos)\n",
            " 1453476/2000000: episode: 1905, duration: 52.221s, episode steps: 1134, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.015526, mae: 2.087457, mean_q: 2.521683, mean_eps: 0.100000\n",
            "📈 Episodio 1906: Recompensa total (clipped): 28.000, Pasos: 852, Mean Reward Calculado: 0.032864 (Recompensa/Pasos)\n",
            " 1454328/2000000: episode: 1906, duration: 39.357s, episode steps: 852, steps per second:  22, episode reward: 28.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.016034, mae: 2.105775, mean_q: 2.542759, mean_eps: 0.100000\n",
            "📈 Episodio 1907: Recompensa total (clipped): 21.000, Pasos: 1089, Mean Reward Calculado: 0.019284 (Recompensa/Pasos)\n",
            " 1455417/2000000: episode: 1907, duration: 49.294s, episode steps: 1089, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.017006, mae: 2.089167, mean_q: 2.521623, mean_eps: 0.100000\n",
            "📈 Episodio 1908: Recompensa total (clipped): 11.000, Pasos: 664, Mean Reward Calculado: 0.016566 (Recompensa/Pasos)\n",
            " 1456081/2000000: episode: 1908, duration: 30.655s, episode steps: 664, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.015358, mae: 2.089785, mean_q: 2.522168, mean_eps: 0.100000\n",
            "📈 Episodio 1909: Recompensa total (clipped): 18.000, Pasos: 648, Mean Reward Calculado: 0.027778 (Recompensa/Pasos)\n",
            " 1456729/2000000: episode: 1909, duration: 29.908s, episode steps: 648, steps per second:  22, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.015856, mae: 2.102023, mean_q: 2.537446, mean_eps: 0.100000\n",
            "📈 Episodio 1910: Recompensa total (clipped): 27.000, Pasos: 1285, Mean Reward Calculado: 0.021012 (Recompensa/Pasos)\n",
            " 1458014/2000000: episode: 1910, duration: 58.644s, episode steps: 1285, steps per second:  22, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.015990, mae: 2.100121, mean_q: 2.535859, mean_eps: 0.100000\n",
            "📈 Episodio 1911: Recompensa total (clipped): 15.000, Pasos: 844, Mean Reward Calculado: 0.017773 (Recompensa/Pasos)\n",
            " 1458858/2000000: episode: 1911, duration: 39.106s, episode steps: 844, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.016689, mae: 2.085961, mean_q: 2.518496, mean_eps: 0.100000\n",
            "📈 Episodio 1912: Recompensa total (clipped): 12.000, Pasos: 755, Mean Reward Calculado: 0.015894 (Recompensa/Pasos)\n",
            " 1459613/2000000: episode: 1912, duration: 34.695s, episode steps: 755, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.016605, mae: 2.098007, mean_q: 2.533835, mean_eps: 0.100000\n",
            "📊 Paso 1,460,000/2,000,000 (73.0%) - 25.7 pasos/seg - ETA: 5.8h - Memoria: 15272.57 MB\n",
            "📈 Episodio 1913: Recompensa total (clipped): 19.000, Pasos: 706, Mean Reward Calculado: 0.026912 (Recompensa/Pasos)\n",
            " 1460319/2000000: episode: 1913, duration: 32.232s, episode steps: 706, steps per second:  22, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.017975, mae: 2.083049, mean_q: 2.511864, mean_eps: 0.100000\n",
            "📈 Episodio 1914: Recompensa total (clipped): 15.000, Pasos: 571, Mean Reward Calculado: 0.026270 (Recompensa/Pasos)\n",
            " 1460890/2000000: episode: 1914, duration: 26.481s, episode steps: 571, steps per second:  22, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.016869, mae: 2.117264, mean_q: 2.557116, mean_eps: 0.100000\n",
            "📈 Episodio 1915: Recompensa total (clipped): 15.000, Pasos: 630, Mean Reward Calculado: 0.023810 (Recompensa/Pasos)\n",
            " 1461520/2000000: episode: 1915, duration: 29.152s, episode steps: 630, steps per second:  22, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.016327, mae: 2.112586, mean_q: 2.550226, mean_eps: 0.100000\n",
            "📈 Episodio 1916: Recompensa total (clipped): 13.000, Pasos: 688, Mean Reward Calculado: 0.018895 (Recompensa/Pasos)\n",
            " 1462208/2000000: episode: 1916, duration: 31.519s, episode steps: 688, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.017193, mae: 2.118900, mean_q: 2.557657, mean_eps: 0.100000\n",
            "📈 Episodio 1917: Recompensa total (clipped): 16.000, Pasos: 655, Mean Reward Calculado: 0.024427 (Recompensa/Pasos)\n",
            " 1462863/2000000: episode: 1917, duration: 29.724s, episode steps: 655, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.018373, mae: 2.112850, mean_q: 2.549501, mean_eps: 0.100000\n",
            "📈 Episodio 1918: Recompensa total (clipped): 22.000, Pasos: 949, Mean Reward Calculado: 0.023182 (Recompensa/Pasos)\n",
            " 1463812/2000000: episode: 1918, duration: 43.366s, episode steps: 949, steps per second:  22, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.017526, mae: 2.115695, mean_q: 2.553273, mean_eps: 0.100000\n",
            "📈 Episodio 1919: Recompensa total (clipped): 20.000, Pasos: 845, Mean Reward Calculado: 0.023669 (Recompensa/Pasos)\n",
            " 1464657/2000000: episode: 1919, duration: 38.405s, episode steps: 845, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.016476, mae: 2.116765, mean_q: 2.553080, mean_eps: 0.100000\n",
            "📈 Episodio 1920: Recompensa total (clipped): 17.000, Pasos: 610, Mean Reward Calculado: 0.027869 (Recompensa/Pasos)\n",
            " 1465267/2000000: episode: 1920, duration: 28.079s, episode steps: 610, steps per second:  22, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.017058, mae: 2.116600, mean_q: 2.554477, mean_eps: 0.100000\n",
            "📈 Episodio 1921: Recompensa total (clipped): 12.000, Pasos: 653, Mean Reward Calculado: 0.018377 (Recompensa/Pasos)\n",
            " 1465920/2000000: episode: 1921, duration: 29.877s, episode steps: 653, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.156 [0.000, 5.000],  loss: 0.016577, mae: 2.117677, mean_q: 2.555738, mean_eps: 0.100000\n",
            "📈 Episodio 1922: Recompensa total (clipped): 33.000, Pasos: 1391, Mean Reward Calculado: 0.023724 (Recompensa/Pasos)\n",
            " 1467311/2000000: episode: 1922, duration: 63.793s, episode steps: 1391, steps per second:  22, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.146 [0.000, 5.000],  loss: 0.016868, mae: 2.117242, mean_q: 2.556115, mean_eps: 0.100000\n",
            "📈 Episodio 1923: Recompensa total (clipped): 17.000, Pasos: 561, Mean Reward Calculado: 0.030303 (Recompensa/Pasos)\n",
            " 1467872/2000000: episode: 1923, duration: 25.651s, episode steps: 561, steps per second:  22, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.016167, mae: 2.112611, mean_q: 2.550739, mean_eps: 0.100000\n",
            "📈 Episodio 1924: Recompensa total (clipped): 25.000, Pasos: 1162, Mean Reward Calculado: 0.021515 (Recompensa/Pasos)\n",
            " 1469034/2000000: episode: 1924, duration: 52.718s, episode steps: 1162, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015653, mae: 2.114374, mean_q: 2.553712, mean_eps: 0.100000\n",
            "📈 Episodio 1925: Recompensa total (clipped): 19.000, Pasos: 1009, Mean Reward Calculado: 0.018831 (Recompensa/Pasos)\n",
            " 1470043/2000000: episode: 1925, duration: 45.597s, episode steps: 1009, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.015891, mae: 2.137946, mean_q: 2.581515, mean_eps: 0.100000\n",
            "📈 Episodio 1926: Recompensa total (clipped): 31.000, Pasos: 1138, Mean Reward Calculado: 0.027241 (Recompensa/Pasos)\n",
            " 1471181/2000000: episode: 1926, duration: 51.756s, episode steps: 1138, steps per second:  22, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015327, mae: 2.115199, mean_q: 2.553850, mean_eps: 0.100000\n",
            "📈 Episodio 1927: Recompensa total (clipped): 23.000, Pasos: 1003, Mean Reward Calculado: 0.022931 (Recompensa/Pasos)\n",
            " 1472184/2000000: episode: 1927, duration: 45.319s, episode steps: 1003, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.016149, mae: 2.125752, mean_q: 2.564818, mean_eps: 0.100000\n",
            "📈 Episodio 1928: Recompensa total (clipped): 11.000, Pasos: 609, Mean Reward Calculado: 0.018062 (Recompensa/Pasos)\n",
            " 1472793/2000000: episode: 1928, duration: 28.153s, episode steps: 609, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.015428, mae: 2.109360, mean_q: 2.544791, mean_eps: 0.100000\n",
            "📈 Episodio 1929: Recompensa total (clipped): 12.000, Pasos: 688, Mean Reward Calculado: 0.017442 (Recompensa/Pasos)\n",
            " 1473481/2000000: episode: 1929, duration: 31.632s, episode steps: 688, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.016664, mae: 2.120859, mean_q: 2.559137, mean_eps: 0.100000\n",
            "📈 Episodio 1930: Recompensa total (clipped): 21.000, Pasos: 821, Mean Reward Calculado: 0.025579 (Recompensa/Pasos)\n",
            " 1474302/2000000: episode: 1930, duration: 37.239s, episode steps: 821, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.017275, mae: 2.119846, mean_q: 2.557949, mean_eps: 0.100000\n",
            "📈 Episodio 1931: Recompensa total (clipped): 15.000, Pasos: 671, Mean Reward Calculado: 0.022355 (Recompensa/Pasos)\n",
            " 1474973/2000000: episode: 1931, duration: 30.579s, episode steps: 671, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.319 [0.000, 5.000],  loss: 0.014800, mae: 2.103191, mean_q: 2.539358, mean_eps: 0.100000\n",
            "📈 Episodio 1932: Recompensa total (clipped): 8.000, Pasos: 383, Mean Reward Calculado: 0.020888 (Recompensa/Pasos)\n",
            " 1475356/2000000: episode: 1932, duration: 17.840s, episode steps: 383, steps per second:  21, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.017565, mae: 2.106652, mean_q: 2.544790, mean_eps: 0.100000\n",
            "📈 Episodio 1933: Recompensa total (clipped): 33.000, Pasos: 1490, Mean Reward Calculado: 0.022148 (Recompensa/Pasos)\n",
            " 1476846/2000000: episode: 1933, duration: 68.481s, episode steps: 1490, steps per second:  22, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.017649, mae: 2.122271, mean_q: 2.562156, mean_eps: 0.100000\n",
            "📈 Episodio 1934: Recompensa total (clipped): 16.000, Pasos: 656, Mean Reward Calculado: 0.024390 (Recompensa/Pasos)\n",
            " 1477502/2000000: episode: 1934, duration: 29.718s, episode steps: 656, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.017745, mae: 2.119639, mean_q: 2.558641, mean_eps: 0.100000\n",
            "📈 Episodio 1935: Recompensa total (clipped): 23.000, Pasos: 949, Mean Reward Calculado: 0.024236 (Recompensa/Pasos)\n",
            " 1478451/2000000: episode: 1935, duration: 42.614s, episode steps: 949, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.017671, mae: 2.111585, mean_q: 2.547922, mean_eps: 0.100000\n",
            "📈 Episodio 1936: Recompensa total (clipped): 15.000, Pasos: 731, Mean Reward Calculado: 0.020520 (Recompensa/Pasos)\n",
            " 1479182/2000000: episode: 1936, duration: 33.087s, episode steps: 731, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.016646, mae: 2.108348, mean_q: 2.545361, mean_eps: 0.100000\n",
            "📈 Episodio 1937: Recompensa total (clipped): 12.000, Pasos: 464, Mean Reward Calculado: 0.025862 (Recompensa/Pasos)\n",
            " 1479646/2000000: episode: 1937, duration: 20.853s, episode steps: 464, steps per second:  22, episode reward: 12.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.377 [0.000, 5.000],  loss: 0.015071, mae: 2.103440, mean_q: 2.543247, mean_eps: 0.100000\n",
            "📊 Paso 1,480,000/2,000,000 (74.0%) - 25.6 pasos/seg - ETA: 5.6h - Memoria: 15262.29 MB\n",
            "📈 Episodio 1938: Recompensa total (clipped): 19.000, Pasos: 781, Mean Reward Calculado: 0.024328 (Recompensa/Pasos)\n",
            " 1480427/2000000: episode: 1938, duration: 35.623s, episode steps: 781, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015055, mae: 2.124656, mean_q: 2.563841, mean_eps: 0.100000\n",
            "📈 Episodio 1939: Recompensa total (clipped): 25.000, Pasos: 1151, Mean Reward Calculado: 0.021720 (Recompensa/Pasos)\n",
            " 1481578/2000000: episode: 1939, duration: 53.093s, episode steps: 1151, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.015133, mae: 2.126724, mean_q: 2.569319, mean_eps: 0.100000\n",
            "📈 Episodio 1940: Recompensa total (clipped): 35.000, Pasos: 1380, Mean Reward Calculado: 0.025362 (Recompensa/Pasos)\n",
            " 1482958/2000000: episode: 1940, duration: 62.699s, episode steps: 1380, steps per second:  22, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.017224, mae: 2.134494, mean_q: 2.576105, mean_eps: 0.100000\n",
            "📈 Episodio 1941: Recompensa total (clipped): 18.000, Pasos: 742, Mean Reward Calculado: 0.024259 (Recompensa/Pasos)\n",
            " 1483700/2000000: episode: 1941, duration: 34.072s, episode steps: 742, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.015719, mae: 2.129581, mean_q: 2.569603, mean_eps: 0.100000\n",
            "📈 Episodio 1942: Recompensa total (clipped): 24.000, Pasos: 924, Mean Reward Calculado: 0.025974 (Recompensa/Pasos)\n",
            " 1484624/2000000: episode: 1942, duration: 42.413s, episode steps: 924, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.017989, mae: 2.126519, mean_q: 2.564550, mean_eps: 0.100000\n",
            "📈 Episodio 1943: Recompensa total (clipped): 18.000, Pasos: 880, Mean Reward Calculado: 0.020455 (Recompensa/Pasos)\n",
            " 1485504/2000000: episode: 1943, duration: 40.526s, episode steps: 880, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.017990, mae: 2.115732, mean_q: 2.551723, mean_eps: 0.100000\n",
            "📈 Episodio 1944: Recompensa total (clipped): 11.000, Pasos: 497, Mean Reward Calculado: 0.022133 (Recompensa/Pasos)\n",
            " 1486001/2000000: episode: 1944, duration: 22.660s, episode steps: 497, steps per second:  22, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.153 [0.000, 5.000],  loss: 0.014286, mae: 2.135705, mean_q: 2.579140, mean_eps: 0.100000\n",
            "📈 Episodio 1945: Recompensa total (clipped): 18.000, Pasos: 746, Mean Reward Calculado: 0.024129 (Recompensa/Pasos)\n",
            " 1486747/2000000: episode: 1945, duration: 33.500s, episode steps: 746, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.017333, mae: 2.133348, mean_q: 2.572336, mean_eps: 0.100000\n",
            "📈 Episodio 1946: Recompensa total (clipped): 21.000, Pasos: 891, Mean Reward Calculado: 0.023569 (Recompensa/Pasos)\n",
            " 1487638/2000000: episode: 1946, duration: 40.337s, episode steps: 891, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.016479, mae: 2.125000, mean_q: 2.562944, mean_eps: 0.100000\n",
            "📈 Episodio 1947: Recompensa total (clipped): 11.000, Pasos: 545, Mean Reward Calculado: 0.020183 (Recompensa/Pasos)\n",
            " 1488183/2000000: episode: 1947, duration: 25.130s, episode steps: 545, steps per second:  22, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 0.016218, mae: 2.131128, mean_q: 2.572415, mean_eps: 0.100000\n",
            "📈 Episodio 1948: Recompensa total (clipped): 15.000, Pasos: 640, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            " 1488823/2000000: episode: 1948, duration: 29.776s, episode steps: 640, steps per second:  21, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.077 [0.000, 5.000],  loss: 0.016917, mae: 2.133080, mean_q: 2.572518, mean_eps: 0.100000\n",
            "📈 Episodio 1949: Recompensa total (clipped): 16.000, Pasos: 657, Mean Reward Calculado: 0.024353 (Recompensa/Pasos)\n",
            " 1489480/2000000: episode: 1949, duration: 29.989s, episode steps: 657, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.014758, mae: 2.119352, mean_q: 2.555094, mean_eps: 0.100000\n",
            "📈 Episodio 1950: Recompensa total (clipped): 15.000, Pasos: 915, Mean Reward Calculado: 0.016393 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 1950, pasos: 1490395)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.25 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 1950 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 15.00\n",
            "   Media últimos 100: 17.25 / 20.0\n",
            "   Mejor promedio histórico: 17.25\n",
            "   Estado: 📈 86.2% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1490395/2000000: episode: 1950, duration: 117.171s, episode steps: 915, steps per second:   8, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016333, mae: 2.120818, mean_q: 2.558630, mean_eps: 0.100000\n",
            "📈 Episodio 1951: Recompensa total (clipped): 8.000, Pasos: 566, Mean Reward Calculado: 0.014134 (Recompensa/Pasos)\n",
            " 1490961/2000000: episode: 1951, duration: 26.350s, episode steps: 566, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.147 [0.000, 5.000],  loss: 0.015450, mae: 2.121073, mean_q: 2.561139, mean_eps: 0.100000\n",
            "📈 Episodio 1952: Recompensa total (clipped): 15.000, Pasos: 660, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            " 1491621/2000000: episode: 1952, duration: 30.311s, episode steps: 660, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.016937, mae: 2.142866, mean_q: 2.584341, mean_eps: 0.100000\n",
            "📈 Episodio 1953: Recompensa total (clipped): 13.000, Pasos: 594, Mean Reward Calculado: 0.021886 (Recompensa/Pasos)\n",
            " 1492215/2000000: episode: 1953, duration: 26.760s, episode steps: 594, steps per second:  22, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.017609, mae: 2.120909, mean_q: 2.559784, mean_eps: 0.100000\n",
            "📈 Episodio 1954: Recompensa total (clipped): 10.000, Pasos: 564, Mean Reward Calculado: 0.017730 (Recompensa/Pasos)\n",
            " 1492779/2000000: episode: 1954, duration: 26.208s, episode steps: 564, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.863 [0.000, 5.000],  loss: 0.017385, mae: 2.122685, mean_q: 2.562332, mean_eps: 0.100000\n",
            "📈 Episodio 1955: Recompensa total (clipped): 15.000, Pasos: 585, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            " 1493364/2000000: episode: 1955, duration: 27.106s, episode steps: 585, steps per second:  22, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.016926, mae: 2.134275, mean_q: 2.576475, mean_eps: 0.100000\n",
            "📈 Episodio 1956: Recompensa total (clipped): 18.000, Pasos: 805, Mean Reward Calculado: 0.022360 (Recompensa/Pasos)\n",
            " 1494169/2000000: episode: 1956, duration: 36.420s, episode steps: 805, steps per second:  22, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.016553, mae: 2.113690, mean_q: 2.549566, mean_eps: 0.100000\n",
            "📈 Episodio 1957: Recompensa total (clipped): 15.000, Pasos: 909, Mean Reward Calculado: 0.016502 (Recompensa/Pasos)\n",
            " 1495078/2000000: episode: 1957, duration: 41.862s, episode steps: 909, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.015674, mae: 2.123542, mean_q: 2.563241, mean_eps: 0.100000\n",
            "📈 Episodio 1958: Recompensa total (clipped): 24.000, Pasos: 813, Mean Reward Calculado: 0.029520 (Recompensa/Pasos)\n",
            " 1495891/2000000: episode: 1958, duration: 36.809s, episode steps: 813, steps per second:  22, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.015861, mae: 2.127741, mean_q: 2.567054, mean_eps: 0.100000\n",
            "📈 Episodio 1959: Recompensa total (clipped): 29.000, Pasos: 1022, Mean Reward Calculado: 0.028376 (Recompensa/Pasos)\n",
            " 1496913/2000000: episode: 1959, duration: 47.113s, episode steps: 1022, steps per second:  22, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.017713, mae: 2.125752, mean_q: 2.561682, mean_eps: 0.100000\n",
            "📈 Episodio 1960: Recompensa total (clipped): 23.000, Pasos: 833, Mean Reward Calculado: 0.027611 (Recompensa/Pasos)\n",
            " 1497746/2000000: episode: 1960, duration: 38.271s, episode steps: 833, steps per second:  22, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.015467, mae: 2.109933, mean_q: 2.545318, mean_eps: 0.100000\n",
            "📈 Episodio 1961: Recompensa total (clipped): 32.000, Pasos: 1386, Mean Reward Calculado: 0.023088 (Recompensa/Pasos)\n",
            " 1499132/2000000: episode: 1961, duration: 64.334s, episode steps: 1386, steps per second:  22, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.017445, mae: 2.118363, mean_q: 2.555162, mean_eps: 0.100000\n",
            "📈 Episodio 1962: Recompensa total (clipped): 15.000, Pasos: 583, Mean Reward Calculado: 0.025729 (Recompensa/Pasos)\n",
            " 1499715/2000000: episode: 1962, duration: 26.588s, episode steps: 583, steps per second:  22, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.017994, mae: 2.116199, mean_q: 2.555333, mean_eps: 0.100000\n",
            "📊 Paso 1,500,000/2,000,000 (75.0%) - 25.5 pasos/seg - ETA: 5.4h - Memoria: 15278.56 MB\n",
            "📈 Episodio 1963: Recompensa total (clipped): 15.000, Pasos: 676, Mean Reward Calculado: 0.022189 (Recompensa/Pasos)\n",
            " 1500391/2000000: episode: 1963, duration: 30.750s, episode steps: 676, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.016342, mae: 2.132653, mean_q: 2.573431, mean_eps: 0.100000\n",
            "📈 Episodio 1964: Recompensa total (clipped): 11.000, Pasos: 728, Mean Reward Calculado: 0.015110 (Recompensa/Pasos)\n",
            " 1501119/2000000: episode: 1964, duration: 32.961s, episode steps: 728, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.014748, mae: 2.132503, mean_q: 2.574942, mean_eps: 0.100000\n",
            "📈 Episodio 1965: Recompensa total (clipped): 7.000, Pasos: 392, Mean Reward Calculado: 0.017857 (Recompensa/Pasos)\n",
            " 1501511/2000000: episode: 1965, duration: 17.920s, episode steps: 392, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.015872, mae: 2.131205, mean_q: 2.573802, mean_eps: 0.100000\n",
            "📈 Episodio 1966: Recompensa total (clipped): 17.000, Pasos: 682, Mean Reward Calculado: 0.024927 (Recompensa/Pasos)\n",
            " 1502193/2000000: episode: 1966, duration: 31.239s, episode steps: 682, steps per second:  22, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.017247, mae: 2.149937, mean_q: 2.592343, mean_eps: 0.100000\n",
            "📈 Episodio 1967: Recompensa total (clipped): 26.000, Pasos: 1010, Mean Reward Calculado: 0.025743 (Recompensa/Pasos)\n",
            " 1503203/2000000: episode: 1967, duration: 46.142s, episode steps: 1010, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.015693, mae: 2.150059, mean_q: 2.594803, mean_eps: 0.100000\n",
            "📈 Episodio 1968: Recompensa total (clipped): 27.000, Pasos: 1037, Mean Reward Calculado: 0.026037 (Recompensa/Pasos)\n",
            " 1504240/2000000: episode: 1968, duration: 48.305s, episode steps: 1037, steps per second:  21, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016247, mae: 2.148376, mean_q: 2.590614, mean_eps: 0.100000\n",
            "📈 Episodio 1969: Recompensa total (clipped): 20.000, Pasos: 730, Mean Reward Calculado: 0.027397 (Recompensa/Pasos)\n",
            " 1504970/2000000: episode: 1969, duration: 33.627s, episode steps: 730, steps per second:  22, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.018873, mae: 2.140001, mean_q: 2.583194, mean_eps: 0.100000\n",
            "📈 Episodio 1970: Recompensa total (clipped): 19.000, Pasos: 907, Mean Reward Calculado: 0.020948 (Recompensa/Pasos)\n",
            " 1505877/2000000: episode: 1970, duration: 40.903s, episode steps: 907, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.017041, mae: 2.135878, mean_q: 2.577538, mean_eps: 0.100000\n",
            "📈 Episodio 1971: Recompensa total (clipped): 18.000, Pasos: 1012, Mean Reward Calculado: 0.017787 (Recompensa/Pasos)\n",
            " 1506889/2000000: episode: 1971, duration: 46.057s, episode steps: 1012, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.016490, mae: 2.132044, mean_q: 2.572331, mean_eps: 0.100000\n",
            "📈 Episodio 1972: Recompensa total (clipped): 24.000, Pasos: 963, Mean Reward Calculado: 0.024922 (Recompensa/Pasos)\n",
            " 1507852/2000000: episode: 1972, duration: 43.476s, episode steps: 963, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.016922, mae: 2.149538, mean_q: 2.592339, mean_eps: 0.100000\n",
            "📈 Episodio 1973: Recompensa total (clipped): 20.000, Pasos: 795, Mean Reward Calculado: 0.025157 (Recompensa/Pasos)\n",
            " 1508647/2000000: episode: 1973, duration: 36.177s, episode steps: 795, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.016576, mae: 2.147436, mean_q: 2.591449, mean_eps: 0.100000\n",
            "📈 Episodio 1974: Recompensa total (clipped): 14.000, Pasos: 780, Mean Reward Calculado: 0.017949 (Recompensa/Pasos)\n",
            " 1509427/2000000: episode: 1974, duration: 35.698s, episode steps: 780, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.017058, mae: 2.149196, mean_q: 2.590741, mean_eps: 0.100000\n",
            "📈 Episodio 1975: Recompensa total (clipped): 32.000, Pasos: 1258, Mean Reward Calculado: 0.025437 (Recompensa/Pasos)\n",
            " 1510685/2000000: episode: 1975, duration: 57.116s, episode steps: 1258, steps per second:  22, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.015749, mae: 2.156937, mean_q: 2.601517, mean_eps: 0.100000\n",
            "📈 Episodio 1976: Recompensa total (clipped): 10.000, Pasos: 606, Mean Reward Calculado: 0.016502 (Recompensa/Pasos)\n",
            " 1511291/2000000: episode: 1976, duration: 27.693s, episode steps: 606, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.018248, mae: 2.165489, mean_q: 2.611154, mean_eps: 0.100000\n",
            "📈 Episodio 1977: Recompensa total (clipped): 16.000, Pasos: 891, Mean Reward Calculado: 0.017957 (Recompensa/Pasos)\n",
            " 1512182/2000000: episode: 1977, duration: 40.472s, episode steps: 891, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.016097, mae: 2.161158, mean_q: 2.606297, mean_eps: 0.100000\n",
            "📈 Episodio 1978: Recompensa total (clipped): 19.000, Pasos: 980, Mean Reward Calculado: 0.019388 (Recompensa/Pasos)\n",
            " 1513162/2000000: episode: 1978, duration: 44.441s, episode steps: 980, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.018628, mae: 2.170732, mean_q: 2.617032, mean_eps: 0.100000\n",
            "📈 Episodio 1979: Recompensa total (clipped): 18.000, Pasos: 668, Mean Reward Calculado: 0.026946 (Recompensa/Pasos)\n",
            " 1513830/2000000: episode: 1979, duration: 30.704s, episode steps: 668, steps per second:  22, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.017750, mae: 2.161205, mean_q: 2.607891, mean_eps: 0.100000\n",
            "📈 Episodio 1980: Recompensa total (clipped): 7.000, Pasos: 373, Mean Reward Calculado: 0.018767 (Recompensa/Pasos)\n",
            " 1514203/2000000: episode: 1980, duration: 17.114s, episode steps: 373, steps per second:  22, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.016803, mae: 2.166346, mean_q: 2.616131, mean_eps: 0.100000\n",
            "📈 Episodio 1981: Recompensa total (clipped): 8.000, Pasos: 419, Mean Reward Calculado: 0.019093 (Recompensa/Pasos)\n",
            " 1514622/2000000: episode: 1981, duration: 19.296s, episode steps: 419, steps per second:  22, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.026 [0.000, 5.000],  loss: 0.016360, mae: 2.155439, mean_q: 2.600935, mean_eps: 0.100000\n",
            "📈 Episodio 1982: Recompensa total (clipped): 12.000, Pasos: 524, Mean Reward Calculado: 0.022901 (Recompensa/Pasos)\n",
            " 1515146/2000000: episode: 1982, duration: 23.982s, episode steps: 524, steps per second:  22, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.016715, mae: 2.172030, mean_q: 2.621782, mean_eps: 0.100000\n",
            "📈 Episodio 1983: Recompensa total (clipped): 19.000, Pasos: 827, Mean Reward Calculado: 0.022975 (Recompensa/Pasos)\n",
            " 1515973/2000000: episode: 1983, duration: 38.396s, episode steps: 827, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.017037, mae: 2.174523, mean_q: 2.623436, mean_eps: 0.100000\n",
            "📈 Episodio 1984: Recompensa total (clipped): 25.000, Pasos: 1052, Mean Reward Calculado: 0.023764 (Recompensa/Pasos)\n",
            " 1517025/2000000: episode: 1984, duration: 47.989s, episode steps: 1052, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.016018, mae: 2.152713, mean_q: 2.596345, mean_eps: 0.100000\n",
            "📈 Episodio 1985: Recompensa total (clipped): 20.000, Pasos: 822, Mean Reward Calculado: 0.024331 (Recompensa/Pasos)\n",
            " 1517847/2000000: episode: 1985, duration: 37.579s, episode steps: 822, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.016172, mae: 2.169030, mean_q: 2.616123, mean_eps: 0.100000\n",
            "📈 Episodio 1986: Recompensa total (clipped): 14.000, Pasos: 629, Mean Reward Calculado: 0.022258 (Recompensa/Pasos)\n",
            " 1518476/2000000: episode: 1986, duration: 28.999s, episode steps: 629, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.017427, mae: 2.188539, mean_q: 2.640790, mean_eps: 0.100000\n",
            "📈 Episodio 1987: Recompensa total (clipped): 15.000, Pasos: 620, Mean Reward Calculado: 0.024194 (Recompensa/Pasos)\n",
            " 1519096/2000000: episode: 1987, duration: 28.692s, episode steps: 620, steps per second:  22, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.018042, mae: 2.171437, mean_q: 2.618605, mean_eps: 0.100000\n",
            "📊 Paso 1,520,000/2,000,000 (76.0%) - 25.4 pasos/seg - ETA: 5.2h - Memoria: 15269.50 MB\n",
            "📈 Episodio 1988: Recompensa total (clipped): 18.000, Pasos: 943, Mean Reward Calculado: 0.019088 (Recompensa/Pasos)\n",
            " 1520039/2000000: episode: 1988, duration: 43.200s, episode steps: 943, steps per second:  22, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.017785, mae: 2.146635, mean_q: 2.591058, mean_eps: 0.100000\n",
            "📈 Episodio 1989: Recompensa total (clipped): 18.000, Pasos: 1012, Mean Reward Calculado: 0.017787 (Recompensa/Pasos)\n",
            " 1521051/2000000: episode: 1989, duration: 46.280s, episode steps: 1012, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.017565, mae: 2.213265, mean_q: 2.673253, mean_eps: 0.100000\n",
            "📈 Episodio 1990: Recompensa total (clipped): 33.000, Pasos: 1497, Mean Reward Calculado: 0.022044 (Recompensa/Pasos)\n",
            " 1522548/2000000: episode: 1990, duration: 69.050s, episode steps: 1497, steps per second:  22, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.017268, mae: 2.204484, mean_q: 2.659096, mean_eps: 0.100000\n",
            "📈 Episodio 1991: Recompensa total (clipped): 11.000, Pasos: 574, Mean Reward Calculado: 0.019164 (Recompensa/Pasos)\n",
            " 1523122/2000000: episode: 1991, duration: 26.580s, episode steps: 574, steps per second:  22, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.016048, mae: 2.197101, mean_q: 2.653543, mean_eps: 0.100000\n",
            "📈 Episodio 1992: Recompensa total (clipped): 19.000, Pasos: 1040, Mean Reward Calculado: 0.018269 (Recompensa/Pasos)\n",
            " 1524162/2000000: episode: 1992, duration: 47.358s, episode steps: 1040, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.018936, mae: 2.204787, mean_q: 2.660941, mean_eps: 0.100000\n",
            "📈 Episodio 1993: Recompensa total (clipped): 27.000, Pasos: 1005, Mean Reward Calculado: 0.026866 (Recompensa/Pasos)\n",
            " 1525167/2000000: episode: 1993, duration: 45.384s, episode steps: 1005, steps per second:  22, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.018277, mae: 2.186847, mean_q: 2.639961, mean_eps: 0.100000\n",
            "📈 Episodio 1994: Recompensa total (clipped): 16.000, Pasos: 743, Mean Reward Calculado: 0.021534 (Recompensa/Pasos)\n",
            " 1525910/2000000: episode: 1994, duration: 34.036s, episode steps: 743, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.018473, mae: 2.195987, mean_q: 2.648335, mean_eps: 0.100000\n",
            "📈 Episodio 1995: Recompensa total (clipped): 19.000, Pasos: 770, Mean Reward Calculado: 0.024675 (Recompensa/Pasos)\n",
            " 1526680/2000000: episode: 1995, duration: 35.086s, episode steps: 770, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.017716, mae: 2.210344, mean_q: 2.667163, mean_eps: 0.100000\n",
            "📈 Episodio 1996: Recompensa total (clipped): 27.000, Pasos: 1083, Mean Reward Calculado: 0.024931 (Recompensa/Pasos)\n",
            " 1527763/2000000: episode: 1996, duration: 49.093s, episode steps: 1083, steps per second:  22, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.017705, mae: 2.208715, mean_q: 2.664337, mean_eps: 0.100000\n",
            "📈 Episodio 1997: Recompensa total (clipped): 17.000, Pasos: 625, Mean Reward Calculado: 0.027200 (Recompensa/Pasos)\n",
            " 1528388/2000000: episode: 1997, duration: 28.542s, episode steps: 625, steps per second:  22, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.019021, mae: 2.234062, mean_q: 2.694420, mean_eps: 0.100000\n",
            "📈 Episodio 1998: Recompensa total (clipped): 7.000, Pasos: 508, Mean Reward Calculado: 0.013780 (Recompensa/Pasos)\n",
            " 1528896/2000000: episode: 1998, duration: 23.363s, episode steps: 508, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.017481, mae: 2.211012, mean_q: 2.670116, mean_eps: 0.100000\n",
            "📈 Episodio 1999: Recompensa total (clipped): 10.000, Pasos: 429, Mean Reward Calculado: 0.023310 (Recompensa/Pasos)\n",
            " 1529325/2000000: episode: 1999, duration: 20.006s, episode steps: 429, steps per second:  21, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.016724, mae: 2.186254, mean_q: 2.639890, mean_eps: 0.100000\n",
            "📈 Episodio 2000: Recompensa total (clipped): 20.000, Pasos: 778, Mean Reward Calculado: 0.025707 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2000, pasos: 1530103)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.27 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2000 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 20.00\n",
            "   Media últimos 100: 18.27 / 20.0\n",
            "   Mejor promedio histórico: 18.27\n",
            "   Estado: 📈 91.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            "💾 Guardado modelo principal lastest: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_model.h5\n",
            "💾 Guardado modelo target lastest: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_target.h5\n",
            "💾 Memoria lastest guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_lastest_memory.pkl\n",
            "💾 Checkpoint lastest guardado (ep: 2000, pasos: 1530103)\n",
            "✅ Checkpoint guardado para episodio 2000\n",
            " 1530103/2000000: episode: 2000, duration: 318.057s, episode steps: 778, steps per second:   2, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.018248, mae: 2.204958, mean_q: 2.659953, mean_eps: 0.100000\n",
            "📈 Episodio 2001: Recompensa total (clipped): 23.000, Pasos: 806, Mean Reward Calculado: 0.028536 (Recompensa/Pasos)\n",
            " 1530909/2000000: episode: 2001, duration: 37.415s, episode steps: 806, steps per second:  22, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.017123, mae: 2.203928, mean_q: 2.659289, mean_eps: 0.100000\n",
            "📈 Episodio 2002: Recompensa total (clipped): 24.000, Pasos: 998, Mean Reward Calculado: 0.024048 (Recompensa/Pasos)\n",
            " 1531907/2000000: episode: 2002, duration: 45.936s, episode steps: 998, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.015 [0.000, 5.000],  loss: 0.017041, mae: 2.181980, mean_q: 2.632975, mean_eps: 0.100000\n",
            "📈 Episodio 2003: Recompensa total (clipped): 31.000, Pasos: 1260, Mean Reward Calculado: 0.024603 (Recompensa/Pasos)\n",
            " 1533167/2000000: episode: 2003, duration: 57.192s, episode steps: 1260, steps per second:  22, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.016199, mae: 2.204328, mean_q: 2.659401, mean_eps: 0.100000\n",
            "📈 Episodio 2004: Recompensa total (clipped): 15.000, Pasos: 782, Mean Reward Calculado: 0.019182 (Recompensa/Pasos)\n",
            " 1533949/2000000: episode: 2004, duration: 35.843s, episode steps: 782, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.017539, mae: 2.205710, mean_q: 2.660704, mean_eps: 0.100000\n",
            "📈 Episodio 2005: Recompensa total (clipped): 8.000, Pasos: 513, Mean Reward Calculado: 0.015595 (Recompensa/Pasos)\n",
            " 1534462/2000000: episode: 2005, duration: 23.416s, episode steps: 513, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.019207, mae: 2.199863, mean_q: 2.651175, mean_eps: 0.100000\n",
            "📈 Episodio 2006: Recompensa total (clipped): 11.000, Pasos: 656, Mean Reward Calculado: 0.016768 (Recompensa/Pasos)\n",
            " 1535118/2000000: episode: 2006, duration: 29.833s, episode steps: 656, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.171 [0.000, 5.000],  loss: 0.017561, mae: 2.179096, mean_q: 2.628511, mean_eps: 0.100000\n",
            "📈 Episodio 2007: Recompensa total (clipped): 16.000, Pasos: 663, Mean Reward Calculado: 0.024133 (Recompensa/Pasos)\n",
            " 1535781/2000000: episode: 2007, duration: 30.508s, episode steps: 663, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.017475, mae: 2.202311, mean_q: 2.658582, mean_eps: 0.100000\n",
            "📈 Episodio 2008: Recompensa total (clipped): 17.000, Pasos: 566, Mean Reward Calculado: 0.030035 (Recompensa/Pasos)\n",
            " 1536347/2000000: episode: 2008, duration: 25.812s, episode steps: 566, steps per second:  22, episode reward: 17.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.014733, mae: 2.190760, mean_q: 2.643107, mean_eps: 0.100000\n",
            "📈 Episodio 2009: Recompensa total (clipped): 29.000, Pasos: 961, Mean Reward Calculado: 0.030177 (Recompensa/Pasos)\n",
            " 1537308/2000000: episode: 2009, duration: 44.277s, episode steps: 961, steps per second:  22, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.019012, mae: 2.183481, mean_q: 2.633782, mean_eps: 0.100000\n",
            "📈 Episodio 2010: Recompensa total (clipped): 21.000, Pasos: 859, Mean Reward Calculado: 0.024447 (Recompensa/Pasos)\n",
            " 1538167/2000000: episode: 2010, duration: 39.352s, episode steps: 859, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.018395, mae: 2.207018, mean_q: 2.661494, mean_eps: 0.100000\n",
            "📈 Episodio 2011: Recompensa total (clipped): 23.000, Pasos: 1090, Mean Reward Calculado: 0.021101 (Recompensa/Pasos)\n",
            " 1539257/2000000: episode: 2011, duration: 49.309s, episode steps: 1090, steps per second:  22, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.016144, mae: 2.189681, mean_q: 2.642245, mean_eps: 0.100000\n",
            "📊 Paso 1,540,000/2,000,000 (77.0%) - 25.3 pasos/seg - ETA: 5.1h - Memoria: 15262.72 MB\n",
            "📈 Episodio 2012: Recompensa total (clipped): 20.000, Pasos: 916, Mean Reward Calculado: 0.021834 (Recompensa/Pasos)\n",
            " 1540173/2000000: episode: 2012, duration: 42.624s, episode steps: 916, steps per second:  21, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.017710, mae: 2.191559, mean_q: 2.642797, mean_eps: 0.100000\n",
            "📈 Episodio 2013: Recompensa total (clipped): 15.000, Pasos: 585, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            " 1540758/2000000: episode: 2013, duration: 27.064s, episode steps: 585, steps per second:  22, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.016048, mae: 2.211957, mean_q: 2.665341, mean_eps: 0.100000\n",
            "📈 Episodio 2014: Recompensa total (clipped): 13.000, Pasos: 706, Mean Reward Calculado: 0.018414 (Recompensa/Pasos)\n",
            " 1541464/2000000: episode: 2014, duration: 32.971s, episode steps: 706, steps per second:  21, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.095 [0.000, 5.000],  loss: 0.015813, mae: 2.199136, mean_q: 2.650814, mean_eps: 0.100000\n",
            "📈 Episodio 2015: Recompensa total (clipped): 17.000, Pasos: 834, Mean Reward Calculado: 0.020384 (Recompensa/Pasos)\n",
            " 1542298/2000000: episode: 2015, duration: 38.397s, episode steps: 834, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.016941, mae: 2.177710, mean_q: 2.624461, mean_eps: 0.100000\n",
            "📈 Episodio 2016: Recompensa total (clipped): 22.000, Pasos: 1094, Mean Reward Calculado: 0.020110 (Recompensa/Pasos)\n",
            " 1543392/2000000: episode: 2016, duration: 50.057s, episode steps: 1094, steps per second:  22, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.017574, mae: 2.187881, mean_q: 2.639847, mean_eps: 0.100000\n",
            "📈 Episodio 2017: Recompensa total (clipped): 20.000, Pasos: 932, Mean Reward Calculado: 0.021459 (Recompensa/Pasos)\n",
            " 1544324/2000000: episode: 2017, duration: 42.845s, episode steps: 932, steps per second:  22, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.017843, mae: 2.197913, mean_q: 2.651938, mean_eps: 0.100000\n",
            "📈 Episodio 2018: Recompensa total (clipped): 26.000, Pasos: 1156, Mean Reward Calculado: 0.022491 (Recompensa/Pasos)\n",
            " 1545480/2000000: episode: 2018, duration: 53.489s, episode steps: 1156, steps per second:  22, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.077 [0.000, 5.000],  loss: 0.015430, mae: 2.192610, mean_q: 2.644876, mean_eps: 0.100000\n",
            "📈 Episodio 2019: Recompensa total (clipped): 25.000, Pasos: 1151, Mean Reward Calculado: 0.021720 (Recompensa/Pasos)\n",
            " 1546631/2000000: episode: 2019, duration: 53.330s, episode steps: 1151, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.018136, mae: 2.179553, mean_q: 2.630729, mean_eps: 0.100000\n",
            "📈 Episodio 2020: Recompensa total (clipped): 13.000, Pasos: 623, Mean Reward Calculado: 0.020867 (Recompensa/Pasos)\n",
            " 1547254/2000000: episode: 2020, duration: 28.463s, episode steps: 623, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.016168, mae: 2.188194, mean_q: 2.641428, mean_eps: 0.100000\n",
            "📈 Episodio 2021: Recompensa total (clipped): 22.000, Pasos: 1076, Mean Reward Calculado: 0.020446 (Recompensa/Pasos)\n",
            " 1548330/2000000: episode: 2021, duration: 49.485s, episode steps: 1076, steps per second:  22, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.018267, mae: 2.188789, mean_q: 2.638904, mean_eps: 0.100000\n",
            "📈 Episodio 2022: Recompensa total (clipped): 26.000, Pasos: 956, Mean Reward Calculado: 0.027197 (Recompensa/Pasos)\n",
            " 1549286/2000000: episode: 2022, duration: 43.347s, episode steps: 956, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.016226, mae: 2.201131, mean_q: 2.655324, mean_eps: 0.100000\n",
            "📈 Episodio 2023: Recompensa total (clipped): 12.000, Pasos: 581, Mean Reward Calculado: 0.020654 (Recompensa/Pasos)\n",
            " 1549867/2000000: episode: 2023, duration: 26.241s, episode steps: 581, steps per second:  22, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.448 [0.000, 5.000],  loss: 0.016629, mae: 2.187447, mean_q: 2.637067, mean_eps: 0.100000\n",
            "📈 Episodio 2024: Recompensa total (clipped): 12.000, Pasos: 875, Mean Reward Calculado: 0.013714 (Recompensa/Pasos)\n",
            " 1550742/2000000: episode: 2024, duration: 39.905s, episode steps: 875, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.017619, mae: 2.204211, mean_q: 2.659637, mean_eps: 0.100000\n",
            "📈 Episodio 2025: Recompensa total (clipped): 21.000, Pasos: 1100, Mean Reward Calculado: 0.019091 (Recompensa/Pasos)\n",
            " 1551842/2000000: episode: 2025, duration: 50.455s, episode steps: 1100, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.015511, mae: 2.191094, mean_q: 2.646268, mean_eps: 0.100000\n",
            "📈 Episodio 2026: Recompensa total (clipped): 13.000, Pasos: 701, Mean Reward Calculado: 0.018545 (Recompensa/Pasos)\n",
            " 1552543/2000000: episode: 2026, duration: 31.899s, episode steps: 701, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.015996, mae: 2.215960, mean_q: 2.671498, mean_eps: 0.100000\n",
            "📈 Episodio 2027: Recompensa total (clipped): 9.000, Pasos: 629, Mean Reward Calculado: 0.014308 (Recompensa/Pasos)\n",
            " 1553172/2000000: episode: 2027, duration: 28.591s, episode steps: 629, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.015932, mae: 2.189129, mean_q: 2.639731, mean_eps: 0.100000\n",
            "📈 Episodio 2028: Recompensa total (clipped): 22.000, Pasos: 841, Mean Reward Calculado: 0.026159 (Recompensa/Pasos)\n",
            " 1554013/2000000: episode: 2028, duration: 38.363s, episode steps: 841, steps per second:  22, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.016558, mae: 2.202357, mean_q: 2.656390, mean_eps: 0.100000\n",
            "📈 Episodio 2029: Recompensa total (clipped): 22.000, Pasos: 1028, Mean Reward Calculado: 0.021401 (Recompensa/Pasos)\n",
            " 1555041/2000000: episode: 2029, duration: 46.761s, episode steps: 1028, steps per second:  22, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.015967, mae: 2.204561, mean_q: 2.658020, mean_eps: 0.100000\n",
            "📈 Episodio 2030: Recompensa total (clipped): 14.000, Pasos: 749, Mean Reward Calculado: 0.018692 (Recompensa/Pasos)\n",
            " 1555790/2000000: episode: 2030, duration: 33.784s, episode steps: 749, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.018694, mae: 2.190711, mean_q: 2.641128, mean_eps: 0.100000\n",
            "📈 Episodio 2031: Recompensa total (clipped): 33.000, Pasos: 1329, Mean Reward Calculado: 0.024831 (Recompensa/Pasos)\n",
            " 1557119/2000000: episode: 2031, duration: 60.415s, episode steps: 1329, steps per second:  22, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.017954, mae: 2.198445, mean_q: 2.652573, mean_eps: 0.100000\n",
            "📈 Episodio 2032: Recompensa total (clipped): 7.000, Pasos: 386, Mean Reward Calculado: 0.018135 (Recompensa/Pasos)\n",
            " 1557505/2000000: episode: 2032, duration: 17.573s, episode steps: 386, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.017183, mae: 2.191072, mean_q: 2.642119, mean_eps: 0.100000\n",
            "📈 Episodio 2033: Recompensa total (clipped): 26.000, Pasos: 954, Mean Reward Calculado: 0.027254 (Recompensa/Pasos)\n",
            " 1558459/2000000: episode: 2033, duration: 43.559s, episode steps: 954, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.018382, mae: 2.200517, mean_q: 2.652970, mean_eps: 0.100000\n",
            "📈 Episodio 2034: Recompensa total (clipped): 20.000, Pasos: 895, Mean Reward Calculado: 0.022346 (Recompensa/Pasos)\n",
            " 1559354/2000000: episode: 2034, duration: 40.735s, episode steps: 895, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015670, mae: 2.200113, mean_q: 2.652736, mean_eps: 0.100000\n",
            "📈 Episodio 2035: Recompensa total (clipped): 7.000, Pasos: 504, Mean Reward Calculado: 0.013889 (Recompensa/Pasos)\n",
            " 1559858/2000000: episode: 2035, duration: 22.825s, episode steps: 504, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.016175, mae: 2.205436, mean_q: 2.658722, mean_eps: 0.100000\n",
            "📊 Paso 1,560,000/2,000,000 (78.0%) - 25.2 pasos/seg - ETA: 4.8h - Memoria: 15277.05 MB\n",
            "📈 Episodio 2036: Recompensa total (clipped): 13.000, Pasos: 661, Mean Reward Calculado: 0.019667 (Recompensa/Pasos)\n",
            " 1560519/2000000: episode: 2036, duration: 30.243s, episode steps: 661, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.016275, mae: 2.200644, mean_q: 2.654449, mean_eps: 0.100000\n",
            "📈 Episodio 2037: Recompensa total (clipped): 15.000, Pasos: 640, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            " 1561159/2000000: episode: 2037, duration: 29.579s, episode steps: 640, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.802 [0.000, 5.000],  loss: 0.016330, mae: 2.185311, mean_q: 2.637679, mean_eps: 0.100000\n",
            "📈 Episodio 2038: Recompensa total (clipped): 32.000, Pasos: 1497, Mean Reward Calculado: 0.021376 (Recompensa/Pasos)\n",
            " 1562656/2000000: episode: 2038, duration: 68.958s, episode steps: 1497, steps per second:  22, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.017640, mae: 2.167852, mean_q: 2.617966, mean_eps: 0.100000\n",
            "📈 Episodio 2039: Recompensa total (clipped): 13.000, Pasos: 621, Mean Reward Calculado: 0.020934 (Recompensa/Pasos)\n",
            " 1563277/2000000: episode: 2039, duration: 28.555s, episode steps: 621, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.017871, mae: 2.177748, mean_q: 2.626194, mean_eps: 0.100000\n",
            "📈 Episodio 2040: Recompensa total (clipped): 8.000, Pasos: 534, Mean Reward Calculado: 0.014981 (Recompensa/Pasos)\n",
            " 1563811/2000000: episode: 2040, duration: 24.377s, episode steps: 534, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.016738, mae: 2.164425, mean_q: 2.610732, mean_eps: 0.100000\n",
            "📈 Episodio 2041: Recompensa total (clipped): 14.000, Pasos: 578, Mean Reward Calculado: 0.024221 (Recompensa/Pasos)\n",
            " 1564389/2000000: episode: 2041, duration: 26.374s, episode steps: 578, steps per second:  22, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.017492, mae: 2.192019, mean_q: 2.643251, mean_eps: 0.100000\n",
            "📈 Episodio 2042: Recompensa total (clipped): 19.000, Pasos: 791, Mean Reward Calculado: 0.024020 (Recompensa/Pasos)\n",
            " 1565180/2000000: episode: 2042, duration: 36.027s, episode steps: 791, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.017037, mae: 2.178332, mean_q: 2.627735, mean_eps: 0.100000\n",
            "📈 Episodio 2043: Recompensa total (clipped): 18.000, Pasos: 775, Mean Reward Calculado: 0.023226 (Recompensa/Pasos)\n",
            " 1565955/2000000: episode: 2043, duration: 35.622s, episode steps: 775, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.015266, mae: 2.165440, mean_q: 2.612556, mean_eps: 0.100000\n",
            "📈 Episodio 2044: Recompensa total (clipped): 14.000, Pasos: 543, Mean Reward Calculado: 0.025783 (Recompensa/Pasos)\n",
            " 1566498/2000000: episode: 2044, duration: 24.601s, episode steps: 543, steps per second:  22, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.016213, mae: 2.184835, mean_q: 2.635001, mean_eps: 0.100000\n",
            "📈 Episodio 2045: Recompensa total (clipped): 5.000, Pasos: 554, Mean Reward Calculado: 0.009025 (Recompensa/Pasos)\n",
            " 1567052/2000000: episode: 2045, duration: 25.659s, episode steps: 554, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.016732, mae: 2.176512, mean_q: 2.626726, mean_eps: 0.100000\n",
            "📈 Episodio 2046: Recompensa total (clipped): 16.000, Pasos: 1022, Mean Reward Calculado: 0.015656 (Recompensa/Pasos)\n",
            " 1568074/2000000: episode: 2046, duration: 47.213s, episode steps: 1022, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.018426, mae: 2.163277, mean_q: 2.607642, mean_eps: 0.100000\n",
            "📈 Episodio 2047: Recompensa total (clipped): 22.000, Pasos: 868, Mean Reward Calculado: 0.025346 (Recompensa/Pasos)\n",
            " 1568942/2000000: episode: 2047, duration: 39.754s, episode steps: 868, steps per second:  22, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.017129, mae: 2.171714, mean_q: 2.618291, mean_eps: 0.100000\n",
            "📈 Episodio 2048: Recompensa total (clipped): 23.000, Pasos: 886, Mean Reward Calculado: 0.025959 (Recompensa/Pasos)\n",
            " 1569828/2000000: episode: 2048, duration: 40.708s, episode steps: 886, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.015182, mae: 2.162060, mean_q: 2.606898, mean_eps: 0.100000\n",
            "📈 Episodio 2049: Recompensa total (clipped): 10.000, Pasos: 533, Mean Reward Calculado: 0.018762 (Recompensa/Pasos)\n",
            " 1570361/2000000: episode: 2049, duration: 24.266s, episode steps: 533, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.015769, mae: 2.194800, mean_q: 2.650672, mean_eps: 0.100000\n",
            "📈 Episodio 2050: Recompensa total (clipped): 7.000, Pasos: 512, Mean Reward Calculado: 0.013672 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2050, pasos: 1570873)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.76 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2050 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 7.00\n",
            "   Media últimos 100: 17.76 / 20.0\n",
            "   Mejor promedio histórico: 17.76\n",
            "   Estado: 📈 88.8% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1570873/2000000: episode: 2050, duration: 81.988s, episode steps: 512, steps per second:   6, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.338 [0.000, 5.000],  loss: 0.016092, mae: 2.206357, mean_q: 2.665189, mean_eps: 0.100000\n",
            "📈 Episodio 2051: Recompensa total (clipped): 10.000, Pasos: 468, Mean Reward Calculado: 0.021368 (Recompensa/Pasos)\n",
            " 1571341/2000000: episode: 2051, duration: 21.447s, episode steps: 468, steps per second:  22, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.018381, mae: 2.211548, mean_q: 2.669414, mean_eps: 0.100000\n",
            "📈 Episodio 2052: Recompensa total (clipped): 13.000, Pasos: 836, Mean Reward Calculado: 0.015550 (Recompensa/Pasos)\n",
            " 1572177/2000000: episode: 2052, duration: 37.838s, episode steps: 836, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.018115, mae: 2.187753, mean_q: 2.637925, mean_eps: 0.100000\n",
            "📈 Episodio 2053: Recompensa total (clipped): 24.000, Pasos: 1005, Mean Reward Calculado: 0.023881 (Recompensa/Pasos)\n",
            " 1573182/2000000: episode: 2053, duration: 47.162s, episode steps: 1005, steps per second:  21, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.016223, mae: 2.175706, mean_q: 2.625191, mean_eps: 0.100000\n",
            "📈 Episodio 2054: Recompensa total (clipped): 10.000, Pasos: 528, Mean Reward Calculado: 0.018939 (Recompensa/Pasos)\n",
            " 1573710/2000000: episode: 2054, duration: 24.230s, episode steps: 528, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.015322, mae: 2.180902, mean_q: 2.631415, mean_eps: 0.100000\n",
            "📈 Episodio 2055: Recompensa total (clipped): 27.000, Pasos: 932, Mean Reward Calculado: 0.028970 (Recompensa/Pasos)\n",
            " 1574642/2000000: episode: 2055, duration: 43.147s, episode steps: 932, steps per second:  22, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.975 [0.000, 5.000],  loss: 0.016903, mae: 2.175452, mean_q: 2.624730, mean_eps: 0.100000\n",
            "📈 Episodio 2056: Recompensa total (clipped): 17.000, Pasos: 731, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            " 1575373/2000000: episode: 2056, duration: 33.058s, episode steps: 731, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.016319, mae: 2.174970, mean_q: 2.623724, mean_eps: 0.100000\n",
            "📈 Episodio 2057: Recompensa total (clipped): 23.000, Pasos: 1217, Mean Reward Calculado: 0.018899 (Recompensa/Pasos)\n",
            " 1576590/2000000: episode: 2057, duration: 55.847s, episode steps: 1217, steps per second:  22, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.017221, mae: 2.183481, mean_q: 2.637464, mean_eps: 0.100000\n",
            "📈 Episodio 2058: Recompensa total (clipped): 18.000, Pasos: 673, Mean Reward Calculado: 0.026746 (Recompensa/Pasos)\n",
            " 1577263/2000000: episode: 2058, duration: 30.921s, episode steps: 673, steps per second:  22, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.017577, mae: 2.195676, mean_q: 2.647893, mean_eps: 0.100000\n",
            "📈 Episodio 2059: Recompensa total (clipped): 13.000, Pasos: 700, Mean Reward Calculado: 0.018571 (Recompensa/Pasos)\n",
            " 1577963/2000000: episode: 2059, duration: 31.964s, episode steps: 700, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.017404, mae: 2.187160, mean_q: 2.638939, mean_eps: 0.100000\n",
            "📈 Episodio 2060: Recompensa total (clipped): 19.000, Pasos: 788, Mean Reward Calculado: 0.024112 (Recompensa/Pasos)\n",
            " 1578751/2000000: episode: 2060, duration: 36.561s, episode steps: 788, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.017038, mae: 2.175303, mean_q: 2.624137, mean_eps: 0.100000\n",
            "📈 Episodio 2061: Recompensa total (clipped): 21.000, Pasos: 881, Mean Reward Calculado: 0.023837 (Recompensa/Pasos)\n",
            " 1579632/2000000: episode: 2061, duration: 40.444s, episode steps: 881, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.016829, mae: 2.173302, mean_q: 2.623250, mean_eps: 0.100000\n",
            "📊 Paso 1,580,000/2,000,000 (79.0%) - 25.2 pasos/seg - ETA: 4.6h - Memoria: 15241.41 MB\n",
            "📈 Episodio 2062: Recompensa total (clipped): 15.000, Pasos: 674, Mean Reward Calculado: 0.022255 (Recompensa/Pasos)\n",
            " 1580306/2000000: episode: 2062, duration: 31.015s, episode steps: 674, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.016728, mae: 2.188188, mean_q: 2.641878, mean_eps: 0.100000\n",
            "📈 Episodio 2063: Recompensa total (clipped): 17.000, Pasos: 632, Mean Reward Calculado: 0.026899 (Recompensa/Pasos)\n",
            " 1580938/2000000: episode: 2063, duration: 28.637s, episode steps: 632, steps per second:  22, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.016603, mae: 2.197534, mean_q: 2.654013, mean_eps: 0.100000\n",
            "📈 Episodio 2064: Recompensa total (clipped): 24.000, Pasos: 968, Mean Reward Calculado: 0.024793 (Recompensa/Pasos)\n",
            " 1581906/2000000: episode: 2064, duration: 44.024s, episode steps: 968, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.018146, mae: 2.216468, mean_q: 2.676213, mean_eps: 0.100000\n",
            "📈 Episodio 2065: Recompensa total (clipped): 13.000, Pasos: 543, Mean Reward Calculado: 0.023941 (Recompensa/Pasos)\n",
            " 1582449/2000000: episode: 2065, duration: 24.374s, episode steps: 543, steps per second:  22, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.018023, mae: 2.194766, mean_q: 2.649655, mean_eps: 0.100000\n",
            "📈 Episodio 2066: Recompensa total (clipped): 20.000, Pasos: 768, Mean Reward Calculado: 0.026042 (Recompensa/Pasos)\n",
            " 1583217/2000000: episode: 2066, duration: 35.055s, episode steps: 768, steps per second:  22, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.017789, mae: 2.223281, mean_q: 2.683510, mean_eps: 0.100000\n",
            "📈 Episodio 2067: Recompensa total (clipped): 11.000, Pasos: 533, Mean Reward Calculado: 0.020638 (Recompensa/Pasos)\n",
            " 1583750/2000000: episode: 2067, duration: 24.153s, episode steps: 533, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.017491, mae: 2.202050, mean_q: 2.658283, mean_eps: 0.100000\n",
            "📈 Episodio 2068: Recompensa total (clipped): 8.000, Pasos: 633, Mean Reward Calculado: 0.012638 (Recompensa/Pasos)\n",
            " 1584383/2000000: episode: 2068, duration: 28.771s, episode steps: 633, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.016131, mae: 2.208294, mean_q: 2.665049, mean_eps: 0.100000\n",
            "📈 Episodio 2069: Recompensa total (clipped): 19.000, Pasos: 861, Mean Reward Calculado: 0.022067 (Recompensa/Pasos)\n",
            " 1585244/2000000: episode: 2069, duration: 39.366s, episode steps: 861, steps per second:  22, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.017019, mae: 2.207139, mean_q: 2.664934, mean_eps: 0.100000\n",
            "📈 Episodio 2070: Recompensa total (clipped): 24.000, Pasos: 1103, Mean Reward Calculado: 0.021759 (Recompensa/Pasos)\n",
            " 1586347/2000000: episode: 2070, duration: 50.203s, episode steps: 1103, steps per second:  22, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.180 [0.000, 5.000],  loss: 0.017264, mae: 2.204701, mean_q: 2.662708, mean_eps: 0.100000\n",
            "📈 Episodio 2071: Recompensa total (clipped): 20.000, Pasos: 806, Mean Reward Calculado: 0.024814 (Recompensa/Pasos)\n",
            " 1587153/2000000: episode: 2071, duration: 36.473s, episode steps: 806, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.016432, mae: 2.205662, mean_q: 2.662662, mean_eps: 0.100000\n",
            "📈 Episodio 2072: Recompensa total (clipped): 21.000, Pasos: 902, Mean Reward Calculado: 0.023282 (Recompensa/Pasos)\n",
            " 1588055/2000000: episode: 2072, duration: 40.595s, episode steps: 902, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.016589, mae: 2.213952, mean_q: 2.669745, mean_eps: 0.100000\n",
            "📈 Episodio 2073: Recompensa total (clipped): 25.000, Pasos: 910, Mean Reward Calculado: 0.027473 (Recompensa/Pasos)\n",
            " 1588965/2000000: episode: 2073, duration: 41.340s, episode steps: 910, steps per second:  22, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.015295, mae: 2.201980, mean_q: 2.657227, mean_eps: 0.100000\n",
            "📈 Episodio 2074: Recompensa total (clipped): 17.000, Pasos: 949, Mean Reward Calculado: 0.017914 (Recompensa/Pasos)\n",
            " 1589914/2000000: episode: 2074, duration: 43.543s, episode steps: 949, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.016793, mae: 2.205948, mean_q: 2.661663, mean_eps: 0.100000\n",
            "📈 Episodio 2075: Recompensa total (clipped): 23.000, Pasos: 953, Mean Reward Calculado: 0.024134 (Recompensa/Pasos)\n",
            " 1590867/2000000: episode: 2075, duration: 43.880s, episode steps: 953, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.016187, mae: 2.239312, mean_q: 2.701414, mean_eps: 0.100000\n",
            "📈 Episodio 2076: Recompensa total (clipped): 36.000, Pasos: 1284, Mean Reward Calculado: 0.028037 (Recompensa/Pasos)\n",
            " 1592151/2000000: episode: 2076, duration: 59.533s, episode steps: 1284, steps per second:  22, episode reward: 36.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.018452, mae: 2.237966, mean_q: 2.700151, mean_eps: 0.100000\n",
            "📈 Episodio 2077: Recompensa total (clipped): 13.000, Pasos: 556, Mean Reward Calculado: 0.023381 (Recompensa/Pasos)\n",
            " 1592707/2000000: episode: 2077, duration: 25.628s, episode steps: 556, steps per second:  22, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.016449, mae: 2.243790, mean_q: 2.707986, mean_eps: 0.100000\n",
            "📈 Episodio 2078: Recompensa total (clipped): 14.000, Pasos: 846, Mean Reward Calculado: 0.016548 (Recompensa/Pasos)\n",
            " 1593553/2000000: episode: 2078, duration: 38.925s, episode steps: 846, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.017497, mae: 2.239956, mean_q: 2.702007, mean_eps: 0.100000\n",
            "📈 Episodio 2079: Recompensa total (clipped): 31.000, Pasos: 1089, Mean Reward Calculado: 0.028466 (Recompensa/Pasos)\n",
            " 1594642/2000000: episode: 2079, duration: 49.746s, episode steps: 1089, steps per second:  22, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.016063, mae: 2.230804, mean_q: 2.692913, mean_eps: 0.100000\n",
            "📈 Episodio 2080: Recompensa total (clipped): 11.000, Pasos: 546, Mean Reward Calculado: 0.020147 (Recompensa/Pasos)\n",
            " 1595188/2000000: episode: 2080, duration: 25.154s, episode steps: 546, steps per second:  22, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.016978, mae: 2.240893, mean_q: 2.705829, mean_eps: 0.100000\n",
            "📈 Episodio 2081: Recompensa total (clipped): 24.000, Pasos: 914, Mean Reward Calculado: 0.026258 (Recompensa/Pasos)\n",
            " 1596102/2000000: episode: 2081, duration: 41.629s, episode steps: 914, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.015705, mae: 2.245844, mean_q: 2.707707, mean_eps: 0.100000\n",
            "📈 Episodio 2082: Recompensa total (clipped): 6.000, Pasos: 581, Mean Reward Calculado: 0.010327 (Recompensa/Pasos)\n",
            " 1596683/2000000: episode: 2082, duration: 26.436s, episode steps: 581, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.017671, mae: 2.232540, mean_q: 2.692550, mean_eps: 0.100000\n",
            "📈 Episodio 2083: Recompensa total (clipped): 9.000, Pasos: 535, Mean Reward Calculado: 0.016822 (Recompensa/Pasos)\n",
            " 1597218/2000000: episode: 2083, duration: 24.836s, episode steps: 535, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.017168, mae: 2.243532, mean_q: 2.707233, mean_eps: 0.100000\n",
            "📈 Episodio 2084: Recompensa total (clipped): 25.000, Pasos: 920, Mean Reward Calculado: 0.027174 (Recompensa/Pasos)\n",
            " 1598138/2000000: episode: 2084, duration: 41.786s, episode steps: 920, steps per second:  22, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016681, mae: 2.236818, mean_q: 2.697301, mean_eps: 0.100000\n",
            "📈 Episodio 2085: Recompensa total (clipped): 29.000, Pasos: 945, Mean Reward Calculado: 0.030688 (Recompensa/Pasos)\n",
            " 1599083/2000000: episode: 2085, duration: 42.834s, episode steps: 945, steps per second:  22, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.018149, mae: 2.232692, mean_q: 2.693481, mean_eps: 0.100000\n",
            "📈 Episodio 2086: Recompensa total (clipped): 16.000, Pasos: 740, Mean Reward Calculado: 0.021622 (Recompensa/Pasos)\n",
            " 1599823/2000000: episode: 2086, duration: 33.581s, episode steps: 740, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.017727, mae: 2.238367, mean_q: 2.700211, mean_eps: 0.100000\n",
            "📊 Paso 1,600,000/2,000,000 (80.0%) - 25.1 pasos/seg - ETA: 4.4h - Memoria: 15282.64 MB\n",
            "📈 Episodio 2087: Recompensa total (clipped): 29.000, Pasos: 1167, Mean Reward Calculado: 0.024850 (Recompensa/Pasos)\n",
            " 1600990/2000000: episode: 2087, duration: 53.810s, episode steps: 1167, steps per second:  22, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.016430, mae: 2.240956, mean_q: 2.704144, mean_eps: 0.100000\n",
            "📈 Episodio 2088: Recompensa total (clipped): 15.000, Pasos: 744, Mean Reward Calculado: 0.020161 (Recompensa/Pasos)\n",
            " 1601734/2000000: episode: 2088, duration: 33.861s, episode steps: 744, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.014609, mae: 2.245779, mean_q: 2.710277, mean_eps: 0.100000\n",
            "📈 Episodio 2089: Recompensa total (clipped): 18.000, Pasos: 664, Mean Reward Calculado: 0.027108 (Recompensa/Pasos)\n",
            " 1602398/2000000: episode: 2089, duration: 30.187s, episode steps: 664, steps per second:  22, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015922, mae: 2.236683, mean_q: 2.699629, mean_eps: 0.100000\n",
            "📈 Episodio 2090: Recompensa total (clipped): 25.000, Pasos: 779, Mean Reward Calculado: 0.032092 (Recompensa/Pasos)\n",
            " 1603177/2000000: episode: 2090, duration: 35.450s, episode steps: 779, steps per second:  22, episode reward: 25.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.018101, mae: 2.218628, mean_q: 2.676708, mean_eps: 0.100000\n",
            "📈 Episodio 2091: Recompensa total (clipped): 10.000, Pasos: 514, Mean Reward Calculado: 0.019455 (Recompensa/Pasos)\n",
            " 1603691/2000000: episode: 2091, duration: 23.219s, episode steps: 514, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.625 [0.000, 5.000],  loss: 0.017830, mae: 2.234819, mean_q: 2.695146, mean_eps: 0.100000\n",
            "📈 Episodio 2092: Recompensa total (clipped): 24.000, Pasos: 915, Mean Reward Calculado: 0.026230 (Recompensa/Pasos)\n",
            " 1604606/2000000: episode: 2092, duration: 41.507s, episode steps: 915, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.016586, mae: 2.247270, mean_q: 2.713106, mean_eps: 0.100000\n",
            "📈 Episodio 2093: Recompensa total (clipped): 26.000, Pasos: 976, Mean Reward Calculado: 0.026639 (Recompensa/Pasos)\n",
            " 1605582/2000000: episode: 2093, duration: 44.010s, episode steps: 976, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.016553, mae: 2.252368, mean_q: 2.716073, mean_eps: 0.100000\n",
            "📈 Episodio 2094: Recompensa total (clipped): 25.000, Pasos: 1052, Mean Reward Calculado: 0.023764 (Recompensa/Pasos)\n",
            " 1606634/2000000: episode: 2094, duration: 47.589s, episode steps: 1052, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.015936, mae: 2.234833, mean_q: 2.696999, mean_eps: 0.100000\n",
            "📈 Episodio 2095: Recompensa total (clipped): 26.000, Pasos: 983, Mean Reward Calculado: 0.026450 (Recompensa/Pasos)\n",
            " 1607617/2000000: episode: 2095, duration: 45.460s, episode steps: 983, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.017360, mae: 2.257794, mean_q: 2.722479, mean_eps: 0.100000\n",
            "📈 Episodio 2096: Recompensa total (clipped): 2.000, Pasos: 407, Mean Reward Calculado: 0.004914 (Recompensa/Pasos)\n",
            " 1608024/2000000: episode: 2096, duration: 18.675s, episode steps: 407, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.017285, mae: 2.257156, mean_q: 2.723622, mean_eps: 0.100000\n",
            "📈 Episodio 2097: Recompensa total (clipped): 14.000, Pasos: 676, Mean Reward Calculado: 0.020710 (Recompensa/Pasos)\n",
            " 1608700/2000000: episode: 2097, duration: 30.959s, episode steps: 676, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.020079, mae: 2.240973, mean_q: 2.702815, mean_eps: 0.100000\n",
            "📈 Episodio 2098: Recompensa total (clipped): 12.000, Pasos: 510, Mean Reward Calculado: 0.023529 (Recompensa/Pasos)\n",
            " 1609210/2000000: episode: 2098, duration: 23.444s, episode steps: 510, steps per second:  22, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.019488, mae: 2.246546, mean_q: 2.711989, mean_eps: 0.100000\n",
            "📈 Episodio 2099: Recompensa total (clipped): 14.000, Pasos: 523, Mean Reward Calculado: 0.026769 (Recompensa/Pasos)\n",
            " 1609733/2000000: episode: 2099, duration: 24.095s, episode steps: 523, steps per second:  22, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.017190, mae: 2.248461, mean_q: 2.711998, mean_eps: 0.100000\n",
            "📈 Episodio 2100: Recompensa total (clipped): 17.000, Pasos: 761, Mean Reward Calculado: 0.022339 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2100, pasos: 1610494)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.07 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2100 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 17.00\n",
            "   Media últimos 100: 18.07 / 20.0\n",
            "   Mejor promedio histórico: 18.07\n",
            "   Estado: 📈 90.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1610494/2000000: episode: 2100, duration: 101.235s, episode steps: 761, steps per second:   8, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.016374, mae: 2.265431, mean_q: 2.733858, mean_eps: 0.100000\n",
            "📈 Episodio 2101: Recompensa total (clipped): 25.000, Pasos: 1094, Mean Reward Calculado: 0.022852 (Recompensa/Pasos)\n",
            " 1611588/2000000: episode: 2101, duration: 50.278s, episode steps: 1094, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.016145, mae: 2.230188, mean_q: 2.686874, mean_eps: 0.100000\n",
            "📈 Episodio 2102: Recompensa total (clipped): 17.000, Pasos: 647, Mean Reward Calculado: 0.026275 (Recompensa/Pasos)\n",
            " 1612235/2000000: episode: 2102, duration: 29.844s, episode steps: 647, steps per second:  22, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.015952, mae: 2.262769, mean_q: 2.728634, mean_eps: 0.100000\n",
            "📈 Episodio 2103: Recompensa total (clipped): 16.000, Pasos: 716, Mean Reward Calculado: 0.022346 (Recompensa/Pasos)\n",
            " 1612951/2000000: episode: 2103, duration: 32.743s, episode steps: 716, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.015578, mae: 2.264420, mean_q: 2.729506, mean_eps: 0.100000\n",
            "📈 Episodio 2104: Recompensa total (clipped): 15.000, Pasos: 833, Mean Reward Calculado: 0.018007 (Recompensa/Pasos)\n",
            " 1613784/2000000: episode: 2104, duration: 37.964s, episode steps: 833, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.017300, mae: 2.261707, mean_q: 2.727860, mean_eps: 0.100000\n",
            "📈 Episodio 2105: Recompensa total (clipped): 13.000, Pasos: 525, Mean Reward Calculado: 0.024762 (Recompensa/Pasos)\n",
            " 1614309/2000000: episode: 2105, duration: 23.937s, episode steps: 525, steps per second:  22, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.017439, mae: 2.235030, mean_q: 2.695777, mean_eps: 0.100000\n",
            "📈 Episodio 2106: Recompensa total (clipped): 19.000, Pasos: 1015, Mean Reward Calculado: 0.018719 (Recompensa/Pasos)\n",
            " 1615324/2000000: episode: 2106, duration: 46.502s, episode steps: 1015, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.016689, mae: 2.241233, mean_q: 2.705993, mean_eps: 0.100000\n",
            "📈 Episodio 2107: Recompensa total (clipped): 12.000, Pasos: 661, Mean Reward Calculado: 0.018154 (Recompensa/Pasos)\n",
            " 1615985/2000000: episode: 2107, duration: 30.319s, episode steps: 661, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.017118, mae: 2.256532, mean_q: 2.720856, mean_eps: 0.100000\n",
            "📈 Episodio 2108: Recompensa total (clipped): 19.000, Pasos: 867, Mean Reward Calculado: 0.021915 (Recompensa/Pasos)\n",
            " 1616852/2000000: episode: 2108, duration: 39.797s, episode steps: 867, steps per second:  22, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.015578, mae: 2.238605, mean_q: 2.701778, mean_eps: 0.100000\n",
            "📈 Episodio 2109: Recompensa total (clipped): 24.000, Pasos: 971, Mean Reward Calculado: 0.024717 (Recompensa/Pasos)\n",
            " 1617823/2000000: episode: 2109, duration: 44.847s, episode steps: 971, steps per second:  22, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.016036, mae: 2.240046, mean_q: 2.700704, mean_eps: 0.100000\n",
            "📈 Episodio 2110: Recompensa total (clipped): 21.000, Pasos: 935, Mean Reward Calculado: 0.022460 (Recompensa/Pasos)\n",
            " 1618758/2000000: episode: 2110, duration: 43.020s, episode steps: 935, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.018184, mae: 2.241004, mean_q: 2.699563, mean_eps: 0.100000\n",
            "📈 Episodio 2111: Recompensa total (clipped): 14.000, Pasos: 700, Mean Reward Calculado: 0.020000 (Recompensa/Pasos)\n",
            " 1619458/2000000: episode: 2111, duration: 32.127s, episode steps: 700, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.016424, mae: 2.250022, mean_q: 2.715064, mean_eps: 0.100000\n",
            "📊 Paso 1,620,000/2,000,000 (81.0%) - 25.0 pasos/seg - ETA: 4.2h - Memoria: 15270.71 MB\n",
            "📈 Episodio 2112: Recompensa total (clipped): 15.000, Pasos: 754, Mean Reward Calculado: 0.019894 (Recompensa/Pasos)\n",
            " 1620212/2000000: episode: 2112, duration: 34.025s, episode steps: 754, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.016720, mae: 2.240536, mean_q: 2.702416, mean_eps: 0.100000\n",
            "📈 Episodio 2113: Recompensa total (clipped): 14.000, Pasos: 667, Mean Reward Calculado: 0.020990 (Recompensa/Pasos)\n",
            " 1620879/2000000: episode: 2113, duration: 30.484s, episode steps: 667, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015610, mae: 2.260814, mean_q: 2.727658, mean_eps: 0.100000\n",
            "📈 Episodio 2114: Recompensa total (clipped): 11.000, Pasos: 434, Mean Reward Calculado: 0.025346 (Recompensa/Pasos)\n",
            " 1621313/2000000: episode: 2114, duration: 19.423s, episode steps: 434, steps per second:  22, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.074 [0.000, 5.000],  loss: 0.015211, mae: 2.240570, mean_q: 2.701672, mean_eps: 0.100000\n",
            "📈 Episodio 2115: Recompensa total (clipped): 16.000, Pasos: 692, Mean Reward Calculado: 0.023121 (Recompensa/Pasos)\n",
            " 1622005/2000000: episode: 2115, duration: 32.173s, episode steps: 692, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.017162, mae: 2.255476, mean_q: 2.719022, mean_eps: 0.100000\n",
            "📈 Episodio 2116: Recompensa total (clipped): 16.000, Pasos: 1010, Mean Reward Calculado: 0.015842 (Recompensa/Pasos)\n",
            " 1623015/2000000: episode: 2116, duration: 46.193s, episode steps: 1010, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.016623, mae: 2.247320, mean_q: 2.711181, mean_eps: 0.100000\n",
            "📈 Episodio 2117: Recompensa total (clipped): 18.000, Pasos: 785, Mean Reward Calculado: 0.022930 (Recompensa/Pasos)\n",
            " 1623800/2000000: episode: 2117, duration: 36.416s, episode steps: 785, steps per second:  22, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.017091, mae: 2.251548, mean_q: 2.715985, mean_eps: 0.100000\n",
            "📈 Episodio 2118: Recompensa total (clipped): 7.000, Pasos: 445, Mean Reward Calculado: 0.015730 (Recompensa/Pasos)\n",
            " 1624245/2000000: episode: 2118, duration: 20.734s, episode steps: 445, steps per second:  21, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.017744, mae: 2.235306, mean_q: 2.697108, mean_eps: 0.100000\n",
            "📈 Episodio 2119: Recompensa total (clipped): 23.000, Pasos: 649, Mean Reward Calculado: 0.035439 (Recompensa/Pasos)\n",
            " 1624894/2000000: episode: 2119, duration: 29.505s, episode steps: 649, steps per second:  22, episode reward: 23.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.016534, mae: 2.243081, mean_q: 2.707395, mean_eps: 0.100000\n",
            "📈 Episodio 2120: Recompensa total (clipped): 16.000, Pasos: 656, Mean Reward Calculado: 0.024390 (Recompensa/Pasos)\n",
            " 1625550/2000000: episode: 2120, duration: 29.809s, episode steps: 656, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.017586, mae: 2.241870, mean_q: 2.702751, mean_eps: 0.100000\n",
            "📈 Episodio 2121: Recompensa total (clipped): 26.000, Pasos: 976, Mean Reward Calculado: 0.026639 (Recompensa/Pasos)\n",
            " 1626526/2000000: episode: 2121, duration: 44.497s, episode steps: 976, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.017486, mae: 2.245632, mean_q: 2.710409, mean_eps: 0.100000\n",
            "📈 Episodio 2122: Recompensa total (clipped): 13.000, Pasos: 712, Mean Reward Calculado: 0.018258 (Recompensa/Pasos)\n",
            " 1627238/2000000: episode: 2122, duration: 32.243s, episode steps: 712, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.016446, mae: 2.239073, mean_q: 2.698738, mean_eps: 0.100000\n",
            "📈 Episodio 2123: Recompensa total (clipped): 11.000, Pasos: 660, Mean Reward Calculado: 0.016667 (Recompensa/Pasos)\n",
            " 1627898/2000000: episode: 2123, duration: 30.306s, episode steps: 660, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.015123, mae: 2.239934, mean_q: 2.700244, mean_eps: 0.100000\n",
            "📈 Episodio 2124: Recompensa total (clipped): 18.000, Pasos: 671, Mean Reward Calculado: 0.026826 (Recompensa/Pasos)\n",
            " 1628569/2000000: episode: 2124, duration: 30.867s, episode steps: 671, steps per second:  22, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.015358, mae: 2.242513, mean_q: 2.704186, mean_eps: 0.100000\n",
            "📈 Episodio 2125: Recompensa total (clipped): 19.000, Pasos: 1014, Mean Reward Calculado: 0.018738 (Recompensa/Pasos)\n",
            " 1629583/2000000: episode: 2125, duration: 46.152s, episode steps: 1014, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.017997, mae: 2.237772, mean_q: 2.695129, mean_eps: 0.100000\n",
            "📈 Episodio 2126: Recompensa total (clipped): 24.000, Pasos: 873, Mean Reward Calculado: 0.027491 (Recompensa/Pasos)\n",
            " 1630456/2000000: episode: 2126, duration: 40.149s, episode steps: 873, steps per second:  22, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.016192, mae: 2.251534, mean_q: 2.715743, mean_eps: 0.100000\n",
            "📈 Episodio 2127: Recompensa total (clipped): 28.000, Pasos: 1291, Mean Reward Calculado: 0.021689 (Recompensa/Pasos)\n",
            " 1631747/2000000: episode: 2127, duration: 59.022s, episode steps: 1291, steps per second:  22, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.015970, mae: 2.261401, mean_q: 2.726524, mean_eps: 0.100000\n",
            "📈 Episodio 2128: Recompensa total (clipped): 25.000, Pasos: 1126, Mean Reward Calculado: 0.022202 (Recompensa/Pasos)\n",
            " 1632873/2000000: episode: 2128, duration: 51.276s, episode steps: 1126, steps per second:  22, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.017028, mae: 2.253627, mean_q: 2.717079, mean_eps: 0.100000\n",
            "📈 Episodio 2129: Recompensa total (clipped): 22.000, Pasos: 760, Mean Reward Calculado: 0.028947 (Recompensa/Pasos)\n",
            " 1633633/2000000: episode: 2129, duration: 34.866s, episode steps: 760, steps per second:  22, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.018271, mae: 2.251279, mean_q: 2.716414, mean_eps: 0.100000\n",
            "📈 Episodio 2130: Recompensa total (clipped): 23.000, Pasos: 994, Mean Reward Calculado: 0.023139 (Recompensa/Pasos)\n",
            " 1634627/2000000: episode: 2130, duration: 45.025s, episode steps: 994, steps per second:  22, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017080, mae: 2.247107, mean_q: 2.709393, mean_eps: 0.100000\n",
            "📈 Episodio 2131: Recompensa total (clipped): 23.000, Pasos: 1039, Mean Reward Calculado: 0.022137 (Recompensa/Pasos)\n",
            " 1635666/2000000: episode: 2131, duration: 47.108s, episode steps: 1039, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.017791, mae: 2.255140, mean_q: 2.719726, mean_eps: 0.100000\n",
            "📈 Episodio 2132: Recompensa total (clipped): 25.000, Pasos: 1077, Mean Reward Calculado: 0.023213 (Recompensa/Pasos)\n",
            " 1636743/2000000: episode: 2132, duration: 49.304s, episode steps: 1077, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.016195, mae: 2.252954, mean_q: 2.717668, mean_eps: 0.100000\n",
            "📈 Episodio 2133: Recompensa total (clipped): 17.000, Pasos: 877, Mean Reward Calculado: 0.019384 (Recompensa/Pasos)\n",
            " 1637620/2000000: episode: 2133, duration: 39.785s, episode steps: 877, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.018575, mae: 2.254227, mean_q: 2.719370, mean_eps: 0.100000\n",
            "📈 Episodio 2134: Recompensa total (clipped): 25.000, Pasos: 959, Mean Reward Calculado: 0.026069 (Recompensa/Pasos)\n",
            " 1638579/2000000: episode: 2134, duration: 44.377s, episode steps: 959, steps per second:  22, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.017162, mae: 2.253586, mean_q: 2.722578, mean_eps: 0.100000\n",
            "📈 Episodio 2135: Recompensa total (clipped): 26.000, Pasos: 1288, Mean Reward Calculado: 0.020186 (Recompensa/Pasos)\n",
            " 1639867/2000000: episode: 2135, duration: 58.449s, episode steps: 1288, steps per second:  22, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.016416, mae: 2.254066, mean_q: 2.719341, mean_eps: 0.100000\n",
            "📊 Paso 1,640,000/2,000,000 (82.0%) - 25.0 pasos/seg - ETA: 4.0h - Memoria: 15228.10 MB\n",
            "📈 Episodio 2136: Recompensa total (clipped): 10.000, Pasos: 369, Mean Reward Calculado: 0.027100 (Recompensa/Pasos)\n",
            " 1640236/2000000: episode: 2136, duration: 17.136s, episode steps: 369, steps per second:  22, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.018579, mae: 2.285158, mean_q: 2.755370, mean_eps: 0.100000\n",
            "📈 Episodio 2137: Recompensa total (clipped): 6.000, Pasos: 360, Mean Reward Calculado: 0.016667 (Recompensa/Pasos)\n",
            " 1640596/2000000: episode: 2137, duration: 16.516s, episode steps: 360, steps per second:  22, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.016973, mae: 2.290084, mean_q: 2.763336, mean_eps: 0.100000\n",
            "📈 Episodio 2138: Recompensa total (clipped): 23.000, Pasos: 901, Mean Reward Calculado: 0.025527 (Recompensa/Pasos)\n",
            " 1641497/2000000: episode: 2138, duration: 40.718s, episode steps: 901, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.017795, mae: 2.271012, mean_q: 2.738423, mean_eps: 0.100000\n",
            "📈 Episodio 2139: Recompensa total (clipped): 12.000, Pasos: 675, Mean Reward Calculado: 0.017778 (Recompensa/Pasos)\n",
            " 1642172/2000000: episode: 2139, duration: 30.621s, episode steps: 675, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016528, mae: 2.278204, mean_q: 2.747719, mean_eps: 0.100000\n",
            "📈 Episodio 2140: Recompensa total (clipped): 14.000, Pasos: 673, Mean Reward Calculado: 0.020802 (Recompensa/Pasos)\n",
            " 1642845/2000000: episode: 2140, duration: 30.737s, episode steps: 673, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.016718, mae: 2.276411, mean_q: 2.750079, mean_eps: 0.100000\n",
            "📈 Episodio 2141: Recompensa total (clipped): 20.000, Pasos: 896, Mean Reward Calculado: 0.022321 (Recompensa/Pasos)\n",
            " 1643741/2000000: episode: 2141, duration: 40.466s, episode steps: 896, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.016873, mae: 2.285142, mean_q: 2.757721, mean_eps: 0.100000\n",
            "📈 Episodio 2142: Recompensa total (clipped): 7.000, Pasos: 470, Mean Reward Calculado: 0.014894 (Recompensa/Pasos)\n",
            " 1644211/2000000: episode: 2142, duration: 21.427s, episode steps: 470, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.087 [0.000, 5.000],  loss: 0.019260, mae: 2.273009, mean_q: 2.741566, mean_eps: 0.100000\n",
            "📈 Episodio 2143: Recompensa total (clipped): 14.000, Pasos: 808, Mean Reward Calculado: 0.017327 (Recompensa/Pasos)\n",
            " 1645019/2000000: episode: 2143, duration: 36.190s, episode steps: 808, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.016931, mae: 2.290677, mean_q: 2.763886, mean_eps: 0.100000\n",
            "📈 Episodio 2144: Recompensa total (clipped): 16.000, Pasos: 657, Mean Reward Calculado: 0.024353 (Recompensa/Pasos)\n",
            " 1645676/2000000: episode: 2144, duration: 30.043s, episode steps: 657, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.018667, mae: 2.286690, mean_q: 2.755894, mean_eps: 0.100000\n",
            "📈 Episodio 2145: Recompensa total (clipped): 10.000, Pasos: 550, Mean Reward Calculado: 0.018182 (Recompensa/Pasos)\n",
            " 1646226/2000000: episode: 2145, duration: 24.899s, episode steps: 550, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.017181, mae: 2.302213, mean_q: 2.774226, mean_eps: 0.100000\n",
            "📈 Episodio 2146: Recompensa total (clipped): 20.000, Pasos: 903, Mean Reward Calculado: 0.022148 (Recompensa/Pasos)\n",
            " 1647129/2000000: episode: 2146, duration: 40.866s, episode steps: 903, steps per second:  22, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.017852, mae: 2.293212, mean_q: 2.765231, mean_eps: 0.100000\n",
            "📈 Episodio 2147: Recompensa total (clipped): 20.000, Pasos: 745, Mean Reward Calculado: 0.026846 (Recompensa/Pasos)\n",
            " 1647874/2000000: episode: 2147, duration: 33.651s, episode steps: 745, steps per second:  22, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.016337, mae: 2.259562, mean_q: 2.722661, mean_eps: 0.100000\n",
            "📈 Episodio 2148: Recompensa total (clipped): 21.000, Pasos: 1086, Mean Reward Calculado: 0.019337 (Recompensa/Pasos)\n",
            " 1648960/2000000: episode: 2148, duration: 49.761s, episode steps: 1086, steps per second:  22, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.017534, mae: 2.284104, mean_q: 2.754294, mean_eps: 0.100000\n",
            "📈 Episodio 2149: Recompensa total (clipped): 7.000, Pasos: 545, Mean Reward Calculado: 0.012844 (Recompensa/Pasos)\n",
            " 1649505/2000000: episode: 2149, duration: 25.132s, episode steps: 545, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.976 [0.000, 5.000],  loss: 0.018000, mae: 2.291040, mean_q: 2.761751, mean_eps: 0.100000\n",
            "📈 Episodio 2150: Recompensa total (clipped): 11.000, Pasos: 565, Mean Reward Calculado: 0.019469 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2150, pasos: 1650070)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.90 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2150 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 11.00\n",
            "   Media últimos 100: 17.90 / 20.0\n",
            "   Mejor promedio histórico: 17.90\n",
            "   Estado: 📈 89.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1650070/2000000: episode: 2150, duration: 257.549s, episode steps: 565, steps per second:   2, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016077, mae: 2.278655, mean_q: 2.750603, mean_eps: 0.100000\n",
            "📈 Episodio 2151: Recompensa total (clipped): 17.000, Pasos: 839, Mean Reward Calculado: 0.020262 (Recompensa/Pasos)\n",
            " 1650909/2000000: episode: 2151, duration: 38.851s, episode steps: 839, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.016684, mae: 2.311209, mean_q: 2.788113, mean_eps: 0.100000\n",
            "📈 Episodio 2152: Recompensa total (clipped): 10.000, Pasos: 542, Mean Reward Calculado: 0.018450 (Recompensa/Pasos)\n",
            " 1651451/2000000: episode: 2152, duration: 25.124s, episode steps: 542, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.017259, mae: 2.309359, mean_q: 2.783311, mean_eps: 0.100000\n",
            "📈 Episodio 2153: Recompensa total (clipped): 25.000, Pasos: 1177, Mean Reward Calculado: 0.021240 (Recompensa/Pasos)\n",
            " 1652628/2000000: episode: 2153, duration: 54.463s, episode steps: 1177, steps per second:  22, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.018263, mae: 2.295722, mean_q: 2.768187, mean_eps: 0.100000\n",
            "📈 Episodio 2154: Recompensa total (clipped): 8.000, Pasos: 580, Mean Reward Calculado: 0.013793 (Recompensa/Pasos)\n",
            " 1653208/2000000: episode: 2154, duration: 26.846s, episode steps: 580, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.018110, mae: 2.307127, mean_q: 2.783798, mean_eps: 0.100000\n",
            "📈 Episodio 2155: Recompensa total (clipped): 16.000, Pasos: 833, Mean Reward Calculado: 0.019208 (Recompensa/Pasos)\n",
            " 1654041/2000000: episode: 2155, duration: 38.500s, episode steps: 833, steps per second:  22, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.015618, mae: 2.315075, mean_q: 2.792592, mean_eps: 0.100000\n",
            "📈 Episodio 2156: Recompensa total (clipped): 11.000, Pasos: 505, Mean Reward Calculado: 0.021782 (Recompensa/Pasos)\n",
            " 1654546/2000000: episode: 2156, duration: 23.306s, episode steps: 505, steps per second:  22, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.016860, mae: 2.303636, mean_q: 2.778018, mean_eps: 0.100000\n",
            "📈 Episodio 2157: Recompensa total (clipped): 30.000, Pasos: 1334, Mean Reward Calculado: 0.022489 (Recompensa/Pasos)\n",
            " 1655880/2000000: episode: 2157, duration: 60.994s, episode steps: 1334, steps per second:  22, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016819, mae: 2.290252, mean_q: 2.760725, mean_eps: 0.100000\n",
            "📈 Episodio 2158: Recompensa total (clipped): 12.000, Pasos: 723, Mean Reward Calculado: 0.016598 (Recompensa/Pasos)\n",
            " 1656603/2000000: episode: 2158, duration: 33.319s, episode steps: 723, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.018750, mae: 2.304472, mean_q: 2.779158, mean_eps: 0.100000\n",
            "📈 Episodio 2159: Recompensa total (clipped): 14.000, Pasos: 773, Mean Reward Calculado: 0.018111 (Recompensa/Pasos)\n",
            " 1657376/2000000: episode: 2159, duration: 35.734s, episode steps: 773, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.019089, mae: 2.295433, mean_q: 2.768437, mean_eps: 0.100000\n",
            "📈 Episodio 2160: Recompensa total (clipped): 14.000, Pasos: 657, Mean Reward Calculado: 0.021309 (Recompensa/Pasos)\n",
            " 1658033/2000000: episode: 2160, duration: 30.081s, episode steps: 657, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.017703, mae: 2.306427, mean_q: 2.780848, mean_eps: 0.100000\n",
            "📈 Episodio 2161: Recompensa total (clipped): 27.000, Pasos: 1523, Mean Reward Calculado: 0.017728 (Recompensa/Pasos)\n",
            " 1659556/2000000: episode: 2161, duration: 69.669s, episode steps: 1523, steps per second:  22, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017321, mae: 2.302870, mean_q: 2.776065, mean_eps: 0.100000\n",
            "📊 Paso 1,660,000/2,000,000 (83.0%) - 24.9 pasos/seg - ETA: 3.8h - Memoria: 15289.64 MB\n",
            "📈 Episodio 2162: Recompensa total (clipped): 21.000, Pasos: 974, Mean Reward Calculado: 0.021561 (Recompensa/Pasos)\n",
            " 1660530/2000000: episode: 2162, duration: 45.220s, episode steps: 974, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.017661, mae: 2.299460, mean_q: 2.772828, mean_eps: 0.100000\n",
            "📈 Episodio 2163: Recompensa total (clipped): 26.000, Pasos: 1406, Mean Reward Calculado: 0.018492 (Recompensa/Pasos)\n",
            " 1661936/2000000: episode: 2163, duration: 64.406s, episode steps: 1406, steps per second:  22, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.018747, mae: 2.312267, mean_q: 2.786613, mean_eps: 0.100000\n",
            "📈 Episodio 2164: Recompensa total (clipped): 23.000, Pasos: 899, Mean Reward Calculado: 0.025584 (Recompensa/Pasos)\n",
            " 1662835/2000000: episode: 2164, duration: 41.186s, episode steps: 899, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.017385, mae: 2.320219, mean_q: 2.798921, mean_eps: 0.100000\n",
            "📈 Episodio 2165: Recompensa total (clipped): 12.000, Pasos: 705, Mean Reward Calculado: 0.017021 (Recompensa/Pasos)\n",
            " 1663540/2000000: episode: 2165, duration: 32.083s, episode steps: 705, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.019133, mae: 2.323824, mean_q: 2.802028, mean_eps: 0.100000\n",
            "📈 Episodio 2166: Recompensa total (clipped): 10.000, Pasos: 520, Mean Reward Calculado: 0.019231 (Recompensa/Pasos)\n",
            " 1664060/2000000: episode: 2166, duration: 23.693s, episode steps: 520, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.020534, mae: 2.328274, mean_q: 2.805816, mean_eps: 0.100000\n",
            "📈 Episodio 2167: Recompensa total (clipped): 5.000, Pasos: 556, Mean Reward Calculado: 0.008993 (Recompensa/Pasos)\n",
            " 1664616/2000000: episode: 2167, duration: 25.519s, episode steps: 556, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.019606, mae: 2.288415, mean_q: 2.759041, mean_eps: 0.100000\n",
            "📈 Episodio 2168: Recompensa total (clipped): 14.000, Pasos: 730, Mean Reward Calculado: 0.019178 (Recompensa/Pasos)\n",
            " 1665346/2000000: episode: 2168, duration: 33.642s, episode steps: 730, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.017110, mae: 2.303664, mean_q: 2.776502, mean_eps: 0.100000\n",
            "📈 Episodio 2169: Recompensa total (clipped): 9.000, Pasos: 539, Mean Reward Calculado: 0.016698 (Recompensa/Pasos)\n",
            " 1665885/2000000: episode: 2169, duration: 24.911s, episode steps: 539, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.016446, mae: 2.290575, mean_q: 2.761099, mean_eps: 0.100000\n",
            "📈 Episodio 2170: Recompensa total (clipped): 21.000, Pasos: 829, Mean Reward Calculado: 0.025332 (Recompensa/Pasos)\n",
            " 1666714/2000000: episode: 2170, duration: 38.092s, episode steps: 829, steps per second:  22, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.017081, mae: 2.285576, mean_q: 2.755982, mean_eps: 0.100000\n",
            "📈 Episodio 2171: Recompensa total (clipped): 10.000, Pasos: 492, Mean Reward Calculado: 0.020325 (Recompensa/Pasos)\n",
            " 1667206/2000000: episode: 2171, duration: 22.745s, episode steps: 492, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.785 [0.000, 5.000],  loss: 0.018801, mae: 2.306626, mean_q: 2.786376, mean_eps: 0.100000\n",
            "📈 Episodio 2172: Recompensa total (clipped): 14.000, Pasos: 644, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            " 1667850/2000000: episode: 2172, duration: 29.466s, episode steps: 644, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.016041, mae: 2.288794, mean_q: 2.758004, mean_eps: 0.100000\n",
            "📈 Episodio 2173: Recompensa total (clipped): 6.000, Pasos: 531, Mean Reward Calculado: 0.011299 (Recompensa/Pasos)\n",
            " 1668381/2000000: episode: 2173, duration: 24.364s, episode steps: 531, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.018938, mae: 2.312257, mean_q: 2.785861, mean_eps: 0.100000\n",
            "📈 Episodio 2174: Recompensa total (clipped): 13.000, Pasos: 661, Mean Reward Calculado: 0.019667 (Recompensa/Pasos)\n",
            " 1669042/2000000: episode: 2174, duration: 30.210s, episode steps: 661, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.016912, mae: 2.298365, mean_q: 2.772058, mean_eps: 0.100000\n",
            "📈 Episodio 2175: Recompensa total (clipped): 18.000, Pasos: 855, Mean Reward Calculado: 0.021053 (Recompensa/Pasos)\n",
            " 1669897/2000000: episode: 2175, duration: 39.083s, episode steps: 855, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.017530, mae: 2.294612, mean_q: 2.764797, mean_eps: 0.100000\n",
            "📈 Episodio 2176: Recompensa total (clipped): 9.000, Pasos: 648, Mean Reward Calculado: 0.013889 (Recompensa/Pasos)\n",
            " 1670545/2000000: episode: 2176, duration: 29.482s, episode steps: 648, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.015779, mae: 2.322189, mean_q: 2.800571, mean_eps: 0.100000\n",
            "📈 Episodio 2177: Recompensa total (clipped): 13.000, Pasos: 695, Mean Reward Calculado: 0.018705 (Recompensa/Pasos)\n",
            " 1671240/2000000: episode: 2177, duration: 31.868s, episode steps: 695, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.488 [0.000, 5.000],  loss: 0.017953, mae: 2.332494, mean_q: 2.815932, mean_eps: 0.100000\n",
            "📈 Episodio 2178: Recompensa total (clipped): 21.000, Pasos: 1139, Mean Reward Calculado: 0.018437 (Recompensa/Pasos)\n",
            " 1672379/2000000: episode: 2178, duration: 52.336s, episode steps: 1139, steps per second:  22, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.018631, mae: 2.302794, mean_q: 2.776861, mean_eps: 0.100000\n",
            "📈 Episodio 2179: Recompensa total (clipped): 12.000, Pasos: 535, Mean Reward Calculado: 0.022430 (Recompensa/Pasos)\n",
            " 1672914/2000000: episode: 2179, duration: 24.449s, episode steps: 535, steps per second:  22, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.017416, mae: 2.316764, mean_q: 2.794667, mean_eps: 0.100000\n",
            "📈 Episodio 2180: Recompensa total (clipped): 13.000, Pasos: 664, Mean Reward Calculado: 0.019578 (Recompensa/Pasos)\n",
            " 1673578/2000000: episode: 2180, duration: 30.177s, episode steps: 664, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.017457, mae: 2.322152, mean_q: 2.800855, mean_eps: 0.100000\n",
            "📈 Episodio 2181: Recompensa total (clipped): 23.000, Pasos: 974, Mean Reward Calculado: 0.023614 (Recompensa/Pasos)\n",
            " 1674552/2000000: episode: 2181, duration: 44.329s, episode steps: 974, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.019595, mae: 2.332405, mean_q: 2.813596, mean_eps: 0.100000\n",
            "📈 Episodio 2182: Recompensa total (clipped): 19.000, Pasos: 1063, Mean Reward Calculado: 0.017874 (Recompensa/Pasos)\n",
            " 1675615/2000000: episode: 2182, duration: 48.644s, episode steps: 1063, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.019574, mae: 2.340492, mean_q: 2.824216, mean_eps: 0.100000\n",
            "📈 Episodio 2183: Recompensa total (clipped): 21.000, Pasos: 885, Mean Reward Calculado: 0.023729 (Recompensa/Pasos)\n",
            " 1676500/2000000: episode: 2183, duration: 39.969s, episode steps: 885, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.018180, mae: 2.312710, mean_q: 2.789228, mean_eps: 0.100000\n",
            "📈 Episodio 2184: Recompensa total (clipped): 21.000, Pasos: 926, Mean Reward Calculado: 0.022678 (Recompensa/Pasos)\n",
            " 1677426/2000000: episode: 2184, duration: 41.945s, episode steps: 926, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.014 [0.000, 5.000],  loss: 0.018739, mae: 2.337330, mean_q: 2.817669, mean_eps: 0.100000\n",
            "📈 Episodio 2185: Recompensa total (clipped): 24.000, Pasos: 863, Mean Reward Calculado: 0.027810 (Recompensa/Pasos)\n",
            " 1678289/2000000: episode: 2185, duration: 39.211s, episode steps: 863, steps per second:  22, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.017291, mae: 2.335253, mean_q: 2.814542, mean_eps: 0.100000\n",
            "📈 Episodio 2186: Recompensa total (clipped): 4.000, Pasos: 526, Mean Reward Calculado: 0.007605 (Recompensa/Pasos)\n",
            " 1678815/2000000: episode: 2186, duration: 23.771s, episode steps: 526, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.016926, mae: 2.322769, mean_q: 2.800041, mean_eps: 0.100000\n",
            "📈 Episodio 2187: Recompensa total (clipped): 10.000, Pasos: 582, Mean Reward Calculado: 0.017182 (Recompensa/Pasos)\n",
            " 1679397/2000000: episode: 2187, duration: 26.634s, episode steps: 582, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.018074, mae: 2.314464, mean_q: 2.793351, mean_eps: 0.100000\n",
            "📊 Paso 1,680,000/2,000,000 (84.0%) - 24.8 pasos/seg - ETA: 3.6h - Memoria: 15284.93 MB\n",
            "📈 Episodio 2188: Recompensa total (clipped): 21.000, Pasos: 1036, Mean Reward Calculado: 0.020270 (Recompensa/Pasos)\n",
            " 1680433/2000000: episode: 2188, duration: 47.752s, episode steps: 1036, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.017376, mae: 2.315236, mean_q: 2.794644, mean_eps: 0.100000\n",
            "📈 Episodio 2189: Recompensa total (clipped): 33.000, Pasos: 1762, Mean Reward Calculado: 0.018729 (Recompensa/Pasos)\n",
            " 1682195/2000000: episode: 2189, duration: 80.021s, episode steps: 1762, steps per second:  22, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.017848, mae: 2.309322, mean_q: 2.785550, mean_eps: 0.100000\n",
            "📈 Episodio 2190: Recompensa total (clipped): 15.000, Pasos: 872, Mean Reward Calculado: 0.017202 (Recompensa/Pasos)\n",
            " 1683067/2000000: episode: 2190, duration: 39.792s, episode steps: 872, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.018669, mae: 2.296105, mean_q: 2.766871, mean_eps: 0.100000\n",
            "📈 Episodio 2191: Recompensa total (clipped): 10.000, Pasos: 636, Mean Reward Calculado: 0.015723 (Recompensa/Pasos)\n",
            " 1683703/2000000: episode: 2191, duration: 29.175s, episode steps: 636, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.016434, mae: 2.311630, mean_q: 2.785660, mean_eps: 0.100000\n",
            "📈 Episodio 2192: Recompensa total (clipped): 25.000, Pasos: 906, Mean Reward Calculado: 0.027594 (Recompensa/Pasos)\n",
            " 1684609/2000000: episode: 2192, duration: 41.622s, episode steps: 906, steps per second:  22, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.020572, mae: 2.300532, mean_q: 2.772207, mean_eps: 0.100000\n",
            "📈 Episodio 2193: Recompensa total (clipped): 20.000, Pasos: 753, Mean Reward Calculado: 0.026560 (Recompensa/Pasos)\n",
            " 1685362/2000000: episode: 2193, duration: 34.287s, episode steps: 753, steps per second:  22, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.019316, mae: 2.321583, mean_q: 2.798764, mean_eps: 0.100000\n",
            "📈 Episodio 2194: Recompensa total (clipped): 7.000, Pasos: 477, Mean Reward Calculado: 0.014675 (Recompensa/Pasos)\n",
            " 1685839/2000000: episode: 2194, duration: 21.662s, episode steps: 477, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.853 [0.000, 5.000],  loss: 0.019280, mae: 2.323357, mean_q: 2.801885, mean_eps: 0.100000\n",
            "📈 Episodio 2195: Recompensa total (clipped): 32.000, Pasos: 1489, Mean Reward Calculado: 0.021491 (Recompensa/Pasos)\n",
            " 1687328/2000000: episode: 2195, duration: 67.606s, episode steps: 1489, steps per second:  22, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.025 [0.000, 5.000],  loss: 0.018476, mae: 2.310568, mean_q: 2.788059, mean_eps: 0.100000\n",
            "📈 Episodio 2196: Recompensa total (clipped): 5.000, Pasos: 365, Mean Reward Calculado: 0.013699 (Recompensa/Pasos)\n",
            " 1687693/2000000: episode: 2196, duration: 17.049s, episode steps: 365, steps per second:  21, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.016317, mae: 2.340407, mean_q: 2.822554, mean_eps: 0.100000\n",
            "📈 Episodio 2197: Recompensa total (clipped): 16.000, Pasos: 733, Mean Reward Calculado: 0.021828 (Recompensa/Pasos)\n",
            " 1688426/2000000: episode: 2197, duration: 33.581s, episode steps: 733, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.018850, mae: 2.315141, mean_q: 2.792687, mean_eps: 0.100000\n",
            "📈 Episodio 2198: Recompensa total (clipped): 16.000, Pasos: 640, Mean Reward Calculado: 0.025000 (Recompensa/Pasos)\n",
            " 1689066/2000000: episode: 2198, duration: 29.249s, episode steps: 640, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.018947, mae: 2.307794, mean_q: 2.781238, mean_eps: 0.100000\n",
            "📈 Episodio 2199: Recompensa total (clipped): 15.000, Pasos: 660, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            " 1689726/2000000: episode: 2199, duration: 30.137s, episode steps: 660, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.018809, mae: 2.318723, mean_q: 2.797533, mean_eps: 0.100000\n",
            "📈 Episodio 2200: Recompensa total (clipped): 31.000, Pasos: 1291, Mean Reward Calculado: 0.024012 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2200, pasos: 1691017)\n",
            "💾 NUEVO MEJOR PROMEDIO: 16.89 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2200 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 31.00\n",
            "   Media últimos 100: 16.89 / 20.0\n",
            "   Mejor promedio histórico: 16.89\n",
            "   Estado: 📈 84.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1691017/2000000: episode: 2200, duration: 133.250s, episode steps: 1291, steps per second:  10, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.017028, mae: 2.307172, mean_q: 2.782487, mean_eps: 0.100000\n",
            "📈 Episodio 2201: Recompensa total (clipped): 7.000, Pasos: 453, Mean Reward Calculado: 0.015453 (Recompensa/Pasos)\n",
            " 1691470/2000000: episode: 2201, duration: 21.049s, episode steps: 453, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.064 [0.000, 5.000],  loss: 0.018184, mae: 2.315811, mean_q: 2.794677, mean_eps: 0.100000\n",
            "📈 Episodio 2202: Recompensa total (clipped): 23.000, Pasos: 725, Mean Reward Calculado: 0.031724 (Recompensa/Pasos)\n",
            " 1692195/2000000: episode: 2202, duration: 33.671s, episode steps: 725, steps per second:  22, episode reward: 23.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.017258, mae: 2.315822, mean_q: 2.793431, mean_eps: 0.100000\n",
            "📈 Episodio 2203: Recompensa total (clipped): 21.000, Pasos: 1066, Mean Reward Calculado: 0.019700 (Recompensa/Pasos)\n",
            " 1693261/2000000: episode: 2203, duration: 48.608s, episode steps: 1066, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.016031, mae: 2.303875, mean_q: 2.778336, mean_eps: 0.100000\n",
            "📈 Episodio 2204: Recompensa total (clipped): 20.000, Pasos: 654, Mean Reward Calculado: 0.030581 (Recompensa/Pasos)\n",
            " 1693915/2000000: episode: 2204, duration: 29.404s, episode steps: 654, steps per second:  22, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.018807, mae: 2.320846, mean_q: 2.799374, mean_eps: 0.100000\n",
            "📈 Episodio 2205: Recompensa total (clipped): 22.000, Pasos: 949, Mean Reward Calculado: 0.023182 (Recompensa/Pasos)\n",
            " 1694864/2000000: episode: 2205, duration: 43.164s, episode steps: 949, steps per second:  22, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.016437, mae: 2.306685, mean_q: 2.782116, mean_eps: 0.100000\n",
            "📈 Episodio 2206: Recompensa total (clipped): 16.000, Pasos: 745, Mean Reward Calculado: 0.021477 (Recompensa/Pasos)\n",
            " 1695609/2000000: episode: 2206, duration: 33.623s, episode steps: 745, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.018957, mae: 2.296786, mean_q: 2.768910, mean_eps: 0.100000\n",
            "📈 Episodio 2207: Recompensa total (clipped): 11.000, Pasos: 635, Mean Reward Calculado: 0.017323 (Recompensa/Pasos)\n",
            " 1696244/2000000: episode: 2207, duration: 28.967s, episode steps: 635, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.019409, mae: 2.302755, mean_q: 2.776412, mean_eps: 0.100000\n",
            "📈 Episodio 2208: Recompensa total (clipped): 24.000, Pasos: 1117, Mean Reward Calculado: 0.021486 (Recompensa/Pasos)\n",
            " 1697361/2000000: episode: 2208, duration: 51.284s, episode steps: 1117, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.017194, mae: 2.296164, mean_q: 2.768031, mean_eps: 0.100000\n",
            "📈 Episodio 2209: Recompensa total (clipped): 20.000, Pasos: 866, Mean Reward Calculado: 0.023095 (Recompensa/Pasos)\n",
            " 1698227/2000000: episode: 2209, duration: 39.735s, episode steps: 866, steps per second:  22, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.017410, mae: 2.302473, mean_q: 2.777126, mean_eps: 0.100000\n",
            "📈 Episodio 2210: Recompensa total (clipped): 27.000, Pasos: 1607, Mean Reward Calculado: 0.016801 (Recompensa/Pasos)\n",
            " 1699834/2000000: episode: 2210, duration: 72.860s, episode steps: 1607, steps per second:  22, episode reward: 27.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.018173, mae: 2.300327, mean_q: 2.774361, mean_eps: 0.100000\n",
            "📊 Paso 1,700,000/2,000,000 (85.0%) - 24.8 pasos/seg - ETA: 3.4h - Memoria: 15291.92 MB\n",
            "📈 Episodio 2211: Recompensa total (clipped): 8.000, Pasos: 594, Mean Reward Calculado: 0.013468 (Recompensa/Pasos)\n",
            " 1700428/2000000: episode: 2211, duration: 27.068s, episode steps: 594, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.017888, mae: 2.313640, mean_q: 2.792689, mean_eps: 0.100000\n",
            "📈 Episodio 2212: Recompensa total (clipped): 17.000, Pasos: 846, Mean Reward Calculado: 0.020095 (Recompensa/Pasos)\n",
            " 1701274/2000000: episode: 2212, duration: 38.350s, episode steps: 846, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.016427, mae: 2.339162, mean_q: 2.824274, mean_eps: 0.100000\n",
            "📈 Episodio 2213: Recompensa total (clipped): 7.000, Pasos: 386, Mean Reward Calculado: 0.018135 (Recompensa/Pasos)\n",
            " 1701660/2000000: episode: 2213, duration: 17.530s, episode steps: 386, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.016239, mae: 2.315087, mean_q: 2.790915, mean_eps: 0.100000\n",
            "📈 Episodio 2214: Recompensa total (clipped): 28.000, Pasos: 1057, Mean Reward Calculado: 0.026490 (Recompensa/Pasos)\n",
            " 1702717/2000000: episode: 2214, duration: 48.817s, episode steps: 1057, steps per second:  22, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.017391, mae: 2.315094, mean_q: 2.793245, mean_eps: 0.100000\n",
            "📈 Episodio 2215: Recompensa total (clipped): 17.000, Pasos: 751, Mean Reward Calculado: 0.022636 (Recompensa/Pasos)\n",
            " 1703468/2000000: episode: 2215, duration: 34.112s, episode steps: 751, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.017255, mae: 2.283561, mean_q: 2.753933, mean_eps: 0.100000\n",
            "📈 Episodio 2216: Recompensa total (clipped): 20.000, Pasos: 788, Mean Reward Calculado: 0.025381 (Recompensa/Pasos)\n",
            " 1704256/2000000: episode: 2216, duration: 36.208s, episode steps: 788, steps per second:  22, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.015931, mae: 2.326587, mean_q: 2.806242, mean_eps: 0.100000\n",
            "📈 Episodio 2217: Recompensa total (clipped): 9.000, Pasos: 507, Mean Reward Calculado: 0.017751 (Recompensa/Pasos)\n",
            " 1704763/2000000: episode: 2217, duration: 23.238s, episode steps: 507, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.016726, mae: 2.314742, mean_q: 2.790818, mean_eps: 0.100000\n",
            "📈 Episodio 2218: Recompensa total (clipped): 34.000, Pasos: 1493, Mean Reward Calculado: 0.022773 (Recompensa/Pasos)\n",
            " 1706256/2000000: episode: 2218, duration: 67.958s, episode steps: 1493, steps per second:  22, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.018246, mae: 2.312837, mean_q: 2.790409, mean_eps: 0.100000\n",
            "📈 Episodio 2219: Recompensa total (clipped): 19.000, Pasos: 938, Mean Reward Calculado: 0.020256 (Recompensa/Pasos)\n",
            " 1707194/2000000: episode: 2219, duration: 42.466s, episode steps: 938, steps per second:  22, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.018979, mae: 2.317330, mean_q: 2.796831, mean_eps: 0.100000\n",
            "📈 Episodio 2220: Recompensa total (clipped): 13.000, Pasos: 609, Mean Reward Calculado: 0.021346 (Recompensa/Pasos)\n",
            " 1707803/2000000: episode: 2220, duration: 27.540s, episode steps: 609, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.020693, mae: 2.289936, mean_q: 2.762552, mean_eps: 0.100000\n",
            "📈 Episodio 2221: Recompensa total (clipped): 15.000, Pasos: 693, Mean Reward Calculado: 0.021645 (Recompensa/Pasos)\n",
            " 1708496/2000000: episode: 2221, duration: 32.082s, episode steps: 693, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.018474, mae: 2.310836, mean_q: 2.789395, mean_eps: 0.100000\n",
            "📈 Episodio 2222: Recompensa total (clipped): 17.000, Pasos: 557, Mean Reward Calculado: 0.030521 (Recompensa/Pasos)\n",
            " 1709053/2000000: episode: 2222, duration: 25.350s, episode steps: 557, steps per second:  22, episode reward: 17.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.017903, mae: 2.294141, mean_q: 2.770543, mean_eps: 0.100000\n",
            "📈 Episodio 2223: Recompensa total (clipped): 19.000, Pasos: 775, Mean Reward Calculado: 0.024516 (Recompensa/Pasos)\n",
            " 1709828/2000000: episode: 2223, duration: 35.021s, episode steps: 775, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.018963, mae: 2.313507, mean_q: 2.791408, mean_eps: 0.100000\n",
            "📈 Episodio 2224: Recompensa total (clipped): 17.000, Pasos: 989, Mean Reward Calculado: 0.017189 (Recompensa/Pasos)\n",
            " 1710817/2000000: episode: 2224, duration: 44.600s, episode steps: 989, steps per second:  22, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.017297, mae: 2.330818, mean_q: 2.812523, mean_eps: 0.100000\n",
            "📈 Episodio 2225: Recompensa total (clipped): 12.000, Pasos: 653, Mean Reward Calculado: 0.018377 (Recompensa/Pasos)\n",
            " 1711470/2000000: episode: 2225, duration: 29.423s, episode steps: 653, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.016678, mae: 2.340998, mean_q: 2.826109, mean_eps: 0.100000\n",
            "📈 Episodio 2226: Recompensa total (clipped): 33.000, Pasos: 1658, Mean Reward Calculado: 0.019903 (Recompensa/Pasos)\n",
            " 1713128/2000000: episode: 2226, duration: 75.587s, episode steps: 1658, steps per second:  22, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.017874, mae: 2.310827, mean_q: 2.787524, mean_eps: 0.100000\n",
            "📈 Episodio 2227: Recompensa total (clipped): 26.000, Pasos: 1063, Mean Reward Calculado: 0.024459 (Recompensa/Pasos)\n",
            " 1714191/2000000: episode: 2227, duration: 48.314s, episode steps: 1063, steps per second:  22, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.018221, mae: 2.321265, mean_q: 2.799280, mean_eps: 0.100000\n",
            "📈 Episodio 2228: Recompensa total (clipped): 27.000, Pasos: 1138, Mean Reward Calculado: 0.023726 (Recompensa/Pasos)\n",
            " 1715329/2000000: episode: 2228, duration: 51.841s, episode steps: 1138, steps per second:  22, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.018189, mae: 2.325355, mean_q: 2.804516, mean_eps: 0.100000\n",
            "📈 Episodio 2229: Recompensa total (clipped): 27.000, Pasos: 929, Mean Reward Calculado: 0.029064 (Recompensa/Pasos)\n",
            " 1716258/2000000: episode: 2229, duration: 42.518s, episode steps: 929, steps per second:  22, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.018402, mae: 2.326795, mean_q: 2.805181, mean_eps: 0.100000\n",
            "📈 Episodio 2230: Recompensa total (clipped): 26.000, Pasos: 827, Mean Reward Calculado: 0.031439 (Recompensa/Pasos)\n",
            " 1717085/2000000: episode: 2230, duration: 37.508s, episode steps: 827, steps per second:  22, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.177 [0.000, 5.000],  loss: 0.017350, mae: 2.318343, mean_q: 2.794961, mean_eps: 0.100000\n",
            "📈 Episodio 2231: Recompensa total (clipped): 22.000, Pasos: 817, Mean Reward Calculado: 0.026928 (Recompensa/Pasos)\n",
            " 1717902/2000000: episode: 2231, duration: 37.114s, episode steps: 817, steps per second:  22, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.153 [0.000, 5.000],  loss: 0.018347, mae: 2.315893, mean_q: 2.791536, mean_eps: 0.100000\n",
            "📈 Episodio 2232: Recompensa total (clipped): 15.000, Pasos: 560, Mean Reward Calculado: 0.026786 (Recompensa/Pasos)\n",
            " 1718462/2000000: episode: 2232, duration: 25.602s, episode steps: 560, steps per second:  22, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.015982, mae: 2.319723, mean_q: 2.796772, mean_eps: 0.100000\n",
            "📊 Paso 1,720,000/2,000,000 (86.0%) - 24.7 pasos/seg - ETA: 3.1h - Memoria: 15243.02 MB\n",
            "📈 Episodio 2233: Recompensa total (clipped): 29.000, Pasos: 1629, Mean Reward Calculado: 0.017802 (Recompensa/Pasos)\n",
            " 1720091/2000000: episode: 2233, duration: 74.299s, episode steps: 1629, steps per second:  22, episode reward: 29.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.017553, mae: 2.316611, mean_q: 2.792905, mean_eps: 0.100000\n",
            "📈 Episodio 2234: Recompensa total (clipped): 18.000, Pasos: 762, Mean Reward Calculado: 0.023622 (Recompensa/Pasos)\n",
            " 1720853/2000000: episode: 2234, duration: 35.737s, episode steps: 762, steps per second:  21, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.016237, mae: 2.297296, mean_q: 2.774731, mean_eps: 0.100000\n",
            "📈 Episodio 2235: Recompensa total (clipped): 25.000, Pasos: 816, Mean Reward Calculado: 0.030637 (Recompensa/Pasos)\n",
            " 1721669/2000000: episode: 2235, duration: 37.519s, episode steps: 816, steps per second:  22, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.066 [0.000, 5.000],  loss: 0.017460, mae: 2.311880, mean_q: 2.790540, mean_eps: 0.100000\n",
            "📈 Episodio 2236: Recompensa total (clipped): 9.000, Pasos: 520, Mean Reward Calculado: 0.017308 (Recompensa/Pasos)\n",
            " 1722189/2000000: episode: 2236, duration: 24.283s, episode steps: 520, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.017517, mae: 2.308016, mean_q: 2.784976, mean_eps: 0.100000\n",
            "📈 Episodio 2237: Recompensa total (clipped): 16.000, Pasos: 885, Mean Reward Calculado: 0.018079 (Recompensa/Pasos)\n",
            " 1723074/2000000: episode: 2237, duration: 40.750s, episode steps: 885, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.017950, mae: 2.295150, mean_q: 2.768791, mean_eps: 0.100000\n",
            "📈 Episodio 2238: Recompensa total (clipped): 16.000, Pasos: 648, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
            " 1723722/2000000: episode: 2238, duration: 29.311s, episode steps: 648, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.017365, mae: 2.312903, mean_q: 2.790664, mean_eps: 0.100000\n",
            "📈 Episodio 2239: Recompensa total (clipped): 29.000, Pasos: 1308, Mean Reward Calculado: 0.022171 (Recompensa/Pasos)\n",
            " 1725030/2000000: episode: 2239, duration: 58.878s, episode steps: 1308, steps per second:  22, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.018727, mae: 2.314387, mean_q: 2.790744, mean_eps: 0.100000\n",
            "📈 Episodio 2240: Recompensa total (clipped): 29.000, Pasos: 1154, Mean Reward Calculado: 0.025130 (Recompensa/Pasos)\n",
            " 1726184/2000000: episode: 2240, duration: 52.825s, episode steps: 1154, steps per second:  22, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.018752, mae: 2.293005, mean_q: 2.766620, mean_eps: 0.100000\n",
            "📈 Episodio 2241: Recompensa total (clipped): 17.000, Pasos: 742, Mean Reward Calculado: 0.022911 (Recompensa/Pasos)\n",
            " 1726926/2000000: episode: 2241, duration: 33.874s, episode steps: 742, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.019826, mae: 2.314200, mean_q: 2.792496, mean_eps: 0.100000\n",
            "📈 Episodio 2242: Recompensa total (clipped): 8.000, Pasos: 562, Mean Reward Calculado: 0.014235 (Recompensa/Pasos)\n",
            " 1727488/2000000: episode: 2242, duration: 25.648s, episode steps: 562, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.019899, mae: 2.295189, mean_q: 2.771121, mean_eps: 0.100000\n",
            "📈 Episodio 2243: Recompensa total (clipped): 9.000, Pasos: 378, Mean Reward Calculado: 0.023810 (Recompensa/Pasos)\n",
            " 1727866/2000000: episode: 2243, duration: 17.364s, episode steps: 378, steps per second:  22, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.016894, mae: 2.287087, mean_q: 2.756896, mean_eps: 0.100000\n",
            "📈 Episodio 2244: Recompensa total (clipped): 29.000, Pasos: 986, Mean Reward Calculado: 0.029412 (Recompensa/Pasos)\n",
            " 1728852/2000000: episode: 2244, duration: 45.485s, episode steps: 986, steps per second:  22, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.016636, mae: 2.294520, mean_q: 2.765932, mean_eps: 0.100000\n",
            "📈 Episodio 2245: Recompensa total (clipped): 27.000, Pasos: 1055, Mean Reward Calculado: 0.025592 (Recompensa/Pasos)\n",
            " 1729907/2000000: episode: 2245, duration: 48.500s, episode steps: 1055, steps per second:  22, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.016803, mae: 2.296870, mean_q: 2.770860, mean_eps: 0.100000\n",
            "📈 Episodio 2246: Recompensa total (clipped): 14.000, Pasos: 694, Mean Reward Calculado: 0.020173 (Recompensa/Pasos)\n",
            " 1730601/2000000: episode: 2246, duration: 31.527s, episode steps: 694, steps per second:  22, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.016068, mae: 2.319643, mean_q: 2.798953, mean_eps: 0.100000\n",
            "📈 Episodio 2247: Recompensa total (clipped): 23.000, Pasos: 761, Mean Reward Calculado: 0.030223 (Recompensa/Pasos)\n",
            " 1731362/2000000: episode: 2247, duration: 34.839s, episode steps: 761, steps per second:  22, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017316, mae: 2.328988, mean_q: 2.810628, mean_eps: 0.100000\n",
            "📈 Episodio 2248: Recompensa total (clipped): 12.000, Pasos: 586, Mean Reward Calculado: 0.020478 (Recompensa/Pasos)\n",
            " 1731948/2000000: episode: 2248, duration: 27.116s, episode steps: 586, steps per second:  22, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.017283, mae: 2.320903, mean_q: 2.799775, mean_eps: 0.100000\n",
            "📈 Episodio 2249: Recompensa total (clipped): 14.000, Pasos: 579, Mean Reward Calculado: 0.024180 (Recompensa/Pasos)\n",
            " 1732527/2000000: episode: 2249, duration: 26.571s, episode steps: 579, steps per second:  22, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.015621, mae: 2.309691, mean_q: 2.790676, mean_eps: 0.100000\n",
            "📈 Episodio 2250: Recompensa total (clipped): 21.000, Pasos: 818, Mean Reward Calculado: 0.025672 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2250, pasos: 1733345)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.86 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2250 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 21.00\n",
            "   Media últimos 100: 17.86 / 20.0\n",
            "   Mejor promedio histórico: 17.86\n",
            "   Estado: 📈 89.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1733345/2000000: episode: 2250, duration: 120.628s, episode steps: 818, steps per second:   7, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.017415, mae: 2.310578, mean_q: 2.787110, mean_eps: 0.100000\n",
            "📈 Episodio 2251: Recompensa total (clipped): 19.000, Pasos: 772, Mean Reward Calculado: 0.024611 (Recompensa/Pasos)\n",
            " 1734117/2000000: episode: 2251, duration: 35.269s, episode steps: 772, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.016596, mae: 2.324528, mean_q: 2.802962, mean_eps: 0.100000\n",
            "📈 Episodio 2252: Recompensa total (clipped): 23.000, Pasos: 970, Mean Reward Calculado: 0.023711 (Recompensa/Pasos)\n",
            " 1735087/2000000: episode: 2252, duration: 44.197s, episode steps: 970, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.018191, mae: 2.316073, mean_q: 2.793664, mean_eps: 0.100000\n",
            "📈 Episodio 2253: Recompensa total (clipped): 22.000, Pasos: 733, Mean Reward Calculado: 0.030014 (Recompensa/Pasos)\n",
            " 1735820/2000000: episode: 2253, duration: 33.873s, episode steps: 733, steps per second:  22, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.016399, mae: 2.317883, mean_q: 2.796420, mean_eps: 0.100000\n",
            "📈 Episodio 2254: Recompensa total (clipped): 22.000, Pasos: 853, Mean Reward Calculado: 0.025791 (Recompensa/Pasos)\n",
            " 1736673/2000000: episode: 2254, duration: 39.715s, episode steps: 853, steps per second:  21, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.017936, mae: 2.323027, mean_q: 2.801678, mean_eps: 0.100000\n",
            "📈 Episodio 2255: Recompensa total (clipped): 19.000, Pasos: 802, Mean Reward Calculado: 0.023691 (Recompensa/Pasos)\n",
            " 1737475/2000000: episode: 2255, duration: 36.867s, episode steps: 802, steps per second:  22, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.017684, mae: 2.311589, mean_q: 2.786780, mean_eps: 0.100000\n",
            "📈 Episodio 2256: Recompensa total (clipped): 22.000, Pasos: 863, Mean Reward Calculado: 0.025492 (Recompensa/Pasos)\n",
            " 1738338/2000000: episode: 2256, duration: 40.135s, episode steps: 863, steps per second:  22, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.018285, mae: 2.322741, mean_q: 2.800916, mean_eps: 0.100000\n",
            "📈 Episodio 2257: Recompensa total (clipped): 7.000, Pasos: 590, Mean Reward Calculado: 0.011864 (Recompensa/Pasos)\n",
            " 1738928/2000000: episode: 2257, duration: 27.344s, episode steps: 590, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.017556, mae: 2.324684, mean_q: 2.803889, mean_eps: 0.100000\n",
            "📈 Episodio 2258: Recompensa total (clipped): 20.000, Pasos: 957, Mean Reward Calculado: 0.020899 (Recompensa/Pasos)\n",
            " 1739885/2000000: episode: 2258, duration: 43.832s, episode steps: 957, steps per second:  22, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.017652, mae: 2.326479, mean_q: 2.804238, mean_eps: 0.100000\n",
            "📊 Paso 1,740,000/2,000,000 (87.0%) - 24.7 pasos/seg - ETA: 2.9h - Memoria: 15305.07 MB\n",
            "📈 Episodio 2259: Recompensa total (clipped): 16.000, Pasos: 649, Mean Reward Calculado: 0.024653 (Recompensa/Pasos)\n",
            " 1740534/2000000: episode: 2259, duration: 30.330s, episode steps: 649, steps per second:  21, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.016274, mae: 2.322146, mean_q: 2.801402, mean_eps: 0.100000\n",
            "📈 Episodio 2260: Recompensa total (clipped): 29.000, Pasos: 1297, Mean Reward Calculado: 0.022359 (Recompensa/Pasos)\n",
            " 1741831/2000000: episode: 2260, duration: 59.722s, episode steps: 1297, steps per second:  22, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.018768, mae: 2.325338, mean_q: 2.803896, mean_eps: 0.100000\n",
            "📈 Episodio 2261: Recompensa total (clipped): 7.000, Pasos: 589, Mean Reward Calculado: 0.011885 (Recompensa/Pasos)\n",
            " 1742420/2000000: episode: 2261, duration: 27.353s, episode steps: 589, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.018162, mae: 2.316980, mean_q: 2.793815, mean_eps: 0.100000\n",
            "📈 Episodio 2262: Recompensa total (clipped): 16.000, Pasos: 690, Mean Reward Calculado: 0.023188 (Recompensa/Pasos)\n",
            " 1743110/2000000: episode: 2262, duration: 32.101s, episode steps: 690, steps per second:  21, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.001 [0.000, 5.000],  loss: 0.017759, mae: 2.306713, mean_q: 2.785214, mean_eps: 0.100000\n",
            "📈 Episodio 2263: Recompensa total (clipped): 21.000, Pasos: 854, Mean Reward Calculado: 0.024590 (Recompensa/Pasos)\n",
            " 1743964/2000000: episode: 2263, duration: 39.405s, episode steps: 854, steps per second:  22, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.018854, mae: 2.319956, mean_q: 2.796562, mean_eps: 0.100000\n",
            "📈 Episodio 2264: Recompensa total (clipped): 18.000, Pasos: 912, Mean Reward Calculado: 0.019737 (Recompensa/Pasos)\n",
            " 1744876/2000000: episode: 2264, duration: 42.334s, episode steps: 912, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.017799, mae: 2.323220, mean_q: 2.803350, mean_eps: 0.100000\n",
            "📈 Episodio 2265: Recompensa total (clipped): 10.000, Pasos: 584, Mean Reward Calculado: 0.017123 (Recompensa/Pasos)\n",
            " 1745460/2000000: episode: 2265, duration: 27.333s, episode steps: 584, steps per second:  21, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.019880, mae: 2.330122, mean_q: 2.810540, mean_eps: 0.100000\n",
            "📈 Episodio 2266: Recompensa total (clipped): 16.000, Pasos: 763, Mean Reward Calculado: 0.020970 (Recompensa/Pasos)\n",
            " 1746223/2000000: episode: 2266, duration: 35.008s, episode steps: 763, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.016045, mae: 2.315640, mean_q: 2.791619, mean_eps: 0.100000\n",
            "📈 Episodio 2267: Recompensa total (clipped): 19.000, Pasos: 1174, Mean Reward Calculado: 0.016184 (Recompensa/Pasos)\n",
            " 1747397/2000000: episode: 2267, duration: 53.620s, episode steps: 1174, steps per second:  22, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.018720, mae: 2.321182, mean_q: 2.800650, mean_eps: 0.100000\n",
            "📈 Episodio 2268: Recompensa total (clipped): 14.000, Pasos: 627, Mean Reward Calculado: 0.022329 (Recompensa/Pasos)\n",
            " 1748024/2000000: episode: 2268, duration: 29.069s, episode steps: 627, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.017638, mae: 2.296876, mean_q: 2.768827, mean_eps: 0.100000\n",
            "📈 Episodio 2269: Recompensa total (clipped): 16.000, Pasos: 660, Mean Reward Calculado: 0.024242 (Recompensa/Pasos)\n",
            " 1748684/2000000: episode: 2269, duration: 30.911s, episode steps: 660, steps per second:  21, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.017305, mae: 2.323705, mean_q: 2.802365, mean_eps: 0.100000\n",
            "📈 Episodio 2270: Recompensa total (clipped): 19.000, Pasos: 719, Mean Reward Calculado: 0.026426 (Recompensa/Pasos)\n",
            " 1749403/2000000: episode: 2270, duration: 32.943s, episode steps: 719, steps per second:  22, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.166 [0.000, 5.000],  loss: 0.019944, mae: 2.321719, mean_q: 2.799612, mean_eps: 0.100000\n",
            "📈 Episodio 2271: Recompensa total (clipped): 29.000, Pasos: 1170, Mean Reward Calculado: 0.024786 (Recompensa/Pasos)\n",
            " 1750573/2000000: episode: 2271, duration: 54.016s, episode steps: 1170, steps per second:  22, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.016144, mae: 2.315945, mean_q: 2.794589, mean_eps: 0.100000\n",
            "📈 Episodio 2272: Recompensa total (clipped): 24.000, Pasos: 872, Mean Reward Calculado: 0.027523 (Recompensa/Pasos)\n",
            " 1751445/2000000: episode: 2272, duration: 40.127s, episode steps: 872, steps per second:  22, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.018082, mae: 2.334006, mean_q: 2.815247, mean_eps: 0.100000\n",
            "📈 Episodio 2273: Recompensa total (clipped): 10.000, Pasos: 498, Mean Reward Calculado: 0.020080 (Recompensa/Pasos)\n",
            " 1751943/2000000: episode: 2273, duration: 22.753s, episode steps: 498, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.016844, mae: 2.350025, mean_q: 2.834344, mean_eps: 0.100000\n",
            "📈 Episodio 2274: Recompensa total (clipped): 20.000, Pasos: 671, Mean Reward Calculado: 0.029806 (Recompensa/Pasos)\n",
            " 1752614/2000000: episode: 2274, duration: 30.927s, episode steps: 671, steps per second:  22, episode reward: 20.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014935, mae: 2.324563, mean_q: 2.802830, mean_eps: 0.100000\n",
            "📈 Episodio 2275: Recompensa total (clipped): 30.000, Pasos: 1123, Mean Reward Calculado: 0.026714 (Recompensa/Pasos)\n",
            " 1753737/2000000: episode: 2275, duration: 51.771s, episode steps: 1123, steps per second:  22, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.015834, mae: 2.328478, mean_q: 2.809665, mean_eps: 0.100000\n",
            "📈 Episodio 2276: Recompensa total (clipped): 9.000, Pasos: 447, Mean Reward Calculado: 0.020134 (Recompensa/Pasos)\n",
            " 1754184/2000000: episode: 2276, duration: 20.428s, episode steps: 447, steps per second:  22, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.017303, mae: 2.339481, mean_q: 2.819824, mean_eps: 0.100000\n",
            "📈 Episodio 2277: Recompensa total (clipped): 19.000, Pasos: 1019, Mean Reward Calculado: 0.018646 (Recompensa/Pasos)\n",
            " 1755203/2000000: episode: 2277, duration: 45.992s, episode steps: 1019, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.017533, mae: 2.336557, mean_q: 2.819098, mean_eps: 0.100000\n",
            "📈 Episodio 2278: Recompensa total (clipped): 26.000, Pasos: 994, Mean Reward Calculado: 0.026157 (Recompensa/Pasos)\n",
            " 1756197/2000000: episode: 2278, duration: 45.372s, episode steps: 994, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.015476, mae: 2.323886, mean_q: 2.801789, mean_eps: 0.100000\n",
            "📈 Episodio 2279: Recompensa total (clipped): 27.000, Pasos: 1103, Mean Reward Calculado: 0.024479 (Recompensa/Pasos)\n",
            " 1757300/2000000: episode: 2279, duration: 50.615s, episode steps: 1103, steps per second:  22, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.017476, mae: 2.329222, mean_q: 2.809142, mean_eps: 0.100000\n",
            "📈 Episodio 2280: Recompensa total (clipped): 33.000, Pasos: 1268, Mean Reward Calculado: 0.026025 (Recompensa/Pasos)\n",
            " 1758568/2000000: episode: 2280, duration: 57.569s, episode steps: 1268, steps per second:  22, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.018226, mae: 2.330773, mean_q: 2.812555, mean_eps: 0.100000\n",
            "📈 Episodio 2281: Recompensa total (clipped): 28.000, Pasos: 1056, Mean Reward Calculado: 0.026515 (Recompensa/Pasos)\n",
            " 1759624/2000000: episode: 2281, duration: 48.221s, episode steps: 1056, steps per second:  22, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.017677, mae: 2.324553, mean_q: 2.807250, mean_eps: 0.100000\n",
            "📊 Paso 1,760,000/2,000,000 (88.0%) - 24.6 pasos/seg - ETA: 2.7h - Memoria: 15320.78 MB\n",
            "📈 Episodio 2282: Recompensa total (clipped): 11.000, Pasos: 533, Mean Reward Calculado: 0.020638 (Recompensa/Pasos)\n",
            " 1760157/2000000: episode: 2282, duration: 24.540s, episode steps: 533, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.015550, mae: 2.323561, mean_q: 2.802355, mean_eps: 0.100000\n",
            "📈 Episodio 2283: Recompensa total (clipped): 23.000, Pasos: 871, Mean Reward Calculado: 0.026406 (Recompensa/Pasos)\n",
            " 1761028/2000000: episode: 2283, duration: 40.167s, episode steps: 871, steps per second:  22, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.014861, mae: 2.337827, mean_q: 2.820506, mean_eps: 0.100000\n",
            "📈 Episodio 2284: Recompensa total (clipped): 26.000, Pasos: 905, Mean Reward Calculado: 0.028729 (Recompensa/Pasos)\n",
            " 1761933/2000000: episode: 2284, duration: 41.647s, episode steps: 905, steps per second:  22, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.016653, mae: 2.339424, mean_q: 2.821333, mean_eps: 0.100000\n",
            "📈 Episodio 2285: Recompensa total (clipped): 25.000, Pasos: 960, Mean Reward Calculado: 0.026042 (Recompensa/Pasos)\n",
            " 1762893/2000000: episode: 2285, duration: 43.780s, episode steps: 960, steps per second:  22, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.017286, mae: 2.346280, mean_q: 2.830504, mean_eps: 0.100000\n",
            "📈 Episodio 2286: Recompensa total (clipped): 24.000, Pasos: 1006, Mean Reward Calculado: 0.023857 (Recompensa/Pasos)\n",
            " 1763899/2000000: episode: 2286, duration: 46.134s, episode steps: 1006, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.016196, mae: 2.370146, mean_q: 2.860264, mean_eps: 0.100000\n",
            "📈 Episodio 2287: Recompensa total (clipped): 24.000, Pasos: 986, Mean Reward Calculado: 0.024341 (Recompensa/Pasos)\n",
            " 1764885/2000000: episode: 2287, duration: 45.044s, episode steps: 986, steps per second:  22, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.018533, mae: 2.346828, mean_q: 2.830279, mean_eps: 0.100000\n",
            "📈 Episodio 2288: Recompensa total (clipped): 20.000, Pasos: 818, Mean Reward Calculado: 0.024450 (Recompensa/Pasos)\n",
            " 1765703/2000000: episode: 2288, duration: 37.261s, episode steps: 818, steps per second:  22, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.017406, mae: 2.377606, mean_q: 2.866820, mean_eps: 0.100000\n",
            "📈 Episodio 2289: Recompensa total (clipped): 25.000, Pasos: 954, Mean Reward Calculado: 0.026205 (Recompensa/Pasos)\n",
            " 1766657/2000000: episode: 2289, duration: 44.063s, episode steps: 954, steps per second:  22, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.016333, mae: 2.341278, mean_q: 2.823749, mean_eps: 0.100000\n",
            "📈 Episodio 2290: Recompensa total (clipped): 27.000, Pasos: 967, Mean Reward Calculado: 0.027921 (Recompensa/Pasos)\n",
            " 1767624/2000000: episode: 2290, duration: 44.794s, episode steps: 967, steps per second:  22, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.018372, mae: 2.363483, mean_q: 2.850508, mean_eps: 0.100000\n",
            "📈 Episodio 2291: Recompensa total (clipped): 16.000, Pasos: 956, Mean Reward Calculado: 0.016736 (Recompensa/Pasos)\n",
            " 1768580/2000000: episode: 2291, duration: 43.400s, episode steps: 956, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.016925, mae: 2.348205, mean_q: 2.831084, mean_eps: 0.100000\n",
            "📈 Episodio 2292: Recompensa total (clipped): 12.000, Pasos: 399, Mean Reward Calculado: 0.030075 (Recompensa/Pasos)\n",
            " 1768979/2000000: episode: 2292, duration: 17.878s, episode steps: 399, steps per second:  22, episode reward: 12.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.017049, mae: 2.378615, mean_q: 2.868068, mean_eps: 0.100000\n",
            "📈 Episodio 2293: Recompensa total (clipped): 19.000, Pasos: 834, Mean Reward Calculado: 0.022782 (Recompensa/Pasos)\n",
            " 1769813/2000000: episode: 2293, duration: 37.911s, episode steps: 834, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.015904, mae: 2.345143, mean_q: 2.827017, mean_eps: 0.100000\n",
            "📈 Episodio 2294: Recompensa total (clipped): 4.000, Pasos: 496, Mean Reward Calculado: 0.008065 (Recompensa/Pasos)\n",
            " 1770309/2000000: episode: 2294, duration: 22.481s, episode steps: 496, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.014767, mae: 2.334064, mean_q: 2.814427, mean_eps: 0.100000\n",
            "📈 Episodio 2295: Recompensa total (clipped): 15.000, Pasos: 698, Mean Reward Calculado: 0.021490 (Recompensa/Pasos)\n",
            " 1771007/2000000: episode: 2295, duration: 31.712s, episode steps: 698, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.755 [0.000, 5.000],  loss: 0.014961, mae: 2.325926, mean_q: 2.804746, mean_eps: 0.100000\n",
            "📈 Episodio 2296: Recompensa total (clipped): 13.000, Pasos: 630, Mean Reward Calculado: 0.020635 (Recompensa/Pasos)\n",
            " 1771637/2000000: episode: 2296, duration: 28.669s, episode steps: 630, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.016715, mae: 2.329250, mean_q: 2.809202, mean_eps: 0.100000\n",
            "📈 Episodio 2297: Recompensa total (clipped): 31.000, Pasos: 1168, Mean Reward Calculado: 0.026541 (Recompensa/Pasos)\n",
            " 1772805/2000000: episode: 2297, duration: 53.065s, episode steps: 1168, steps per second:  22, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.018637, mae: 2.329939, mean_q: 2.809005, mean_eps: 0.100000\n",
            "📈 Episodio 2298: Recompensa total (clipped): 28.000, Pasos: 1062, Mean Reward Calculado: 0.026365 (Recompensa/Pasos)\n",
            " 1773867/2000000: episode: 2298, duration: 48.630s, episode steps: 1062, steps per second:  22, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.016346, mae: 2.338892, mean_q: 2.819874, mean_eps: 0.100000\n",
            "📈 Episodio 2299: Recompensa total (clipped): 33.000, Pasos: 1348, Mean Reward Calculado: 0.024481 (Recompensa/Pasos)\n",
            " 1775215/2000000: episode: 2299, duration: 62.091s, episode steps: 1348, steps per second:  22, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.017126, mae: 2.336125, mean_q: 2.816127, mean_eps: 0.100000\n",
            "📈 Episodio 2300: Recompensa total (clipped): 21.000, Pasos: 887, Mean Reward Calculado: 0.023675 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2300, pasos: 1776102)\n",
            "💾 NUEVO MEJOR PROMEDIO: 19.71 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2300 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 21.00\n",
            "   Media últimos 100: 19.71 / 20.0\n",
            "   Mejor promedio histórico: 19.71\n",
            "   Estado: 📈 98.6% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1776102/2000000: episode: 2300, duration: 107.576s, episode steps: 887, steps per second:   8, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.015893, mae: 2.318459, mean_q: 2.795508, mean_eps: 0.100000\n",
            "📈 Episodio 2301: Recompensa total (clipped): 5.000, Pasos: 372, Mean Reward Calculado: 0.013441 (Recompensa/Pasos)\n",
            " 1776474/2000000: episode: 2301, duration: 16.973s, episode steps: 372, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.032 [0.000, 5.000],  loss: 0.016565, mae: 2.361463, mean_q: 2.848431, mean_eps: 0.100000\n",
            "📈 Episodio 2302: Recompensa total (clipped): 20.000, Pasos: 945, Mean Reward Calculado: 0.021164 (Recompensa/Pasos)\n",
            " 1777419/2000000: episode: 2302, duration: 42.673s, episode steps: 945, steps per second:  22, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.017227, mae: 2.345093, mean_q: 2.829422, mean_eps: 0.100000\n",
            "📈 Episodio 2303: Recompensa total (clipped): 8.000, Pasos: 415, Mean Reward Calculado: 0.019277 (Recompensa/Pasos)\n",
            " 1777834/2000000: episode: 2303, duration: 18.797s, episode steps: 415, steps per second:  22, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.020605, mae: 2.345891, mean_q: 2.829147, mean_eps: 0.100000\n",
            "📈 Episodio 2304: Recompensa total (clipped): 22.000, Pasos: 1003, Mean Reward Calculado: 0.021934 (Recompensa/Pasos)\n",
            " 1778837/2000000: episode: 2304, duration: 45.436s, episode steps: 1003, steps per second:  22, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.019054, mae: 2.334126, mean_q: 2.816253, mean_eps: 0.100000\n",
            "📈 Episodio 2305: Recompensa total (clipped): 11.000, Pasos: 734, Mean Reward Calculado: 0.014986 (Recompensa/Pasos)\n",
            " 1779571/2000000: episode: 2305, duration: 33.369s, episode steps: 734, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.129 [0.000, 5.000],  loss: 0.016994, mae: 2.336169, mean_q: 2.816513, mean_eps: 0.100000\n",
            "📊 Paso 1,780,000/2,000,000 (89.0%) - 24.6 pasos/seg - ETA: 2.5h - Memoria: 15290.38 MB\n",
            "📈 Episodio 2306: Recompensa total (clipped): 29.000, Pasos: 928, Mean Reward Calculado: 0.031250 (Recompensa/Pasos)\n",
            " 1780499/2000000: episode: 2306, duration: 42.182s, episode steps: 928, steps per second:  22, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.017211, mae: 2.339130, mean_q: 2.820809, mean_eps: 0.100000\n",
            "📈 Episodio 2307: Recompensa total (clipped): 24.000, Pasos: 926, Mean Reward Calculado: 0.025918 (Recompensa/Pasos)\n",
            " 1781425/2000000: episode: 2307, duration: 42.072s, episode steps: 926, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.017050, mae: 2.338252, mean_q: 2.819261, mean_eps: 0.100000\n",
            "📈 Episodio 2308: Recompensa total (clipped): 21.000, Pasos: 963, Mean Reward Calculado: 0.021807 (Recompensa/Pasos)\n",
            " 1782388/2000000: episode: 2308, duration: 43.746s, episode steps: 963, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.017060, mae: 2.342758, mean_q: 2.824956, mean_eps: 0.100000\n",
            "📈 Episodio 2309: Recompensa total (clipped): 12.000, Pasos: 647, Mean Reward Calculado: 0.018547 (Recompensa/Pasos)\n",
            " 1783035/2000000: episode: 2309, duration: 29.582s, episode steps: 647, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.017495, mae: 2.342035, mean_q: 2.823853, mean_eps: 0.100000\n",
            "📈 Episodio 2310: Recompensa total (clipped): 10.000, Pasos: 753, Mean Reward Calculado: 0.013280 (Recompensa/Pasos)\n",
            " 1783788/2000000: episode: 2310, duration: 34.312s, episode steps: 753, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.018453, mae: 2.345032, mean_q: 2.825382, mean_eps: 0.100000\n",
            "📈 Episodio 2311: Recompensa total (clipped): 25.000, Pasos: 956, Mean Reward Calculado: 0.026151 (Recompensa/Pasos)\n",
            " 1784744/2000000: episode: 2311, duration: 43.992s, episode steps: 956, steps per second:  22, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.017045, mae: 2.332916, mean_q: 2.814317, mean_eps: 0.100000\n",
            "📈 Episodio 2312: Recompensa total (clipped): 26.000, Pasos: 1000, Mean Reward Calculado: 0.026000 (Recompensa/Pasos)\n",
            " 1785744/2000000: episode: 2312, duration: 45.626s, episode steps: 1000, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.017339, mae: 2.346713, mean_q: 2.830960, mean_eps: 0.100000\n",
            "📈 Episodio 2313: Recompensa total (clipped): 26.000, Pasos: 1046, Mean Reward Calculado: 0.024857 (Recompensa/Pasos)\n",
            " 1786790/2000000: episode: 2313, duration: 47.983s, episode steps: 1046, steps per second:  22, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.019611, mae: 2.329962, mean_q: 2.811087, mean_eps: 0.100000\n",
            "📈 Episodio 2314: Recompensa total (clipped): 19.000, Pasos: 1060, Mean Reward Calculado: 0.017925 (Recompensa/Pasos)\n",
            " 1787850/2000000: episode: 2314, duration: 48.001s, episode steps: 1060, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.017848, mae: 2.335151, mean_q: 2.815232, mean_eps: 0.100000\n",
            "📈 Episodio 2315: Recompensa total (clipped): 7.000, Pasos: 473, Mean Reward Calculado: 0.014799 (Recompensa/Pasos)\n",
            " 1788323/2000000: episode: 2315, duration: 21.383s, episode steps: 473, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.408 [0.000, 5.000],  loss: 0.017503, mae: 2.325533, mean_q: 2.803566, mean_eps: 0.100000\n",
            "📈 Episodio 2316: Recompensa total (clipped): 28.000, Pasos: 1170, Mean Reward Calculado: 0.023932 (Recompensa/Pasos)\n",
            " 1789493/2000000: episode: 2316, duration: 52.979s, episode steps: 1170, steps per second:  22, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.017235, mae: 2.345627, mean_q: 2.827272, mean_eps: 0.100000\n",
            "📈 Episodio 2317: Recompensa total (clipped): 11.000, Pasos: 512, Mean Reward Calculado: 0.021484 (Recompensa/Pasos)\n",
            " 1790005/2000000: episode: 2317, duration: 23.429s, episode steps: 512, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.017284, mae: 2.343228, mean_q: 2.823407, mean_eps: 0.100000\n",
            "📈 Episodio 2318: Recompensa total (clipped): 23.000, Pasos: 1148, Mean Reward Calculado: 0.020035 (Recompensa/Pasos)\n",
            " 1791153/2000000: episode: 2318, duration: 51.593s, episode steps: 1148, steps per second:  22, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.016127, mae: 2.371463, mean_q: 2.860495, mean_eps: 0.100000\n",
            "📈 Episodio 2319: Recompensa total (clipped): 21.000, Pasos: 851, Mean Reward Calculado: 0.024677 (Recompensa/Pasos)\n",
            " 1792004/2000000: episode: 2319, duration: 38.034s, episode steps: 851, steps per second:  22, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.016439, mae: 2.380513, mean_q: 2.872954, mean_eps: 0.100000\n",
            "📈 Episodio 2320: Recompensa total (clipped): 28.000, Pasos: 952, Mean Reward Calculado: 0.029412 (Recompensa/Pasos)\n",
            " 1792956/2000000: episode: 2320, duration: 43.317s, episode steps: 952, steps per second:  22, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.015954, mae: 2.365562, mean_q: 2.851203, mean_eps: 0.100000\n",
            "📈 Episodio 2321: Recompensa total (clipped): 10.000, Pasos: 513, Mean Reward Calculado: 0.019493 (Recompensa/Pasos)\n",
            " 1793469/2000000: episode: 2321, duration: 23.397s, episode steps: 513, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.017769, mae: 2.378541, mean_q: 2.867716, mean_eps: 0.100000\n",
            "📈 Episodio 2322: Recompensa total (clipped): 21.000, Pasos: 903, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            " 1794372/2000000: episode: 2322, duration: 40.958s, episode steps: 903, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.017941, mae: 2.367124, mean_q: 2.855260, mean_eps: 0.100000\n",
            "📈 Episodio 2323: Recompensa total (clipped): 26.000, Pasos: 971, Mean Reward Calculado: 0.026777 (Recompensa/Pasos)\n",
            " 1795343/2000000: episode: 2323, duration: 44.431s, episode steps: 971, steps per second:  22, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.016617, mae: 2.367652, mean_q: 2.854994, mean_eps: 0.100000\n",
            "📈 Episodio 2324: Recompensa total (clipped): 12.000, Pasos: 761, Mean Reward Calculado: 0.015769 (Recompensa/Pasos)\n",
            " 1796104/2000000: episode: 2324, duration: 34.537s, episode steps: 761, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.016751, mae: 2.353549, mean_q: 2.837859, mean_eps: 0.100000\n",
            "📈 Episodio 2325: Recompensa total (clipped): 21.000, Pasos: 893, Mean Reward Calculado: 0.023516 (Recompensa/Pasos)\n",
            " 1796997/2000000: episode: 2325, duration: 40.445s, episode steps: 893, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.017264, mae: 2.348418, mean_q: 2.829041, mean_eps: 0.100000\n",
            "📈 Episodio 2326: Recompensa total (clipped): 16.000, Pasos: 751, Mean Reward Calculado: 0.021305 (Recompensa/Pasos)\n",
            " 1797748/2000000: episode: 2326, duration: 33.830s, episode steps: 751, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.018698, mae: 2.369235, mean_q: 2.853866, mean_eps: 0.100000\n",
            "📈 Episodio 2327: Recompensa total (clipped): 17.000, Pasos: 936, Mean Reward Calculado: 0.018162 (Recompensa/Pasos)\n",
            " 1798684/2000000: episode: 2327, duration: 43.327s, episode steps: 936, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.017959, mae: 2.367394, mean_q: 2.854141, mean_eps: 0.100000\n",
            "📈 Episodio 2328: Recompensa total (clipped): 17.000, Pasos: 971, Mean Reward Calculado: 0.017508 (Recompensa/Pasos)\n",
            " 1799655/2000000: episode: 2328, duration: 43.860s, episode steps: 971, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.017616, mae: 2.361226, mean_q: 2.845945, mean_eps: 0.100000\n",
            "📊 Paso 1,800,000/2,000,000 (90.0%) - 24.5 pasos/seg - ETA: 2.3h - Memoria: 15271.64 MB\n",
            "📈 Episodio 2329: Recompensa total (clipped): 17.000, Pasos: 960, Mean Reward Calculado: 0.017708 (Recompensa/Pasos)\n",
            " 1800615/2000000: episode: 2329, duration: 43.047s, episode steps: 960, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.016723, mae: 2.345230, mean_q: 2.827971, mean_eps: 0.100000\n",
            "📈 Episodio 2330: Recompensa total (clipped): 18.000, Pasos: 868, Mean Reward Calculado: 0.020737 (Recompensa/Pasos)\n",
            " 1801483/2000000: episode: 2330, duration: 39.020s, episode steps: 868, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.017787, mae: 2.375163, mean_q: 2.864503, mean_eps: 0.100000\n",
            "📈 Episodio 2331: Recompensa total (clipped): 17.000, Pasos: 752, Mean Reward Calculado: 0.022606 (Recompensa/Pasos)\n",
            " 1802235/2000000: episode: 2331, duration: 33.786s, episode steps: 752, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017709, mae: 2.360586, mean_q: 2.844021, mean_eps: 0.100000\n",
            "📈 Episodio 2332: Recompensa total (clipped): 14.000, Pasos: 525, Mean Reward Calculado: 0.026667 (Recompensa/Pasos)\n",
            " 1802760/2000000: episode: 2332, duration: 24.063s, episode steps: 525, steps per second:  22, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.015385, mae: 2.365601, mean_q: 2.850385, mean_eps: 0.100000\n",
            "📈 Episodio 2333: Recompensa total (clipped): 16.000, Pasos: 630, Mean Reward Calculado: 0.025397 (Recompensa/Pasos)\n",
            " 1803390/2000000: episode: 2333, duration: 28.734s, episode steps: 630, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.990 [0.000, 5.000],  loss: 0.017410, mae: 2.347285, mean_q: 2.829469, mean_eps: 0.100000\n",
            "📈 Episodio 2334: Recompensa total (clipped): 24.000, Pasos: 910, Mean Reward Calculado: 0.026374 (Recompensa/Pasos)\n",
            " 1804300/2000000: episode: 2334, duration: 41.360s, episode steps: 910, steps per second:  22, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.015645, mae: 2.372490, mean_q: 2.859044, mean_eps: 0.100000\n",
            "📈 Episodio 2335: Recompensa total (clipped): 13.000, Pasos: 673, Mean Reward Calculado: 0.019316 (Recompensa/Pasos)\n",
            " 1804973/2000000: episode: 2335, duration: 30.492s, episode steps: 673, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.018175, mae: 2.356828, mean_q: 2.838534, mean_eps: 0.100000\n",
            "📈 Episodio 2336: Recompensa total (clipped): 31.000, Pasos: 1179, Mean Reward Calculado: 0.026293 (Recompensa/Pasos)\n",
            " 1806152/2000000: episode: 2336, duration: 53.784s, episode steps: 1179, steps per second:  22, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.016168, mae: 2.350924, mean_q: 2.833798, mean_eps: 0.100000\n",
            "📈 Episodio 2337: Recompensa total (clipped): 32.000, Pasos: 1202, Mean Reward Calculado: 0.026622 (Recompensa/Pasos)\n",
            " 1807354/2000000: episode: 2337, duration: 54.545s, episode steps: 1202, steps per second:  22, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.017422, mae: 2.362183, mean_q: 2.848085, mean_eps: 0.100000\n",
            "📈 Episodio 2338: Recompensa total (clipped): 9.000, Pasos: 506, Mean Reward Calculado: 0.017787 (Recompensa/Pasos)\n",
            " 1807860/2000000: episode: 2338, duration: 23.288s, episode steps: 506, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.017582, mae: 2.357604, mean_q: 2.839976, mean_eps: 0.100000\n",
            "📈 Episodio 2339: Recompensa total (clipped): 14.000, Pasos: 791, Mean Reward Calculado: 0.017699 (Recompensa/Pasos)\n",
            " 1808651/2000000: episode: 2339, duration: 35.844s, episode steps: 791, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.016296, mae: 2.368605, mean_q: 2.855042, mean_eps: 0.100000\n",
            "📈 Episodio 2340: Recompensa total (clipped): 9.000, Pasos: 583, Mean Reward Calculado: 0.015437 (Recompensa/Pasos)\n",
            " 1809234/2000000: episode: 2340, duration: 26.581s, episode steps: 583, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.017588, mae: 2.352789, mean_q: 2.836187, mean_eps: 0.100000\n",
            "📈 Episodio 2341: Recompensa total (clipped): 35.000, Pasos: 1632, Mean Reward Calculado: 0.021446 (Recompensa/Pasos)\n",
            " 1810866/2000000: episode: 2341, duration: 73.646s, episode steps: 1632, steps per second:  22, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.016734, mae: 2.357794, mean_q: 2.842990, mean_eps: 0.100000\n",
            "📈 Episodio 2342: Recompensa total (clipped): 17.000, Pasos: 705, Mean Reward Calculado: 0.024113 (Recompensa/Pasos)\n",
            " 1811571/2000000: episode: 2342, duration: 32.468s, episode steps: 705, steps per second:  22, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.018776, mae: 2.349978, mean_q: 2.831291, mean_eps: 0.100000\n",
            "📈 Episodio 2343: Recompensa total (clipped): 5.000, Pasos: 357, Mean Reward Calculado: 0.014006 (Recompensa/Pasos)\n",
            " 1811928/2000000: episode: 2343, duration: 16.640s, episode steps: 357, steps per second:  21, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.017217, mae: 2.365078, mean_q: 2.849103, mean_eps: 0.100000\n",
            "📈 Episodio 2344: Recompensa total (clipped): 28.000, Pasos: 1081, Mean Reward Calculado: 0.025902 (Recompensa/Pasos)\n",
            " 1813009/2000000: episode: 2344, duration: 49.292s, episode steps: 1081, steps per second:  22, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.018881, mae: 2.353572, mean_q: 2.837162, mean_eps: 0.100000\n",
            "📈 Episodio 2345: Recompensa total (clipped): 26.000, Pasos: 1019, Mean Reward Calculado: 0.025515 (Recompensa/Pasos)\n",
            " 1814028/2000000: episode: 2345, duration: 45.973s, episode steps: 1019, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.016717, mae: 2.347792, mean_q: 2.830488, mean_eps: 0.100000\n",
            "📈 Episodio 2346: Recompensa total (clipped): 18.000, Pasos: 642, Mean Reward Calculado: 0.028037 (Recompensa/Pasos)\n",
            " 1814670/2000000: episode: 2346, duration: 29.450s, episode steps: 642, steps per second:  22, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.016604, mae: 2.376442, mean_q: 2.864410, mean_eps: 0.100000\n",
            "📈 Episodio 2347: Recompensa total (clipped): 35.000, Pasos: 1682, Mean Reward Calculado: 0.020809 (Recompensa/Pasos)\n",
            " 1816352/2000000: episode: 2347, duration: 77.159s, episode steps: 1682, steps per second:  22, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.017813, mae: 2.351242, mean_q: 2.833596, mean_eps: 0.100000\n",
            "📈 Episodio 2348: Recompensa total (clipped): 17.000, Pasos: 779, Mean Reward Calculado: 0.021823 (Recompensa/Pasos)\n",
            " 1817131/2000000: episode: 2348, duration: 35.829s, episode steps: 779, steps per second:  22, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.017874, mae: 2.340950, mean_q: 2.827006, mean_eps: 0.100000\n",
            "📈 Episodio 2349: Recompensa total (clipped): 9.000, Pasos: 632, Mean Reward Calculado: 0.014241 (Recompensa/Pasos)\n",
            " 1817763/2000000: episode: 2349, duration: 28.427s, episode steps: 632, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.018271, mae: 2.353810, mean_q: 2.837230, mean_eps: 0.100000\n",
            "📈 Episodio 2350: Recompensa total (clipped): 22.000, Pasos: 865, Mean Reward Calculado: 0.025434 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2350, pasos: 1818628)\n",
            "💾 NUEVO MEJOR PROMEDIO: 19.49 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2350 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 22.00\n",
            "   Media últimos 100: 19.49 / 20.0\n",
            "   Mejor promedio histórico: 19.49\n",
            "   Estado: 📈 97.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1818628/2000000: episode: 2350, duration: 276.376s, episode steps: 865, steps per second:   3, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.019652, mae: 2.347340, mean_q: 2.829968, mean_eps: 0.100000\n",
            "📈 Episodio 2351: Recompensa total (clipped): 8.000, Pasos: 622, Mean Reward Calculado: 0.012862 (Recompensa/Pasos)\n",
            " 1819250/2000000: episode: 2351, duration: 28.661s, episode steps: 622, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016513, mae: 2.353026, mean_q: 2.836169, mean_eps: 0.100000\n",
            "📊 Paso 1,820,000/2,000,000 (91.0%) - 24.4 pasos/seg - ETA: 2.0h - Memoria: 15297.18 MB\n",
            "📈 Episodio 2352: Recompensa total (clipped): 15.000, Pasos: 829, Mean Reward Calculado: 0.018094 (Recompensa/Pasos)\n",
            " 1820079/2000000: episode: 2352, duration: 37.833s, episode steps: 829, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.016389, mae: 2.347331, mean_q: 2.832112, mean_eps: 0.100000\n",
            "📈 Episodio 2353: Recompensa total (clipped): 7.000, Pasos: 509, Mean Reward Calculado: 0.013752 (Recompensa/Pasos)\n",
            " 1820588/2000000: episode: 2353, duration: 23.247s, episode steps: 509, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.016699, mae: 2.374550, mean_q: 2.861593, mean_eps: 0.100000\n",
            "📈 Episodio 2354: Recompensa total (clipped): 24.000, Pasos: 1070, Mean Reward Calculado: 0.022430 (Recompensa/Pasos)\n",
            " 1821658/2000000: episode: 2354, duration: 49.335s, episode steps: 1070, steps per second:  22, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.017230, mae: 2.356356, mean_q: 2.841353, mean_eps: 0.100000\n",
            "📈 Episodio 2355: Recompensa total (clipped): 13.000, Pasos: 623, Mean Reward Calculado: 0.020867 (Recompensa/Pasos)\n",
            " 1822281/2000000: episode: 2355, duration: 28.297s, episode steps: 623, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.015880, mae: 2.366614, mean_q: 2.852309, mean_eps: 0.100000\n",
            "📈 Episodio 2356: Recompensa total (clipped): 34.000, Pasos: 1309, Mean Reward Calculado: 0.025974 (Recompensa/Pasos)\n",
            " 1823590/2000000: episode: 2356, duration: 59.057s, episode steps: 1309, steps per second:  22, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.018178, mae: 2.366763, mean_q: 2.853053, mean_eps: 0.100000\n",
            "📈 Episodio 2357: Recompensa total (clipped): 10.000, Pasos: 664, Mean Reward Calculado: 0.015060 (Recompensa/Pasos)\n",
            " 1824254/2000000: episode: 2357, duration: 30.549s, episode steps: 664, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.017556, mae: 2.393713, mean_q: 2.886288, mean_eps: 0.100000\n",
            "📈 Episodio 2358: Recompensa total (clipped): 23.000, Pasos: 973, Mean Reward Calculado: 0.023638 (Recompensa/Pasos)\n",
            " 1825227/2000000: episode: 2358, duration: 43.791s, episode steps: 973, steps per second:  22, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.017038, mae: 2.377825, mean_q: 2.867105, mean_eps: 0.100000\n",
            "📈 Episodio 2359: Recompensa total (clipped): 7.000, Pasos: 434, Mean Reward Calculado: 0.016129 (Recompensa/Pasos)\n",
            " 1825661/2000000: episode: 2359, duration: 19.571s, episode steps: 434, steps per second:  22, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.017680, mae: 2.355464, mean_q: 2.836939, mean_eps: 0.100000\n",
            "📈 Episodio 2360: Recompensa total (clipped): 19.000, Pasos: 830, Mean Reward Calculado: 0.022892 (Recompensa/Pasos)\n",
            " 1826491/2000000: episode: 2360, duration: 37.474s, episode steps: 830, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.017644, mae: 2.361497, mean_q: 2.844111, mean_eps: 0.100000\n",
            "📈 Episodio 2361: Recompensa total (clipped): 13.000, Pasos: 603, Mean Reward Calculado: 0.021559 (Recompensa/Pasos)\n",
            " 1827094/2000000: episode: 2361, duration: 27.385s, episode steps: 603, steps per second:  22, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.017314, mae: 2.363282, mean_q: 2.847167, mean_eps: 0.100000\n",
            "📈 Episodio 2362: Recompensa total (clipped): 19.000, Pasos: 905, Mean Reward Calculado: 0.020994 (Recompensa/Pasos)\n",
            " 1827999/2000000: episode: 2362, duration: 41.650s, episode steps: 905, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.017325, mae: 2.367820, mean_q: 2.855852, mean_eps: 0.100000\n",
            "📈 Episodio 2363: Recompensa total (clipped): 24.000, Pasos: 1508, Mean Reward Calculado: 0.015915 (Recompensa/Pasos)\n",
            " 1829507/2000000: episode: 2363, duration: 68.614s, episode steps: 1508, steps per second:  22, episode reward: 24.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.017896, mae: 2.371331, mean_q: 2.857727, mean_eps: 0.100000\n",
            "📈 Episodio 2364: Recompensa total (clipped): 14.000, Pasos: 627, Mean Reward Calculado: 0.022329 (Recompensa/Pasos)\n",
            " 1830134/2000000: episode: 2364, duration: 28.259s, episode steps: 627, steps per second:  22, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.016821, mae: 2.360905, mean_q: 2.844117, mean_eps: 0.100000\n",
            "📈 Episodio 2365: Recompensa total (clipped): 17.000, Pasos: 752, Mean Reward Calculado: 0.022606 (Recompensa/Pasos)\n",
            " 1830886/2000000: episode: 2365, duration: 34.227s, episode steps: 752, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.016586, mae: 2.351877, mean_q: 2.835827, mean_eps: 0.100000\n",
            "📈 Episodio 2366: Recompensa total (clipped): 6.000, Pasos: 500, Mean Reward Calculado: 0.012000 (Recompensa/Pasos)\n",
            " 1831386/2000000: episode: 2366, duration: 22.552s, episode steps: 500, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.860 [0.000, 5.000],  loss: 0.016459, mae: 2.371750, mean_q: 2.860987, mean_eps: 0.100000\n",
            "📈 Episodio 2367: Recompensa total (clipped): 7.000, Pasos: 482, Mean Reward Calculado: 0.014523 (Recompensa/Pasos)\n",
            " 1831868/2000000: episode: 2367, duration: 22.183s, episode steps: 482, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.016054, mae: 2.356291, mean_q: 2.841547, mean_eps: 0.100000\n",
            "📈 Episodio 2368: Recompensa total (clipped): 8.000, Pasos: 493, Mean Reward Calculado: 0.016227 (Recompensa/Pasos)\n",
            " 1832361/2000000: episode: 2368, duration: 22.719s, episode steps: 493, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.019071, mae: 2.395078, mean_q: 2.887629, mean_eps: 0.100000\n",
            "📈 Episodio 2369: Recompensa total (clipped): 28.000, Pasos: 1035, Mean Reward Calculado: 0.027053 (Recompensa/Pasos)\n",
            " 1833396/2000000: episode: 2369, duration: 47.370s, episode steps: 1035, steps per second:  22, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.015928, mae: 2.356170, mean_q: 2.841373, mean_eps: 0.100000\n",
            "📈 Episodio 2370: Recompensa total (clipped): 23.000, Pasos: 854, Mean Reward Calculado: 0.026932 (Recompensa/Pasos)\n",
            " 1834250/2000000: episode: 2370, duration: 38.348s, episode steps: 854, steps per second:  22, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.018338, mae: 2.372918, mean_q: 2.861400, mean_eps: 0.100000\n",
            "📈 Episodio 2371: Recompensa total (clipped): 16.000, Pasos: 1296, Mean Reward Calculado: 0.012346 (Recompensa/Pasos)\n",
            " 1835546/2000000: episode: 2371, duration: 58.769s, episode steps: 1296, steps per second:  22, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.016553, mae: 2.372872, mean_q: 2.859579, mean_eps: 0.100000\n",
            "📈 Episodio 2372: Recompensa total (clipped): 25.000, Pasos: 982, Mean Reward Calculado: 0.025458 (Recompensa/Pasos)\n",
            " 1836528/2000000: episode: 2372, duration: 45.017s, episode steps: 982, steps per second:  22, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.017577, mae: 2.354696, mean_q: 2.838786, mean_eps: 0.100000\n",
            "📈 Episodio 2373: Recompensa total (clipped): 11.000, Pasos: 738, Mean Reward Calculado: 0.014905 (Recompensa/Pasos)\n",
            " 1837266/2000000: episode: 2373, duration: 33.368s, episode steps: 738, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.017526, mae: 2.357344, mean_q: 2.841841, mean_eps: 0.100000\n",
            "📈 Episodio 2374: Recompensa total (clipped): 21.000, Pasos: 817, Mean Reward Calculado: 0.025704 (Recompensa/Pasos)\n",
            " 1838083/2000000: episode: 2374, duration: 36.745s, episode steps: 817, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.018547, mae: 2.362954, mean_q: 2.846909, mean_eps: 0.100000\n",
            "📈 Episodio 2375: Recompensa total (clipped): 21.000, Pasos: 822, Mean Reward Calculado: 0.025547 (Recompensa/Pasos)\n",
            " 1838905/2000000: episode: 2375, duration: 37.233s, episode steps: 822, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.016508, mae: 2.351860, mean_q: 2.834386, mean_eps: 0.100000\n",
            "📈 Episodio 2376: Recompensa total (clipped): 17.000, Pasos: 869, Mean Reward Calculado: 0.019563 (Recompensa/Pasos)\n",
            " 1839774/2000000: episode: 2376, duration: 38.993s, episode steps: 869, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.018124, mae: 2.370298, mean_q: 2.856526, mean_eps: 0.100000\n",
            "📊 Paso 1,840,000/2,000,000 (92.0%) - 24.4 pasos/seg - ETA: 1.8h - Memoria: 15291.19 MB\n",
            "📈 Episodio 2377: Recompensa total (clipped): 18.000, Pasos: 740, Mean Reward Calculado: 0.024324 (Recompensa/Pasos)\n",
            " 1840514/2000000: episode: 2377, duration: 33.676s, episode steps: 740, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.015472, mae: 2.366890, mean_q: 2.854425, mean_eps: 0.100000\n",
            "📈 Episodio 2378: Recompensa total (clipped): 17.000, Pasos: 893, Mean Reward Calculado: 0.019037 (Recompensa/Pasos)\n",
            " 1841407/2000000: episode: 2378, duration: 40.603s, episode steps: 893, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.015798, mae: 2.368614, mean_q: 2.854017, mean_eps: 0.100000\n",
            "📈 Episodio 2379: Recompensa total (clipped): 21.000, Pasos: 920, Mean Reward Calculado: 0.022826 (Recompensa/Pasos)\n",
            " 1842327/2000000: episode: 2379, duration: 41.471s, episode steps: 920, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.016511, mae: 2.372273, mean_q: 2.863109, mean_eps: 0.100000\n",
            "📈 Episodio 2380: Recompensa total (clipped): 17.000, Pasos: 717, Mean Reward Calculado: 0.023710 (Recompensa/Pasos)\n",
            " 1843044/2000000: episode: 2380, duration: 33.031s, episode steps: 717, steps per second:  22, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017307, mae: 2.358672, mean_q: 2.843252, mean_eps: 0.100000\n",
            "📈 Episodio 2381: Recompensa total (clipped): 19.000, Pasos: 820, Mean Reward Calculado: 0.023171 (Recompensa/Pasos)\n",
            " 1843864/2000000: episode: 2381, duration: 37.442s, episode steps: 820, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.017011, mae: 2.373641, mean_q: 2.861906, mean_eps: 0.100000\n",
            "📈 Episodio 2382: Recompensa total (clipped): 15.000, Pasos: 701, Mean Reward Calculado: 0.021398 (Recompensa/Pasos)\n",
            " 1844565/2000000: episode: 2382, duration: 32.187s, episode steps: 701, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.018240, mae: 2.375014, mean_q: 2.865948, mean_eps: 0.100000\n",
            "📈 Episodio 2383: Recompensa total (clipped): 15.000, Pasos: 777, Mean Reward Calculado: 0.019305 (Recompensa/Pasos)\n",
            " 1845342/2000000: episode: 2383, duration: 35.277s, episode steps: 777, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.018503, mae: 2.378623, mean_q: 2.866515, mean_eps: 0.100000\n",
            "📈 Episodio 2384: Recompensa total (clipped): 16.000, Pasos: 575, Mean Reward Calculado: 0.027826 (Recompensa/Pasos)\n",
            " 1845917/2000000: episode: 2384, duration: 26.103s, episode steps: 575, steps per second:  22, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.019020, mae: 2.339523, mean_q: 2.819904, mean_eps: 0.100000\n",
            "📈 Episodio 2385: Recompensa total (clipped): 33.000, Pasos: 1257, Mean Reward Calculado: 0.026253 (Recompensa/Pasos)\n",
            " 1847174/2000000: episode: 2385, duration: 56.736s, episode steps: 1257, steps per second:  22, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.017741, mae: 2.372690, mean_q: 2.858983, mean_eps: 0.100000\n",
            "📈 Episodio 2386: Recompensa total (clipped): 9.000, Pasos: 529, Mean Reward Calculado: 0.017013 (Recompensa/Pasos)\n",
            " 1847703/2000000: episode: 2386, duration: 23.839s, episode steps: 529, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.017231, mae: 2.360051, mean_q: 2.847402, mean_eps: 0.100000\n",
            "📈 Episodio 2387: Recompensa total (clipped): 16.000, Pasos: 648, Mean Reward Calculado: 0.024691 (Recompensa/Pasos)\n",
            " 1848351/2000000: episode: 2387, duration: 29.504s, episode steps: 648, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.018801, mae: 2.369398, mean_q: 2.854707, mean_eps: 0.100000\n",
            "📈 Episodio 2388: Recompensa total (clipped): 14.000, Pasos: 757, Mean Reward Calculado: 0.018494 (Recompensa/Pasos)\n",
            " 1849108/2000000: episode: 2388, duration: 33.980s, episode steps: 757, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.015725, mae: 2.360246, mean_q: 2.843606, mean_eps: 0.100000\n",
            "📈 Episodio 2389: Recompensa total (clipped): 24.000, Pasos: 1057, Mean Reward Calculado: 0.022706 (Recompensa/Pasos)\n",
            " 1850165/2000000: episode: 2389, duration: 47.925s, episode steps: 1057, steps per second:  22, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.017596, mae: 2.357740, mean_q: 2.841110, mean_eps: 0.100000\n",
            "📈 Episodio 2390: Recompensa total (clipped): 13.000, Pasos: 464, Mean Reward Calculado: 0.028017 (Recompensa/Pasos)\n",
            " 1850629/2000000: episode: 2390, duration: 21.079s, episode steps: 464, steps per second:  22, episode reward: 13.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.015603, mae: 2.338129, mean_q: 2.820901, mean_eps: 0.100000\n",
            "📈 Episodio 2391: Recompensa total (clipped): 20.000, Pasos: 1229, Mean Reward Calculado: 0.016273 (Recompensa/Pasos)\n",
            " 1851858/2000000: episode: 2391, duration: 55.682s, episode steps: 1229, steps per second:  22, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015709, mae: 2.360906, mean_q: 2.847786, mean_eps: 0.100000\n",
            "📈 Episodio 2392: Recompensa total (clipped): 11.000, Pasos: 667, Mean Reward Calculado: 0.016492 (Recompensa/Pasos)\n",
            " 1852525/2000000: episode: 2392, duration: 30.177s, episode steps: 667, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.135 [0.000, 5.000],  loss: 0.018117, mae: 2.352448, mean_q: 2.839912, mean_eps: 0.100000\n",
            "📈 Episodio 2393: Recompensa total (clipped): 6.000, Pasos: 499, Mean Reward Calculado: 0.012024 (Recompensa/Pasos)\n",
            " 1853024/2000000: episode: 2393, duration: 22.660s, episode steps: 499, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015596, mae: 2.349287, mean_q: 2.832991, mean_eps: 0.100000\n",
            "📈 Episodio 2394: Recompensa total (clipped): 29.000, Pasos: 1247, Mean Reward Calculado: 0.023256 (Recompensa/Pasos)\n",
            " 1854271/2000000: episode: 2394, duration: 56.014s, episode steps: 1247, steps per second:  22, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.017170, mae: 2.365799, mean_q: 2.852796, mean_eps: 0.100000\n",
            "📈 Episodio 2395: Recompensa total (clipped): 17.000, Pasos: 694, Mean Reward Calculado: 0.024496 (Recompensa/Pasos)\n",
            " 1854965/2000000: episode: 2395, duration: 31.189s, episode steps: 694, steps per second:  22, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.015808, mae: 2.352644, mean_q: 2.836709, mean_eps: 0.100000\n",
            "📈 Episodio 2396: Recompensa total (clipped): 21.000, Pasos: 939, Mean Reward Calculado: 0.022364 (Recompensa/Pasos)\n",
            " 1855904/2000000: episode: 2396, duration: 42.135s, episode steps: 939, steps per second:  22, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.016520, mae: 2.359239, mean_q: 2.845248, mean_eps: 0.100000\n",
            "📈 Episodio 2397: Recompensa total (clipped): 15.000, Pasos: 692, Mean Reward Calculado: 0.021676 (Recompensa/Pasos)\n",
            " 1856596/2000000: episode: 2397, duration: 31.747s, episode steps: 692, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016657, mae: 2.362730, mean_q: 2.848893, mean_eps: 0.100000\n",
            "📈 Episodio 2398: Recompensa total (clipped): 14.000, Pasos: 662, Mean Reward Calculado: 0.021148 (Recompensa/Pasos)\n",
            " 1857258/2000000: episode: 2398, duration: 30.577s, episode steps: 662, steps per second:  22, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.016160, mae: 2.385121, mean_q: 2.875646, mean_eps: 0.100000\n",
            "📈 Episodio 2399: Recompensa total (clipped): 21.000, Pasos: 988, Mean Reward Calculado: 0.021255 (Recompensa/Pasos)\n",
            " 1858246/2000000: episode: 2399, duration: 44.396s, episode steps: 988, steps per second:  22, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.017708, mae: 2.360739, mean_q: 2.846215, mean_eps: 0.100000\n",
            "📈 Episodio 2400: Recompensa total (clipped): 15.000, Pasos: 733, Mean Reward Calculado: 0.020464 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2400, pasos: 1858979)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.88 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2400 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 15.00\n",
            "   Media últimos 100: 17.88 / 20.0\n",
            "   Mejor promedio histórico: 17.88\n",
            "   Estado: 📈 89.4% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1858979/2000000: episode: 2400, duration: 105.812s, episode steps: 733, steps per second:   7, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.016365, mae: 2.356100, mean_q: 2.840307, mean_eps: 0.100000\n",
            "📊 Paso 1,860,000/2,000,000 (93.0%) - 24.3 pasos/seg - ETA: 1.6h - Memoria: 15308.60 MB\n",
            "📈 Episodio 2401: Recompensa total (clipped): 24.000, Pasos: 1024, Mean Reward Calculado: 0.023438 (Recompensa/Pasos)\n",
            " 1860003/2000000: episode: 2401, duration: 46.321s, episode steps: 1024, steps per second:  22, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.018721, mae: 2.351215, mean_q: 2.834248, mean_eps: 0.100000\n",
            "📈 Episodio 2402: Recompensa total (clipped): 13.000, Pasos: 853, Mean Reward Calculado: 0.015240 (Recompensa/Pasos)\n",
            " 1860856/2000000: episode: 2402, duration: 38.552s, episode steps: 853, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.016290, mae: 2.366078, mean_q: 2.854477, mean_eps: 0.100000\n",
            "📈 Episodio 2403: Recompensa total (clipped): 17.000, Pasos: 667, Mean Reward Calculado: 0.025487 (Recompensa/Pasos)\n",
            " 1861523/2000000: episode: 2403, duration: 30.386s, episode steps: 667, steps per second:  22, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.016067, mae: 2.362603, mean_q: 2.854632, mean_eps: 0.100000\n",
            "📈 Episodio 2404: Recompensa total (clipped): 12.000, Pasos: 672, Mean Reward Calculado: 0.017857 (Recompensa/Pasos)\n",
            " 1862195/2000000: episode: 2404, duration: 30.617s, episode steps: 672, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.016229, mae: 2.375744, mean_q: 2.865053, mean_eps: 0.100000\n",
            "📈 Episodio 2405: Recompensa total (clipped): 10.000, Pasos: 641, Mean Reward Calculado: 0.015601 (Recompensa/Pasos)\n",
            " 1862836/2000000: episode: 2405, duration: 29.012s, episode steps: 641, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.016351, mae: 2.372411, mean_q: 2.861348, mean_eps: 0.100000\n",
            "📈 Episodio 2406: Recompensa total (clipped): 22.000, Pasos: 844, Mean Reward Calculado: 0.026066 (Recompensa/Pasos)\n",
            " 1863680/2000000: episode: 2406, duration: 38.293s, episode steps: 844, steps per second:  22, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.017819, mae: 2.370281, mean_q: 2.859379, mean_eps: 0.100000\n",
            "📈 Episodio 2407: Recompensa total (clipped): 31.000, Pasos: 1198, Mean Reward Calculado: 0.025876 (Recompensa/Pasos)\n",
            " 1864878/2000000: episode: 2407, duration: 55.101s, episode steps: 1198, steps per second:  22, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.015313, mae: 2.355140, mean_q: 2.839890, mean_eps: 0.100000\n",
            "📈 Episodio 2408: Recompensa total (clipped): 31.000, Pasos: 1433, Mean Reward Calculado: 0.021633 (Recompensa/Pasos)\n",
            " 1866311/2000000: episode: 2408, duration: 65.232s, episode steps: 1433, steps per second:  22, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.017843, mae: 2.370044, mean_q: 2.856690, mean_eps: 0.100000\n",
            "📈 Episodio 2409: Recompensa total (clipped): 22.000, Pasos: 820, Mean Reward Calculado: 0.026829 (Recompensa/Pasos)\n",
            " 1867131/2000000: episode: 2409, duration: 37.600s, episode steps: 820, steps per second:  22, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.017737, mae: 2.363018, mean_q: 2.850276, mean_eps: 0.100000\n",
            "📈 Episodio 2410: Recompensa total (clipped): 14.000, Pasos: 811, Mean Reward Calculado: 0.017263 (Recompensa/Pasos)\n",
            " 1867942/2000000: episode: 2410, duration: 37.418s, episode steps: 811, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.018185, mae: 2.357248, mean_q: 2.842955, mean_eps: 0.100000\n",
            "📈 Episodio 2411: Recompensa total (clipped): 13.000, Pasos: 681, Mean Reward Calculado: 0.019090 (Recompensa/Pasos)\n",
            " 1868623/2000000: episode: 2411, duration: 30.702s, episode steps: 681, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.016472, mae: 2.364076, mean_q: 2.851711, mean_eps: 0.100000\n",
            "📈 Episodio 2412: Recompensa total (clipped): 14.000, Pasos: 731, Mean Reward Calculado: 0.019152 (Recompensa/Pasos)\n",
            " 1869354/2000000: episode: 2412, duration: 33.065s, episode steps: 731, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.017995, mae: 2.356469, mean_q: 2.841497, mean_eps: 0.100000\n",
            "📈 Episodio 2413: Recompensa total (clipped): 16.000, Pasos: 629, Mean Reward Calculado: 0.025437 (Recompensa/Pasos)\n",
            " 1869983/2000000: episode: 2413, duration: 27.957s, episode steps: 629, steps per second:  22, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.016432, mae: 2.364297, mean_q: 2.851049, mean_eps: 0.100000\n",
            "📈 Episodio 2414: Recompensa total (clipped): 9.000, Pasos: 388, Mean Reward Calculado: 0.023196 (Recompensa/Pasos)\n",
            " 1870371/2000000: episode: 2414, duration: 17.260s, episode steps: 388, steps per second:  22, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.014823, mae: 2.357985, mean_q: 2.845615, mean_eps: 0.100000\n",
            "📈 Episodio 2415: Recompensa total (clipped): 20.000, Pasos: 855, Mean Reward Calculado: 0.023392 (Recompensa/Pasos)\n",
            " 1871226/2000000: episode: 2415, duration: 38.461s, episode steps: 855, steps per second:  22, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.015580, mae: 2.362523, mean_q: 2.850471, mean_eps: 0.100000\n",
            "📈 Episodio 2416: Recompensa total (clipped): 16.000, Pasos: 757, Mean Reward Calculado: 0.021136 (Recompensa/Pasos)\n",
            " 1871983/2000000: episode: 2416, duration: 34.666s, episode steps: 757, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.016786, mae: 2.350344, mean_q: 2.834729, mean_eps: 0.100000\n",
            "📈 Episodio 2417: Recompensa total (clipped): 20.000, Pasos: 958, Mean Reward Calculado: 0.020877 (Recompensa/Pasos)\n",
            " 1872941/2000000: episode: 2417, duration: 44.147s, episode steps: 958, steps per second:  22, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.016395, mae: 2.351826, mean_q: 2.837965, mean_eps: 0.100000\n",
            "📈 Episodio 2418: Recompensa total (clipped): 25.000, Pasos: 843, Mean Reward Calculado: 0.029656 (Recompensa/Pasos)\n",
            " 1873784/2000000: episode: 2418, duration: 37.654s, episode steps: 843, steps per second:  22, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.015732, mae: 2.346678, mean_q: 2.830501, mean_eps: 0.100000\n",
            "📈 Episodio 2419: Recompensa total (clipped): 28.000, Pasos: 1118, Mean Reward Calculado: 0.025045 (Recompensa/Pasos)\n",
            " 1874902/2000000: episode: 2419, duration: 50.187s, episode steps: 1118, steps per second:  22, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.016423, mae: 2.350031, mean_q: 2.834054, mean_eps: 0.100000\n",
            "📈 Episodio 2420: Recompensa total (clipped): 17.000, Pasos: 769, Mean Reward Calculado: 0.022107 (Recompensa/Pasos)\n",
            " 1875671/2000000: episode: 2420, duration: 35.073s, episode steps: 769, steps per second:  22, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.015748, mae: 2.350813, mean_q: 2.836632, mean_eps: 0.100000\n",
            "📈 Episodio 2421: Recompensa total (clipped): 22.000, Pasos: 982, Mean Reward Calculado: 0.022403 (Recompensa/Pasos)\n",
            " 1876653/2000000: episode: 2421, duration: 44.878s, episode steps: 982, steps per second:  22, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.017838, mae: 2.363854, mean_q: 2.853026, mean_eps: 0.100000\n",
            "📈 Episodio 2422: Recompensa total (clipped): 15.000, Pasos: 768, Mean Reward Calculado: 0.019531 (Recompensa/Pasos)\n",
            " 1877421/2000000: episode: 2422, duration: 35.141s, episode steps: 768, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.017162, mae: 2.351156, mean_q: 2.834168, mean_eps: 0.100000\n",
            "📈 Episodio 2423: Recompensa total (clipped): 28.000, Pasos: 1209, Mean Reward Calculado: 0.023160 (Recompensa/Pasos)\n",
            " 1878630/2000000: episode: 2423, duration: 55.540s, episode steps: 1209, steps per second:  22, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.016979, mae: 2.347844, mean_q: 2.829142, mean_eps: 0.100000\n",
            "📈 Episodio 2424: Recompensa total (clipped): 12.000, Pasos: 776, Mean Reward Calculado: 0.015464 (Recompensa/Pasos)\n",
            " 1879406/2000000: episode: 2424, duration: 35.050s, episode steps: 776, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.016826, mae: 2.352819, mean_q: 2.834844, mean_eps: 0.100000\n",
            "📊 Paso 1,880,000/2,000,000 (94.0%) - 24.3 pasos/seg - ETA: 1.4h - Memoria: 15299.03 MB\n",
            "📈 Episodio 2425: Recompensa total (clipped): 21.000, Pasos: 1029, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            " 1880435/2000000: episode: 2425, duration: 46.593s, episode steps: 1029, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.016681, mae: 2.353613, mean_q: 2.838277, mean_eps: 0.100000\n",
            "📈 Episodio 2426: Recompensa total (clipped): 19.000, Pasos: 813, Mean Reward Calculado: 0.023370 (Recompensa/Pasos)\n",
            " 1881248/2000000: episode: 2426, duration: 37.325s, episode steps: 813, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.017464, mae: 2.371553, mean_q: 2.857790, mean_eps: 0.100000\n",
            "📈 Episodio 2427: Recompensa total (clipped): 9.000, Pasos: 706, Mean Reward Calculado: 0.012748 (Recompensa/Pasos)\n",
            " 1881954/2000000: episode: 2427, duration: 32.552s, episode steps: 706, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.016566, mae: 2.370031, mean_q: 2.856378, mean_eps: 0.100000\n",
            "📈 Episodio 2428: Recompensa total (clipped): 15.000, Pasos: 667, Mean Reward Calculado: 0.022489 (Recompensa/Pasos)\n",
            " 1882621/2000000: episode: 2428, duration: 30.425s, episode steps: 667, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.017951, mae: 2.364968, mean_q: 2.850134, mean_eps: 0.100000\n",
            "📈 Episodio 2429: Recompensa total (clipped): 8.000, Pasos: 425, Mean Reward Calculado: 0.018824 (Recompensa/Pasos)\n",
            " 1883046/2000000: episode: 2429, duration: 18.987s, episode steps: 425, steps per second:  22, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.014721, mae: 2.351992, mean_q: 2.834121, mean_eps: 0.100000\n",
            "📈 Episodio 2430: Recompensa total (clipped): 8.000, Pasos: 515, Mean Reward Calculado: 0.015534 (Recompensa/Pasos)\n",
            " 1883561/2000000: episode: 2430, duration: 23.103s, episode steps: 515, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.016775, mae: 2.374480, mean_q: 2.862523, mean_eps: 0.100000\n",
            "📈 Episodio 2431: Recompensa total (clipped): 25.000, Pasos: 1068, Mean Reward Calculado: 0.023408 (Recompensa/Pasos)\n",
            " 1884629/2000000: episode: 2431, duration: 48.533s, episode steps: 1068, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.016525, mae: 2.350873, mean_q: 2.832501, mean_eps: 0.100000\n",
            "📈 Episodio 2432: Recompensa total (clipped): 15.000, Pasos: 814, Mean Reward Calculado: 0.018428 (Recompensa/Pasos)\n",
            " 1885443/2000000: episode: 2432, duration: 36.387s, episode steps: 814, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.017969, mae: 2.368157, mean_q: 2.856850, mean_eps: 0.100000\n",
            "📈 Episodio 2433: Recompensa total (clipped): 15.000, Pasos: 660, Mean Reward Calculado: 0.022727 (Recompensa/Pasos)\n",
            " 1886103/2000000: episode: 2433, duration: 30.090s, episode steps: 660, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.017886, mae: 2.366424, mean_q: 2.852427, mean_eps: 0.100000\n",
            "📈 Episodio 2434: Recompensa total (clipped): 26.000, Pasos: 987, Mean Reward Calculado: 0.026342 (Recompensa/Pasos)\n",
            " 1887090/2000000: episode: 2434, duration: 45.111s, episode steps: 987, steps per second:  22, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.016718, mae: 2.375588, mean_q: 2.864197, mean_eps: 0.100000\n",
            "📈 Episodio 2435: Recompensa total (clipped): 21.000, Pasos: 906, Mean Reward Calculado: 0.023179 (Recompensa/Pasos)\n",
            " 1887996/2000000: episode: 2435, duration: 41.048s, episode steps: 906, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.017610, mae: 2.363222, mean_q: 2.847959, mean_eps: 0.100000\n",
            "📈 Episodio 2436: Recompensa total (clipped): 4.000, Pasos: 339, Mean Reward Calculado: 0.011799 (Recompensa/Pasos)\n",
            " 1888335/2000000: episode: 2436, duration: 15.347s, episode steps: 339, steps per second:  22, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.017021, mae: 2.323039, mean_q: 2.800660, mean_eps: 0.100000\n",
            "📈 Episodio 2437: Recompensa total (clipped): 16.000, Pasos: 926, Mean Reward Calculado: 0.017279 (Recompensa/Pasos)\n",
            " 1889261/2000000: episode: 2437, duration: 42.160s, episode steps: 926, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.017099, mae: 2.346766, mean_q: 2.828768, mean_eps: 0.100000\n",
            "📈 Episodio 2438: Recompensa total (clipped): 28.000, Pasos: 1372, Mean Reward Calculado: 0.020408 (Recompensa/Pasos)\n",
            " 1890633/2000000: episode: 2438, duration: 61.339s, episode steps: 1372, steps per second:  22, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016625, mae: 2.369311, mean_q: 2.855499, mean_eps: 0.100000\n",
            "📈 Episodio 2439: Recompensa total (clipped): 15.000, Pasos: 943, Mean Reward Calculado: 0.015907 (Recompensa/Pasos)\n",
            " 1891576/2000000: episode: 2439, duration: 43.455s, episode steps: 943, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.015733, mae: 2.343046, mean_q: 2.825434, mean_eps: 0.100000\n",
            "📈 Episodio 2440: Recompensa total (clipped): 12.000, Pasos: 567, Mean Reward Calculado: 0.021164 (Recompensa/Pasos)\n",
            " 1892143/2000000: episode: 2440, duration: 26.443s, episode steps: 567, steps per second:  21, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.354 [0.000, 5.000],  loss: 0.014489, mae: 2.361892, mean_q: 2.847208, mean_eps: 0.100000\n",
            "📈 Episodio 2441: Recompensa total (clipped): 10.000, Pasos: 536, Mean Reward Calculado: 0.018657 (Recompensa/Pasos)\n",
            " 1892679/2000000: episode: 2441, duration: 24.862s, episode steps: 536, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.016689, mae: 2.350046, mean_q: 2.831071, mean_eps: 0.100000\n",
            "📈 Episodio 2442: Recompensa total (clipped): 35.000, Pasos: 1187, Mean Reward Calculado: 0.029486 (Recompensa/Pasos)\n",
            " 1893866/2000000: episode: 2442, duration: 53.659s, episode steps: 1187, steps per second:  22, episode reward: 35.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.018412, mae: 2.358090, mean_q: 2.841250, mean_eps: 0.100000\n",
            "📈 Episodio 2443: Recompensa total (clipped): 24.000, Pasos: 1027, Mean Reward Calculado: 0.023369 (Recompensa/Pasos)\n",
            " 1894893/2000000: episode: 2443, duration: 46.278s, episode steps: 1027, steps per second:  22, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016876, mae: 2.354267, mean_q: 2.840973, mean_eps: 0.100000\n",
            "📈 Episodio 2444: Recompensa total (clipped): 22.000, Pasos: 931, Mean Reward Calculado: 0.023631 (Recompensa/Pasos)\n",
            " 1895824/2000000: episode: 2444, duration: 42.254s, episode steps: 931, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.015889, mae: 2.344847, mean_q: 2.827498, mean_eps: 0.100000\n",
            "📈 Episodio 2445: Recompensa total (clipped): 19.000, Pasos: 907, Mean Reward Calculado: 0.020948 (Recompensa/Pasos)\n",
            " 1896731/2000000: episode: 2445, duration: 41.007s, episode steps: 907, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.017443, mae: 2.352736, mean_q: 2.835814, mean_eps: 0.100000\n",
            "📈 Episodio 2446: Recompensa total (clipped): 7.000, Pasos: 550, Mean Reward Calculado: 0.012727 (Recompensa/Pasos)\n",
            " 1897281/2000000: episode: 2446, duration: 25.079s, episode steps: 550, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.896 [0.000, 5.000],  loss: 0.017624, mae: 2.346683, mean_q: 2.825269, mean_eps: 0.100000\n",
            "📈 Episodio 2447: Recompensa total (clipped): 21.000, Pasos: 931, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
            " 1898212/2000000: episode: 2447, duration: 42.293s, episode steps: 931, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.016436, mae: 2.352785, mean_q: 2.837425, mean_eps: 0.100000\n",
            "📈 Episodio 2448: Recompensa total (clipped): 21.000, Pasos: 794, Mean Reward Calculado: 0.026448 (Recompensa/Pasos)\n",
            " 1899006/2000000: episode: 2448, duration: 35.560s, episode steps: 794, steps per second:  22, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.017990, mae: 2.344745, mean_q: 2.824367, mean_eps: 0.100000\n",
            "📈 Episodio 2449: Recompensa total (clipped): 10.000, Pasos: 474, Mean Reward Calculado: 0.021097 (Recompensa/Pasos)\n",
            " 1899480/2000000: episode: 2449, duration: 21.464s, episode steps: 474, steps per second:  22, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.016914, mae: 2.337515, mean_q: 2.817301, mean_eps: 0.100000\n",
            "📊 Paso 1,900,000/2,000,000 (95.0%) - 24.3 pasos/seg - ETA: 1.1h - Memoria: 15284.36 MB\n",
            "📈 Episodio 2450: Recompensa total (clipped): 27.000, Pasos: 814, Mean Reward Calculado: 0.033170 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2450, pasos: 1900294)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.50 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2450 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 27.00\n",
            "   Media últimos 100: 17.50 / 20.0\n",
            "   Mejor promedio histórico: 17.50\n",
            "   Estado: 📈 87.5% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1900294/2000000: episode: 2450, duration: 75.793s, episode steps: 814, steps per second:  11, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.015946, mae: 2.355395, mean_q: 2.839330, mean_eps: 0.100000\n",
            "📈 Episodio 2451: Recompensa total (clipped): 21.000, Pasos: 783, Mean Reward Calculado: 0.026820 (Recompensa/Pasos)\n",
            " 1901077/2000000: episode: 2451, duration: 35.370s, episode steps: 783, steps per second:  22, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.017039, mae: 2.376034, mean_q: 2.864254, mean_eps: 0.100000\n",
            "📈 Episodio 2452: Recompensa total (clipped): 18.000, Pasos: 697, Mean Reward Calculado: 0.025825 (Recompensa/Pasos)\n",
            " 1901774/2000000: episode: 2452, duration: 31.468s, episode steps: 697, steps per second:  22, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.016238, mae: 2.358944, mean_q: 2.842789, mean_eps: 0.100000\n",
            "📈 Episodio 2453: Recompensa total (clipped): 24.000, Pasos: 1151, Mean Reward Calculado: 0.020851 (Recompensa/Pasos)\n",
            " 1902925/2000000: episode: 2453, duration: 52.349s, episode steps: 1151, steps per second:  22, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.016672, mae: 2.354153, mean_q: 2.836647, mean_eps: 0.100000\n",
            "📈 Episodio 2454: Recompensa total (clipped): 7.000, Pasos: 533, Mean Reward Calculado: 0.013133 (Recompensa/Pasos)\n",
            " 1903458/2000000: episode: 2454, duration: 23.816s, episode steps: 533, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.154 [0.000, 5.000],  loss: 0.017340, mae: 2.332205, mean_q: 2.809915, mean_eps: 0.100000\n",
            "📈 Episodio 2455: Recompensa total (clipped): 18.000, Pasos: 939, Mean Reward Calculado: 0.019169 (Recompensa/Pasos)\n",
            " 1904397/2000000: episode: 2455, duration: 42.365s, episode steps: 939, steps per second:  22, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.018474, mae: 2.339074, mean_q: 2.817935, mean_eps: 0.100000\n",
            "📈 Episodio 2456: Recompensa total (clipped): 23.000, Pasos: 1131, Mean Reward Calculado: 0.020336 (Recompensa/Pasos)\n",
            " 1905528/2000000: episode: 2456, duration: 51.789s, episode steps: 1131, steps per second:  22, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.015224, mae: 2.362661, mean_q: 2.848995, mean_eps: 0.100000\n",
            "📈 Episodio 2457: Recompensa total (clipped): 12.000, Pasos: 532, Mean Reward Calculado: 0.022556 (Recompensa/Pasos)\n",
            " 1906060/2000000: episode: 2457, duration: 24.397s, episode steps: 532, steps per second:  22, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.018060, mae: 2.369221, mean_q: 2.855748, mean_eps: 0.100000\n",
            "📈 Episodio 2458: Recompensa total (clipped): 15.000, Pasos: 851, Mean Reward Calculado: 0.017626 (Recompensa/Pasos)\n",
            " 1906911/2000000: episode: 2458, duration: 38.565s, episode steps: 851, steps per second:  22, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.017037, mae: 2.350202, mean_q: 2.831780, mean_eps: 0.100000\n",
            "📈 Episodio 2459: Recompensa total (clipped): 4.000, Pasos: 527, Mean Reward Calculado: 0.007590 (Recompensa/Pasos)\n",
            " 1907438/2000000: episode: 2459, duration: 24.408s, episode steps: 527, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015992, mae: 2.321963, mean_q: 2.797439, mean_eps: 0.100000\n",
            "📈 Episodio 2460: Recompensa total (clipped): 25.000, Pasos: 1007, Mean Reward Calculado: 0.024826 (Recompensa/Pasos)\n",
            " 1908445/2000000: episode: 2460, duration: 45.847s, episode steps: 1007, steps per second:  22, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.016413, mae: 2.363037, mean_q: 2.847196, mean_eps: 0.100000\n",
            "📈 Episodio 2461: Recompensa total (clipped): 27.000, Pasos: 1079, Mean Reward Calculado: 0.025023 (Recompensa/Pasos)\n",
            " 1909524/2000000: episode: 2461, duration: 48.971s, episode steps: 1079, steps per second:  22, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.018576, mae: 2.339163, mean_q: 2.822050, mean_eps: 0.100000\n",
            "📈 Episodio 2462: Recompensa total (clipped): 13.000, Pasos: 713, Mean Reward Calculado: 0.018233 (Recompensa/Pasos)\n",
            " 1910237/2000000: episode: 2462, duration: 32.800s, episode steps: 713, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.016167, mae: 2.353280, mean_q: 2.836444, mean_eps: 0.100000\n",
            "📈 Episodio 2463: Recompensa total (clipped): 32.000, Pasos: 1254, Mean Reward Calculado: 0.025518 (Recompensa/Pasos)\n",
            " 1911491/2000000: episode: 2463, duration: 57.044s, episode steps: 1254, steps per second:  22, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.016379, mae: 2.367643, mean_q: 2.851721, mean_eps: 0.100000\n",
            "📈 Episodio 2464: Recompensa total (clipped): 12.000, Pasos: 782, Mean Reward Calculado: 0.015345 (Recompensa/Pasos)\n",
            " 1912273/2000000: episode: 2464, duration: 35.208s, episode steps: 782, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.017949, mae: 2.348482, mean_q: 2.829385, mean_eps: 0.100000\n",
            "📈 Episodio 2465: Recompensa total (clipped): 32.000, Pasos: 1265, Mean Reward Calculado: 0.025296 (Recompensa/Pasos)\n",
            " 1913538/2000000: episode: 2465, duration: 56.717s, episode steps: 1265, steps per second:  22, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.016872, mae: 2.357838, mean_q: 2.843239, mean_eps: 0.100000\n",
            "📈 Episodio 2466: Recompensa total (clipped): 16.000, Pasos: 850, Mean Reward Calculado: 0.018824 (Recompensa/Pasos)\n",
            " 1914388/2000000: episode: 2466, duration: 38.168s, episode steps: 850, steps per second:  22, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.017115, mae: 2.356750, mean_q: 2.840922, mean_eps: 0.100000\n",
            "📈 Episodio 2467: Recompensa total (clipped): 15.000, Pasos: 612, Mean Reward Calculado: 0.024510 (Recompensa/Pasos)\n",
            " 1915000/2000000: episode: 2467, duration: 27.633s, episode steps: 612, steps per second:  22, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.016173, mae: 2.347856, mean_q: 2.832033, mean_eps: 0.100000\n",
            "📈 Episodio 2468: Recompensa total (clipped): 13.000, Pasos: 687, Mean Reward Calculado: 0.018923 (Recompensa/Pasos)\n",
            " 1915687/2000000: episode: 2468, duration: 31.061s, episode steps: 687, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.016261, mae: 2.362755, mean_q: 2.847003, mean_eps: 0.100000\n",
            "📈 Episodio 2469: Recompensa total (clipped): 11.000, Pasos: 536, Mean Reward Calculado: 0.020522 (Recompensa/Pasos)\n",
            " 1916223/2000000: episode: 2469, duration: 24.344s, episode steps: 536, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.017054, mae: 2.329011, mean_q: 2.808023, mean_eps: 0.100000\n",
            "📈 Episodio 2470: Recompensa total (clipped): 19.000, Pasos: 898, Mean Reward Calculado: 0.021158 (Recompensa/Pasos)\n",
            " 1917121/2000000: episode: 2470, duration: 40.277s, episode steps: 898, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.016044, mae: 2.352952, mean_q: 2.834207, mean_eps: 0.100000\n",
            "📈 Episodio 2471: Recompensa total (clipped): 19.000, Pasos: 694, Mean Reward Calculado: 0.027378 (Recompensa/Pasos)\n",
            " 1917815/2000000: episode: 2471, duration: 31.271s, episode steps: 694, steps per second:  22, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.019621, mae: 2.357534, mean_q: 2.840436, mean_eps: 0.100000\n",
            "📈 Episodio 2472: Recompensa total (clipped): 16.000, Pasos: 750, Mean Reward Calculado: 0.021333 (Recompensa/Pasos)\n",
            " 1918565/2000000: episode: 2472, duration: 33.731s, episode steps: 750, steps per second:  22, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.017692, mae: 2.352191, mean_q: 2.835624, mean_eps: 0.100000\n",
            "📈 Episodio 2473: Recompensa total (clipped): 29.000, Pasos: 1151, Mean Reward Calculado: 0.025195 (Recompensa/Pasos)\n",
            " 1919716/2000000: episode: 2473, duration: 51.928s, episode steps: 1151, steps per second:  22, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.015003, mae: 2.355937, mean_q: 2.840329, mean_eps: 0.100000\n",
            "📊 Paso 1,920,000/2,000,000 (96.0%) - 24.2 pasos/seg - ETA: 0.9h - Memoria: 15305.39 MB\n",
            "📈 Episodio 2474: Recompensa total (clipped): 21.000, Pasos: 905, Mean Reward Calculado: 0.023204 (Recompensa/Pasos)\n",
            " 1920621/2000000: episode: 2474, duration: 40.999s, episode steps: 905, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.016345, mae: 2.355587, mean_q: 2.838233, mean_eps: 0.100000\n",
            "📈 Episodio 2475: Recompensa total (clipped): 32.000, Pasos: 1472, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            " 1922093/2000000: episode: 2475, duration: 65.852s, episode steps: 1472, steps per second:  22, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.015761, mae: 2.352341, mean_q: 2.834973, mean_eps: 0.100000\n",
            "📈 Episodio 2476: Recompensa total (clipped): 24.000, Pasos: 1359, Mean Reward Calculado: 0.017660 (Recompensa/Pasos)\n",
            " 1923452/2000000: episode: 2476, duration: 61.509s, episode steps: 1359, steps per second:  22, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.018266, mae: 2.364090, mean_q: 2.850325, mean_eps: 0.100000\n",
            "📈 Episodio 2477: Recompensa total (clipped): 10.000, Pasos: 637, Mean Reward Calculado: 0.015699 (Recompensa/Pasos)\n",
            " 1924089/2000000: episode: 2477, duration: 28.766s, episode steps: 637, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.015790, mae: 2.360662, mean_q: 2.848420, mean_eps: 0.100000\n",
            "📈 Episodio 2478: Recompensa total (clipped): 27.000, Pasos: 1093, Mean Reward Calculado: 0.024703 (Recompensa/Pasos)\n",
            " 1925182/2000000: episode: 2478, duration: 49.343s, episode steps: 1093, steps per second:  22, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.017317, mae: 2.344364, mean_q: 2.824313, mean_eps: 0.100000\n",
            "📈 Episodio 2479: Recompensa total (clipped): 20.000, Pasos: 952, Mean Reward Calculado: 0.021008 (Recompensa/Pasos)\n",
            " 1926134/2000000: episode: 2479, duration: 43.016s, episode steps: 952, steps per second:  22, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.017091, mae: 2.350748, mean_q: 2.832407, mean_eps: 0.100000\n",
            "📈 Episodio 2480: Recompensa total (clipped): 15.000, Pasos: 666, Mean Reward Calculado: 0.022523 (Recompensa/Pasos)\n",
            " 1926800/2000000: episode: 2480, duration: 29.843s, episode steps: 666, steps per second:  22, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.016163, mae: 2.356630, mean_q: 2.838496, mean_eps: 0.100000\n",
            "📈 Episodio 2481: Recompensa total (clipped): 26.000, Pasos: 1293, Mean Reward Calculado: 0.020108 (Recompensa/Pasos)\n",
            " 1928093/2000000: episode: 2481, duration: 58.356s, episode steps: 1293, steps per second:  22, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.016692, mae: 2.346592, mean_q: 2.827672, mean_eps: 0.100000\n",
            "📈 Episodio 2482: Recompensa total (clipped): 27.000, Pasos: 1200, Mean Reward Calculado: 0.022500 (Recompensa/Pasos)\n",
            " 1929293/2000000: episode: 2482, duration: 53.736s, episode steps: 1200, steps per second:  22, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.017790, mae: 2.359897, mean_q: 2.845253, mean_eps: 0.100000\n",
            "📈 Episodio 2483: Recompensa total (clipped): 23.000, Pasos: 765, Mean Reward Calculado: 0.030065 (Recompensa/Pasos)\n",
            " 1930058/2000000: episode: 2483, duration: 34.680s, episode steps: 765, steps per second:  22, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.017020, mae: 2.340086, mean_q: 2.819591, mean_eps: 0.100000\n",
            "📈 Episodio 2484: Recompensa total (clipped): 12.000, Pasos: 675, Mean Reward Calculado: 0.017778 (Recompensa/Pasos)\n",
            " 1930733/2000000: episode: 2484, duration: 30.520s, episode steps: 675, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.016571, mae: 2.369947, mean_q: 2.859439, mean_eps: 0.100000\n",
            "📈 Episodio 2485: Recompensa total (clipped): 7.000, Pasos: 647, Mean Reward Calculado: 0.010819 (Recompensa/Pasos)\n",
            " 1931380/2000000: episode: 2485, duration: 29.396s, episode steps: 647, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.379 [0.000, 5.000],  loss: 0.018186, mae: 2.368520, mean_q: 2.856844, mean_eps: 0.100000\n",
            "📈 Episodio 2486: Recompensa total (clipped): 25.000, Pasos: 1051, Mean Reward Calculado: 0.023787 (Recompensa/Pasos)\n",
            " 1932431/2000000: episode: 2486, duration: 47.704s, episode steps: 1051, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.892 [0.000, 5.000],  loss: 0.017166, mae: 2.367099, mean_q: 2.852089, mean_eps: 0.100000\n",
            "📈 Episodio 2487: Recompensa total (clipped): 7.000, Pasos: 373, Mean Reward Calculado: 0.018767 (Recompensa/Pasos)\n",
            " 1932804/2000000: episode: 2487, duration: 17.063s, episode steps: 373, steps per second:  22, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.316 [0.000, 5.000],  loss: 0.015577, mae: 2.339056, mean_q: 2.818322, mean_eps: 0.100000\n",
            "📈 Episodio 2488: Recompensa total (clipped): 16.000, Pasos: 694, Mean Reward Calculado: 0.023055 (Recompensa/Pasos)\n",
            " 1933498/2000000: episode: 2488, duration: 31.205s, episode steps: 694, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.016362, mae: 2.372890, mean_q: 2.861499, mean_eps: 0.100000\n",
            "📈 Episodio 2489: Recompensa total (clipped): 19.000, Pasos: 1070, Mean Reward Calculado: 0.017757 (Recompensa/Pasos)\n",
            " 1934568/2000000: episode: 2489, duration: 48.209s, episode steps: 1070, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.015572, mae: 2.360097, mean_q: 2.845486, mean_eps: 0.100000\n",
            "📈 Episodio 2490: Recompensa total (clipped): 6.000, Pasos: 626, Mean Reward Calculado: 0.009585 (Recompensa/Pasos)\n",
            " 1935194/2000000: episode: 2490, duration: 28.805s, episode steps: 626, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.017358, mae: 2.353877, mean_q: 2.836865, mean_eps: 0.100000\n",
            "📈 Episodio 2491: Recompensa total (clipped): 18.000, Pasos: 712, Mean Reward Calculado: 0.025281 (Recompensa/Pasos)\n",
            " 1935906/2000000: episode: 2491, duration: 32.098s, episode steps: 712, steps per second:  22, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.017443, mae: 2.371631, mean_q: 2.860226, mean_eps: 0.100000\n",
            "📈 Episodio 2492: Recompensa total (clipped): 29.000, Pasos: 1251, Mean Reward Calculado: 0.023181 (Recompensa/Pasos)\n",
            " 1937157/2000000: episode: 2492, duration: 56.593s, episode steps: 1251, steps per second:  22, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.017528, mae: 2.377332, mean_q: 2.865798, mean_eps: 0.100000\n",
            "📈 Episodio 2493: Recompensa total (clipped): 17.000, Pasos: 800, Mean Reward Calculado: 0.021250 (Recompensa/Pasos)\n",
            " 1937957/2000000: episode: 2493, duration: 35.813s, episode steps: 800, steps per second:  22, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.017511, mae: 2.365476, mean_q: 2.849803, mean_eps: 0.100000\n",
            "📈 Episodio 2494: Recompensa total (clipped): 25.000, Pasos: 1088, Mean Reward Calculado: 0.022978 (Recompensa/Pasos)\n",
            " 1939045/2000000: episode: 2494, duration: 48.963s, episode steps: 1088, steps per second:  22, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.017098, mae: 2.360697, mean_q: 2.845157, mean_eps: 0.100000\n",
            "📈 Episodio 2495: Recompensa total (clipped): 3.000, Pasos: 381, Mean Reward Calculado: 0.007874 (Recompensa/Pasos)\n",
            " 1939426/2000000: episode: 2495, duration: 17.411s, episode steps: 381, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.787 [0.000, 5.000],  loss: 0.020478, mae: 2.332109, mean_q: 2.810709, mean_eps: 0.100000\n",
            "📈 Episodio 2496: Recompensa total (clipped): 9.000, Pasos: 454, Mean Reward Calculado: 0.019824 (Recompensa/Pasos)\n",
            " 1939880/2000000: episode: 2496, duration: 20.149s, episode steps: 454, steps per second:  23, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.016610, mae: 2.357860, mean_q: 2.841762, mean_eps: 0.100000\n",
            "📊 Paso 1,940,000/2,000,000 (97.0%) - 24.2 pasos/seg - ETA: 0.7h - Memoria: 15291.54 MB\n",
            "📈 Episodio 2497: Recompensa total (clipped): 12.000, Pasos: 753, Mean Reward Calculado: 0.015936 (Recompensa/Pasos)\n",
            " 1940633/2000000: episode: 2497, duration: 33.816s, episode steps: 753, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.016921, mae: 2.362759, mean_q: 2.849827, mean_eps: 0.100000\n",
            "📈 Episodio 2498: Recompensa total (clipped): 12.000, Pasos: 524, Mean Reward Calculado: 0.022901 (Recompensa/Pasos)\n",
            " 1941157/2000000: episode: 2498, duration: 23.777s, episode steps: 524, steps per second:  22, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.018548, mae: 2.357153, mean_q: 2.838490, mean_eps: 0.100000\n",
            "📈 Episodio 2499: Recompensa total (clipped): 16.000, Pasos: 669, Mean Reward Calculado: 0.023916 (Recompensa/Pasos)\n",
            " 1941826/2000000: episode: 2499, duration: 30.606s, episode steps: 669, steps per second:  22, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.017049, mae: 2.354061, mean_q: 2.837006, mean_eps: 0.100000\n",
            "📈 Episodio 2500: Recompensa total (clipped): 22.000, Pasos: 977, Mean Reward Calculado: 0.022518 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2500, pasos: 1942803)\n",
            "💾 NUEVO MEJOR PROMEDIO: 18.05 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2500 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 22.00\n",
            "   Media últimos 100: 18.05 / 20.0\n",
            "   Mejor promedio histórico: 18.05\n",
            "   Estado: 📈 90.3% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1942803/2000000: episode: 2500, duration: 97.387s, episode steps: 977, steps per second:  10, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.016454, mae: 2.359797, mean_q: 2.842708, mean_eps: 0.100000\n",
            "📈 Episodio 2501: Recompensa total (clipped): 25.000, Pasos: 1061, Mean Reward Calculado: 0.023563 (Recompensa/Pasos)\n",
            " 1943864/2000000: episode: 2501, duration: 48.511s, episode steps: 1061, steps per second:  22, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.016698, mae: 2.347581, mean_q: 2.830299, mean_eps: 0.100000\n",
            "📈 Episodio 2502: Recompensa total (clipped): 15.000, Pasos: 694, Mean Reward Calculado: 0.021614 (Recompensa/Pasos)\n",
            " 1944558/2000000: episode: 2502, duration: 31.851s, episode steps: 694, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.016665, mae: 2.334234, mean_q: 2.813149, mean_eps: 0.100000\n",
            "📈 Episodio 2503: Recompensa total (clipped): 12.000, Pasos: 640, Mean Reward Calculado: 0.018750 (Recompensa/Pasos)\n",
            " 1945198/2000000: episode: 2503, duration: 28.382s, episode steps: 640, steps per second:  23, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.015739, mae: 2.348263, mean_q: 2.831765, mean_eps: 0.100000\n",
            "📈 Episodio 2504: Recompensa total (clipped): 11.000, Pasos: 663, Mean Reward Calculado: 0.016591 (Recompensa/Pasos)\n",
            " 1945861/2000000: episode: 2504, duration: 30.062s, episode steps: 663, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.016061, mae: 2.341824, mean_q: 2.820439, mean_eps: 0.100000\n",
            "📈 Episodio 2505: Recompensa total (clipped): 29.000, Pasos: 1272, Mean Reward Calculado: 0.022799 (Recompensa/Pasos)\n",
            " 1947133/2000000: episode: 2505, duration: 57.569s, episode steps: 1272, steps per second:  22, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016318, mae: 2.352036, mean_q: 2.831843, mean_eps: 0.100000\n",
            "📈 Episodio 2506: Recompensa total (clipped): 25.000, Pasos: 1229, Mean Reward Calculado: 0.020342 (Recompensa/Pasos)\n",
            " 1948362/2000000: episode: 2506, duration: 55.714s, episode steps: 1229, steps per second:  22, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.015507, mae: 2.359046, mean_q: 2.843846, mean_eps: 0.100000\n",
            "📈 Episodio 2507: Recompensa total (clipped): 31.000, Pasos: 1054, Mean Reward Calculado: 0.029412 (Recompensa/Pasos)\n",
            " 1949416/2000000: episode: 2507, duration: 47.351s, episode steps: 1054, steps per second:  22, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016612, mae: 2.343289, mean_q: 2.823440, mean_eps: 0.100000\n",
            "📈 Episodio 2508: Recompensa total (clipped): 7.000, Pasos: 474, Mean Reward Calculado: 0.014768 (Recompensa/Pasos)\n",
            " 1949890/2000000: episode: 2508, duration: 21.401s, episode steps: 474, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.017881, mae: 2.376467, mean_q: 2.865594, mean_eps: 0.100000\n",
            "📈 Episodio 2509: Recompensa total (clipped): 20.000, Pasos: 1065, Mean Reward Calculado: 0.018779 (Recompensa/Pasos)\n",
            " 1950955/2000000: episode: 2509, duration: 48.587s, episode steps: 1065, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015454, mae: 2.368064, mean_q: 2.855429, mean_eps: 0.100000\n",
            "📈 Episodio 2510: Recompensa total (clipped): 24.000, Pasos: 1036, Mean Reward Calculado: 0.023166 (Recompensa/Pasos)\n",
            " 1951991/2000000: episode: 2510, duration: 45.975s, episode steps: 1036, steps per second:  23, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.016905, mae: 2.352161, mean_q: 2.835214, mean_eps: 0.100000\n",
            "📈 Episodio 2511: Recompensa total (clipped): 25.000, Pasos: 1012, Mean Reward Calculado: 0.024704 (Recompensa/Pasos)\n",
            " 1953003/2000000: episode: 2511, duration: 45.291s, episode steps: 1012, steps per second:  22, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.015495, mae: 2.365382, mean_q: 2.849716, mean_eps: 0.100000\n",
            "📈 Episodio 2512: Recompensa total (clipped): 22.000, Pasos: 855, Mean Reward Calculado: 0.025731 (Recompensa/Pasos)\n",
            " 1953858/2000000: episode: 2512, duration: 38.432s, episode steps: 855, steps per second:  22, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.017652, mae: 2.359218, mean_q: 2.843528, mean_eps: 0.100000\n",
            "📈 Episodio 2513: Recompensa total (clipped): 10.000, Pasos: 676, Mean Reward Calculado: 0.014793 (Recompensa/Pasos)\n",
            " 1954534/2000000: episode: 2513, duration: 30.644s, episode steps: 676, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.015912, mae: 2.343417, mean_q: 2.825466, mean_eps: 0.100000\n",
            "📈 Episodio 2514: Recompensa total (clipped): 11.000, Pasos: 571, Mean Reward Calculado: 0.019264 (Recompensa/Pasos)\n",
            " 1955105/2000000: episode: 2514, duration: 26.095s, episode steps: 571, steps per second:  22, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.016741, mae: 2.357892, mean_q: 2.840538, mean_eps: 0.100000\n",
            "📈 Episodio 2515: Recompensa total (clipped): 7.000, Pasos: 443, Mean Reward Calculado: 0.015801 (Recompensa/Pasos)\n",
            " 1955548/2000000: episode: 2515, duration: 19.974s, episode steps: 443, steps per second:  22, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.016694, mae: 2.358465, mean_q: 2.842914, mean_eps: 0.100000\n",
            "📈 Episodio 2516: Recompensa total (clipped): 9.000, Pasos: 689, Mean Reward Calculado: 0.013062 (Recompensa/Pasos)\n",
            " 1956237/2000000: episode: 2516, duration: 31.217s, episode steps: 689, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.385 [0.000, 5.000],  loss: 0.017273, mae: 2.364039, mean_q: 2.847321, mean_eps: 0.100000\n",
            "📈 Episodio 2517: Recompensa total (clipped): 14.000, Pasos: 531, Mean Reward Calculado: 0.026365 (Recompensa/Pasos)\n",
            " 1956768/2000000: episode: 2517, duration: 23.782s, episode steps: 531, steps per second:  22, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.337 [0.000, 5.000],  loss: 0.016269, mae: 2.352076, mean_q: 2.837054, mean_eps: 0.100000\n",
            "📈 Episodio 2518: Recompensa total (clipped): 16.000, Pasos: 589, Mean Reward Calculado: 0.027165 (Recompensa/Pasos)\n",
            " 1957357/2000000: episode: 2518, duration: 26.578s, episode steps: 589, steps per second:  22, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.136 [0.000, 5.000],  loss: 0.018430, mae: 2.393738, mean_q: 2.886258, mean_eps: 0.100000\n",
            "📈 Episodio 2519: Recompensa total (clipped): 27.000, Pasos: 831, Mean Reward Calculado: 0.032491 (Recompensa/Pasos)\n",
            " 1958188/2000000: episode: 2519, duration: 37.203s, episode steps: 831, steps per second:  22, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.017374, mae: 2.364426, mean_q: 2.850792, mean_eps: 0.100000\n",
            "📈 Episodio 2520: Recompensa total (clipped): 4.000, Pasos: 495, Mean Reward Calculado: 0.008081 (Recompensa/Pasos)\n",
            " 1958683/2000000: episode: 2520, duration: 22.235s, episode steps: 495, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.016509, mae: 2.353473, mean_q: 2.836202, mean_eps: 0.100000\n",
            "📈 Episodio 2521: Recompensa total (clipped): 13.000, Pasos: 619, Mean Reward Calculado: 0.021002 (Recompensa/Pasos)\n",
            " 1959302/2000000: episode: 2521, duration: 28.325s, episode steps: 619, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.016951, mae: 2.348155, mean_q: 2.831945, mean_eps: 0.100000\n",
            "📊 Paso 1,960,000/2,000,000 (98.0%) - 24.2 pasos/seg - ETA: 0.5h - Memoria: 15290.12 MB\n",
            "📈 Episodio 2522: Recompensa total (clipped): 23.000, Pasos: 1032, Mean Reward Calculado: 0.022287 (Recompensa/Pasos)\n",
            " 1960334/2000000: episode: 2522, duration: 46.222s, episode steps: 1032, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.016310, mae: 2.362108, mean_q: 2.847345, mean_eps: 0.100000\n",
            "📈 Episodio 2523: Recompensa total (clipped): 22.000, Pasos: 877, Mean Reward Calculado: 0.025086 (Recompensa/Pasos)\n",
            " 1961211/2000000: episode: 2523, duration: 39.008s, episode steps: 877, steps per second:  22, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.016919, mae: 2.369639, mean_q: 2.856429, mean_eps: 0.100000\n",
            "📈 Episodio 2524: Recompensa total (clipped): 30.000, Pasos: 1380, Mean Reward Calculado: 0.021739 (Recompensa/Pasos)\n",
            " 1962591/2000000: episode: 2524, duration: 62.006s, episode steps: 1380, steps per second:  22, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.016649, mae: 2.359843, mean_q: 2.845153, mean_eps: 0.100000\n",
            "📈 Episodio 2525: Recompensa total (clipped): 24.000, Pasos: 1280, Mean Reward Calculado: 0.018750 (Recompensa/Pasos)\n",
            " 1963871/2000000: episode: 2525, duration: 56.996s, episode steps: 1280, steps per second:  22, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.016664, mae: 2.374877, mean_q: 2.863115, mean_eps: 0.100000\n",
            "📈 Episodio 2526: Recompensa total (clipped): 13.000, Pasos: 625, Mean Reward Calculado: 0.020800 (Recompensa/Pasos)\n",
            " 1964496/2000000: episode: 2526, duration: 27.967s, episode steps: 625, steps per second:  22, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.015711, mae: 2.374670, mean_q: 2.862271, mean_eps: 0.100000\n",
            "📈 Episodio 2527: Recompensa total (clipped): 14.000, Pasos: 970, Mean Reward Calculado: 0.014433 (Recompensa/Pasos)\n",
            " 1965466/2000000: episode: 2527, duration: 43.503s, episode steps: 970, steps per second:  22, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.016278, mae: 2.356116, mean_q: 2.840695, mean_eps: 0.100000\n",
            "📈 Episodio 2528: Recompensa total (clipped): 18.000, Pasos: 744, Mean Reward Calculado: 0.024194 (Recompensa/Pasos)\n",
            " 1966210/2000000: episode: 2528, duration: 33.416s, episode steps: 744, steps per second:  22, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.016648, mae: 2.367227, mean_q: 2.853855, mean_eps: 0.100000\n",
            "📈 Episodio 2529: Recompensa total (clipped): 19.000, Pasos: 832, Mean Reward Calculado: 0.022837 (Recompensa/Pasos)\n",
            " 1967042/2000000: episode: 2529, duration: 37.573s, episode steps: 832, steps per second:  22, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.847 [0.000, 5.000],  loss: 0.015639, mae: 2.354537, mean_q: 2.838576, mean_eps: 0.100000\n",
            "📈 Episodio 2530: Recompensa total (clipped): 21.000, Pasos: 869, Mean Reward Calculado: 0.024166 (Recompensa/Pasos)\n",
            " 1967911/2000000: episode: 2530, duration: 38.949s, episode steps: 869, steps per second:  22, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.016357, mae: 2.366246, mean_q: 2.850216, mean_eps: 0.100000\n",
            "📈 Episodio 2531: Recompensa total (clipped): 18.000, Pasos: 570, Mean Reward Calculado: 0.031579 (Recompensa/Pasos)\n",
            " 1968481/2000000: episode: 2531, duration: 25.965s, episode steps: 570, steps per second:  22, episode reward: 18.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.017683, mae: 2.381149, mean_q: 2.869377, mean_eps: 0.100000\n",
            "📈 Episodio 2532: Recompensa total (clipped): 21.000, Pasos: 706, Mean Reward Calculado: 0.029745 (Recompensa/Pasos)\n",
            " 1969187/2000000: episode: 2532, duration: 31.679s, episode steps: 706, steps per second:  22, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.017271, mae: 2.372276, mean_q: 2.861398, mean_eps: 0.100000\n",
            "📈 Episodio 2533: Recompensa total (clipped): 13.000, Pasos: 558, Mean Reward Calculado: 0.023297 (Recompensa/Pasos)\n",
            " 1969745/2000000: episode: 2533, duration: 25.092s, episode steps: 558, steps per second:  22, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.016125, mae: 2.350772, mean_q: 2.832860, mean_eps: 0.100000\n",
            "📈 Episodio 2534: Recompensa total (clipped): 15.000, Pasos: 916, Mean Reward Calculado: 0.016376 (Recompensa/Pasos)\n",
            " 1970661/2000000: episode: 2534, duration: 40.608s, episode steps: 916, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.016310, mae: 2.358390, mean_q: 2.843308, mean_eps: 0.100000\n",
            "📈 Episodio 2535: Recompensa total (clipped): 5.000, Pasos: 351, Mean Reward Calculado: 0.014245 (Recompensa/Pasos)\n",
            " 1971012/2000000: episode: 2535, duration: 15.671s, episode steps: 351, steps per second:  22, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.016214, mae: 2.336013, mean_q: 2.820671, mean_eps: 0.100000\n",
            "📈 Episodio 2536: Recompensa total (clipped): 11.000, Pasos: 738, Mean Reward Calculado: 0.014905 (Recompensa/Pasos)\n",
            " 1971750/2000000: episode: 2536, duration: 33.417s, episode steps: 738, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.015862, mae: 2.337645, mean_q: 2.818930, mean_eps: 0.100000\n",
            "📈 Episodio 2537: Recompensa total (clipped): 17.000, Pasos: 931, Mean Reward Calculado: 0.018260 (Recompensa/Pasos)\n",
            " 1972681/2000000: episode: 2537, duration: 41.779s, episode steps: 931, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.018062, mae: 2.349187, mean_q: 2.831830, mean_eps: 0.100000\n",
            "📈 Episodio 2538: Recompensa total (clipped): 18.000, Pasos: 989, Mean Reward Calculado: 0.018200 (Recompensa/Pasos)\n",
            " 1973670/2000000: episode: 2538, duration: 43.721s, episode steps: 989, steps per second:  23, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.017386, mae: 2.337086, mean_q: 2.818106, mean_eps: 0.100000\n",
            "📈 Episodio 2539: Recompensa total (clipped): 34.000, Pasos: 1379, Mean Reward Calculado: 0.024656 (Recompensa/Pasos)\n",
            " 1975049/2000000: episode: 2539, duration: 62.299s, episode steps: 1379, steps per second:  22, episode reward: 34.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.016664, mae: 2.332236, mean_q: 2.814139, mean_eps: 0.100000\n",
            "📈 Episodio 2540: Recompensa total (clipped): 18.000, Pasos: 888, Mean Reward Calculado: 0.020270 (Recompensa/Pasos)\n",
            " 1975937/2000000: episode: 2540, duration: 40.531s, episode steps: 888, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.017086, mae: 2.340875, mean_q: 2.821982, mean_eps: 0.100000\n",
            "📈 Episodio 2541: Recompensa total (clipped): 11.000, Pasos: 660, Mean Reward Calculado: 0.016667 (Recompensa/Pasos)\n",
            " 1976597/2000000: episode: 2541, duration: 30.069s, episode steps: 660, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.015529, mae: 2.346336, mean_q: 2.828410, mean_eps: 0.100000\n",
            "📈 Episodio 2542: Recompensa total (clipped): 16.000, Pasos: 691, Mean Reward Calculado: 0.023155 (Recompensa/Pasos)\n",
            " 1977288/2000000: episode: 2542, duration: 30.755s, episode steps: 691, steps per second:  22, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.017019, mae: 2.328453, mean_q: 2.807901, mean_eps: 0.100000\n",
            "📈 Episodio 2543: Recompensa total (clipped): 14.000, Pasos: 545, Mean Reward Calculado: 0.025688 (Recompensa/Pasos)\n",
            " 1977833/2000000: episode: 2543, duration: 24.930s, episode steps: 545, steps per second:  22, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.016885, mae: 2.335539, mean_q: 2.816502, mean_eps: 0.100000\n",
            "📈 Episodio 2544: Recompensa total (clipped): 12.000, Pasos: 691, Mean Reward Calculado: 0.017366 (Recompensa/Pasos)\n",
            " 1978524/2000000: episode: 2544, duration: 31.149s, episode steps: 691, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.018657, mae: 2.334700, mean_q: 2.814370, mean_eps: 0.100000\n",
            "📈 Episodio 2545: Recompensa total (clipped): 13.000, Pasos: 557, Mean Reward Calculado: 0.023339 (Recompensa/Pasos)\n",
            " 1979081/2000000: episode: 2545, duration: 25.070s, episode steps: 557, steps per second:  22, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.864 [0.000, 5.000],  loss: 0.018094, mae: 2.329048, mean_q: 2.807013, mean_eps: 0.100000\n",
            "📈 Episodio 2546: Recompensa total (clipped): 13.000, Pasos: 659, Mean Reward Calculado: 0.019727 (Recompensa/Pasos)\n",
            " 1979740/2000000: episode: 2546, duration: 29.812s, episode steps: 659, steps per second:  22, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.016319, mae: 2.348000, mean_q: 2.830569, mean_eps: 0.100000\n",
            "📊 Paso 1,980,000/2,000,000 (99.0%) - 24.2 pasos/seg - ETA: 0.2h - Memoria: 15303.11 MB\n",
            "📈 Episodio 2547: Recompensa total (clipped): 8.000, Pasos: 559, Mean Reward Calculado: 0.014311 (Recompensa/Pasos)\n",
            " 1980299/2000000: episode: 2547, duration: 25.190s, episode steps: 559, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.015553, mae: 2.357975, mean_q: 2.842939, mean_eps: 0.100000\n",
            "📈 Episodio 2548: Recompensa total (clipped): 28.000, Pasos: 1370, Mean Reward Calculado: 0.020438 (Recompensa/Pasos)\n",
            " 1981669/2000000: episode: 2548, duration: 61.524s, episode steps: 1370, steps per second:  22, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.012 [0.000, 5.000],  loss: 0.016538, mae: 2.362718, mean_q: 2.851286, mean_eps: 0.100000\n",
            "📈 Episodio 2549: Recompensa total (clipped): 12.000, Pasos: 680, Mean Reward Calculado: 0.017647 (Recompensa/Pasos)\n",
            " 1982349/2000000: episode: 2549, duration: 30.645s, episode steps: 680, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.017164, mae: 2.348437, mean_q: 2.831932, mean_eps: 0.100000\n",
            "📈 Episodio 2550: Recompensa total (clipped): 5.000, Pasos: 560, Mean Reward Calculado: 0.008929 (Recompensa/Pasos)\n",
            "💾 Guardado modelo principal best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_model.h5\n",
            "💾 Guardado modelo target best: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_target.h5\n",
            "💾 Memoria best guardada: checkpoints/DDQN_REPLAY/DDQN_REPLAY_best_memory.pkl\n",
            "💾 Checkpoint best guardado (ep: 2550, pasos: 1982909)\n",
            "💾 NUEVO MEJOR PROMEDIO: 17.44 - Guardado en checkpoints/DDQN_REPLAY\n",
            "\n",
            "📊 EPISODIO 2550 - PROGRESO HACIA OBJETIVO\n",
            "   Reward actual: 5.00\n",
            "   Media últimos 100: 17.44 / 20.0\n",
            "   Mejor promedio histórico: 17.44\n",
            "   Estado: 📈 87.2% del objetivo\n",
            "   Episodios en objetivo: 0\n",
            "   Episodios consecutivos en objetivo: 0\n",
            " 1982909/2000000: episode: 2550, duration: 95.305s, episode steps: 560, steps per second:   6, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.016106, mae: 2.355143, mean_q: 2.840424, mean_eps: 0.100000\n",
            "📈 Episodio 2551: Recompensa total (clipped): 19.000, Pasos: 758, Mean Reward Calculado: 0.025066 (Recompensa/Pasos)\n",
            " 1983667/2000000: episode: 2551, duration: 34.527s, episode steps: 758, steps per second:  22, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.017124, mae: 2.365704, mean_q: 2.852521, mean_eps: 0.100000\n",
            "📈 Episodio 2552: Recompensa total (clipped): 18.000, Pasos: 915, Mean Reward Calculado: 0.019672 (Recompensa/Pasos)\n",
            " 1984582/2000000: episode: 2552, duration: 41.933s, episode steps: 915, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.018298, mae: 2.375167, mean_q: 2.863982, mean_eps: 0.100000\n",
            "📈 Episodio 2553: Recompensa total (clipped): 23.000, Pasos: 1102, Mean Reward Calculado: 0.020871 (Recompensa/Pasos)\n",
            " 1985684/2000000: episode: 2553, duration: 49.795s, episode steps: 1102, steps per second:  22, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.017568, mae: 2.366040, mean_q: 2.853591, mean_eps: 0.100000\n",
            "📈 Episodio 2554: Recompensa total (clipped): 7.000, Pasos: 524, Mean Reward Calculado: 0.013359 (Recompensa/Pasos)\n",
            " 1986208/2000000: episode: 2554, duration: 24.023s, episode steps: 524, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.309 [0.000, 5.000],  loss: 0.017419, mae: 2.380578, mean_q: 2.873052, mean_eps: 0.100000\n",
            "📈 Episodio 2555: Recompensa total (clipped): 31.000, Pasos: 1315, Mean Reward Calculado: 0.023574 (Recompensa/Pasos)\n",
            " 1987523/2000000: episode: 2555, duration: 59.949s, episode steps: 1315, steps per second:  22, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.017722, mae: 2.366475, mean_q: 2.853154, mean_eps: 0.100000\n",
            "📈 Episodio 2556: Recompensa total (clipped): 19.000, Pasos: 884, Mean Reward Calculado: 0.021493 (Recompensa/Pasos)\n",
            " 1988407/2000000: episode: 2556, duration: 40.240s, episode steps: 884, steps per second:  22, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.016764, mae: 2.344679, mean_q: 2.827050, mean_eps: 0.100000\n",
            "📈 Episodio 2557: Recompensa total (clipped): 32.000, Pasos: 1145, Mean Reward Calculado: 0.027948 (Recompensa/Pasos)\n",
            " 1989552/2000000: episode: 2557, duration: 52.350s, episode steps: 1145, steps per second:  22, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.016544, mae: 2.367469, mean_q: 2.853689, mean_eps: 0.100000\n",
            "📈 Episodio 2558: Recompensa total (clipped): 27.000, Pasos: 1053, Mean Reward Calculado: 0.025641 (Recompensa/Pasos)\n",
            " 1990605/2000000: episode: 2558, duration: 47.691s, episode steps: 1053, steps per second:  22, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.016946, mae: 2.358807, mean_q: 2.845217, mean_eps: 0.100000\n",
            "📈 Episodio 2559: Recompensa total (clipped): 20.000, Pasos: 775, Mean Reward Calculado: 0.025806 (Recompensa/Pasos)\n",
            " 1991380/2000000: episode: 2559, duration: 34.865s, episode steps: 775, steps per second:  22, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.017601, mae: 2.371689, mean_q: 2.859400, mean_eps: 0.100000\n",
            "📈 Episodio 2560: Recompensa total (clipped): 20.000, Pasos: 976, Mean Reward Calculado: 0.020492 (Recompensa/Pasos)\n",
            " 1992356/2000000: episode: 2560, duration: 44.132s, episode steps: 976, steps per second:  22, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.016165, mae: 2.353547, mean_q: 2.837630, mean_eps: 0.100000\n",
            "📈 Episodio 2561: Recompensa total (clipped): 10.000, Pasos: 497, Mean Reward Calculado: 0.020121 (Recompensa/Pasos)\n",
            " 1992853/2000000: episode: 2561, duration: 22.536s, episode steps: 497, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.015976, mae: 2.379231, mean_q: 2.869580, mean_eps: 0.100000\n",
            "📈 Episodio 2562: Recompensa total (clipped): 27.000, Pasos: 1235, Mean Reward Calculado: 0.021862 (Recompensa/Pasos)\n",
            " 1994088/2000000: episode: 2562, duration: 55.364s, episode steps: 1235, steps per second:  22, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.017546, mae: 2.365216, mean_q: 2.851372, mean_eps: 0.100000\n",
            "📈 Episodio 2563: Recompensa total (clipped): 25.000, Pasos: 1015, Mean Reward Calculado: 0.024631 (Recompensa/Pasos)\n",
            " 1995103/2000000: episode: 2563, duration: 45.402s, episode steps: 1015, steps per second:  22, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.018787, mae: 2.380676, mean_q: 2.869332, mean_eps: 0.100000\n",
            "📈 Episodio 2564: Recompensa total (clipped): 33.000, Pasos: 1650, Mean Reward Calculado: 0.020000 (Recompensa/Pasos)\n",
            " 1996753/2000000: episode: 2564, duration: 73.554s, episode steps: 1650, steps per second:  22, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.041 [0.000, 5.000],  loss: 0.018035, mae: 2.357346, mean_q: 2.842208, mean_eps: 0.100000\n",
            "📈 Episodio 2565: Recompensa total (clipped): 28.000, Pasos: 1038, Mean Reward Calculado: 0.026975 (Recompensa/Pasos)\n",
            " 1997791/2000000: episode: 2565, duration: 46.934s, episode steps: 1038, steps per second:  22, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.943 [0.000, 5.000],  loss: 0.018330, mae: 2.357179, mean_q: 2.842984, mean_eps: 0.100000\n",
            "📈 Episodio 2566: Recompensa total (clipped): 9.000, Pasos: 670, Mean Reward Calculado: 0.013433 (Recompensa/Pasos)\n",
            " 1998461/2000000: episode: 2566, duration: 30.051s, episode steps: 670, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.017721, mae: 2.360573, mean_q: 2.844354, mean_eps: 0.100000\n",
            "📈 Episodio 2567: Recompensa total (clipped): 15.000, Pasos: 765, Mean Reward Calculado: 0.019608 (Recompensa/Pasos)\n",
            " 1999226/2000000: episode: 2567, duration: 34.373s, episode steps: 765, steps per second:  22, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.017996, mae: 2.384358, mean_q: 2.873570, mean_eps: 0.100000\n",
            "📊 Paso 2,000,000/2,000,000 (100.0%) - 24.1 pasos/seg - ETA: 0.0h - Memoria: 15307.94 MB\n",
            "\n",
            "🏁 RESUMEN FINAL DEL ENTRENAMIENTO\n",
            "   Total episodios: 2567\n",
            "   Media final últimos 100: 17.93\n",
            "   Objetivo (20.0): ❌ NO ALCANZADO\n",
            "   Mejor promedio histórico: 18.04\n",
            "   Episodios que alcanzaron objetivo: 0\n",
            "💾 Métricas finales guardadas en: checkpoints/DDQN_REPLAY/final_metrics.json\n",
            "done, took 82924.445 seconds\n",
            "Entrenamiento completado en 1382.07 minutos\n",
            "🛡️ Protegiendo checkpoint 'lastest' existente (ep: 2550)\n",
            "❌ NO se guardará un nuevo checkpoint con episodio 0\n",
            "🛡️ Protegiendo checkpoint 'best' existente (ep: 2550)\n",
            "❌ NO se guardará un nuevo checkpoint con episodio 0\n",
            "🎯 EVALUANDO MODELO DDQN_REPLAY\n",
            "📊 Evaluando por 10 episodios...\n",
            "   Episodio 1/10: reward (clip) = 5.0: reward (real) = 45.0\n",
            "   Episodio 2/10: reward (clip) = 8.0: reward (real) = 125.0\n",
            "   Episodio 3/10: reward (clip) = 11.0: reward (real) = 325.0\n",
            "   Episodio 4/10: reward (clip) = 5.0: reward (real) = 45.0\n",
            "   Episodio 5/10: reward (clip) = 6.0: reward (real) = 65.0\n",
            "   Episodio 6/10: reward (clip) = 10.0: reward (real) = 140.0\n",
            "   Episodio 7/10: reward (clip) = 10.0: reward (real) = 160.0\n",
            "   Episodio 8/10: reward (clip) = 12.0: reward (real) = 165.0\n",
            "   Episodio 9/10: reward (clip) = 4.0: reward (real) = 35.0\n",
            "   Episodio 10/10: reward (clip) = 8.0: reward (real) = 100.0\n",
            "\n",
            "📊 RESULTADOS DE EVALUACIÓN:\n",
            "   Media: 7.90 ❌\n",
            "   Desviación: ±2.66\n",
            "   Máximo: 12.00\n",
            "   Mínimo: 4.00\n",
            "   Episodios sobre 20.0: 0 / 10\n",
            "📈 Progreso: 39.5% del objetivo\n",
            "📊 Recompensa promedio para DDQN_REPLAY: 7.90\n",
            "🎯 EVALUANDO MODELO DUELING_DQN_REPLAY\n",
            "📊 Evaluando por 10 episodios...\n",
            "   Episodio 1/10: reward (clip) = 18.0: reward (real) = 485.0\n",
            "   Episodio 2/10: reward (clip) = 2.0: reward (real) = 15.0\n",
            "   Episodio 3/10: reward (clip) = 8.0: reward (real) = 105.0\n",
            "   Episodio 4/10: reward (clip) = 13.0: reward (real) = 225.0\n",
            "   Episodio 5/10: reward (clip) = 5.0: reward (real) = 75.0\n",
            "   Episodio 6/10: reward (clip) = 6.0: reward (real) = 105.0\n",
            "   Episodio 7/10: reward (clip) = 12.0: reward (real) = 210.0\n",
            "   Episodio 8/10: reward (clip) = 8.0: reward (real) = 120.0\n",
            "   Episodio 9/10: reward (clip) = 11.0: reward (real) = 150.0\n",
            "   Episodio 10/10: reward (clip) = 9.0: reward (real) = 125.0\n",
            "\n",
            "📊 RESULTADOS DE EVALUACIÓN:\n",
            "   Media: 9.20 ❌\n",
            "   Desviación: ±4.31\n",
            "   Máximo: 18.00\n",
            "   Mínimo: 2.00\n",
            "   Episodios sobre 20.0: 0 / 10\n",
            "📈 Progreso: 46.0% del objetivo\n",
            "\n",
            "✅ SOLUCIÓN EXITOSA - Entrenamiento completado\n",
            "\n",
            "🥇 El mejor modelo es DDQN_REPLAY con recompensa promedio 9.20\n",
            "📹 Grabando video del modelo DDQN_REPLAY en: checkpoints/videos/DDQN_REPLAY_20250702_184915.mp4...\n",
            "  Episodio 1: Recompensa Total = 120.0 (Pasos: 677)\n",
            "💾 Guardando video con 677 frames en checkpoints/videos/DDQN_REPLAY_20250702_184915.mp4...\n",
            "❌ Error al grabar el video: maximum recursion depth exceeded\n",
            "📈 El modelo necesita más entrenamiento para alcanzar media 20.0\n"
          ]
        }
      ],
      "source": [
        "# --- Bloque de Ejecución Principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GeOu1FJeC2W"
      },
      "source": [
        "---\n",
        "## **PARTE 4** - *Análisis del entrenamiento*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3EJYd-eC2W"
      },
      "source": [
        "### 4. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VYYFEoFeC2W"
      },
      "outputs": [],
      "source": [
        "Modelos_a_procesar = ['DQN','DDQN', 'DDQN_REPLAY', 'DUELING_DQN_REPLAY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68dzFiUS2U6L"
      },
      "outputs": [],
      "source": [
        "file_ok = 'output-train-DDQN'\n",
        "file = 'output-train-DDQN'\n",
        "ruta_del_archivo = f'{file}.txt'\n",
        "ruta_archivo_csv = f'{file_ok}.csv'\n",
        "\n",
        "def parse_datos_episodio_flexible_desde_archivo(ruta_archivo):\n",
        "    \"\"\"\n",
        "    Analiza el contenido de un archivo dado para extraer campos de datos específicos para cada episodio,\n",
        "    estrictamente comprobando el formato de la segunda línea para evitar datos \"basura\".\n",
        "    Además, calcula la \"Recompensa acumulada media\" de forma progresiva.\n",
        "\n",
        "    Argumentos:\n",
        "        ruta_archivo (str): La ruta al archivo de texto de entrada que contiene la información del episodio.\n",
        "\n",
        "    Retorna:\n",
        "        pandas.DataFrame: Un DataFrame con las columnas especificadas para cada episodio,\n",
        "                          incluyendo la \"Recompensa acumulada media\",\n",
        "                          o None si el archivo no se puede leer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Intenta abrir y leer el archivo\n",
        "        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
        "            texto = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: No se encontró el archivo '{ruta_archivo}'. Por favor, verifica la ruta.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error al leer el archivo: {e}\")\n",
        "        return None\n",
        "\n",
        "    datos = [] # Lista para almacenar los diccionarios de cada episodio\n",
        "    lineas = texto.strip().split('\\n') # Divide el texto en líneas y elimina espacios en blanco al inicio/final\n",
        "    i = 0 # Índice para recorrer las líneas\n",
        "\n",
        "    while i < len(lineas):\n",
        "        linea1 = lineas[i]\n",
        "\n",
        "        # Expresión regular para la primera línea del episodio (Ej: 📈 Episodio X: Recompensa total...)\n",
        "        # Permanece estricta ya que es el marcador de inicio de un episodio.\n",
        "        coincidencia1 = re.match(\n",
        "            r\"📈 Episodio (\\d+): Recompensa total \\(clipped\\): ([\\d.]+), Pasos: (\\d+), Mean Reward Calculado: ([\\d.]+)\",\n",
        "            linea1\n",
        "        )\n",
        "\n",
        "        if coincidencia1:\n",
        "            episodio = int(coincidencia1.group(1))\n",
        "            # Si la primera línea coincide, extrae los datos principales del episodio\n",
        "            datos_episodio = {\n",
        "                'Episodio': episodio,\n",
        "                'Recompensa total': None,\n",
        "                'Pasos': int(coincidencia1.group(3)),\n",
        "                'Mean Reward Calculado': float(coincidencia1.group(4))\n",
        "            }\n",
        "\n",
        "            # Inicializa las columnas de la segunda línea a None por si no se encuentran\n",
        "            datos_episodio['duracion (s)'] = None\n",
        "            datos_episodio['steps per second'] = None\n",
        "            datos_episodio['Recompensa Episodio'] = None\n",
        "            datos_episodio['mean action'] = None\n",
        "            datos_episodio['loss'] = None\n",
        "\n",
        "\n",
        "            segunda_linea_encontrada = False\n",
        "            j = i + 1 # Inicia la búsqueda de la segunda línea a partir de la siguiente\n",
        "\n",
        "            # Bucle interno para buscar la línea de métricas detalladas (la \"segunda línea\")\n",
        "            while j < len(lineas):\n",
        "                potencial_linea2 = lineas[j]\n",
        "\n",
        "                # Criterio clave REVISADO: Buscamos \"episode: <numero_episodio_actual>,\" en la línea.\n",
        "                # Esto es más robusto ante variaciones de espacios o \"basura\" antes de las métricas.\n",
        "                numero_episodio_actual = coincidencia1.group(1) # Obtenemos el número de episodio de la línea 1\n",
        "                if re.search(rf\"episode:\\s*{re.escape(numero_episodio_actual)},\", potencial_linea2):\n",
        "\n",
        "                    # Si esta línea contiene el número de episodio al que corresponde,\n",
        "                    # es muy probable que sea nuestra línea de métricas detalladas.\n",
        "                    # Extrae las métricas individuales de forma flexible.\n",
        "                    duracion_coincidencia = re.search(r\"duration:\\s*([\\d.]+)s\", potencial_linea2)\n",
        "                    pasos_por_segundo_coincidencia = re.search(r\"steps\\s*(?:per\\s*)?second:\\s*(\\d+)\", potencial_linea2)\n",
        "                    recompensa_episodio_coincidencia = re.search(r\"episode\\s*reward:\\s*([\\d.]+)\", potencial_linea2)\n",
        "                    accion_media_coincidencia = re.search(r\"mean\\s*action:\\s*([\\d.]+)\", potencial_linea2)\n",
        "                    perdida_coincidencia = re.search(r\"loss:\\s*([-\\d.]+)\", potencial_linea2) # Maneja '--'\n",
        "\n",
        "                    datos_episodio['duracion (s)'] = float(duracion_coincidencia.group(1)) if duracion_coincidencia else 0\n",
        "                    datos_episodio['steps per second'] = int(pasos_por_segundo_coincidencia.group(1)) if pasos_por_segundo_coincidencia else None\n",
        "                    datos_episodio['Recompensa Episodio'] = float(recompensa_episodio_coincidencia.group(1)) if recompensa_episodio_coincidencia else None\n",
        "                    datos_episodio['mean action'] = float(accion_media_coincidencia.group(1)) if accion_media_coincidencia else None\n",
        "\n",
        "                    # Manejo especial para 'loss', que puede ser '--'\n",
        "                    if perdida_coincidencia and perdida_coincidencia.group(1) != '--':\n",
        "                        datos_episodio['loss'] = float(perdida_coincidencia.group(1))\n",
        "                    else:\n",
        "                        datos_episodio['loss'] = None\n",
        "\n",
        "                    segunda_linea_encontrada = True\n",
        "                    i = j # Actualiza el índice principal 'i' a la posición de esta segunda línea\n",
        "                    break # Sale del bucle interno, ya encontramos nuestra línea2\n",
        "\n",
        "                # Si encontramos el inicio de un NUEVO episodio, significa que la línea de métricas\n",
        "                # para el episodio actual no existe o no tiene el formato esperado.\n",
        "                elif re.match(r\"📈 Episodio \\d+:\", potencial_linea2):\n",
        "                    break # Salimos del bucle interno sin encontrar la segunda línea\n",
        "\n",
        "                j += 1 # Avanza a la siguiente línea para buscar la segunda línea\n",
        "\n",
        "            datos.append(datos_episodio) # Añade los datos del episodio (completos o con Nones)\n",
        "            i += 1 # Incrementa el índice principal 'i'. El bucle principal continuará desde aquí.\n",
        "                   # Si se encontró la línea2, 'i' ya se actualizó a 'j' y este i+=1 lo mueve a j+1.\n",
        "                   # Si no se encontró la línea2, 'i' sigue en su valor original y este i+=1 lo mueve a i+1.\n",
        "        else:\n",
        "            i += 1 # Si la línea actual no es el inicio de un episodio, simplemente avanza a la siguiente línea\n",
        "                   # (esto ignora las líneas de \"basura\" que no son ni inicio de episodio ni la línea de métricas esperada).\n",
        "\n",
        "    df = pd.DataFrame(datos)\n",
        "    # Calcular el promedio de los últimos 100 episodios (media móvil)\n",
        "    # min_periods=1 asegura que la media se calcule incluso si hay menos de 100 episodios al principio\n",
        "    df['Recompensa total'] = round(df['Recompensa Episodio'].rolling(window=100, min_periods=1).mean(), 2)\n",
        "\n",
        "    # --- CÁLCULO DE TIEMPO ACUMULADO ---\n",
        "    # Convertir 'duration (s)' a tipo numérico, forzando errores a NaN\n",
        "    df['duration (s)'] = pd.to_numeric(df['duracion (s)'], errors='coerce')\n",
        "    # Rellenar cualquier NaN en duration (s) con 0 para el cálculo acumulado si es apropiado\n",
        "    df['duration (s)'] = df['duration (s)'].fillna(0)\n",
        "    # Calcular el tiempo acumulado sumando las duraciones individuales\n",
        "    df['Tiempo acumulado'] = df['duration (s)'].cumsum()\n",
        "    # --- FIN CÁLCULO DE TIEMPO ACUMULADO ---\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---\n",
        "# Asegúrate de que el fichero esté en la misma carpeta, o proporciona la ruta completa\n",
        "df_desde_archivo = parse_datos_episodio_flexible_desde_archivo(ruta_del_archivo)\n",
        "# Opcional: guardar el DataFrame en un archivo CSV\n",
        "df_desde_archivo.to_csv(f'{file_ok}.csv', index=False)\n",
        "\n",
        "if df_desde_archivo is not None:\n",
        "    print(\"Primeras 10 filas del DataFrame (con Recompensa acumulada media):\")\n",
        "    print(df_desde_archivo.head)\n",
        "    print(duracion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vROUgLfqeC2X"
      },
      "outputs": [],
      "source": [
        "ruta_archivo_csv = f'{file_ok}.csv'\n",
        "# Lee el CSV. Pandas es muy bueno infiriendo el formato por defecto.\n",
        "dt_total = pd.read_csv(ruta_archivo_csv, sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYyH6veIeC2X"
      },
      "outputs": [],
      "source": [
        "# Crea la figura y los ejes para el gráfico\n",
        "plt.figure(figsize=(12, 7)) # Aumenta el tamaño para mejor visualización\n",
        "\n",
        "# Dibuja la recompensa por cada episodio\n",
        "plt.plot(dt_total['Episodio'], dt_total['Recompensa Episodio'],\n",
        "         label='Recompensa/Episodio',\n",
        "         alpha=0.7, # Hazla un poco transparente para que la media se vea bien\n",
        "         color='skyblue')\n",
        "\n",
        "# Dibuja la recompensa acumulada media sobre el mismo gráfico\n",
        "plt.plot(dt_total['Episodio'], dt_total['Recompensa total'],\n",
        "         label='Recompensa Media (100)',\n",
        "         color='red',\n",
        "         linewidth=2) # Haz la línea más gruesa para que destaque\n",
        "\n",
        "# Añade título y etiquetas\n",
        "plt.title('Recompensa por Episodio y Recompensa Acumulada Media durante el Entrenamiento')\n",
        "plt.xlabel('Número de Episodio')\n",
        "plt.ylabel('Recompensa')\n",
        "\n",
        "# Añade una leyenda para identificar cada línea\n",
        "plt.legend()\n",
        "\n",
        "# Añade una cuadrícula para facilitar la lectura\n",
        "plt.grid(True)\n",
        "\n",
        "# Muestra el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLwg6jxveC2X"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de cómo trazar la pérdida (si está disponible)\n",
        "#if 'loss' in dt_total:\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dt_total['loss'])\n",
        "plt.title('Pérdida durante el Entrenamiento')\n",
        "plt.xlabel('Paso de Entrenamiento') # O Episodio, dependiendo de cómo lo registre keras-rl\n",
        "plt.ylabel('Pérdida')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DduyUBXTeC2X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "dt_total[\"Pasos\"].astype(int).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0yvsAhxeC2X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Tiempo acumulado')\n",
        "tiempo_acumulado_horas = dt_total['Tiempo acumulado'] / 3600\n",
        "plt.xlabel('Paso de Entrenamiento') # O Episodio, dependiendo de cómo lo registre keras-rl\n",
        "plt.ylabel('Horas ejecución')\n",
        "tiempo_acumulado_horas.astype(float).plot()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfc49ee0",
        "outputId": "b81bce22-7474-4ad7-ac86-16935240e06c"
      },
      "source": [
        "%pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.24.3)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}