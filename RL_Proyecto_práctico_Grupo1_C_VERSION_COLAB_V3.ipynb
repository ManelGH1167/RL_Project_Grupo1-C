{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "* Alumno 1:\n",
        "* Alumno 2: Abdelilah BENALI\n",
        "* Alumno 3: Jair Cuesta Cifuentes\n",
        "* Alumno 4: Manel González Huete\n",
        "* Alumno 5: Francisco Manzanas Mogrovejo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# # ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "# mount='/content/gdrive'\n",
        "# drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "# try:\n",
        "#   from google.colab import drive\n",
        "#   IN_COLAB=True\n",
        "# except:\n",
        "#   IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6n7MIefJ21i"
      },
      "outputs": [],
      "source": [
        "# # Switch to the directory on the Google Drive that you want to use\n",
        "# import os\n",
        "# if IN_COLAB:\n",
        "#   print(\"We're running Colab\")\n",
        "\n",
        "#   if IN_COLAB:\n",
        "#     # Mount the Google Drive at mount\n",
        "#     print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "#     drive.mount(mount)\n",
        "\n",
        "#     # Create drive_root if it doesn't exist\n",
        "#     create_drive_root = True\n",
        "#     if create_drive_root:\n",
        "#       print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "#       os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "#     # Change to the directory\n",
        "#     print(\"\\nColab: Changing directory to \", drive_root)\n",
        "#     %cd $drive_root\n",
        "# # Verify we're in the correct working directory\n",
        "# %pwd\n",
        "# print(\"Archivos en el directorio: \")\n",
        "# print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‼️ Crear un entorno virtual para instalar las versiones correctas de los módulos.**\n",
        "\n"
      ],
      "metadata": {
        "id": "LZbBr9Bb3Yu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar virtualenv\n",
        "!pip install virtualenv --quiet\n",
        "\n",
        "# 2. Crear el entorno virtual llamado \"miar_rl\"\n",
        "!virtualenv miar_rl\n",
        "\n",
        "# 3. Instala paquetes DENTRO del entorno virtual con versiones exactas\n",
        "!./miar_rl/bin/pip install numpy==1.23.5 --quiet\n",
        "!./miar_rl/bin/pip install gym==0.17.3 --quiet\n",
        "!./miar_rl/bin/pip install tensorflow==2.12.1 keras==2.12.0 --quiet\n",
        "!./miar_rl/bin/pip install git+https://github.com/Kojoley/atari-py.git@1.2.2 --quiet\n",
        "!./miar_rl/bin/pip install keras-rl2==1.0.5 --quiet\n",
        "\n",
        "# Librerías adicionales\n",
        "!./miar_rl/bin/pip install Pillow\n",
        "!./miar_rl/bin/pip install matplotlib\n",
        "!./miar_rl/bin/pip install tqdm"
      ],
      "metadata": {
        "id": "dLaINiX8q312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addaeb61-900e-48fa-80e7-565368d1387e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/6.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/6.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hcreated virtual environment CPython3.11.13.final.0-64 in 412ms\n",
            "  creator CPython3Posix(dest=/content/miar_rl, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.1.1, setuptools==80.3.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gym]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m171.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m187.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [tensorflow]\n",
            "\u001b[1A\u001b[2K  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'atari-py' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'atari-py'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "Successfully installed Pillow-11.2.1\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.58.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in ./miar_rl/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./miar_rl/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./miar_rl/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./miar_rl/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.58.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Installing collected packages: python-dateutil, pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pyparsing-3.2.3 python-dateutil-2.9.0.post0\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IN_COLAB = True\n",
        "\n",
        "# if IN_COLAB:\n",
        "#   %pip uninstall -y tensorflow keras keras-rl2\n",
        "#   %pip install tensorflow==2.6.0\n",
        "#   %pip install keras-rl2==1.0.5\n",
        "#   %pip install gym==0.17.3\n",
        "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "\n",
        "\n",
        "#   %pip install tensorflow==2.8\n",
        "# if not IN_LOCAL:\n",
        "#   %pip install numpy==1.23.5\n",
        "#   %pip install gym==0.17\n",
        "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "#   %pip install pyglet==1.5.0\n",
        "#   %pip install h5py==3.1.0\n",
        "#   %pip install Pillow==9.5.0\n",
        "#   %pip install keras-rl2==1.0.5\n",
        "#   %pip install Keras==2.2.4\n",
        "#   %pip install tensorflow==2.5.3\n",
        "#   %pip install torch==2.0.1\n",
        "#   %pip install agents==1.4.0\n",
        "#   %pip install matplotlib==3.4.3\n",
        "#   %pip install tqdm\n",
        "\n"
      ],
      "metadata": {
        "id": "qw8CfOtrexpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "### Importar librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‼️ No es posible ejecutar con el entorno en las celdas. Se debe crear un script y ejectutarlo directo en shell con Shell escape (!).**"
      ],
      "metadata": {
        "id": "9aUofv4q3hx6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a640a15-72db-4a64-d147-5a4ee75bbd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cell1.py\n",
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\"  # 👈 Asegura el backend antes de que matplotlib lo lea\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "from collections import deque\n",
        "from tqdm import trange\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "print(\"✅ Código cargado con éxito en entorno virtual\")\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell1.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dis8x3Qcyncm",
        "outputId": "68d4dc61-3ed1-4b1d-a8bf-fe3e1df9d345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 17:06:21.970397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750525581.989901    2695 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750525581.995907    2695 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-21 17:06:22.015664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/cell1.py\", line 16, in <module>\n",
            "    from rl.agents.dqn import DQNAgent\n",
            "  File \"/content/miar_rl/lib/python3.11/site-packages/rl/agents/__init__.py\", line 1, in <module>\n",
            "    from .dqn import DQNAgent, NAFAgent, ContinuousDQNAgent\n",
            "  File \"/content/miar_rl/lib/python3.11/site-packages/rl/agents/dqn.py\", line 8, in <module>\n",
            "    from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
            "  File \"/content/miar_rl/lib/python3.11/site-packages/rl/policy.py\", line 3, in <module>\n",
            "    from rl.util import *\n",
            "  File \"/content/miar_rl/lib/python3.11/site-packages/rl/util.py\", line 3, in <module>\n",
            "    from tensorflow.keras.models import model_from_config, Sequential, Model, model_from_config\n",
            "ImportError: cannot import name 'model_from_config' from 'tensorflow.keras.models' (/content/miar_rl/lib/python3.11/site-packages/keras/_tf_keras/keras/models/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "### Crear el entorno\n",
        "Nuestro entorno es el juego Space Invaders, de Atari"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‼️ Puesto que se debe trabajar en un solo script, para \"dividir\" en celdas, se podría importar el script de la celda anterior, aunque se ejecutaría nuevamente.**"
      ],
      "metadata": {
        "id": "MupkI2D64q8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cell2.py\n",
        "# Importar TODO lo de cell1.py\n",
        "from cell1 import *\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "print(f\"Entorno {env_name} creado con nb_actions = {nb_actions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ewgfkA04_k",
        "outputId": "a2dd016c-cd18-44fa-86d3-96f310e04bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell2.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa5MtHWm048d",
        "outputId": "4286d2c4-1eba-436a-b177-be64ad4890cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 16:02:11.437199: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:11.524974: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:11.525591: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 16:02:13.663758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "✅ Código cargado con éxito en entorno virtual\n",
            "TensorFlow version: 2.12.1\n",
            "Entorno SpaceInvaders-v0 creado con nb_actions = 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cell3.py\n",
        "from cell2 import *\n",
        "\n",
        "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
        "print(\"El número de acciones posibles es : \", nb_actions)\n",
        "print(\"Las acciones posibles son : \", env.env.get_action_meanings())\n",
        "\n",
        "# Creamos la versión one-hot encoded de nuestras acciones\n",
        "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
        "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzjuXAiS5WSH",
        "outputId": "9f6d784e-7569-417f-b8fa-126854235474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell3.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYj0v22j5Yt3",
        "outputId": "5fc630ba-515a-4edc-ae33-1c97753de1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 16:02:19.424009: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:19.511292: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:19.518434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 16:02:21.081989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "✅ Código cargado con éxito en entorno virtual\n",
            "TensorFlow version: 2.12.1\n",
            "Entorno SpaceInvaders-v0 creado con nb_actions = 6\n",
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZspU28hA6VKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBgaaL8o02bK"
      },
      "source": [
        "### Clase \"processor\" para Atari\n",
        "\n",
        "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGEZUcpGb2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdddcbd4-5268-411e-c507-849093e8aa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell4.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cell4.py\n",
        "from cell3 import *\n",
        "from rl.core import Processor\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\"\n",
        "        Preprocesamiento simple: convierte a escala de grises y normaliza\n",
        "        \"\"\"\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell4.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ISBd7nOP1MH",
        "outputId": "a8d833e4-2b45-4cbe-fbb9-469efcbb62bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 16:02:26.111212: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:26.193029: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:26.193592: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 16:02:27.257723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "✅ Código cargado con éxito en entorno virtual\n",
            "TensorFlow version: 2.12.1\n",
            "Entorno SpaceInvaders-v0 creado con nb_actions = 6\n",
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QkJzYEa02bK"
      },
      "source": [
        "### Revisar el entorno de juego"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‼️ Como estamos trabajando en un entorno \"fuera\" de colab plt.show() no es factible. Se guardan las imágenes y después se muestran**"
      ],
      "metadata": {
        "id": "5YwqRAoURI-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfJX0AyY02bK",
        "outputId": "9cf0f52d-90ab-46fa-ff2d-9b31ec913652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cell5.py\n",
        "from cell4 import *\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Crear carpeta para guardar imágenes si no existe\n",
        "save_dir = \"/content/imagenes_guardadas\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(22):\n",
        "    if i > 20:\n",
        "        filename = os.path.join(save_dir, f\"frame_{i}.png\")\n",
        "        plt.imsave(filename, observation)\n",
        "\n",
        "    observation, reward, done, info = env.step(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell5.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHrL9bENQAgJ",
        "outputId": "2b7db255-f672-4f2f-d175-7859cbf66bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 16:02:30.212065: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:30.259337: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:30.259838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 16:02:30.999353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "✅ Código cargado con éxito en entorno virtual\n",
            "TensorFlow version: 2.12.1\n",
            "Entorno SpaceInvaders-v0 creado con nb_actions = 6\n",
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Ruta de la carpeta donde guardaste las imágenes\n",
        "image_dir = \"/content/imagenes_guardadas\"\n",
        "\n",
        "# Obtener la lista de archivos y ordenarlos (por si acaso)\n",
        "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
        "\n",
        "# Mostrar una a una\n",
        "for filename in image_files:\n",
        "    image_path = os.path.join(image_dir, filename)\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(filename)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "nqSV0mAbQ27h",
        "outputId": "cd21bf5a-e30e-482d-927e-cd552b414e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOOxIehJQ2rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0YdDO0402bK",
        "outputId": "ab542bbe-abb0-475a-f37e-7526646e8f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cell6.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cell6.py\n",
        "from cell5 import *\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "processor = AtariProcessor()\n",
        "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
        "\n",
        "save_dir = \"/content/imagenes_preprocesadas\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "filename = os.path.join(save_dir, \"preprocesada.png\")\n",
        "plt.imsave(filename, processor.process_state_batch(obs_preprocessed), cmap='gray')\n",
        "\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)\n",
        "print(f\"Imagen guardada en: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python cell6.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF9C6P_WQCmI",
        "outputId": "4813a17e-c8e3-4588-db9c-b2d0ef57cbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 16:02:33.933948: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:33.981252: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 16:02:33.981746: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 16:02:34.747591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "✅ Código cargado con éxito en entorno virtual\n",
            "TensorFlow version: 2.12.1\n",
            "Entorno SpaceInvaders-v0 creado con nb_actions = 6\n",
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n",
            "(210, 160, 3)\n",
            "(84, 84)\n",
            "Imagen guardada en: /content/imagenes_preprocesadas/preprocesada.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "image_path = \"/content/imagenes_preprocesadas/preprocesada.png\"\n",
        "img = Image.open(image_path)\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "f80Rtu4cRT1h",
        "outputId": "db87c347-4a21-4d97-81bc-3974f52a0878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgdy4D2D02bK"
      },
      "source": [
        "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main - TEST I - DQN - Inicial"
      ],
      "metadata": {
        "id": "f22D0jV192U7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‼️ Para evitar ejecutar _todo_ cada vez, se integra lo necesario en un solo script.**"
      ],
      "metadata": {
        "id": "tlz7KNZr5zNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punto de partida"
      ],
      "metadata": {
        "id": "nHWqilKpp7Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import numpy as np\n",
        "import gym\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "\n",
        "# Definir dimensiones\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "ENV_NAME = 'SpaceInvaders-v0'\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Procesador de observaciones\n",
        "# ------------------------------------------------------------\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        return batch.astype('float32') / 255.\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Crear entorno y procesador\n",
        "# ------------------------------------------------------------\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "processor = AtariProcessor()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Definir el modelo DQN en TensorFlow\n",
        "# ------------------------------------------------------------\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Configurar DQN Agent\n",
        "# ------------------------------------------------------------\n",
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=1.0, value_min=0.1,\n",
        "                              value_test=0.05, nb_steps=1000000)\n",
        "\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=50000,\n",
        "               gamma=0.99,\n",
        "               target_model_update=10000,\n",
        "               train_interval=4,\n",
        "               delta_clip=1.0)\n",
        "\n",
        "dqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Entrenamiento\n",
        "# ------------------------------------------------------------\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# Guardar pesos entrenados\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "# Evaluar el agente\n",
        "history = dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "# Extraer recompensas por episodio\n",
        "episode_rewards = history.history['episode_reward']\n",
        "\n",
        "# Calcular estadísticas\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "\n",
        "print(f\"Recompensa promedio en test: {mean_reward:.2f}\")\n",
        "print(f\"Desviación estándar: {std_reward:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGvzxp_Xdhix",
        "outputId": "ee30cbed-6658-4db2-ea01-618a1938836b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw_a40j26VPx",
        "outputId": "1f43507d-8bf3-4896-e71b-f9a148171289"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 18:20:19.775104: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-21 18:20:19.777934: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:20:19.829366: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:20:19.829851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 18:20:20.577403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " permute (Permute)           (None, 84, 84, 4)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,687,206\n",
            "Trainable params: 1,687,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "2025-06-21 18:20:23.117662: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-06-21 18:20:23.207320: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "2025-06-21 18:20:23.235000: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_2_1/bias/Assign' id:204 op device:{requested: '', assigned: ''} def:{{{node conv2d_2_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_2_1/bias, conv2d_2_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Training for 50000 steps ...\n",
            "/content/miar_rl/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-06-21 18:20:23.624673: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/BiasAdd' id:125 op device:{requested: '', assigned: ''} def:{{{node dense_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_1/MatMul, dense_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:20:23.641464: W tensorflow/c/c_api.cc:300] Operation '{name:'count_3/Assign' id:405 op device:{requested: '', assigned: ''} def:{{{node count_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_3, count_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "   420/50000: episode: 1, duration: 1.741s, episode steps: 420, steps per second: 241, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1131/50000: episode: 2, duration: 2.798s, episode steps: 711, steps per second: 254, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1941/50000: episode: 3, duration: 3.174s, episode steps: 810, steps per second: 255, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2831/50000: episode: 4, duration: 3.518s, episode steps: 890, steps per second: 253, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3206/50000: episode: 5, duration: 1.478s, episode steps: 375, steps per second: 254, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3703/50000: episode: 6, duration: 1.980s, episode steps: 497, steps per second: 251, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4638/50000: episode: 7, duration: 3.674s, episode steps: 935, steps per second: 254, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6179/50000: episode: 8, duration: 6.110s, episode steps: 1541, steps per second: 252, episode reward: 17.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7378/50000: episode: 9, duration: 4.700s, episode steps: 1199, steps per second: 255, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7968/50000: episode: 10, duration: 2.346s, episode steps: 590, steps per second: 251, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8699/50000: episode: 11, duration: 2.906s, episode steps: 731, steps per second: 252, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9125/50000: episode: 12, duration: 1.686s, episode steps: 426, steps per second: 253, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9553/50000: episode: 13, duration: 1.705s, episode steps: 428, steps per second: 251, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10429/50000: episode: 14, duration: 3.467s, episode steps: 876, steps per second: 253, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10919/50000: episode: 15, duration: 1.942s, episode steps: 490, steps per second: 252, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11445/50000: episode: 16, duration: 2.096s, episode steps: 526, steps per second: 251, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11924/50000: episode: 17, duration: 1.905s, episode steps: 479, steps per second: 252, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12899/50000: episode: 18, duration: 3.829s, episode steps: 975, steps per second: 255, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13684/50000: episode: 19, duration: 3.110s, episode steps: 785, steps per second: 252, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14286/50000: episode: 20, duration: 2.390s, episode steps: 602, steps per second: 252, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14903/50000: episode: 21, duration: 2.441s, episode steps: 617, steps per second: 253, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15358/50000: episode: 22, duration: 1.799s, episode steps: 455, steps per second: 253, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16212/50000: episode: 23, duration: 3.360s, episode steps: 854, steps per second: 254, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16893/50000: episode: 24, duration: 2.721s, episode steps: 681, steps per second: 250, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17420/50000: episode: 25, duration: 2.077s, episode steps: 527, steps per second: 254, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18157/50000: episode: 26, duration: 2.901s, episode steps: 737, steps per second: 254, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18807/50000: episode: 27, duration: 2.570s, episode steps: 650, steps per second: 253, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19578/50000: episode: 28, duration: 3.068s, episode steps: 771, steps per second: 251, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20211/50000: episode: 29, duration: 2.513s, episode steps: 633, steps per second: 252, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21056/50000: episode: 30, duration: 3.318s, episode steps: 845, steps per second: 255, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21650/50000: episode: 31, duration: 2.337s, episode steps: 594, steps per second: 254, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22683/50000: episode: 32, duration: 4.088s, episode steps: 1033, steps per second: 253, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23373/50000: episode: 33, duration: 2.722s, episode steps: 690, steps per second: 253, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23976/50000: episode: 34, duration: 2.378s, episode steps: 603, steps per second: 254, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24664/50000: episode: 35, duration: 2.712s, episode steps: 688, steps per second: 254, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25392/50000: episode: 36, duration: 2.903s, episode steps: 728, steps per second: 251, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25891/50000: episode: 37, duration: 1.996s, episode steps: 499, steps per second: 250, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26460/50000: episode: 38, duration: 2.257s, episode steps: 569, steps per second: 252, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26866/50000: episode: 39, duration: 1.605s, episode steps: 406, steps per second: 253, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27529/50000: episode: 40, duration: 2.604s, episode steps: 663, steps per second: 255, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28342/50000: episode: 41, duration: 3.210s, episode steps: 813, steps per second: 253, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29419/50000: episode: 42, duration: 4.240s, episode steps: 1077, steps per second: 254, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30031/50000: episode: 43, duration: 2.425s, episode steps: 612, steps per second: 252, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30558/50000: episode: 44, duration: 2.067s, episode steps: 527, steps per second: 255, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31647/50000: episode: 45, duration: 4.318s, episode steps: 1089, steps per second: 252, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32377/50000: episode: 46, duration: 2.877s, episode steps: 730, steps per second: 254, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32975/50000: episode: 47, duration: 2.356s, episode steps: 598, steps per second: 254, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33580/50000: episode: 48, duration: 2.383s, episode steps: 605, steps per second: 254, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34148/50000: episode: 49, duration: 2.290s, episode steps: 568, steps per second: 248, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34804/50000: episode: 50, duration: 2.593s, episode steps: 656, steps per second: 253, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35459/50000: episode: 51, duration: 2.583s, episode steps: 655, steps per second: 254, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35966/50000: episode: 52, duration: 2.000s, episode steps: 507, steps per second: 254, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36582/50000: episode: 53, duration: 2.434s, episode steps: 616, steps per second: 253, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37335/50000: episode: 54, duration: 3.003s, episode steps: 753, steps per second: 251, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38448/50000: episode: 55, duration: 4.369s, episode steps: 1113, steps per second: 255, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38846/50000: episode: 56, duration: 1.573s, episode steps: 398, steps per second: 253, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39409/50000: episode: 57, duration: 2.225s, episode steps: 563, steps per second: 253, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40075/50000: episode: 58, duration: 2.666s, episode steps: 666, steps per second: 250, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40785/50000: episode: 59, duration: 2.823s, episode steps: 710, steps per second: 251, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41826/50000: episode: 60, duration: 4.091s, episode steps: 1041, steps per second: 254, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42659/50000: episode: 61, duration: 3.276s, episode steps: 833, steps per second: 254, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43331/50000: episode: 62, duration: 2.689s, episode steps: 672, steps per second: 250, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43777/50000: episode: 63, duration: 1.755s, episode steps: 446, steps per second: 254, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44457/50000: episode: 64, duration: 2.674s, episode steps: 680, steps per second: 254, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44913/50000: episode: 65, duration: 1.814s, episode steps: 456, steps per second: 251, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45937/50000: episode: 66, duration: 4.085s, episode steps: 1024, steps per second: 251, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46548/50000: episode: 67, duration: 2.476s, episode steps: 611, steps per second: 247, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47658/50000: episode: 68, duration: 4.426s, episode steps: 1110, steps per second: 251, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48224/50000: episode: 69, duration: 2.267s, episode steps: 566, steps per second: 250, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48767/50000: episode: 70, duration: 2.179s, episode steps: 543, steps per second: 249, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49452/50000: episode: 71, duration: 2.724s, episode steps: 685, steps per second: 251, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "done, took 198.076 seconds\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 5.000, steps: 680\n",
            "Episode 2: reward: 5.000, steps: 649\n",
            "Episode 3: reward: 9.000, steps: 683\n",
            "Episode 4: reward: 1.000, steps: 386\n",
            "Episode 5: reward: 8.000, steps: 932\n",
            "Episode 6: reward: 7.000, steps: 718\n",
            "Episode 7: reward: 10.000, steps: 697\n",
            "Episode 8: reward: 2.000, steps: 637\n",
            "Episode 9: reward: 2.000, steps: 349\n",
            "Episode 10: reward: 12.000, steps: 939\n",
            "Recompensa promedio en test: 6.10\n",
            "Desviación estándar: 3.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main - TEST I - DQN - V2"
      ],
      "metadata": {
        "id": "yETDh5rn-xA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sólo mejora la organización"
      ],
      "metadata": {
        "id": "wyk-mPGEpx6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main2.py\n",
        "import numpy as np\n",
        "import gym\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Permute\n",
        "from tensorflow.keras.optimizers import legacy as keras_legacy_optimizers\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.core import Processor\n",
        "\n",
        "# Parámetros\n",
        "ENV_NAME = 'SpaceInvaders-v0'\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "SEED = 123\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Procesador personalizado\n",
        "# ------------------------------------------------------------\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation).resize(INPUT_SHAPE).convert('L')\n",
        "        return np.array(img).astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        return batch.astype('float32') / 255.0\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Clase contenedora del agente\n",
        "# ------------------------------------------------------------\n",
        "class DQNAgentWrapper:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.env.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        tf.random.set_seed(SEED)\n",
        "\n",
        "        self.nb_actions = self.env.action_space.n\n",
        "        self.processor = AtariProcessor()\n",
        "        self.model = self.build_model()\n",
        "        self.memory = SequentialMemory(limit=100000, window_length=WINDOW_LENGTH)\n",
        "        self.policy = LinearAnnealedPolicy(\n",
        "            EpsGreedyQPolicy(),\n",
        "            attr='eps',\n",
        "            value_max=1.0,\n",
        "            value_min=0.1,\n",
        "            value_test=0.05,\n",
        "            nb_steps=50000\n",
        "        )\n",
        "        self.agent = self.build_agent()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
        "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
        "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(self.nb_actions, activation='linear'))\n",
        "        return model\n",
        "\n",
        "    def build_agent(self):\n",
        "        # Usa Adam legacy para compatibilidad total con keras-rl2\n",
        "        adam_legacy = keras_legacy_optimizers.Adam(learning_rate=0.00025)\n",
        "        dqn = DQNAgent(\n",
        "            model=self.model,\n",
        "            nb_actions=self.nb_actions,\n",
        "            memory=self.memory,\n",
        "            processor=self.processor,\n",
        "            policy=self.policy,\n",
        "            nb_steps_warmup=10000,\n",
        "            gamma=0.99,\n",
        "            target_model_update=1000,\n",
        "            train_interval=4,\n",
        "            delta_clip=1.0\n",
        "        )\n",
        "        dqn.compile(adam_legacy, metrics=['mae'])\n",
        "        return dqn\n",
        "\n",
        "    def train(self, steps=50000):\n",
        "        self.agent.fit(self.env, nb_steps=steps, visualize=False, verbose=2)\n",
        "\n",
        "    def test(self, episodes=3, render=False):\n",
        "        self.agent.test(self.env, nb_episodes=episodes, visualize=render)\n",
        "\n",
        "    def save(self, path='dqn_weights.h5f'):\n",
        "        self.agent.save_weights(path, overwrite=True)\n",
        "\n",
        "    def load(self, path='dqn_weights.h5f'):\n",
        "        self.agent.load_weights(path)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Entrenamiento\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    dqn_wrapper = DQNAgentWrapper(ENV_NAME)\n",
        "\n",
        "    print(\"Entrenando el agente...\")\n",
        "    dqn_wrapper.train()\n",
        "\n",
        "    print(\"Evaluando el agente...\")\n",
        "    dqn_wrapper.test()\n",
        "\n",
        "    print(\"Guardando pesos...\")\n",
        "    dqn_wrapper.save(f'dqn_{ENV_NAME}_weights.h5f')"
      ],
      "metadata": {
        "id": "Rlr8_LNl-woB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1070078-9564-4cfc-80cb-fb03d3bf909f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python main2.py\n"
      ],
      "metadata": {
        "id": "Lt-XFdAt-wdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3063812-e11c-4e79-d3b5-dd2b2f11d3f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 18:24:49.849606: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-21 18:24:49.852354: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:24:49.904276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:24:49.904780: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 18:24:50.651771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-06-21 18:24:52.433747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-06-21 18:24:52.444609: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "2025-06-21 18:24:52.472376: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1_1/bias/Assign' id:244 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1_1/bias, dense_1_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Entrenando el agente...\n",
            "Training for 50000 steps ...\n",
            "/content/miar_rl/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-06-21 18:24:52.863082: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/BiasAdd' id:125 op device:{requested: '', assigned: ''} def:{{{node dense_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_1/MatMul, dense_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:24:52.879171: W tensorflow/c/c_api.cc:300] Operation '{name:'count_3/Assign' id:395 op device:{requested: '', assigned: ''} def:{{{node count_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_3, count_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "   420/50000: episode: 1, duration: 1.737s, episode steps: 420, steps per second: 242, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   991/50000: episode: 2, duration: 2.267s, episode steps: 571, steps per second: 252, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1707/50000: episode: 3, duration: 2.840s, episode steps: 716, steps per second: 252, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2445/50000: episode: 4, duration: 2.968s, episode steps: 738, steps per second: 249, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3204/50000: episode: 5, duration: 3.021s, episode steps: 759, steps per second: 251, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3846/50000: episode: 6, duration: 2.567s, episode steps: 642, steps per second: 250, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5247/50000: episode: 7, duration: 5.542s, episode steps: 1401, steps per second: 253, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5739/50000: episode: 8, duration: 1.950s, episode steps: 492, steps per second: 252, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6342/50000: episode: 9, duration: 2.388s, episode steps: 603, steps per second: 253, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7161/50000: episode: 10, duration: 3.279s, episode steps: 819, steps per second: 250, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7938/50000: episode: 11, duration: 3.046s, episode steps: 777, steps per second: 255, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8880/50000: episode: 12, duration: 3.729s, episode steps: 942, steps per second: 253, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9407/50000: episode: 13, duration: 2.089s, episode steps: 527, steps per second: 252, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "/content/miar_rl/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-06-21 18:25:32.774311: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1_1/BiasAdd' id:249 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_1_1/MatMul, dense_1_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:25:33.032402: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_3/AddN' id:491 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:25:33.065424: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv2d_2/bias/v/Assign' id:789 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv2d_2/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv2d_2/bias/v, training/Adam/conv2d_2/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            " 10225/50000: episode: 14, duration: 9.265s, episode steps: 818, steps per second:  88, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.732 [0.000, 5.000],  loss: 0.004909, mae: 0.033626, mean_q: 0.056478, mean_eps: 0.817948\n",
            " 10844/50000: episode: 15, duration: 19.132s, episode steps: 619, steps per second:  32, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.006680, mae: 0.037112, mean_q: 0.059689, mean_eps: 0.810388\n",
            " 11564/50000: episode: 16, duration: 21.044s, episode steps: 720, steps per second:  34, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.007328, mae: 0.052056, mean_q: 0.076062, mean_eps: 0.798364\n",
            " 12110/50000: episode: 17, duration: 16.647s, episode steps: 546, steps per second:  33, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.007263, mae: 0.060881, mean_q: 0.083807, mean_eps: 0.786952\n",
            " 12912/50000: episode: 18, duration: 23.839s, episode steps: 802, steps per second:  34, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.006478, mae: 0.079386, mean_q: 0.112451, mean_eps: 0.774820\n",
            " 13295/50000: episode: 19, duration: 11.904s, episode steps: 383, steps per second:  32, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.005210, mae: 0.084006, mean_q: 0.113239, mean_eps: 0.764164\n",
            " 14105/50000: episode: 20, duration: 24.944s, episode steps: 810, steps per second:  32, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.007151, mae: 0.093487, mean_q: 0.127644, mean_eps: 0.753400\n",
            " 14804/50000: episode: 21, duration: 19.745s, episode steps: 699, steps per second:  35, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.007295, mae: 0.121252, mean_q: 0.167980, mean_eps: 0.739828\n",
            " 15476/50000: episode: 22, duration: 18.958s, episode steps: 672, steps per second:  35, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.007871, mae: 0.135120, mean_q: 0.184039, mean_eps: 0.727516\n",
            " 16222/50000: episode: 23, duration: 20.970s, episode steps: 746, steps per second:  36, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.006509, mae: 0.141510, mean_q: 0.189813, mean_eps: 0.714736\n",
            " 17160/50000: episode: 24, duration: 26.374s, episode steps: 938, steps per second:  36, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.006128, mae: 0.164807, mean_q: 0.216855, mean_eps: 0.699580\n",
            " 17850/50000: episode: 25, duration: 19.227s, episode steps: 690, steps per second:  36, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.007091, mae: 0.201982, mean_q: 0.262386, mean_eps: 0.684928\n",
            " 18789/50000: episode: 26, duration: 26.342s, episode steps: 939, steps per second:  36, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.005773, mae: 0.212849, mean_q: 0.274872, mean_eps: 0.670240\n",
            " 20140/50000: episode: 27, duration: 37.836s, episode steps: 1351, steps per second:  36, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.007255, mae: 0.225662, mean_q: 0.294662, mean_eps: 0.649648\n",
            " 20842/50000: episode: 28, duration: 21.789s, episode steps: 702, steps per second:  32, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.006885, mae: 0.263147, mean_q: 0.341704, mean_eps: 0.631180\n",
            " 21634/50000: episode: 29, duration: 23.136s, episode steps: 792, steps per second:  34, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007132, mae: 0.308955, mean_q: 0.397083, mean_eps: 0.617716\n",
            " 22283/50000: episode: 30, duration: 20.089s, episode steps: 649, steps per second:  32, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.007449, mae: 0.339952, mean_q: 0.431034, mean_eps: 0.604756\n",
            " 23048/50000: episode: 31, duration: 23.799s, episode steps: 765, steps per second:  32, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.008587, mae: 0.371793, mean_q: 0.470162, mean_eps: 0.592048\n",
            " 24137/50000: episode: 32, duration: 30.650s, episode steps: 1089, steps per second:  36, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.008080, mae: 0.401831, mean_q: 0.509725, mean_eps: 0.575344\n",
            " 24499/50000: episode: 33, duration: 10.024s, episode steps: 362, steps per second:  36, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.008214, mae: 0.440474, mean_q: 0.555659, mean_eps: 0.562276\n",
            " 25154/50000: episode: 34, duration: 18.361s, episode steps: 655, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.007788, mae: 0.441487, mean_q: 0.557294, mean_eps: 0.553132\n",
            " 25579/50000: episode: 35, duration: 11.968s, episode steps: 425, steps per second:  36, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.010434, mae: 0.474376, mean_q: 0.595743, mean_eps: 0.543412\n",
            " 26102/50000: episode: 36, duration: 14.731s, episode steps: 523, steps per second:  36, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.008344, mae: 0.480956, mean_q: 0.604530, mean_eps: 0.534880\n",
            " 26562/50000: episode: 37, duration: 12.820s, episode steps: 460, steps per second:  36, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.007896, mae: 0.502455, mean_q: 0.627787, mean_eps: 0.526024\n",
            " 26915/50000: episode: 38, duration: 9.823s, episode steps: 353, steps per second:  36, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.009125, mae: 0.503554, mean_q: 0.629391, mean_eps: 0.518716\n",
            " 28017/50000: episode: 39, duration: 31.116s, episode steps: 1102, steps per second:  35, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.008890, mae: 0.539557, mean_q: 0.672700, mean_eps: 0.505612\n",
            " 28557/50000: episode: 40, duration: 16.482s, episode steps: 540, steps per second:  33, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.009945, mae: 0.600719, mean_q: 0.746023, mean_eps: 0.490816\n",
            " 29442/50000: episode: 41, duration: 26.261s, episode steps: 885, steps per second:  34, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.009904, mae: 0.633030, mean_q: 0.783670, mean_eps: 0.478000\n",
            " 30442/50000: episode: 42, duration: 30.943s, episode steps: 1000, steps per second:  32, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.011389, mae: 0.697810, mean_q: 0.864093, mean_eps: 0.461044\n",
            " 31291/50000: episode: 43, duration: 25.493s, episode steps: 849, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012048, mae: 0.734215, mean_q: 0.907944, mean_eps: 0.444412\n",
            " 31940/50000: episode: 44, duration: 18.244s, episode steps: 649, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.011538, mae: 0.737269, mean_q: 0.910152, mean_eps: 0.430948\n",
            " 33108/50000: episode: 45, duration: 32.865s, episode steps: 1168, steps per second:  36, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013330, mae: 0.760690, mean_q: 0.935986, mean_eps: 0.414604\n",
            " 33591/50000: episode: 46, duration: 13.471s, episode steps: 483, steps per second:  36, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.010059, mae: 0.775374, mean_q: 0.953099, mean_eps: 0.399736\n",
            " 34098/50000: episode: 47, duration: 14.200s, episode steps: 507, steps per second:  36, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.009889, mae: 0.783278, mean_q: 0.964560, mean_eps: 0.390808\n",
            " 34767/50000: episode: 48, duration: 18.803s, episode steps: 669, steps per second:  36, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.012638, mae: 0.772237, mean_q: 0.950784, mean_eps: 0.380224\n",
            " 35154/50000: episode: 49, duration: 11.003s, episode steps: 387, steps per second:  35, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.010071, mae: 0.770907, mean_q: 0.950908, mean_eps: 0.370720\n",
            " 36445/50000: episode: 50, duration: 37.287s, episode steps: 1291, steps per second:  35, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.012891, mae: 0.825569, mean_q: 1.017573, mean_eps: 0.355600\n",
            " 37087/50000: episode: 51, duration: 19.681s, episode steps: 642, steps per second:  33, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.011748, mae: 0.834581, mean_q: 1.026043, mean_eps: 0.338212\n",
            " 37443/50000: episode: 52, duration: 10.086s, episode steps: 356, steps per second:  35, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: 0.012049, mae: 0.832861, mean_q: 1.020851, mean_eps: 0.329248\n",
            " 38108/50000: episode: 53, duration: 20.475s, episode steps: 665, steps per second:  32, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.011685, mae: 0.812272, mean_q: 1.000054, mean_eps: 0.320068\n",
            " 38797/50000: episode: 54, duration: 20.505s, episode steps: 689, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.013125, mae: 0.868953, mean_q: 1.067867, mean_eps: 0.307864\n",
            " 39480/50000: episode: 55, duration: 21.294s, episode steps: 683, steps per second:  32, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013645, mae: 0.870089, mean_q: 1.068290, mean_eps: 0.295516\n",
            " 40588/50000: episode: 56, duration: 32.814s, episode steps: 1108, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.013128, mae: 0.907153, mean_q: 1.115805, mean_eps: 0.279424\n",
            " 41233/50000: episode: 57, duration: 18.281s, episode steps: 645, steps per second:  35, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.012366, mae: 0.934056, mean_q: 1.146000, mean_eps: 0.263620\n",
            " 41865/50000: episode: 58, duration: 17.723s, episode steps: 632, steps per second:  36, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.248 [0.000, 5.000],  loss: 0.013057, mae: 0.972323, mean_q: 1.191837, mean_eps: 0.252100\n",
            " 42864/50000: episode: 59, duration: 28.040s, episode steps: 999, steps per second:  36, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013869, mae: 0.986334, mean_q: 1.209821, mean_eps: 0.237448\n",
            " 43775/50000: episode: 60, duration: 25.453s, episode steps: 911, steps per second:  36, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.014863, mae: 1.009386, mean_q: 1.242522, mean_eps: 0.220276\n",
            " 44413/50000: episode: 61, duration: 18.020s, episode steps: 638, steps per second:  35, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.012203, mae: 1.054846, mean_q: 1.296390, mean_eps: 0.206308\n",
            " 45324/50000: episode: 62, duration: 26.535s, episode steps: 911, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013627, mae: 1.103604, mean_q: 1.353204, mean_eps: 0.192376\n",
            " 45984/50000: episode: 63, duration: 20.693s, episode steps: 660, steps per second:  32, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015609, mae: 1.148229, mean_q: 1.405882, mean_eps: 0.178264\n",
            " 46327/50000: episode: 64, duration: 9.803s, episode steps: 343, steps per second:  35, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.014005, mae: 1.178562, mean_q: 1.439549, mean_eps: 0.169228\n",
            " 47462/50000: episode: 65, duration: 33.647s, episode steps: 1135, steps per second:  34, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.015112, mae: 1.169938, mean_q: 1.429863, mean_eps: 0.155908\n",
            " 48605/50000: episode: 66, duration: 35.723s, episode steps: 1143, steps per second:  32, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.015175, mae: 1.207741, mean_q: 1.479617, mean_eps: 0.135388\n",
            " 49011/50000: episode: 67, duration: 12.600s, episode steps: 406, steps per second:  32, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.016169, mae: 1.263039, mean_q: 1.540152, mean_eps: 0.121456\n",
            " 49996/50000: episode: 68, duration: 27.790s, episode steps: 985, steps per second:  35, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.017169, mae: 1.282320, mean_q: 1.568468, mean_eps: 0.108964\n",
            "done, took 1206.384 seconds\n",
            "Evaluando el agente...\n",
            "Testing for 3 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 505\n",
            "Episode 2: reward: 0.000, steps: 508\n",
            "Episode 3: reward: 4.000, steps: 520\n",
            "Guardando pesos...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_6zjH3r-wbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main - TEST III\n",
        "\n",
        "- Aumentar nb_steps_warmup\n",
        "Para que el agente explore más antes de comenzar a entrenar.\n",
        "Antes: 10000 o 50000\n",
        "Propuesto: 20000 o 30000\n",
        "\n",
        "- Reducir train_interval\n",
        "Para que la red se actualice con mayor frecuencia durante el entrenamiento.\n",
        "Antes: 4\n",
        "Propuesto: 1 o 2\n",
        "\n",
        "- Ajustar target_model_update\n",
        "Sincronizar la red objetivo más seguido para mejorar la estabilidad.\n",
        "Antes: 1000\n",
        "Propuesto: 500\n",
        "\n",
        "- Hacer la política epsilon decay más lenta\n",
        "Para una exploración más gradual y estable.\n",
        "En LinearAnnealedPolicy, subir nb_steps de 50000 a 100000 o más.\n",
        "\n",
        "- Disminuir tasa de aprendizaje (learning_rate)\n",
        "Para que el modelo aprenda de forma más suave y estable.\n",
        "Antes: 0.00025\n",
        "Propuesto: 0.0001"
      ],
      "metadata": {
        "id": "aPxsojLF-oWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main3.py\n",
        "import numpy as np\n",
        "import gym\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Permute\n",
        "from tensorflow.keras.optimizers import legacy as keras_legacy_optimizers\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.core import Processor\n",
        "\n",
        "# Parámetros\n",
        "ENV_NAME = 'SpaceInvaders-v0'\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "SEED = 123\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Procesador personalizado\n",
        "# ------------------------------------------------------------\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation).resize(INPUT_SHAPE).convert('L')\n",
        "        return np.array(img).astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        return batch.astype('float32') / 255.0\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Clase contenedora del agente\n",
        "# ------------------------------------------------------------\n",
        "class DQNAgentWrapper:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.env.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        tf.random.set_seed(SEED)\n",
        "\n",
        "        self.nb_actions = self.env.action_space.n\n",
        "        self.processor = AtariProcessor()\n",
        "        self.model = self.build_model()\n",
        "        self.memory = SequentialMemory(limit=100000, window_length=WINDOW_LENGTH)\n",
        "        self.policy = LinearAnnealedPolicy(\n",
        "            EpsGreedyQPolicy(),\n",
        "            attr='eps',\n",
        "            value_max=1.0,\n",
        "            value_min=0.1,\n",
        "            value_test=0.05,\n",
        "            nb_steps=100000  # más lento para explorar mejor\n",
        "        )\n",
        "        self.agent = self.build_agent()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
        "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
        "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(self.nb_actions, activation='linear'))\n",
        "        return model\n",
        "\n",
        "    def build_agent(self):\n",
        "        # Adam legacy con lr menor\n",
        "        adam_legacy = keras_legacy_optimizers.Adam(learning_rate=0.0001)\n",
        "        dqn = DQNAgent(\n",
        "            model=self.model,\n",
        "            nb_actions=self.nb_actions,\n",
        "            memory=self.memory,\n",
        "            processor=self.processor,\n",
        "            policy=self.policy,\n",
        "            nb_steps_warmup=20000,      # warmup más largo\n",
        "            gamma=0.99,\n",
        "            target_model_update=500,    # sincronización más frecuente\n",
        "            train_interval=2,           # entrenamiento cada 2 pasos\n",
        "            delta_clip=1.0\n",
        "        )\n",
        "        dqn.compile(adam_legacy, metrics=['mae'])\n",
        "        return dqn\n",
        "\n",
        "    def train(self, steps=50000):\n",
        "        self.agent.fit(self.env, nb_steps=steps, visualize=False, verbose=2)\n",
        "\n",
        "    def test(self, episodes=3, render=False):\n",
        "        self.agent.test(self.env, nb_episodes=episodes, visualize=render)\n",
        "\n",
        "    def save(self, path='dqn_weights.h5f'):\n",
        "        self.agent.save_weights(path, overwrite=True)\n",
        "\n",
        "    def load(self, path='dqn_weights.h5f'):\n",
        "        self.agent.load_weights(path)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Entrenamiento\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    dqn_wrapper = DQNAgentWrapper(ENV_NAME)\n",
        "\n",
        "    print(\"Entrenando el agente...\")\n",
        "    dqn_wrapper.train()\n",
        "\n",
        "    print(\"Evaluando el agente...\")\n",
        "    dqn_wrapper.test()\n",
        "\n",
        "    print(\"Guardando pesos...\")\n",
        "    dqn_wrapper.save(f'dqn_{ENV_NAME}_weights.h5f')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQN7A8eccO5y",
        "outputId": "f89fe3c8-597d-4560-ab7a-e412bb744e03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./miar_rl/bin/python main3.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG2MyaCTqj7-",
        "outputId": "fe322cb1-85d3-4de8-e73e-9098e399b451"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-21 18:45:56.135279: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-21 18:45:56.137970: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:45:56.189953: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-06-21 18:45:56.190439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-21 18:45:56.946017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-06-21 18:45:58.735377: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-06-21 18:45:58.746279: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "2025-06-21 18:45:58.773751: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_2_1/kernel/Assign' id:189 op device:{requested: '', assigned: ''} def:{{{node conv2d_2_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_2_1/kernel, conv2d_2_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Entrenando el agente...\n",
            "Training for 50000 steps ...\n",
            "/content/miar_rl/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-06-21 18:45:59.171418: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/BiasAdd' id:125 op device:{requested: '', assigned: ''} def:{{{node dense_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_1/MatMul, dense_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:45:59.187439: W tensorflow/c/c_api.cc:300] Operation '{name:'count_1/Assign' id:375 op device:{requested: '', assigned: ''} def:{{{node count_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_1, count_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "   420/50000: episode: 1, duration: 1.735s, episode steps: 420, steps per second: 242, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1257/50000: episode: 2, duration: 3.389s, episode steps: 837, steps per second: 247, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2012/50000: episode: 3, duration: 3.006s, episode steps: 755, steps per second: 251, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2625/50000: episode: 4, duration: 2.450s, episode steps: 613, steps per second: 250, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3521/50000: episode: 5, duration: 3.588s, episode steps: 896, steps per second: 250, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4030/50000: episode: 6, duration: 2.056s, episode steps: 509, steps per second: 248, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4436/50000: episode: 7, duration: 1.619s, episode steps: 406, steps per second: 251, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5229/50000: episode: 8, duration: 3.158s, episode steps: 793, steps per second: 251, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6298/50000: episode: 9, duration: 4.241s, episode steps: 1069, steps per second: 252, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7138/50000: episode: 10, duration: 3.369s, episode steps: 840, steps per second: 249, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8102/50000: episode: 11, duration: 3.819s, episode steps: 964, steps per second: 252, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8703/50000: episode: 12, duration: 2.407s, episode steps: 601, steps per second: 250, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9327/50000: episode: 13, duration: 2.508s, episode steps: 624, steps per second: 249, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10095/50000: episode: 14, duration: 3.082s, episode steps: 768, steps per second: 249, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10787/50000: episode: 15, duration: 2.750s, episode steps: 692, steps per second: 252, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11765/50000: episode: 16, duration: 3.899s, episode steps: 978, steps per second: 251, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12405/50000: episode: 17, duration: 2.572s, episode steps: 640, steps per second: 249, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13237/50000: episode: 18, duration: 3.347s, episode steps: 832, steps per second: 249, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13679/50000: episode: 19, duration: 1.758s, episode steps: 442, steps per second: 251, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14211/50000: episode: 20, duration: 2.117s, episode steps: 532, steps per second: 251, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15064/50000: episode: 21, duration: 3.404s, episode steps: 853, steps per second: 251, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15565/50000: episode: 22, duration: 2.034s, episode steps: 501, steps per second: 246, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16430/50000: episode: 23, duration: 3.442s, episode steps: 865, steps per second: 251, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17324/50000: episode: 24, duration: 3.550s, episode steps: 894, steps per second: 252, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17726/50000: episode: 25, duration: 1.613s, episode steps: 402, steps per second: 249, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18105/50000: episode: 26, duration: 1.530s, episode steps: 379, steps per second: 248, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18946/50000: episode: 27, duration: 3.355s, episode steps: 841, steps per second: 251, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19770/50000: episode: 28, duration: 3.278s, episode steps: 824, steps per second: 251, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "/content/miar_rl/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-06-21 18:47:19.292987: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1_1/BiasAdd' id:249 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_1_1/MatMul, dense_1_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:47:19.557625: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_3/AddN' id:491 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-06-21 18:47:19.591289: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv2d_2/bias/v/Assign' id:789 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv2d_2/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv2d_2/bias/v, training/Adam/conv2d_2/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            " 20949/50000: episode: 29, duration: 52.373s, episode steps: 1179, steps per second:  23, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.006376, mae: 0.031629, mean_q: 0.051385, mean_eps: 0.815725\n",
            " 22038/50000: episode: 30, duration: 58.832s, episode steps: 1089, steps per second:  19, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.005783, mae: 0.074555, mean_q: 0.104827, mean_eps: 0.806563\n",
            " 22571/50000: episode: 31, duration: 30.803s, episode steps: 533, steps per second:  17, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.007553, mae: 0.107383, mean_q: 0.145509, mean_eps: 0.799264\n",
            " 23013/50000: episode: 32, duration: 25.660s, episode steps: 442, steps per second:  17, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.005172, mae: 0.119966, mean_q: 0.164154, mean_eps: 0.794872\n",
            " 24004/50000: episode: 33, duration: 51.737s, episode steps: 991, steps per second:  19, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006305, mae: 0.160071, mean_q: 0.211214, mean_eps: 0.788428\n",
            " 24608/50000: episode: 34, duration: 31.492s, episode steps: 604, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.006567, mae: 0.208089, mean_q: 0.271821, mean_eps: 0.781255\n",
            " 25450/50000: episode: 35, duration: 43.840s, episode steps: 842, steps per second:  19, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.006785, mae: 0.247101, mean_q: 0.319625, mean_eps: 0.774748\n",
            " 25975/50000: episode: 36, duration: 27.328s, episode steps: 525, steps per second:  19, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.005922, mae: 0.285086, mean_q: 0.363635, mean_eps: 0.768592\n",
            " 27249/50000: episode: 37, duration: 66.094s, episode steps: 1274, steps per second:  19, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.007263, mae: 0.375379, mean_q: 0.479293, mean_eps: 0.760492\n",
            " 27848/50000: episode: 38, duration: 31.374s, episode steps: 599, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.007074, mae: 0.419825, mean_q: 0.529643, mean_eps: 0.752068\n",
            " 28215/50000: episode: 39, duration: 19.261s, episode steps: 367, steps per second:  19, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.007750, mae: 0.460884, mean_q: 0.584575, mean_eps: 0.747721\n",
            " 28877/50000: episode: 40, duration: 34.581s, episode steps: 662, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.005978, mae: 0.514332, mean_q: 0.645711, mean_eps: 0.743086\n",
            " 29549/50000: episode: 41, duration: 37.640s, episode steps: 672, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.007473, mae: 0.558381, mean_q: 0.698434, mean_eps: 0.737083\n",
            " 30514/50000: episode: 42, duration: 53.528s, episode steps: 965, steps per second:  18, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.007547, mae: 0.623380, mean_q: 0.775859, mean_eps: 0.729721\n",
            " 31599/50000: episode: 43, duration: 56.253s, episode steps: 1085, steps per second:  19, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007000, mae: 0.691518, mean_q: 0.856976, mean_eps: 0.720496\n",
            " 32113/50000: episode: 44, duration: 26.655s, episode steps: 514, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.005992, mae: 0.733228, mean_q: 0.906540, mean_eps: 0.713296\n",
            " 32609/50000: episode: 45, duration: 25.809s, episode steps: 496, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.007278, mae: 0.773478, mean_q: 0.954410, mean_eps: 0.708751\n",
            " 33109/50000: episode: 46, duration: 25.997s, episode steps: 500, steps per second:  19, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.007443, mae: 0.790155, mean_q: 0.973127, mean_eps: 0.704269\n",
            " 33559/50000: episode: 47, duration: 23.386s, episode steps: 450, steps per second:  19, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007930, mae: 0.818531, mean_q: 1.009220, mean_eps: 0.699994\n",
            " 34469/50000: episode: 48, duration: 47.235s, episode steps: 910, steps per second:  19, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.007512, mae: 0.875088, mean_q: 1.077144, mean_eps: 0.693874\n",
            " 35124/50000: episode: 49, duration: 34.085s, episode steps: 655, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.008211, mae: 0.952515, mean_q: 1.172505, mean_eps: 0.686836\n",
            " 36171/50000: episode: 50, duration: 58.268s, episode steps: 1047, steps per second:  18, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.008391, mae: 1.046996, mean_q: 1.286878, mean_eps: 0.679177\n",
            " 37056/50000: episode: 51, duration: 48.367s, episode steps: 885, steps per second:  18, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.008437, mae: 1.124065, mean_q: 1.378075, mean_eps: 0.670483\n",
            " 37707/50000: episode: 52, duration: 33.850s, episode steps: 651, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.009874, mae: 1.170826, mean_q: 1.434414, mean_eps: 0.663571\n",
            " 38042/50000: episode: 53, duration: 17.489s, episode steps: 335, steps per second:  19, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.009077, mae: 1.200211, mean_q: 1.471991, mean_eps: 0.659134\n",
            " 38708/50000: episode: 54, duration: 34.833s, episode steps: 666, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.009886, mae: 1.254883, mean_q: 1.535551, mean_eps: 0.654634\n",
            " 39356/50000: episode: 55, duration: 33.899s, episode steps: 648, steps per second:  19, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010519, mae: 1.318434, mean_q: 1.611866, mean_eps: 0.648721\n",
            " 39762/50000: episode: 56, duration: 21.173s, episode steps: 406, steps per second:  19, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.010231, mae: 1.349254, mean_q: 1.650769, mean_eps: 0.643978\n",
            " 40302/50000: episode: 57, duration: 28.263s, episode steps: 540, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.010280, mae: 1.375162, mean_q: 1.684978, mean_eps: 0.639721\n",
            " 40958/50000: episode: 58, duration: 34.238s, episode steps: 656, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.008765, mae: 1.416887, mean_q: 1.729764, mean_eps: 0.634339\n",
            " 41663/50000: episode: 59, duration: 36.841s, episode steps: 705, steps per second:  19, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.010097, mae: 1.426867, mean_q: 1.742286, mean_eps: 0.628210\n",
            " 42693/50000: episode: 60, duration: 53.914s, episode steps: 1030, steps per second:  19, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.010643, mae: 1.497835, mean_q: 1.826668, mean_eps: 0.620398\n",
            " 43385/50000: episode: 61, duration: 36.310s, episode steps: 692, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010389, mae: 1.540117, mean_q: 1.878632, mean_eps: 0.612649\n",
            " 43833/50000: episode: 62, duration: 23.474s, episode steps: 448, steps per second:  19, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.008626, mae: 1.580311, mean_q: 1.923881, mean_eps: 0.607519\n",
            " 44616/50000: episode: 63, duration: 40.997s, episode steps: 783, steps per second:  19, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.010439, mae: 1.600153, mean_q: 1.948400, mean_eps: 0.601984\n",
            " 45278/50000: episode: 64, duration: 34.633s, episode steps: 662, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010823, mae: 1.651078, mean_q: 2.011937, mean_eps: 0.595486\n",
            " 45920/50000: episode: 65, duration: 33.441s, episode steps: 642, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.010438, mae: 1.676563, mean_q: 2.041177, mean_eps: 0.589618\n",
            " 46399/50000: episode: 66, duration: 25.051s, episode steps: 479, steps per second:  19, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.010557, mae: 1.688571, mean_q: 2.054960, mean_eps: 0.584569\n",
            " 46978/50000: episode: 67, duration: 30.202s, episode steps: 579, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.011446, mae: 1.714303, mean_q: 2.085117, mean_eps: 0.579808\n",
            " 48081/50000: episode: 68, duration: 57.647s, episode steps: 1103, steps per second:  19, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010680, mae: 1.773121, mean_q: 2.159141, mean_eps: 0.572239\n",
            " 49019/50000: episode: 69, duration: 48.938s, episode steps: 938, steps per second:  19, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.010593, mae: 1.841253, mean_q: 2.238600, mean_eps: 0.563050\n",
            " 49594/50000: episode: 70, duration: 30.076s, episode steps: 575, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.010972, mae: 1.939142, mean_q: 2.358728, mean_eps: 0.556246\n",
            "done, took 1666.361 seconds\n",
            "Evaluando el agente...\n",
            "Testing for 3 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 966\n",
            "Episode 2: reward: 8.000, steps: 683\n",
            "Episode 3: reward: 6.000, steps: 796\n",
            "Guardando pesos...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUSLRw7C02bM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}